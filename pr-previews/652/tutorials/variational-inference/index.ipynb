{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using Pkg; Pkg.activate(; temp=true)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "This post will look at **variational inference (VI)**, an optimization approach to _approximate_ Bayesian inference, and how to use it in Turing.jl as an alternative to other approaches such as MCMC.\nThis post will focus on the usage of VI in Turing rather than the principles and theory underlying VI.\nIf you are interested in understanding the mathematics you can checkout [our write-up]({{<meta dev-variational-inference>}}) or any other resource online (there are a lot of great ones).\n\nLet's start with a minimal example. \nConsider a `Turing.Model`, which we denote as `model`.\nApproximating the posterior associated with `model` via VI is as simple as"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#| eval: false\nm = model(data...)               # instantiate model on the data\nq_init = q_fullrank_gaussian(m)  # initial variational approximation\nvi(m, q_init, 1000) # perform VI with the default algorithm on `m` for 1000 iterations"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Thus, it's no more work than standard MCMC sampling in Turing.\nThe default algorithm uses stochastic gradient descent to minimize the (exclusive) KL divergence.\nThis is commonly referred to as *automatic differentiation variational inference*[^KTRGB2017], *stochastic gradient VI*[^TL2014], and *black-box variational inference*[^RGB2014] with the reparameterization gradient[^KW2014][^RMW2014][^TL2014].\n\nTo get a bit more into what we can do with VI, let's look at a more concrete example.\nWe will reproduce the [tutorial on Bayesian linear regression]({{<meta linear-regression>}}) using VI instead of MCMC.\nAfter that, we will discuss how to customize the behavior of `vi` for more advanced usage.\n\nLet's first import the relevant packages:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using Random\nusing Turing\nusing Turing: Variational\nusing AdvancedVI\nusing Plots\n\nRandom.seed!(42);"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Bayesian Linear Regression Example\nLet's start by setting up our example.\nWe will re-use the [Bayesian linear regression]({{<meta linear-regression>}}) example.\nAs we'll see, there is really no additional work required to apply variational inference to a more complex `Model`."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using FillArrays\nusing RDatasets\n\nusing LinearAlgebra\n\n# Import the \"Default\" dataset.\ndata = RDatasets.dataset(\"datasets\", \"mtcars\");\n\n# Show the first six rows of the dataset.\nfirst(data, 6)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Function to split samples.\nfunction split_data(df, at=0.70)\n    r = size(df, 1)\n    index = Int(round(r * at))\n    train = df[1:index, :]\n    test = df[(index + 1):end, :]\n    return train, test\nend\n\n# A handy helper function to rescale our dataset.\nfunction standardize(x)\n    return (x .- mean(x; dims=1)) ./ std(x; dims=1)\nend\n\nfunction standardize(x, orig)\n    return (x .- mean(orig; dims=1)) ./ std(orig; dims=1)\nend\n\n# Another helper function to unstandardize our datasets.\nfunction unstandardize(x, orig)\n    return x .* std(orig; dims=1) .+ mean(orig; dims=1)\nend\n\nfunction unstandardize(x, mean_train, std_train)\n    return x .* std_train .+ mean_train\nend"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Remove the model column.\nselect!(data, Not(:Model))\n\n# Split our dataset 70%/30% into training/test sets.\ntrain, test = split_data(data, 0.7)\ntrain_unstandardized = copy(train)\n\n# Standardize both datasets.\nstd_train = standardize(Matrix(train))\nstd_test = standardize(Matrix(test), Matrix(train))\n\n# Save dataframe versions of our dataset.\ntrain_cut = DataFrame(std_train, names(data))\ntest_cut = DataFrame(std_test, names(data))\n\n# Create our labels. These are the values we are trying to predict.\ntrain_label = train_cut[:, :MPG]\ntest_label = test_cut[:, :MPG]\n\n# Get the list of columns to keep.\nremove_names = filter(x -> !in(x, [\"MPG\"]), names(data))\n\n# Filter the test and train sets.\ntrain = Matrix(train_cut[:, remove_names]);\ntest = Matrix(test_cut[:, remove_names]);"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Bayesian linear regression.\n@model function linear_regression(x, y, n_obs, n_vars, ::Type{T}=Vector{Float64}) where {T}\n    # Set variance prior.\n    σ² ~ truncated(Normal(0, 100); lower=0)\n\n    # Set intercept prior.\n    intercept ~ Normal(0, 3)\n\n    # Set the priors on our coefficients.\n    coefficients ~ MvNormal(Zeros(n_vars), 10.0 * I)\n\n    # Calculate all the mu terms.\n    mu = intercept .+ x * coefficients\n    return y ~ MvNormal(mu, σ² * I)\nend;"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "n_obs, n_vars = size(train)\nm = linear_regression(train, train_label, n_obs, n_vars);"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Basic Usage\nTo run VI, we must first set a *variational family*.\nFor instance, the most commonly used family is the mean-field Gaussian family.\nFor this, Turing provides functions that automatically construct the initialization corresponding to the model `m`:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "q_init = q_meanfield_gaussian(m);"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "`vi` will automatically recognize the variational family through the type of `q_init`.\nHere is a detailed documentation for the constructor:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@doc(Variational.q_meanfield_gaussian)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "As we can see, the precise initialization can be customized through the keyword arguments.\n\nLet's run VI with the default setting:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "n_iters = 1000\nq_avg, q_last, info, state = vi(m, q_init, n_iters; show_progress=false);"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The default setting uses the `AdvancedVI.RepGradELBO` objective, which corresponds to a variant of what is known as *automatic differentiation VI*[^KTRGB2017] or *stochastic gradient VI*[^TL2014] or *black-box VI*[^RGB2014] with the reparameterization gradient[^KW2014][^RMW2014][^TL2014].\nThe default optimizer we use is `AdvancedVI.DoWG`[^KMJ2023] combined with a proximal operator.\n(The use of proximal operators with VI on a location-scale family is discussed in detail by J. Domke[^D2020][^DGG2023] and others[^KOWMG2023].)\nWe will take a deeper look into the returned values and the keyword arguments in the following subsections.\nFirst, here is the full documentation for `vi`:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@doc(Variational.vi)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Values Returned by `vi`\nThe main output of the algorithm is `q_avg`, the average of the parameters generated by the optimization algorithm.\nFor computing `q_avg`, the default setting uses what is known as polynomial averaging[^SZ2013].\nUsually, `q_avg` will perform better than the last-iterate `q_last`.\nFor instance, we can compare the ELBO of the two:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@info(\"Objective of q_avg and q_last\",\n    ELBO_q_avg = estimate_objective(AdvancedVI.RepGradELBO(32), q_avg, LogDensityFunction(m)),\n    ELBO_q_last = estimate_objective(AdvancedVI.RepGradELBO(32), q_last, LogDensityFunction(m))\n)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We can see that `ELBO_q_avg` is slightly more optimal.\n\nNow, `info` contains information generated during optimization that could be useful for diagnostics.\nFor the default setting, which is `RepGradELBO`, it contains the ELBO estimated at each step, which can be plotted as follows:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "Plots.plot([i.elbo for i in info], xlabel=\"Iterations\", ylabel=\"ELBO\", label=\"info\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Since the ELBO is estimated by a small number of samples, it appears noisy.\nFurthermore, at each step, the ELBO is evaluated on `q_last`, not `q_avg`, which is the actual output that we care about.\nTo obtain more accurate ELBO estimates evaluated on `q_avg`, we have to define a custom callback function.\n\n## Custom Callback Functions\nTo inspect the progress of optimization in more detail, one can define a custom callback function.\nFor example, the following callback function estimates the ELBO on `q_avg` every 10 steps with a larger number of samples:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "function callback(; stat, averaged_params, restructure, kwargs...)\n    if mod(stat.iteration, 10) == 1\n        q_avg = restructure(averaged_params)\n        obj = AdvancedVI.RepGradELBO(128)\n        elbo_avg = estimate_objective(obj, q_avg, LogDensityFunction(m))\n        (elbo_avg = elbo_avg,)\n    else\n        nothing\n    end\nend;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The `NamedTuple` returned by `callback` will be appended to the corresponding entry of `info`, and it will also be displayed on the progress meter if `show_progress` is set as `true`.\n\nThe custom callback can be supplied to `vi` as a keyword argument:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "q_mf, _, info_mf, _ = vi(m, q_init, n_iters; show_progress=false, callback=callback);"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Let's plot the result:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "iters = 1:10:length(info_mf)\nelbo_mf = [i.elbo_avg for i in info_mf[iters]]\nPlots.plot!(iters, elbo_mf, xlabel=\"Iterations\", ylabel=\"ELBO\", label=\"callback\", ylims=(-200,Inf))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We can see that the ELBO values are less noisy and progress more smoothly due to averaging.\n\n## Using Different Optimisers\nThe default optimiser we use is a proximal variant of DoWG[^KMJ2023].\nFor Gaussian variational families, this works well as a default option.\nSometimes, the step size of `AdvancedVI.DoWG` could be too large, resulting in unstable behavior.\n(In this case, we recommend trying `AdvancedVI.DoG`[^IHC2023])\nOr, for whatever reason, it might be desirable to use a different optimiser.\nOur implementation supports any optimiser that implements the [Optimisers.jl](https://fluxml.ai/Optimisers.jl/stable/) interface.\n\nFor instance, let's try using `Optimisers.Adam`[^KB2014], which is a popular choice.\nSince `AdvancedVI` does not implement a proximal operator for `Optimisers.Adam`, we must use the `AdvancedVI.ClipScale()` projection operator, which ensures that the scale matrix of the variational approximation is positive definite.\n(See the paper by J. Domke 2020[^D2020] for more detail about the use of a projection operator.)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using Optimisers\n\n_, _, info_adam, _ = vi(m, q_init, n_iters; show_progress=false, callback=callback, optimizer=Optimisers.Adam(3e-3), operator=ClipScale());"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "iters = 1:10:length(info_mf)\nelbo_adam = [i.elbo_avg for i in info_adam[iters]]\nPlots.plot(iters, elbo_mf, xlabel=\"Iterations\", ylabel=\"ELBO\", label=\"DoWG\")\nPlots.plot!(iters, elbo_adam, xlabel=\"Iterations\", ylabel=\"ELBO\", label=\"Adam\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Compared to the default option `AdvancedVI.DoWG()`, we can see that `Optimisers.Adam(3e-3)` is converging more slowly.\nWith more step size tuning, it is possible that `Optimisers.Adam` could perform better or equal.\nThat is, most common optimisers require some degree of tuning to perform better or comparably to `AdvancedVI.DoWG()` or `AdvancedVI.DoG()`, which do not require much tuning at all.\nDue to this fact, they are referred to as parameter-free optimizers.\n\n## Using Full-Rank Variational Families\nSo far, we have only used the mean-field Gaussian family.\nThis, however, approximates the posterior covariance with a diagonal matrix.\nTo model the full covariance matrix, we can use the *full-rank* Gaussian family[^TL2014][^KTRGB2017]:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "q_init_fr = q_fullrank_gaussian(m);"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@doc(Variational.q_fullrank_gaussian)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The term *full-rank* might seem a bit peculiar since covariance matrices are always full-rank.\nThis term, however, traditionally comes from the fact that full-rank families use full-rank factors in addition to the diagonal of the covariance.\n\nIn contrast to the mean-field family, the full-rank family will often result in more computation per optimization step and slower convergence, especially in high dimensions:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "q_fr, _, info_fr, _ = vi(m, q_init_fr, n_iters; show_progress=false, callback)\n\nPlots.plot(elbo_mf, xlabel=\"Iterations\", ylabel=\"ELBO\", label=\"Mean-Field\", ylims=(-200, Inf))\n\nelbo_fr = [i.elbo_avg for i in info_fr[iters]]\nPlots.plot!(elbo_fr, xlabel=\"Iterations\", ylabel=\"ELBO\", label=\"Full-Rank\", ylims=(-200, Inf))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "However, we can see that the full-rank families achieve a higher ELBO in the end.\nDue to the relationship between the ELBO and the Kullback-Leibler divergence, this indicates that the full-rank covariance is much more accurate.\nThis trade-off between statistical accuracy and optimization speed is often referred to as the *statistical-computational trade-off*.\nThe fact that we can control this trade-off through the choice of variational family is a strength, rather than a limitation, of variational inference.\n\nWe can also visualize the covariance matrix."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "heatmap(cov(rand(q_fr, 100_000), dims=2))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Obtaining Summary Statistics\n\nLet's inspect the resulting variational approximation in more detail and compare it against MCMC.\nTo obtain summary statistics from VI, we can draw samples from the resulting variational approximation:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "z = rand(q_fr, 100_000);"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Now, we can, for example, look at expectations:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "avg = vec(mean(z; dims=2))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The vector has the same ordering as the parameters in the model, *e.g.* in this case `σ²` has index `1`, `intercept` has index `2` and `coefficients` has indices `3:12`. If  you forget or you might want to do something programmatically with the result, you can obtain the `sym → indices` mapping as follows:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using Bijectors: bijector\n\n_, sym2range = bijector(m, Val(true));\nsym2range"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "For example, we can check the sample distribution and mean value of `σ²`:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "histogram(z[1, :])\navg[union(sym2range[:σ²]...)]"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "avg[union(sym2range[:intercept]...)]"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "avg[union(sym2range[:coefficients]...)]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "For further convenience, we can wrap the samples into a `Chains` object to summarize the results."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "varinf = Turing.DynamicPPL.VarInfo(m)\nvns_and_values = Turing.DynamicPPL.varname_and_value_leaves(Turing.DynamicPPL.values_as(varinf, OrderedDict))\nvarnames = map(first, vns_and_values)\nvi_chain = Chains(reshape(z', (size(z,2), size(z,1), 1)), varnames)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "(Since we're drawing independent samples, we can simply ignore the ESS and Rhat metrics.)\nUnfortunately, extracting `varnames` is a bit verbose at the moment, but hopefully will become simpler in the near future. \n\nLet's compare this against samples from `NUTS`:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "mcmc_chain = sample(m, NUTS(), 10_000, drop_warmup=true, progress=false);\n\nvi_mean = mean(vi_chain)[:, 2]\nmcmc_mean = mean(mcmc_chain, names(mcmc_chain, :parameters))[:, 2]\n\nplot(mcmc_mean; xticks=1:1:length(mcmc_mean), label=\"mean of NUTS\")\nplot!(vi_mean; label=\"mean of VI\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "That looks pretty good! But let's see how the predictive distributions looks for the two.\n\n## Making Predictions\n\nSimilarily to the linear regression tutorial, we're going to compare to multivariate ordinary linear regression using the `GLM` package:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Import the GLM package.\nusing GLM\n\n# Perform multivariate OLS.\nols = lm(\n    @formula(MPG ~ Cyl + Disp + HP + DRat + WT + QSec + VS + AM + Gear + Carb), train_cut\n)\n\n# Store our predictions in the original dataframe.\ntrain_cut.OLSPrediction = unstandardize(GLM.predict(ols), train_unstandardized.MPG)\ntest_cut.OLSPrediction = unstandardize(GLM.predict(ols, test_cut), train_unstandardized.MPG);"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Make a prediction given an input vector, using mean parameter values from a chain.\nfunction prediction(chain, x)\n    p = get_params(chain)\n    α = mean(p.intercept)\n    β = collect(mean.(p.coefficients))\n    return α .+ x * β\nend"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Unstandardize the dependent variable.\ntrain_cut.MPG = unstandardize(train_cut.MPG, train_unstandardized.MPG)\ntest_cut.MPG = unstandardize(test_cut.MPG, train_unstandardized.MPG);"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Show the first side rows of the modified dataframe.\nfirst(test_cut, 6)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Construct the Chains from the Variational Approximations\nz_mf = rand(q_mf, 10_000);\nz_fr = rand(q_fr, 10_000);\n\nvi_mf_chain = Chains(reshape(z_mf', (size(z_mf,2), size(z_mf,1), 1)), varnames);\nvi_fr_chain = Chains(reshape(z_fr', (size(z_fr,2), size(z_fr,1), 1)), varnames);"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Calculate the predictions for the training and testing sets using the samples `z` from variational posterior\ntrain_cut.VIMFPredictions = unstandardize(\n    prediction(vi_mf_chain, train), train_unstandardized.MPG\n)\ntest_cut.VIMFPredictions = unstandardize(\n    prediction(vi_mf_chain, test), train_unstandardized.MPG\n)\n\ntrain_cut.VIFRPredictions = unstandardize(\n    prediction(vi_fr_chain, train), train_unstandardized.MPG\n)\ntest_cut.VIFRPredictions = unstandardize(\n    prediction(vi_fr_chain, test), train_unstandardized.MPG\n)\n\ntrain_cut.BayesPredictions = unstandardize(\n    prediction(mcmc_chain, train), train_unstandardized.MPG\n)\ntest_cut.BayesPredictions = unstandardize(\n    prediction(mcmc_chain, test), train_unstandardized.MPG\n);"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "vi_mf_loss1 = mean((train_cut.VIMFPredictions - train_cut.MPG) .^ 2)\nvi_fr_loss1 = mean((train_cut.VIFRPredictions - train_cut.MPG) .^ 2)\nbayes_loss1 = mean((train_cut.BayesPredictions - train_cut.MPG) .^ 2)\nols_loss1 = mean((train_cut.OLSPrediction - train_cut.MPG) .^ 2)\n\nvi_mf_loss2 = mean((test_cut.VIMFPredictions - test_cut.MPG) .^ 2)\nvi_fr_loss2 = mean((test_cut.VIFRPredictions - test_cut.MPG) .^ 2)\nbayes_loss2 = mean((test_cut.BayesPredictions - test_cut.MPG) .^ 2)\nols_loss2 = mean((test_cut.OLSPrediction - test_cut.MPG) .^ 2)\n\nprintln(\"Training set:\n    VI Mean-Field loss: $vi_mf_loss1\n    VI Full-Rank loss: $vi_fr_loss1\n    Bayes loss: $bayes_loss1\n    OLS loss: $ols_loss1\nTest set:\n    VI Mean-Field loss: $vi_mf_loss2\n    VI Full-Rank loss: $vi_fr_loss2\n    Bayes loss: $bayes_loss2\n    OLS loss: $ols_loss2\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Interestingly the squared difference between true- and mean-prediction on the test-set is actually *better* for the full-rank variational posterior than for the \"true\" posterior obtained by MCMC sampling using `NUTS`.\nBut, as Bayesians, we know that the mean doesn't tell the entire story. One quick check is to look at the mean predictions ± standard deviation of the two different approaches:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "preds_vi_mf = mapreduce(hcat, 1:5:size(vi_mf_chain, 1)) do i\n    return unstandardize(prediction(vi_mf_chain[i], test), train_unstandardized.MPG)\nend\n\np1 = scatter(\n    1:size(test, 1),\n    mean(preds_vi_mf; dims=2);\n    yerr=std(preds_vi_mf; dims=2),\n    label=\"prediction (mean ± std)\",\n    size=(900, 500),\n    markersize=8,\n)\nscatter!(1:size(test, 1), unstandardize(test_label, train_unstandardized.MPG); label=\"true\")\nxaxis!(1:size(test, 1))\nylims!(10, 40)\ntitle!(\"VI Mean-Field\")\n\npreds_vi_fr = mapreduce(hcat, 1:5:size(vi_mf_chain, 1)) do i\n    return unstandardize(prediction(vi_fr_chain[i], test), train_unstandardized.MPG)\nend\n\np2 = scatter(\n    1:size(test, 1),\n    mean(preds_vi_fr; dims=2);\n    yerr=std(preds_vi_fr; dims=2),\n    label=\"prediction (mean ± std)\",\n    size=(900, 500),\n    markersize=8,\n)\nscatter!(1:size(test, 1), unstandardize(test_label, train_unstandardized.MPG); label=\"true\")\nxaxis!(1:size(test, 1))\nylims!(10, 40)\ntitle!(\"VI Full-Rank\")\n\npreds_mcmc = mapreduce(hcat, 1:5:size(mcmc_chain, 1)) do i\n    return unstandardize(prediction(mcmc_chain[i], test), train_unstandardized.MPG)\nend\n\np3 = scatter(\n    1:size(test, 1),\n    mean(preds_mcmc; dims=2);\n    yerr=std(preds_mcmc; dims=2),\n    label=\"prediction (mean ± std)\",\n    size=(900, 500),\n    markersize=8,\n)\nscatter!(1:size(test, 1), unstandardize(test_label, train_unstandardized.MPG); label=\"true\")\nxaxis!(1:size(test, 1))\nylims!(10, 40)\ntitle!(\"MCMC (NUTS)\")\n\nplot(p1, p2, p3; layout=(1, 3), size=(900, 250), label=\"\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We can see that the full-rank VI approximation is very close to the predictions from MCMC samples.\nAlso, the coverage of full-rank VI and MCMC is much better the crude mean-field approximation.\n\n[^KMJ2023]: Khaled, A., Mishchenko, K., & Jin, C. (2023). DoWG unleashed: An efficient universal parameter-free gradient descent method. In *Advances in Neural Information Processing Systems*, 36.\n[^D2020]: Domke, J. (2020). Provable smoothness guarantees for black-box variational inference. In *Proceedings of the International Conference on Machine Learning*. PMLR.\n[^DGG2023]: Domke, J., Gower, R., & Garrigos, G. (2023). Provable convergence guarantees for black-box variational inference. In *Advances in Neural Information Processing Systems*, 36.\n[^IHC2023]: Ivgi, M., Hinder, O., & Carmon, Y. (2023). DoG is SGD’s best friend: A parameter-free dynamic step size schedule. In *Proceedings of the International Conference on Machine Learning*. PMLR.\n[^KB2014]: Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. In *Proceedings of the International Conference on Learning Representations*.\n[^KOWMG2023]: Kim, K., Oh, J., Wu, K., Ma, Y., & Gardner, J. (2023). On the convergence of black-box variational inference. In *Advances in Neural Information Processing Systems*, 36.\n[^KTRGB2017]: Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., & Blei, D. M. (2017). Automatic differentiation variational inference. *Journal of Machine Learning Research*, 18(14).\n[^KW2014]: Kingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. In *Proceedings of the International Conference on Learning Representations.*\n[^RGB2014]: Ranganath, R., Gerrish, S., & Blei, D. (2014). Black box variational inference. In *Proceedings of the International Conference on Artificial intelligence and statistics*. PMLR.\n[^RMW2014]: Rezende, D. J., Mohamed, S., & Wierstra, D (2014). Stochastic backpropagation and approximate inference in deep generative models. In *Proceedings of the International Conference on Machine Learning*. PMLR.\n[^SZ2013]: Shamir, O., & Zhang, T. (2013). Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In *Proceedings of the International Conference on Machine Learning.* PMLR.\n[^TL2014]: Titsias, M., & Lázaro-Gredilla, M. (2014). Doubly stochastic variational Bayes for non-conjugate inference. In *Proceedings of the International Conference on Machine Learning*. PMLR."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
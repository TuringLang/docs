{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"A Mini Turing Implementation II: Contexts\"\n",
        "engine: julia\n",
        "aliases:\n",
        "  - ../../../tutorials/16-contexts/index.html\n",
        "---\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "#| output: false\n",
        "using Pkg;\n",
        "Pkg.instantiate();\n",
        "```\n",
        "\n",
        "In the [Mini Turing]({{< meta minituring >}}) tutorial we developed a miniature version of the Turing language, to illustrate its core design. A passing mention was made of contexts. In this tutorial we develop that aspect of our mini Turing language further to demonstrate how and why contexts are an important part of Turing's design.\n",
        "\n",
        "::: {.callout-important}\n",
        "Note: The way Turing actually uses contexts changed somewhat in releases 0.39 and 0.40. The content of this page remains relevant, the principles of how contexts operate remain the same, and concepts like leaf and parent contexts still exist. However, we've moved away from using contexts for quite as many things as we used to. Most importantly, whether to accumulate the log joint, log prior, or log likelihood is no longer done using different contexts. Please keep this in mind as you read this page: The principles remain, but the details have changed. We will update this page once the refactoring of internals that is happening around releases like 0.39 and 0.40 is done.\n",
        ":::\n",
        "\n",
        "# Mini Turing expanded, now with more contexts\n",
        "\n",
        "If you haven't read [Mini Turing]({{< meta minituring >}}) yet, you should do that first. We start by repeating verbatim much of the code from there. Define the type for holding values for variables:\n",
        "\n",
        "```{julia}\n",
        "import MacroTools, Random, AbstractMCMC\n",
        "using Distributions: Normal, logpdf\n",
        "using MCMCChains: Chains\n",
        "using AbstractMCMC: sample\n",
        "\n",
        "struct VarInfo{V,L}\n",
        "    values::V\n",
        "    logps::L\n",
        "end\n",
        "\n",
        "VarInfo() = VarInfo(Dict{Symbol,Float64}(), Dict{Symbol,Float64}())\n",
        "\n",
        "function Base.setindex!(varinfo::VarInfo, (value, logp), var_id)\n",
        "    varinfo.values[var_id] = value\n",
        "    varinfo.logps[var_id] = logp\n",
        "    return varinfo\n",
        "end\n",
        "```\n",
        "\n",
        "Define the macro that expands `~` expressions to calls to `assume` and `observe`:\n",
        "\n",
        "```{julia}\n",
        "# Methods will be defined for these later.\n",
        "function assume end\n",
        "function observe end\n",
        "\n",
        "macro mini_model(expr)\n",
        "    return esc(mini_model(expr))\n",
        "end\n",
        "\n",
        "function mini_model(expr)\n",
        "    # Split the function definition into a dictionary with its name, arguments, body etc.\n",
        "    def = MacroTools.splitdef(expr)\n",
        "\n",
        "    # Replace tildes in the function body with calls to `assume` or `observe`\n",
        "    def[:body] = MacroTools.postwalk(def[:body]) do sub_expr\n",
        "        if MacroTools.@capture(sub_expr, var_ ~ dist_)\n",
        "            if var in def[:args]\n",
        "                # If the variable is an argument of the model function, it is observed\n",
        "                return :($(observe)(context, varinfo, $dist, $(Meta.quot(var)), $var))\n",
        "            else\n",
        "                # Otherwise it is unobserved\n",
        "                return :($var = $(assume)(context, varinfo, $dist, $(Meta.quot(var))))\n",
        "            end\n",
        "        else\n",
        "            return sub_expr\n",
        "        end\n",
        "    end\n",
        "\n",
        "    # Add `context` and `varinfo` arguments to the model function\n",
        "    def[:args] = vcat(:varinfo, :context, def[:args])\n",
        "\n",
        "    # Reassemble the function definition from its name, arguments, body etc.\n",
        "    return MacroTools.combinedef(def)\n",
        "end\n",
        "\n",
        "\n",
        "struct MiniModel{F,D} <: AbstractMCMC.AbstractModel\n",
        "    f::F\n",
        "    data::D # a NamedTuple of all the data\n",
        "end\n",
        "```\n",
        "\n",
        "Define an example model:\n",
        "\n",
        "```{julia}\n",
        "@mini_model function m(x)\n",
        "    a ~ Normal(0.5, 1)\n",
        "    b ~ Normal(a, 2)\n",
        "    x ~ Normal(b, 0.5)\n",
        "    return nothing\n",
        "end;\n",
        "\n",
        "mini_m = MiniModel(m, (x=3.0,))\n",
        "```\n",
        "\n",
        "Previously in the mini Turing case, at this point we defined `SamplingContext`, a structure that holds a random number generator and a sampler, and gets passed to `observe` and `assume`. We then used it to implement a simple Metropolis-Hastings sampler.\n",
        "\n",
        "The notion of a context may have seemed overly complicated just to implement the sampler, but there are other things we may want to do with a model than sample from the posterior. Having the context passing in place lets us do that without having to touch the above macro at all. For instance, let's say we want to evaluate the log joint probability of the model for a given set of data and parameters. Using a new context type we can use the previously defined `model` function, but change its behavior by changing what the `observe` and `assume` functions do.\n",
        "\n",
        "\n",
        "\n",
        "```{julia}\n",
        "struct JointContext end\n",
        "\n",
        "function observe(context::JointContext, varinfo, dist, var_id, var_value)\n",
        "    logp = logpdf(dist, var_value)\n",
        "    varinfo[var_id] = (var_value, logp)\n",
        "    return nothing\n",
        "end\n",
        "\n",
        "function assume(context::JointContext, varinfo, dist, var_id)\n",
        "    if !haskey(varinfo.values, var_id)\n",
        "        error(\"Can't evaluate the log probability if the variable $(var_id) is not set.\")\n",
        "    end\n",
        "    var_value = varinfo.values[var_id]\n",
        "    logp = logpdf(dist, var_value)\n",
        "    varinfo[var_id] = (var_value, logp)\n",
        "    return var_value\n",
        "end\n",
        "\n",
        "function logjoint(model, parameter_values::NamedTuple)\n",
        "    vi = VarInfo()\n",
        "    for (var_id, value) in pairs(parameter_values)\n",
        "        # Set the log prob to NaN for now. These will get overwritten when model.f is\n",
        "        # called with JointContext.\n",
        "        vi[var_id] = (value, NaN)\n",
        "    end\n",
        "    model.f(vi, JointContext(), values(model.data)...)\n",
        "    return sum(values(vi.logps))\n",
        "end\n",
        "\n",
        "logjoint(mini_m, (a=0.5, b=1.0))\n",
        "```\n",
        "\n",
        "When using the `JointContext` no sampling whatsoever happens in calling `mini_m`. Rather only the log probability of each given variable value is evaluated. `logjoint` then sums these results to get the total log joint probability.\n",
        "\n",
        "We can similarly define a context for evaluating the log prior probability:\n",
        "\n",
        "```{julia}\n",
        "struct PriorContext end\n",
        "\n",
        "function observe(context::PriorContext, varinfo, dist, var_id, var_value)\n",
        "    # Since we are evaluating the prior, the log probability of all the observations\n",
        "    # is set to 0. This has the effect of ignoring the likelihood.\n",
        "    varinfo[var_id] = (var_value, 0.0)\n",
        "    return nothing\n",
        "end\n",
        "\n",
        "function assume(context::PriorContext, varinfo, dist, var_id)\n",
        "    if !haskey(varinfo.values, var_id)\n",
        "        error(\"Can't evaluate the log probability if the variable $(var_id) is not set.\")\n",
        "    end\n",
        "    var_value = varinfo.values[var_id]\n",
        "    logp = logpdf(dist, var_value)\n",
        "    varinfo[var_id] = (var_value, logp)\n",
        "    return var_value\n",
        "end\n",
        "\n",
        "function logprior(model, parameter_values::NamedTuple)\n",
        "    vi = VarInfo()\n",
        "    for (var_id, value) in pairs(parameter_values)\n",
        "        vi[var_id] = (value, NaN)\n",
        "    end\n",
        "    model.f(vi, PriorContext(), values(model.data)...)\n",
        "    return sum(values(vi.logps))\n",
        "end\n",
        "\n",
        "logprior(mini_m, (a=0.5, b=1.0))\n",
        "```\n",
        "\n",
        "Notice that the definition of `assume(context::PriorContext, args...)` is identical to the one for `JointContext`, and `logprior` and `logjoint` are also identical except for the context type they create. There's clearly an opportunity here for some refactoring using abstract types, but that's outside the scope of this tutorial. Rather, the point here is to demonstrate that we can extract different sorts of things from our model by defining different context types, and specialising `observe` and `assume` for them.\n",
        "\n",
        "\n",
        "## Contexts within contexts\n",
        "\n",
        "Let's use the above two contexts to provide a slightly more general definition of the `SamplingContext` and the Metropolis-Hastings sampler we wrote in the mini Turing tutorial.\n",
        "\n",
        "```{julia}\n",
        "struct SamplingContext{S<:AbstractMCMC.AbstractSampler,R<:Random.AbstractRNG}\n",
        "    rng::R\n",
        "    sampler::S\n",
        "    subcontext::Union{PriorContext, JointContext}\n",
        "end\n",
        "```\n",
        "\n",
        "The new aspect here is the `subcontext` field. Note that this is a context within a context! The idea is that we don't need to hard code how the MCMC sampler evaluates the log probability, but rather can pass that work onto the subcontext. This way the same sampler can be used to sample from either the joint or the prior distribution.\n",
        "\n",
        "The methods for `SamplingContext` are largely as in the our earlier mini Turing case, except they now pass some of the work onto the subcontext:\n",
        "\n",
        "```{julia}\n",
        "function observe(context::SamplingContext, args...)\n",
        "    # Sampling doesn't affect the observed values, so nothing to do here other than pass to\n",
        "    # the subcontext.\n",
        "    return observe(context.subcontext, args...)\n",
        "end\n",
        "\n",
        "struct PriorSampler <: AbstractMCMC.AbstractSampler end\n",
        "\n",
        "function assume(context::SamplingContext{PriorSampler}, varinfo, dist, var_id)\n",
        "    sample = Random.rand(context.rng, dist)\n",
        "    varinfo[var_id] = (sample, NaN)\n",
        "    # Once the value has been sampled, let the subcontext handle evaluating the log\n",
        "    # probability.\n",
        "    return assume(context.subcontext, varinfo, dist, var_id)\n",
        "end;\n",
        "\n",
        "# The subcontext field of the MHSampler determines which distribution this sampler\n",
        "# samples from.\n",
        "struct MHSampler{D, T<:Real} <: AbstractMCMC.AbstractSampler\n",
        "    sigma::T\n",
        "    subcontext::D\n",
        "end\n",
        "\n",
        "MHSampler(subcontext) = MHSampler(1, subcontext)\n",
        "\n",
        "function assume(context::SamplingContext{<:MHSampler}, varinfo, dist, var_id)\n",
        "    sampler = context.sampler\n",
        "    old_value = varinfo.values[var_id]\n",
        "\n",
        "    # propose a random-walk step, i.e, add the current value to a random \n",
        "    # value sampled from a Normal distribution centered at 0\n",
        "    value = rand(context.rng, Normal(old_value, sampler.sigma))\n",
        "    varinfo[var_id] = (value, NaN)\n",
        "    # Once the value has been sampled, let the subcontext handle evaluating the log\n",
        "    # probability.\n",
        "    return assume(context.subcontext, varinfo, dist, var_id)\n",
        "end;\n",
        "\n",
        "# The following three methods are identical to before, except for passing\n",
        "# `sampler.subcontext` to the context SamplingContext.\n",
        "function AbstractMCMC.step(\n",
        "    rng::Random.AbstractRNG, model::MiniModel, sampler::MHSampler; kwargs...\n",
        ")\n",
        "    vi = VarInfo()\n",
        "    ctx = SamplingContext(rng, PriorSampler(), sampler.subcontext)\n",
        "    model.f(vi, ctx, values(model.data)...)\n",
        "    return vi, vi\n",
        "end\n",
        "\n",
        "function AbstractMCMC.step(\n",
        "    rng::Random.AbstractRNG,\n",
        "    model::MiniModel,\n",
        "    sampler::MHSampler,\n",
        "    prev_state::VarInfo; # is just the old trace\n",
        "    kwargs...,\n",
        ")\n",
        "    vi = prev_state\n",
        "    new_vi = deepcopy(vi)\n",
        "    ctx = SamplingContext(rng, sampler, sampler.subcontext)\n",
        "    model.f(new_vi, ctx, values(model.data)...)\n",
        "\n",
        "    # Compute log acceptance probability\n",
        "    # Since the proposal is symmetric the computation can be simplified\n",
        "    logα = sum(values(new_vi.logps)) - sum(values(vi.logps))\n",
        "\n",
        "    # Accept proposal with computed acceptance probability\n",
        "    if -Random.randexp(rng) < logα\n",
        "        return new_vi, new_vi\n",
        "    else\n",
        "        return prev_state, prev_state\n",
        "    end\n",
        "end;\n",
        "\n",
        "function AbstractMCMC.bundle_samples(\n",
        "    samples, model::MiniModel, ::MHSampler, ::Any, ::Type{Chains}; kwargs...\n",
        ")\n",
        "    # We get a vector of traces\n",
        "    values = [sample.values for sample in samples]\n",
        "    params = [key for key in keys(values[1]) if key ∉ keys(model.data)]\n",
        "    vals = reduce(hcat, [value[p] for value in values] for p in params)\n",
        "    # Composing the `Chains` data-structure, of which analyzing infrastructure is provided\n",
        "    chains = Chains(vals, params)\n",
        "    return chains\n",
        "end;\n",
        "```\n",
        "\n",
        "We can use this to sample from the joint distribution just like before:\n",
        "\n",
        "```{julia}\n",
        "sample(MiniModel(m, (x=3.0,)), MHSampler(JointContext()), 1_000_000; chain_type=Chains, progress=false)\n",
        "```\n",
        "\n",
        "or we can choose to sample from the prior instead\n",
        "\n",
        "```{julia}\n",
        "sample(MiniModel(m, (x=3.0,)), MHSampler(PriorContext()), 1_000_000; chain_type=Chains, progress=false)\n",
        "```\n",
        "\n",
        "Of course, using an MCMC algorithm to sample from the prior is unnecessary and silly (`PriorSampler` exists, after all), but the point is to illustrate the flexibility of the context system. We could, for instance, use the same setup to implement an _Approximate Bayesian Computation_ (ABC) algorithm. \n",
        "\n",
        "\n",
        "The use of contexts also goes far beyond just evaluating log probabilities and sampling. Some examples from Turing are\n",
        "\n",
        "* `FixedContext`, which fixes some variables to given values and removes them completely from the evaluation of any log probabilities. They power the `Turing.fix` and `Turing.unfix` functions.\n",
        "* `ConditionContext` conditions the model on fixed values for some parameters. They are used by `Turing.condition` and `Turing.decondition`, i.e. the `model | (parameter=value,)` syntax. The difference between `fix` and `condition` is whether the log probability for the corresponding variable is included in the overall log density. \n",
        "\n",
        "* `PriorExtractorContext` collects information about what the prior distribution of each variable is.\n",
        "* `PrefixContext` adds prefixes to variable names, allowing models to be used within other models without variable name collisions.\n",
        "* `PointwiseLikelihoodContext` records the log likelihood of each individual variable.\n",
        "* `DebugContext` collects useful debugging information while executing the model.\n",
        "\n",
        "All of the above are what Turing calls _parent contexts_, which is to say that they all keep a subcontext just like our above `SamplingContext` did. Their implementations of `assume` and `observe` call the implementation of the subcontext once they are done doing their own work of fixing/conditioning/prefixing/etc. Contexts are often chained, so that e.g. a `DebugContext` may wrap within it a `PrefixContext`, which may in turn wrap a `ConditionContext`, etc. The only contexts that _don't_ have a subcontext in the Turing are the ones for evaluating the prior, likelihood, and joint distributions. These are called _leaf contexts_.\n",
        "\n",
        "The above version of mini Turing is still much simpler than the full Turing language, but the principles of how contexts are used are the same."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
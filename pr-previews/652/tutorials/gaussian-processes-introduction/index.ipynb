{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#| echo: false\n#| output: false\nusing Pkg;\nPkg.instantiate();"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "[JuliaGPs](https://github.com/JuliaGaussianProcesses/#welcome-to-juliagps) packages integrate well with Turing.jl because they implement the Distributions.jl\ninterface.\nYou should be able to understand what is going on in this tutorial if you know what a GP is.\nFor a more in-depth understanding of the\n[JuliaGPs](https://github.com/JuliaGaussianProcesses/#welcome-to-juliagps) functionality\nused here, please consult the\n[JuliaGPs](https://github.com/JuliaGaussianProcesses/#welcome-to-juliagps) docs.\n\nIn this tutorial, we will model the putting dataset discussed in Chapter 21 of\n[Bayesian Data Analysis](http://www.stat.columbia.edu/%7Egelman/book/).\nThe dataset comprises the result of measuring how often a golfer successfully gets the ball\nin the hole, depending on how far away from it they are.\nThe goal of inference is to estimate the probability of any given shot being successful at a\ngiven distance.\n\n### Let's download the data and take a look at it:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using CSV, DataFrames\n\ndf = CSV.read(\"golf.dat\", DataFrame; delim=' ', ignorerepeated=true)\ndf[1:5, :]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We've printed the first 5 rows of the dataset (which comprises only 19 rows in total).\nObserve it has three columns:\n\n 1. `distance` -- how far away from the hole. I'll refer to `distance` as `d` throughout the rest of this tutorial\n 2. `n` -- how many shots were taken from a given distance\n 3. `y` -- how many shots were successful from a given distance\n\nWe will use a Binomial model for the data, whose success probability is parametrised by a\ntransformation of a GP. Something along the lines of:\n$$\n\\begin{aligned}\nf & \\sim \\operatorname{GP}(0, k) \\\\\ny_j \\mid f(d_j) & \\sim \\operatorname{Binomial}(n_j, g(f(d_j))) \\\\\ng(x) & := \\frac{1}{1 + e^{-x}}\n\\end{aligned}\n$$\n\nTo do this, let's define our Turing.jl model:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using AbstractGPs, LogExpFunctions, Turing\n\n@model function putting_model(d, n; jitter=1e-4)\n    v ~ Gamma(2, 1)\n    l ~ Gamma(4, 1)\n    f = GP(v * with_lengthscale(SEKernel(), l))\n    f_latent ~ f(d, jitter)\n    y ~ product_distribution(Binomial.(n, logistic.(f_latent)))\n    return (fx=f(d, jitter), f_latent=f_latent, y=y)\nend"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We first define an `AbstractGPs.GP`, which represents a distribution over functions, and\nis entirely separate from Turing.jl.\nWe place a prior over its variance `v` and length-scale `l`.\n`f(d, jitter)` constructs the multivariate Gaussian comprising the random variables\nin `f` whose indices are in `d` (plus a bit of independent Gaussian noise with variance\n`jitter` -- see [the docs](https://juliagaussianprocesses.github.io/AbstractGPs.jl/dev/api/#FiniteGP-and-AbstractGP)\nfor more details).\n`f(d, jitter)` has the type `AbstractMvNormal`, and is the bit of AbstractGPs.jl that implements the\nDistributions.jl interface, so it's legal to put it on the right-hand side\nof a `~`.\nFrom this you should deduce that `f_latent` is distributed according to a multivariate\nGaussian.\nThe remaining lines comprise standard Turing.jl code that is encountered in other tutorials\nand Turing documentation.\n\nBefore performing inference, we might want to inspect the prior that our model places over\nthe data, to see whether there is anything obviously wrong.\nThese kinds of prior predictive checks are straightforward to perform using Turing.jl, since\nit is possible to sample from the prior easily by just calling the model:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "m = putting_model(Float64.(df.distance), df.n)\nm().y"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We make use of this to see what kinds of datasets we simulate from the prior:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using Plots\n\nfunction plot_data(d, n, y, xticks, yticks)\n    ylims = (0, round(maximum(n), RoundUp; sigdigits=2))\n    margin = -0.5 * Plots.mm\n    plt = plot(; xticks=xticks, yticks=yticks, ylims=ylims, margin=margin, grid=false)\n    bar!(plt, d, n; color=:red, label=\"\", alpha=0.5)\n    bar!(plt, d, y; label=\"\", color=:blue, alpha=0.7)\n    return plt\nend\n\n# Construct model and run some prior predictive checks.\nm = putting_model(Float64.(df.distance), df.n)\nhists = map(1:20) do j\n    xticks = j > 15 ? :auto : nothing\n    yticks = rem(j, 5) == 1 ? :auto : nothing\n    return plot_data(df.distance, df.n, m().y, xticks, yticks)\nend\nplot(hists...; layout=(4, 5))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In this case, the only prior knowledge I have is that the proportion of successful shots\nought to decrease monotonically as the distance from the hole increases, which should show\nup in the data as the blue lines generally go down as we move from left to right on each\ngraph.\nUnfortunately, there is not a simple way to enforce monotonicity in the samples from a GP,\nand we can see this in some of the plots above, so we must hope that we have enough data to\nensure that this relationship holds approximately under the posterior.\nIn any case, you can judge for yourself whether you think this is the most useful\nvisualisation that we can perform -- if you think there is something better to look at,\nplease let us know!\n\nMoving on, we generate samples from the posterior using the default `NUTS` sampler.\nWe'll make use of [ReverseDiff.jl](https://github.com/JuliaDiff/ReverseDiff.jl), as it has\nbetter performance than [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl/) on\nthis example. See the [automatic differentiation docs]({{< meta usage-automatic-differentiation >}}) for more info."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using Random, ReverseDiff\n\nm_post = m | (y=df.y,)\nchn = sample(Xoshiro(123456), m_post, NUTS(; adtype=AutoReverseDiff()), 1_000, progress=false)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We can use these samples and the `posterior` function from `AbstractGPs` to sample from the\nposterior probability of success at any distance we choose:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "d_pred = 1:0.2:21\nsamples = map(returned(m_post, chn)[1:10:end]) do x\n    return logistic.(rand(posterior(x.fx, x.f_latent)(d_pred, 1e-4)))\nend\np = plot()\nplot!(d_pred, reduce(hcat, samples); label=\"\", color=:blue, alpha=0.2)\nscatter!(df.distance, df.y ./ df.n; label=\"\", color=:red)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We can see that the general trend is indeed down as the distance from the hole increases,\nand that if we move away from the data, the posterior uncertainty quickly inflates.\nThis suggests that the model is probably going to do a reasonable job of interpolating\nbetween observed data, but less good a job at extrapolating to larger distances."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia 1.11",
      "language": "julia",
      "name": "julia-1.11"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
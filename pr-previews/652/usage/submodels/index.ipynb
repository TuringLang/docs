{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using Turing\nusing Random: Xoshiro, seed!\nseed!(468)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In Turing.jl, you can define models and use them as components of larger models (i.e., _submodels_), using the `to_submodel` function.\nIn this way, you can (for example) define a model once and use it in multiple places:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function inner()\n    a ~ Normal()\n    return a + 100\nend\n\n@model function outer()\n    # This line adds the variable `x.a` to the chain.\n    # The inner variable `a` is prefixed with the\n    # left-hand side of the `~` operator, i.e. `x`.\n    x ~ to_submodel(inner())\n    # Here, the value of x will be `a + 100` because\n    # that is the return value of the submodel.\n    b ~ Normal(x)\nend"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "If we sample from this model, we would expect that `x.a` should be close to zero, and `b` close to 100:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "rand(outer())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Manipulating submodels\n\n### Conditioning\n\nIn general, everything that can be done to a model 'carries over' to when it is used as a submodel.\nFor example, you can condition a variable in a submodel in two ways:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# From the outside; the prefix `x` must be applied because\n# from the perspective of `outer`, the variable is called\n# `x.a`.\nouter_conditioned1 = outer() | (@varname(x.a) => 1);\nrand(Xoshiro(468), outer_conditioned1)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Or equivalently, from the inside:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function outer_conditioned2()\n    # The prefix doesn't need to be applied here because\n    # `inner` itself has no knowledge of the prefix.\n    x ~ to_submodel(inner() | (@varname(a) => 1))\n    b ~ Normal(x)\nend\nrand(Xoshiro(468), outer_conditioned2())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In both cases the variable `x.a` does not appear.\n\nNote, however, that you cannot condition on the return value of a submodel.\nThus, for example, if we had:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function inner_sensible()\n    a ~ Normal()\n    return a\nend\n\n@model function outer()\n    x ~ to_submodel(inner())\n    b ~ Normal(x)\nend"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "and we tried to condition on `x`, it would be silently ignored, even though `x` is equal to `a`.\n\nThe reason for this is because it is entirely coincidental that the return value of the submodel is equal to `a`.\nIn general, a return value can be anything, and conditioning on it is in general not a meaningful operation.\n\n### Prefixing\n\nPrefixing is the only place where submodel behaviour is 'special' compared to that of ordinary models.\n\nBy default, all variables in a submodel are prefixed with the left-hand side of the tilde-statement.\nThis is done to avoid clashes if the same submodel is used multiple times in a model.\n\nYou can disable this by passing `false` as the second argument to `to_submodel`:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function outer_no_prefix()\n    x ~ to_submodel(inner(), false)\n    b ~ Normal(x)\nend\nrand(outer_no_prefix())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Accessing submodel variables\n\nIn all of the examples above, `x` is equal to `a + 100` because that is the return value of the submodel.\nTo access the actual latent variables in the submodel itself, the simplest option is to include the variable in the return value of the submodel:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function inner_with_retval()\n    a ~ Normal()\n    # You can return anything you like from the model,\n    # but if you want to access the latent variables, they\n    # should be included in the return value.\n    return (; a=a, a_plus_100=a + 100)\nend\n@model function outer_with_retval()\n    # The variable `x` will now contain the return value of the submodel,\n    # which is a named tuple with `a` and `a_plus_100`.\n    x ~ to_submodel(inner_with_retval())\n    # You can access the value of x.a directly, because\n    # x is a NamedTuple which contains `a`. Since `b` is\n    # centred on `x.a`, it should be close to 0, not 100.\n    b ~ Normal(x.a)\nend\nrand(Xoshiro(468), outer_with_retval())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "You can also manually access the value by looking inside the special `__varinfo__` object.\n\n::: {.callout-warning}\nThis relies on DynamicPPL internals and we do not recommend doing this unless you have no other option, e.g., if the submodel is defined in a different package which you do not control.\n:::"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function outer_with_varinfo()\n    x ~ to_submodel(inner())\n    # Access the value of x.a\n    a_value = __varinfo__[@varname(x.a)]\n    b ~ Normal(a_value)\nend\nrand(Xoshiro(468), outer_with_varinfo())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Example: linear models\n\nHere is a motivating example for the use of submodels.\nSuppose we want to fit a (very simplified) regression model to some data $x$ and $y$, where\n\n$$\\begin{align}\nc_0 &\\sim \\text{Normal}(0, 5) \\\\\nc_1 &\\sim \\text{Normal}(0, 5) \\\\\n\\mu &= c_0 + c_1x \\\\\ny &\\sim d\n\\end{align}$$\n\nwhere $d$ is _some_ distribution parameterised by the value of $\\mu$, which we don't know the exact form of.\n\nIn practice, what we would do is to write several different models, one for each function $f$:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function normal(x, y)\n    c0 ~ Normal(0, 5)\n    c1 ~ Normal(0, 5)\n    mu = c0 .+ c1 .* x\n    # Assume that y = mu, and that the noise in `y` is\n    # normally distributed with standard deviation sigma\n    sigma ~ truncated(Cauchy(0, 3); lower=0)\n    for i in eachindex(y)\n        y[i] ~ Normal(mu[i], sigma)\n    end\nend\n\n@model function logpoisson(x, y)\n    c0 ~ Normal(0, 5)\n    c1 ~ Normal(0, 5)\n    mu = c0 .+ c1 .* x\n    # exponentiate mu because the rate parameter of\n    # a Poisson distribution must be positive\n    for i in eachindex(y)\n        y[i] ~ Poisson(exp(mu[i]))\n    end\nend\n\n# and so on..."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "::: {.callout-note}\nYou could use `arraydist` to avoid the loops: for example, in `logpoisson`, one could write `y ~ arraydist(Poisson.(exp.(mu)))`, but for simplicity in this tutorial we spell it out fully.\n:::\n\nWe would then fit all of our models and use some criterion to test which model is most suitable (see e.g. [Wikipedia](https://en.wikipedia.org/wiki/Model_selection), or section 3.4 of Bishop's *Pattern Recognition and Machine Learning*).\n\nHowever, the code above is quite repetitive.\nFor example, if we wanted to adjust the priors on `c0` and `c1`, we would have to do it in each model separately.\nIf this was any other kind of code, we would naturally think of extracting the common parts into a separate function.\nIn this case we can do exactly that with a submodel:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function priors(x)\n    c0 ~ Normal(0, 5)\n    c1 ~ Normal(0, 5)\n    mu = c0 .+ c1 .* x\n    return (; c0=c0, c1=c1, mu=mu)\nend\n\n@model function normal(x, y)\n    ps = to_submodel(priors(x))\n    sigma ~ truncated(Cauchy(0, 3); lower=0)\n    for i in eachindex(y)\n        y[i] ~ Normal(ps.mu[i], sigma)\n    end\nend\n\n@model function logpoisson(x, y)\n    ps = to_submodel(priors(x))\n    for i in eachindex(y)\n        y[i] ~ Poisson(exp(ps.mu[i]))\n    end\nend"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "One could go even further and extract the `y` section into its own submodel as well, which would bring us to a generalised linear modelling interface that does not actually require the user to define their own Turing models at all:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function normal_family(mu, y)\n    sigma ~ truncated(Cauchy(0, 3); lower=0)\n    for i in eachindex(y)\n        y[i] ~ Normal(mu[i], sigma)\n    end\n    return nothing\nend\n\n@model function logpoisson_family(mu, y)\n    for i in eachindex(y)\n        y[i] ~ Poisson(exp(mu[i]))\n    end\n    return nothing\nend\n\n# An end-user could just use this function. Of course,\n# a more thorough interface would also allow the user to\n# specify priors, etc.\nfunction make_model(x, y, family::Symbol)\n    if family == :normal\n        family_model = normal_family\n    elseif family == :logpoisson\n        family_model = logpoisson_family\n    else\n        error(\"unknown family: `$family`\")\n    end\n\n    @model function general(x, y)\n        ps ~ to_submodel(priors(x), false)\n        _n ~ to_submodel(family_model(ps.mu, y), false)\n    end\n    return general(x, y)\nend\n\nsample(make_model([1, 2, 3], [1, 2, 3], :normal), NUTS(), 1000; progress=false)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "While this final example really showcases the composability of submodels, it also illustrates a minor syntactic drawback.\nWhen we create a submodel from `family_model(ps.mu, y)`, in principle, we do not really care about its return value because it is not used anywhere else in the model.\nIdeally, we should therefore not need to place anything on the left-hand side of `to_submodel`.\nHowever, because the special behaviour of `to_submodel` relies on the tilde operator, and the tilde operator requires a left-hand side, we have to use a dummy variable (here `_n`).\n\nFurthermore, because the left-hand side of a tilde-statement must be a valid variable name, we cannot use destructuring syntax on the left-hand side of `to_submodel`, even if the return value is a NamedTuple.\nThus, for example, the following is not allowed:\n\n```julia\n(; c0, c1, mu) ~ to_submodel(priors(x))\n```\n\nTo use destructuring syntax, you would have to add a separate line:\n\n```julia\nps = to_submodel(priors(x))\n(; c0, c1, mu) = ps\n```\n\n## Submodels versus distributions\n\nFinally, we end with a discussion of why some of the behaviour for submodels above has come about.\nThis is slightly more behind-the-scenes and therefore will likely be of most interest to Turing developers.\n\nFundamentally, submodels are to be compared against distributions: both of them can appear on the right-hand side of a tilde statement.\nHowever, distributions only have one 'output', i.e., the value that is sampled from them:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "dist = Normal()\nrand(dist)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Another point to bear in mind is that, given a sample from `dist`, asking for its log-probability is a meaningful calculation."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "logpdf(dist, rand(dist))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In contrast, models (and hence submodels) have two different outputs: the latent variables, and the return value.\nThese are accessed respectively using `rand(model)` and `model()`:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function f()\n    a ~ Normal()\n    return \"hello, world.\"\nend\n\nmodel = f()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Latent variables\nrand(model)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Return value\nmodel()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Just like for distributions, one can indeed ask for the log-probability of the latent variables (although we have to specify whether we want the joint, likelihood, or prior):"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "logjoint(model, rand(model))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "But it does not make sense to ask for the log-probability of the return value (which in this case is a string, and in general, could be literally any object).\n\nThe fact that we have what looks like a unified notation for these is a bit of a lie, since it hides this distinction.\nIn particular, for `x ~ distr`, `x` is assigned the value of `rand(distr)`; but for `y ~ submodel`, `y` is assigned the value of `submodel()`.\nThis is why, for example, it is impossible to condition on `y` in `y ~ ...`; we can only condition on `x` in `x ~ dist`.\n\nEventually we would like to make this more logically consistent.\nIn particular, it is clear that `y ~ submodel` should return not one but two objects: the latent variables and the return value.\nFurthermore, it should be possible to condition on the latent variables, but not on the return value.\nSee [this issue](https://github.com/TuringLang/Turing.jl/issues/2485) for an ongoing discussion of the best way to accomplish this.\n\nIt should be mentioned that extracting the latent variables from a submodel is not entirely trivial since the submodel is run using the same `VarInfo` as the parent model (i.e., we would have to do a before-and-after comparison to see which _new_ variables were added by the submodel).\n\nAlso, we are still working out the exact data structure that should be used to represent the latent variables.\nIn the examples above `rand(model)` returns a `NamedTuple`, but this actually causes loss of information because the keys of a `NamedTuple` are `Symbol`s, whereas we really want to use `VarName`s.\nSee [this issue](https://github.com/TuringLang/DynamicPPL.jl/issues/900) for a current proposal."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia 1.11",
      "language": "julia",
      "name": "julia-1.11"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": "# Install necessary dependencies.\nusing Pkg\nPkg.activate(; temp=true)\nPkg.add([])",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| echo: false\n#| output: false\nusing Pkg;\nPkg.instantiate();\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Introduction\n\n### The Code\n\nFor each sampler, we will use the same code to plot sampler paths. The block below loads the relevant libraries and defines a function for plotting the sampler's trajectory across the posterior.\n\nThe Turing model definition used here is not especially practical, but it is designed in such a way as to produce visually interesting posterior surfaces to show how different samplers move along the distribution.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\nENV[\"GKS_ENCODING\"] = \"utf-8\" # Allows the use of unicode characters in Plots.jl\nusing Plots\nusing StatsPlots\nusing Turing\nusing Random\nusing Bijectors\n\n# Set a seed.\nRandom.seed!(0)\n\n# Define a strange model.\n@model function gdemo(x)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    bumps = sin(m) + cos(m)\n    m = m + 5 * bumps\n    for i in eachindex(x)\n        x[i] ~ Normal(m, sqrt(s²))\n    end\n    return s², m\nend\n\n# Define our data points.\nx = [1.5, 2.0, 13.0, 2.1, 0.0]\n\n# Set up the model call, sample from the prior.\nmodel = gdemo(x)\n\n# Evaluate surface at coordinates.\nevaluate(m1, m2) = logjoint(model, (m=m2, s²=invlink.(Ref(InverseGamma(2, 3)), m1)))\n\nfunction plot_sampler(chain; label=\"\")\n    # Extract values from chain.\n    val = get(chain, [:s², :m, :lp])\n    ss = link.(Ref(InverseGamma(2, 3)), val.s²)\n    ms = val.m\n    lps = val.lp\n\n    # How many surface points to sample.\n    granularity = 100\n\n    # Range start/stop points.\n    spread = 0.5\n    σ_start = minimum(ss) - spread * std(ss)\n    σ_stop = maximum(ss) + spread * std(ss)\n    μ_start = minimum(ms) - spread * std(ms)\n    μ_stop = maximum(ms) + spread * std(ms)\n    σ_rng = collect(range(σ_start; stop=σ_stop, length=granularity))\n    μ_rng = collect(range(μ_start; stop=μ_stop, length=granularity))\n\n    # Make surface plot.\n    p = surface(\n        σ_rng,\n        μ_rng,\n        evaluate;\n        camera=(30, 65),\n        #   ticks=nothing,\n        colorbar=false,\n        color=:inferno,\n        title=label,\n    )\n\n    line_range = 1:length(ms)\n\n    scatter3d!(\n        ss[line_range],\n        ms[line_range],\n        lps[line_range];\n        mc=:viridis,\n        marker_z=collect(line_range),\n        msw=0,\n        legend=false,\n        colorbar=false,\n        alpha=0.5,\n        xlabel=\"σ\",\n        ylabel=\"μ\",\n        zlabel=\"Log probability\",\n        title=label,\n    )\n\n    return p\nend;\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| output: false\nsetprogress!(false)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Samplers\n\n### Gibbs\n\nGibbs sampling tends to exhibit a \"jittery\" trajectory. The example below combines HMC and PG sampling to traverse the posterior.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "c = sample(model, Gibbs(:s² => HMC(0.01, 5), :m => PG(20)), 1000)\nplot_sampler(c)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### HMC\n\nHamiltonian Monte Carlo (HMC) sampling is a typical sampler to use, as it tends to be fairly good at converging in an efficient manner. It can often be tricky to set the correct parameters for this sampler however, and the NUTS sampler is often easier to run if you don't want to spend too much time fiddling with step size and the number of steps to take. Note however that HMC does not explore the positive values μ very well, likely due to the leapfrog and step size parameter settings.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "c = sample(model, HMC(0.01, 10), 1000)\nplot_sampler(c)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### HMCDA\n\nThe HMCDA sampler is an implementation of the Hamiltonian Monte Carlo with Dual Averaging algorithm found in the paper \"The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo\" by Hoffman and Gelman (2011). The paper can be found on [arXiv](https://arxiv.org/abs/1111.4246) for the interested reader.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "c = sample(model, HMCDA(200, 0.65, 0.3), 1000)\nplot_sampler(c)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### MH\n\nMetropolis-Hastings (MH) sampling is one of the earliest Markov Chain Monte Carlo methods. MH sampling does not \"move\" a lot, unlike many of the other samplers implemented in Turing. Typically a much longer chain is required to converge to an appropriate parameter estimate.\n\nThe plot below only uses 1,000 iterations of Metropolis-Hastings.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "c = sample(model, MH(), 1000)\nplot_sampler(c)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "As you can see, the MH sampler doesn't move parameter estimates very often.\n\n### NUTS\n\nThe No U-Turn Sampler (NUTS) is an implementation of the algorithm found in the paper \"The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo\" by Hoffman and Gelman (2011). The paper can be found on [arXiv](https://arxiv.org/abs/1111.4246) for the interested reader.\n\nNUTS tends to be very good at traversing complex posteriors quickly.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "c = sample(model, NUTS(0.65), 1000)\nplot_sampler(c)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The only parameter that needs to be set other than the number of iterations to run is the target acceptance rate. In the Hoffman and Gelman paper, they note that a target acceptance rate of 0.65 is typical.\n\nHere is a plot showing a very high acceptance rate. Note that it appears to \"stick\" to a mode and is not particularly good at exploring the posterior as compared to the 0.65 target acceptance ratio case.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "c = sample(model, NUTS(0.95), 1000)\nplot_sampler(c)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "An exceptionally low acceptance rate will show very few moves on the posterior:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "c = sample(model, NUTS(0.2), 1000)\nplot_sampler(c)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### PG\n\nThe Particle Gibbs (PG) sampler is an implementation of an algorithm from the paper \"Particle Markov chain Monte Carlo methods\" by Andrieu, Doucet, and Holenstein (2010). The interested reader can learn more [here](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2009.00736.x).\n\nThe two parameters are the number of particles, and the number of iterations. The plot below shows the use of 20 particles.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "c = sample(model, PG(20), 1000)\nplot_sampler(c)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Next, we plot using 50 particles.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "c = sample(model, PG(50), 1000)\nplot_sampler(c)",
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia",
      "language": "julia"
    }
  },
  "nbformat": 4
}
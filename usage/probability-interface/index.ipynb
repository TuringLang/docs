{
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": "# Install necessary dependencies.\nusing Pkg\nPkg.activate(; temp=true)\nPkg.add([\"Turing\", \"DynamicPPL\", \"Random\"])",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| echo: false\n#| output: false\nusing Pkg;\nPkg.instantiate();\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The easiest way to manipulate and query Turing models is via the DynamicPPL probability interface.\n\nLet's use a simple model of normally-distributed data as an example.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "using Turing\nusing DynamicPPL\nusing Random\n\n@model function gdemo(n)\n    μ ~ Normal(0, 1)\n    x ~ MvNormal(fill(μ, n), I)\nend",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We generate some data using `μ = 0`:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "Random.seed!(1776)\ndataset = randn(100)\ndataset[1:5]",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Conditioning and Deconditioning\n\nBayesian models can be transformed with two main operations, conditioning and deconditioning (also known as marginalisation).\nConditioning takes a variable and fixes its value as known.\nWe do this by passing a model and a collection of conditioned variables to `|`, or its alias, `condition`:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# (equivalently)\n# conditioned_model = condition(gdemo(length(dataset)), (x=dataset, μ=0))\nconditioned_model = gdemo(length(dataset)) | (x=dataset, μ=0)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This operation can be reversed by applying `decondition`:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "original_model = decondition(conditioned_model)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We can also decondition only some of the variables:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "partially_conditioned = decondition(conditioned_model, :μ)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We can see which of the variables in a model have been conditioned with `DynamicPPL.conditioned`:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "DynamicPPL.conditioned(partially_conditioned)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "> Sometimes it is helpful to define convenience functions for conditioning on some variable(s).\n> For instance, in this example we might want to define a version of `gdemo` that conditions on some observations of `x`:\n> \n> ```julia\n> gdemo(x::AbstractVector{<:Real}) = gdemo(length(x)) | (; x)\n> ```\n> \n> For illustrative purposes, however, we do not use this function in the examples below.\n\n## Probabilities and Densities\n\nWe often want to calculate the (unnormalised) probability density for an event.\nThis probability might be a prior, a likelihood, or a posterior (joint) density.\nDynamicPPL provides convenient functions for this.\nTo begin, let's define a model `gdemo`, condition it on a dataset, and draw a sample.\nThe returned sample only contains `μ`, since the value of `x` has already been fixed:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "model = gdemo(length(dataset)) | (x=dataset,)\n\nRandom.seed!(124)\nsample = rand(model)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We can then calculate the joint probability of a set of samples (here drawn from the prior) with `logjoint`.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "logjoint(model, sample)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "For models with many variables `rand(model)` can be prohibitively slow since it returns a `NamedTuple` of samples from the prior distribution of the unconditioned variables.\nWe recommend working with samples of type `DataStructures.OrderedDict` in this case (which Turing re-exports, so can be used directly):",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "Random.seed!(124)\nsample_dict = rand(OrderedDict, model)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "`logjoint` can also be used on this sample:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "logjoint(model, sample_dict)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The prior probability and the likelihood of a set of samples can be calculated with the functions `logprior` and `loglikelihood` respectively.\nThe log joint probability is the sum of these two quantities:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "logjoint(model, sample) ≈ loglikelihood(model, sample) + logprior(model, sample)",
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "logjoint(model, sample_dict) ≈ loglikelihood(model, sample_dict) + logprior(model, sample_dict)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Example: Cross-validation\n\nTo give an example of the probability interface in use, we can use it to estimate the performance of our model using cross-validation.\nIn cross-validation, we split the dataset into several equal parts.\nThen, we choose one of these sets to serve as the validation set.\nHere, we measure fit using the cross entropy (Bayes loss).[^1]\n(For the sake of simplicity, in the following code, we enforce that `nfolds` must divide the number of data points.\nFor a more competent implementation, see [MLUtils.jl](https://juliaml.github.io/MLUtils.jl/dev/api/#MLUtils.kfolds).)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# Calculate the train/validation splits across `nfolds` partitions, assume `length(dataset)` divides `nfolds`\nfunction kfolds(dataset::Array{<:Real}, nfolds::Int)\n    fold_size, remaining = divrem(length(dataset), nfolds)\n    if remaining != 0\n        error(\"The number of folds must divide the number of data points.\")\n    end\n    first_idx = firstindex(dataset)\n    last_idx = lastindex(dataset)\n    splits = map(0:(nfolds - 1)) do i\n        start_idx = first_idx + i * fold_size\n        end_idx = start_idx + fold_size\n        train_set_indices = [first_idx:(start_idx - 1); end_idx:last_idx]\n        return (view(dataset, train_set_indices), view(dataset, start_idx:(end_idx - 1)))\n    end\n    return splits\nend\n\nfunction cross_val(\n    dataset::Vector{<:Real};\n    nfolds::Int=5,\n    nsamples::Int=1_000,\n    rng::Random.AbstractRNG=Random.default_rng(),\n)\n    # Initialize `loss` in a way such that the loop below does not change its type\n    model = gdemo(1) | (x=[first(dataset)],)\n    loss = zero(logjoint(model, rand(rng, model)))\n\n    for (train, validation) in kfolds(dataset, nfolds)\n        # First, we train the model on the training set, i.e., we obtain samples from the posterior.\n        # For normally-distributed data, the posterior can be computed in closed form.\n        # For general models, however, typically samples will be generated using MCMC with Turing.\n        posterior = Normal(mean(train), 1)\n        samples = rand(rng, posterior, nsamples)\n\n        # Evaluation on the validation set.\n        validation_model = gdemo(length(validation)) | (x=validation,)\n        loss += sum(samples) do sample\n            logjoint(validation_model, (μ=sample,))\n        end\n    end\n\n    return loss\nend\n\ncross_val(dataset)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "[^1]: See [ParetoSmooth.jl](https://github.com/TuringLang/ParetoSmooth.jl) for a faster and more accurate implementation of cross-validation than the one provided here.",
      "metadata": {}
    }
  ],
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia",
      "language": "julia"
    }
  },
  "nbformat": 4
}
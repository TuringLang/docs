{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#| echo: false\n#| output: false\nusing Pkg;\nPkg.instantiate();"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In time series analysis we are often interested in understanding how various real-life circumstances impact our quantity of interest.\nThese can be, for instance, season, day of week, or time of day.\nTo analyse this it is useful to decompose time series into simpler components (corresponding to relevant circumstances)\nand infer their relevance.\nIn this tutorial we are going to use Turing for time series analysis and learn about useful ways to decompose time series.\n\n# Modelling time series\n\nBefore we start coding, let us talk about what exactly we mean with time series decomposition.\nIn a nutshell, it is a divide-and-conquer approach where we express a time series as a sum or a product of simpler series.\nFor instance, the time series $f(t)$ can be decomposed into a sum of $n$ components\n\n$$f(t) = \\sum_{i=1}^n f_i(t),$$\n\nor we can decompose $g(t)$ into a product of $m$ components\n\n$$g(t) = \\prod_{i=1}^m g_i(t).$$\n\nWe refer to this as *additive* or *multiplicative* decomposition respectively.\nThis type of decomposition is great since it lets us reason about individual components, which makes encoding prior information and interpreting model predictions very easy.\nTwo common components are *trends*, which represent the overall change of the time series (often assumed to be linear),\nand *cyclic effects* which contribute oscillating effects around the trend.\nLet us simulate some data with an additive linear trend and oscillating effects."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using Turing\nusing FillArrays\nusing StatsPlots\n\nusing LinearAlgebra\nusing Random\nusing Statistics\n\nRandom.seed!(12345)\n\ntrue_sin_freq = 2\ntrue_sin_amp = 5\ntrue_cos_freq = 7\ntrue_cos_amp = 2.5\ntmax = 10\nβ_true = 2\nα_true = -1\ntt = 0:0.05:tmax\nf₁(t) = α_true + β_true * t\nf₂(t) = true_sin_amp * sinpi(2 * t * true_sin_freq / tmax)\nf₃(t) = true_cos_amp * cospi(2 * t * true_cos_freq / tmax)\nf(t) = f₁(t) + f₂(t) + f₃(t)\n\nplot(f, tt; label=\"f(t)\", title=\"Observed time series\", legend=:topleft, linewidth=3)\nplot!(\n    [f₁, f₂, f₃],\n    tt;\n    label=[\"f₁(t)\" \"f₂(t)\" \"f₃(t)\"],\n    style=[:dot :dash :dashdot],\n    linewidth=1,\n)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Even though we use simple components, combining them can give rise to fairly complex time series.\nIn this time series, cyclic effects are just added on top of the trend.\nIf we instead multiply the components the cyclic effects cause the series to oscillate\nbetween larger and larger values, since they get scaled by the trend."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "g(t) = f₁(t) * f₂(t) * f₃(t)\n\nplot(g, tt; label=\"f(t)\", title=\"Observed time series\", legend=:topleft, linewidth=3)\nplot!([f₁, f₂, f₃], tt; label=[\"f₁(t)\" \"f₂(t)\" \"f₃(t)\"], linewidth=1)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Unlike $f$, $g$ oscillates around $0$ since it is being multiplied with sines and cosines.\nTo let a multiplicative decomposition oscillate around the trend we could define it as\n$\\tilde{g}(t) = f₁(t) * (1 + f₂(t)) * (1 + f₃(t)),$\nbut for convenience we will leave it as is.\nThe inference machinery is the same for both cases.\n\n# Model fitting\n\nHaving discussed time series decomposition, let us fit a model to the time series above and recover the true parameters.\nBefore building our model, we standardise the time axis to $[0, 1]$ and subtract the max of the time series.\nThis helps convergence while maintaining interpretability and the correct scales for the cyclic components."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "σ_true = 0.35\nt = collect(tt[begin:3:end])\nt_min, t_max = extrema(t)\nx = (t .- t_min) ./ (t_max - t_min)\nyf = f.(t) .+ σ_true .* randn(size(t))\nyf_max = maximum(yf)\nyf = yf .- yf_max\n\nscatter(x, yf; title=\"Standardised data\", legend=false)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Let us now build our model.\nWe want to assume a linear trend, and cyclic effects.\nEncoding a linear trend is easy enough, but what about cyclical effects?\nWe will take a scattergun approach, and create multiple cyclical features using both sine and cosine functions and let our inference machinery figure out which to keep.\nTo do this, we define how long a one period should be, and create features in reference to said period.\nHow long a period should be is problem dependent, but as an example let us say it is $1$ year.\nIf we then find evidence for a cyclic effect with a frequency of 2, that would mean a biannual effect. A frequency of 4 would mean quarterly etc.\nSince we are using synthetic data, we are simply going to let the period be 1, which is the entire length of the time series."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "freqs = 1:10\nnum_freqs = length(freqs)\nperiod = 1\ncyclic_features = [sinpi.(2 .* freqs' .* x ./ period) cospi.(2 .* freqs' .* x ./ period)]\n\nplot_freqs = [1, 3, 5]\nfreq_ptl = plot(\n    cyclic_features[:, plot_freqs];\n    label=permutedims([\"sin(2π$(f)x)\" for f in plot_freqs]),\n    title=\"Cyclical features subset\",\n)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Having constructed the cyclical features, we can finally build our model. The model we will implement looks like this\n\n$$\nf(t) = \\alpha + \\beta_t t + \\sum_{i=1}^F \\beta_{\\sin{},i} \\sin{}(2\\pi f_i t) + \\sum_{i=1}^F \\beta_{\\cos{},i} \\cos{}(2\\pi f_i t),\n$$\n\nwith a Gaussian likelihood $y \\sim \\mathcal{N}(f(t), \\sigma^2)$.\nFor convenience we are treating the cyclical feature weights $\\beta_{\\sin{},i}$ and $\\beta_{\\cos{},i}$ the same in code and weight them with $\\beta_c$.\nAnd just because it is so easy, we parameterise our model with the operation with which to apply the cyclic effects.\nThis lets us use the exact same code for both additive and multiplicative models.\nFinally, we plot prior predictive samples to make sure our priors make sense."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function decomp_model(t, c, op)\n    α ~ Normal(0, 10)\n    βt ~ Normal(0, 2)\n    βc ~ MvNormal(Zeros(size(c, 2)), I)\n    σ ~ truncated(Normal(0, 0.1); lower=0)\n\n    cyclic = c * βc\n    trend = α .+ βt .* t\n    μ = op(trend, cyclic)\n    y ~ MvNormal(μ, σ^2 * I)\n    return (; trend, cyclic)\nend\n\ny_prior_samples = mapreduce(hcat, 1:100) do _\n    rand(decomp_model(t, cyclic_features, +)).y\nend\nplot(t, y_prior_samples; linewidth=1, alpha=0.5, color=1, label=\"\", title=\"Prior samples\")\nscatter!(t, yf; color=2, label=\"Data\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "With the model specified and with a reasonable prior we can now let Turing decompose the time series for us!"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using MCMCChains: get_sections\n\nfunction mean_ribbon(samples)\n    qs = quantile(samples)\n    low = qs[:, Symbol(\"2.5%\")]\n    up = qs[:, Symbol(\"97.5%\")]\n    m = mean(samples)[:, :mean]\n    return m, (m - low, up - m)\nend\n\nfunction get_decomposition(model, x, cyclic_features, chain, op)\n    chain_params = get_sections(chain, :parameters)\n    return returned(model(x, cyclic_features, op), chain_params)\nend\n\nfunction plot_fit(x, y, decomp, ymax)\n    trend = mapreduce(x -> x.trend, hcat, decomp)\n    cyclic = mapreduce(x -> x.cyclic, hcat, decomp)\n\n    trend_plt = plot(\n        x,\n        trend .+ ymax;\n        color=1,\n        label=nothing,\n        alpha=0.2,\n        title=\"Trend\",\n        xlabel=\"Time\",\n        ylabel=\"f₁(t)\",\n    )\n    ls = [ones(length(t)) t] \\ y\n    α̂, β̂ = ls[1], ls[2:end]\n    plot!(\n        trend_plt,\n        t,\n        α̂ .+ t .* β̂ .+ ymax;\n        label=\"Least squares trend\",\n        color=5,\n        linewidth=4,\n    )\n\n    scatter!(trend_plt, x, y .+ ymax; label=nothing, color=2, legend=:topleft)\n    cyclic_plt = plot(\n        x,\n        cyclic;\n        color=1,\n        label=nothing,\n        alpha=0.2,\n        title=\"Cyclic effect\",\n        xlabel=\"Time\",\n        ylabel=\"f₂(t)\",\n    )\n    return trend_plt, cyclic_plt\nend\n\nchain = sample(decomp_model(x, cyclic_features, +) | (; y=yf), NUTS(), 2000, progress=false)\nyf_samples = predict(decomp_model(x, cyclic_features, +), chain)\nm, conf = mean_ribbon(yf_samples)\npredictive_plt = plot(\n    t,\n    m .+ yf_max;\n    ribbon=conf,\n    label=\"Posterior density\",\n    title=\"Posterior decomposition\",\n    xlabel=\"Time\",\n    ylabel=\"f(t)\",\n)\nscatter!(predictive_plt, t, yf .+ yf_max; color=2, label=\"Data\", legend=:topleft)\n\ndecomp = get_decomposition(decomp_model, x, cyclic_features, chain, +)\ndecomposed_plt = plot_fit(t, yf, decomp, yf_max)\nplot(predictive_plt, decomposed_plt...; layout=(3, 1), size=(700, 1000))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#| echo: false\nlet\n    @assert mean(ess(chain)[:, :ess]) > 500 \"Mean ESS: $(mean(ess(chain)[:, :ess])) - not > 500\"\n    lower_quantile = m .- conf[1] # 2.5% quantile\n    upper_quantile = m .+ conf[2] # 97.5% quantile\n    @assert mean(lower_quantile .≤ yf .≤ upper_quantile) ≥ 0.9 \"Surprisingly few observations in predicted 95% interval: $(mean(lower_quantile .≤ yf .≤ upper_quantile))\"\nend"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Inference is successful and the posterior beautifully captures the data.\nWe see that the least squares linear fit deviates somewhat from the posterior trend.\nSince our model takes cyclic effects into account separately,\nwe get a better estimate of the true overall trend than if we would have just fitted a line.\nBut what frequency content did the model identify?"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "function plot_cyclic_features(βsin, βcos)\n    labels = reshape([\"freq = $i\" for i in freqs], 1, :)\n    colors = collect(freqs)'\n    style = reshape([i <= 10 ? :solid : :dash for i in 1:length(labels)], 1, :)\n    sin_features_plt = density(\n        βsin[:, :, 1];\n        title=\"Sine features posterior\",\n        label=labels,\n        ylabel=\"Density\",\n        xlabel=\"Weight\",\n        color=colors,\n        linestyle=style,\n        legend=nothing,\n    )\n    cos_features_plt = density(\n        βcos[:, :, 1];\n        title=\"Cosine features posterior\",\n        ylabel=\"Density\",\n        xlabel=\"Weight\",\n        label=nothing,\n        color=colors,\n        linestyle=style,\n    )\n\n    return seasonal_features_plt = plot(\n        sin_features_plt,\n        cos_features_plt;\n        layout=(2, 1),\n        size=(800, 600),\n        legend=:outerright,\n    )\nend\n\nβc = Array(group(chain, :βc))\nplot_cyclic_features(βc[:, begin:num_freqs, :], βc[:, (num_freqs + 1):end, :])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Plotting the posterior over the cyclic features reveals that the model managed to extract the true frequency content.\n\nSince we wrote our model to accept a combining operator, we can easily run the same analysis for a multiplicative model."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "yg = g.(t) .+ σ_true .* randn(size(t))\n\ny_prior_samples = mapreduce(hcat, 1:100) do _\n    rand(decomp_model(t, cyclic_features, .*)).y\nend\nplot(t, y_prior_samples; linewidth=1, alpha=0.5, color=1, label=\"\", title=\"Prior samples\")\nscatter!(t, yf; color=2, label=\"Data\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "chain = sample(decomp_model(x, cyclic_features, .*) | (; y=yg), NUTS(), 2000, progress=false)\nyg_samples = predict(decomp_model(x, cyclic_features, .*), chain)\nm, conf = mean_ribbon(yg_samples)\npredictive_plt = plot(\n    t,\n    m;\n    ribbon=conf,\n    label=\"Posterior density\",\n    title=\"Posterior decomposition\",\n    xlabel=\"Time\",\n    ylabel=\"g(t)\",\n)\nscatter!(predictive_plt, t, yg; color=2, label=\"Data\", legend=:topleft)\n\ndecomp = get_decomposition(decomp_model, x, cyclic_features, chain, .*)\ndecomposed_plt = plot_fit(t, yg, decomp, 0)\nplot(predictive_plt, decomposed_plt...; layout=(3, 1), size=(700, 1000))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#| echo: false\nlet\n    @assert mean(ess(chain)[:, :ess]) > 500 \"Mean ESS: $(mean(ess(chain)[:, :ess])) - not > 500\"\n    lower_quantile = m .- conf[1] # 2.5% quantile\n    upper_quantile = m .+ conf[2] # 97.5% quantile\n    @assert mean(lower_quantile .≤ yg .≤ upper_quantile) ≥ 0.9 \"Surprisingly few observations in predicted 95% interval: $(mean(lower_quantile .≤ yg .≤ upper_quantile))\"\nend"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The model fits! What about the infered cyclic components?"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "βc = Array(group(chain, :βc))\nplot_cyclic_features(βc[:, begin:num_freqs, :], βc[:, (num_freqs + 1):end, :])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "While multiplicative model fits to the data, it does not recover the true parameters for this dataset.\n\n# Wrapping up\n\nIn this tutorial we have seen how to implement and fit time series models using additive and multiplicative decomposition.\nWe also saw how to visualise the model fit, and how to interpret learned cyclical components."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia 1.11",
      "language": "julia",
      "name": "julia-1.11"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
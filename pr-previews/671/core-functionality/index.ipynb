{
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": "# Install necessary dependencies.\nusing Pkg\nPkg.activate(; temp=true)\nPkg.add([])",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| echo: false\n#| output: false\nusing Pkg;\nPkg.instantiate();\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This article provides an overview of the core functionality in Turing.jl, which are likely to be used across a wide range of models.\n\n## Basics\n\n### Introduction\n\nA probabilistic program is a Julia function wrapped in a `@model` macro.\nIn this function, arbitrary Julia code can be used, but to ensure correctness of inference it should not have external effects or modify global state.\n\nTo specify distributions of random variables, Turing models use `~` notation: `x ~ distr` where `x` is an identifier.\nThis resembles the notation used in statistical models.\nFor example, the model:\n\n$$\\begin{align}\na &\\sim \\text{Normal}(0, 1) \\\\\nx &\\sim \\text{Normal}(a, 1)\n\\end{align}$$\n\nis written in Turing as:\n\n```julia\nusing Turing\n\n@model function mymodel()\n    a ~ Normal(0, 1)\n    x ~ Normal(a, 1)\nend\n```\n\n### Tilde-statements\n\nIndexing and field access is supported, so that `x[i] ~ distr` and `x.field ~ distr` are valid statements.\nHowever, in these cases, `x` must be defined in the scope of the model function.\n`distr` is typically either a distribution from Distributions.jl (see [this page]({{< meta usage-custom-distribution >}}) for implementing custom distributions), or another Turing model wrapped in `to_submodel()` (see [this page]({{< meta usage-submodels >}}) for submodels).\n\nThere are two classes of tilde-statements: _observe_ statements, where the left-hand side contains an observed value, and _assume_ statements, where the left-hand side is not observed.\nThese respectively correspond to likelihood and prior terms.\n\nIt is easier to start by explaining when a variable is treated as an observed value.\nThis can happen in one of two ways:\n\n- The variable is passed as one of the arguments to the model function; or\n- The value of the variable in the model is explicitly conditioned or fixed.\n\n> Note that it is not enough for the variable to be defined in the current scope. For example, in\n> \n> ```julia\n> @model function mymodel(x)\n>     y = x + 1\n>     y ~ Normal(0, 1)\n> end\n> ```\n> \n> `y` is not treated as an observed value.\n\nIn such a case, `x` is considered to be an observed value, assumed to have been drawn from the distribution `distr`.\nThe likelihood (if needed) is computed using `loglikelihood(distr, x)`.\n\nOn the other hand, if neither of the above are true, then this is treated as an assume-statement: inside the probabilistic program, this samples a new variable called `x`, distributed according to `distr`, and places it in the current scope.\n\n### Simple Gaussian Demo\n\nBelow is a simple Gaussian demo illustrating the basic usage of Turing.jl.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# Import packages.\nusing Turing\nusing StatsPlots\n\n# Define a simple Normal model with unknown mean and variance.\n@model function gdemo(x, y)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    x ~ Normal(m, sqrt(s²))\n    return y ~ Normal(m, sqrt(s²))\nend\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In Turing.jl, MCMC sampling is performed using the `sample()` function, which (at its most basic) takes a model, a sampler, and the number of samples to draw.\n\nFor this model, the prior expectation of `s²` is `mean(InverseGamma(2, 3)) = 3/(2 - 1) = 3`, and the prior expectation of `m` is 0.\nWe can check this using the `Prior` sampler:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| output: false\nsetprogress!(false)\n```",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "p1 = sample(gdemo(missing, missing), Prior(), 100000)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "To perform inference, we simply need to specify the sampling algorithm we want to use.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#  Run sampler, collect results.\nc1 = sample(gdemo(1.5, 2), SMC(), 1000)\nc2 = sample(gdemo(1.5, 2), PG(10), 1000)\nc3 = sample(gdemo(1.5, 2), HMC(0.1, 5), 1000)\nc4 = sample(gdemo(1.5, 2), Gibbs(:m => PG(10), :s² => HMC(0.1, 5)), 1000)\nc5 = sample(gdemo(1.5, 2), HMCDA(0.15, 0.65), 1000)\nc6 = sample(gdemo(1.5, 2), NUTS(0.65), 1000)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The arguments for each sampler are:\n\n  - SMC: number of particles.\n  - PG: number of particles, number of iterations.\n  - HMC: leapfrog step size, leapfrog step numbers.\n  - Gibbs: component sampler 1, component sampler 2, ...\n  - HMCDA: total leapfrog length, target accept ratio.\n  - NUTS: number of adaptation steps (optional), target accept ratio.\n\nMore information about each sampler can be found in [Turing.jl's API docs](https://turinglang.org/Turing.jl).\n\nThe `MCMCChains` module (which is re-exported by Turing) provides plotting tools for the `Chain` objects returned by a `sample` function.\nSee the [MCMCChains](https://github.com/TuringLang/MCMCChains.jl) repository for more information on the suite of tools available for diagnosing MCMC chains.\n\n```julia\n# Summarise results\ndescribe(c3)\n\n# Plot results\nplot(c3)\nsavefig(\"gdemo-plot.png\")\n```\n\n### Conditioning on data\n\nUsing this syntax, a probabilistic model is defined in Turing.\nThe model function generated by Turing can then be used to condition the model on data.\nSubsequently, the `sample` function can be used to generate samples from the posterior distribution.\n\nIn the following example, the defined model is conditioned to the data (`arg_1 = 1`, `arg_2 = 2`) by passing the arguments `1` and `2` to the model function.\n\n```julia\n@model function model_name(arg_1, arg_2)\n    arg_1 ~ ...\n    arg_2 ~ ...\nend\n```\n\nThe conditioned model can then be passed onto the sample function to run posterior inference.\n\n```julia\nmodel = model_name(1, 2)\nchn = sample(model, HMC(0.5, 20), 1000) # Sample with HMC.\n```\n\nAlternatively, one can also use the conditioning operator `|` to condition the model on data.\nIn this case, the model does not need to be defined with `arg_1` and `arg_2` as parameters.\n\n```julia\n@model function model_name()\n    arg_1 ~ ...\n    arg_2 ~ ...\nend\n\n# Condition the model on data.\nmodel = model_name() | (arg_1 = 1, arg_2 = 2) \n```\n\n### Analysing MCMC chains\n\nThe returned chain contains samples of the variables in the model.\n\n```julia\nvar_1 = mean(chn[:var_1]) # Taking the mean of a variable named var_1.\n```\n\nThe key (`:var_1`) can either be a `Symbol` or a `String`.\nFor example, to fetch `x[1]`, one can use `chn[Symbol(\"x[1]\")]` or `chn[\"x[1]\"]`.\nIf you want to retrieve all parameters associated with a specific symbol, you can use `group`.\nAs an example, if you have the parameters `\"x[1]\"`, `\"x[2]\"`, and `\"x[3]\"`, calling `group(chn, :x)` or `group(chn, \"x\")` will return a new chain with only `\"x[1]\"`, `\"x[2]\"`, and `\"x[3]\"`.\n\n### Tilde-statement ordering\n\nTuring does not have a declarative form.\nThus, the ordering of tilde-statements in a Turing model is important: random variables cannot be used until they have been first declared in a tilde-statement.\nFor example, the following example works:\n\n```julia\n# Define a simple Normal model with unknown mean and variance.\n@model function model_function(y)\n    s ~ Poisson(1)\n    y ~ Normal(s, 1)\n    return y\nend\n\nsample(model_function(10), SMC(), 100)\n```\n\nBut if we switch the `s ~ Poisson(1)` and `y ~ Normal(s, 1)` lines, the model will no longer sample correctly:\n\n```julia\n# Define a simple Normal model with unknown mean and variance.\n@model function model_function(y)\n    y ~ Normal(s, 1)\n    s ~ Poisson(1)\n    return y\nend\n\nsample(model_function(10), SMC(), 100)\n```\n\n### Sampling Multiple Chains\n\nTuring supports distributed and threaded parallel sampling.\nTo do so, call `sample(model, sampler, parallel_type, n, n_chains)`, where `parallel_type` can be either `MCMCThreads()` or `MCMCDistributed()` for thread and parallel sampling, respectively.\n\nHaving multiple chains in the same object is valuable for evaluating convergence.\nSome diagnostic functions like `gelmandiag` require multiple chains.\n\nIf you want to sample multiple chains without using parallelism, you can use `MCMCSerial()`:\n\n```julia\n# Sample 3 chains in a serial fashion.\nchains = sample(model, sampler, MCMCSerial(), 1000, 3)\n```\n\nThe `chains` variable now contains a `Chains` object which can be indexed by chain. To pull out the first chain from the `chains` object, use `chains[:,:,1]`. The method is the same if you use either of the below parallel sampling methods.\n\n#### Multithreaded sampling\n\nIf you wish to perform multithreaded sampling, you can call `sample` with the following signature:\n\n```julia\n# Sample four chains using multiple threads, each with 1000 samples.\nsample(model, sampler, MCMCThreads(), 1000, 4)\n```\n\nBe aware that Turing cannot add threads for you -- you must have started your Julia instance with multiple threads to experience any kind of parallelism.\nSee the [Julia documentation](https://docs.julialang.org/en/v1/manual/parallel-computing/#man-multithreading-1) for details on how to achieve this.\n\n#### Distributed sampling\n\nTo perform distributed sampling (using multiple processes), you must first import `Distributed`.\n\nProcess parallel sampling can be done like so:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| eval: false\n# Load Distributed to add processes and the @everywhere macro.\nusing Distributed\n\n# Load Turing.\nusing Turing\n\n# Add four processes to use for sampling.\naddprocs(4; exeflags=\"--project=$(Base.active_project())\")\n\n# Initialise everything on all the processes.\n# Note: Make sure to do this after you've already loaded Turing,\n#       so each process does not have to precompile.\n#       Parallel sampling may fail silently if you do not do this.\n@everywhere using Turing\n\n# Define a model on all processes.\n@everywhere @model function gdemo(x)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n\n    for i in eachindex(x)\n        x[i] ~ Normal(m, sqrt(s²))\n    end\nend\n\n# Declare the model instance everywhere.\n@everywhere model = gdemo([1.5, 2.0])\n\n# Sample four chains using multiple processes, each with 1000 samples.\nsample(model, NUTS(), MCMCDistributed(), 1000, 4)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Sampling from an Unconditional Distribution (The Prior)\n\nTuring allows you to sample from a declared model's prior. If you wish to draw a chain from the prior to inspect your prior distributions, you can run",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| eval: false\nchain = sample(model, Prior(), n_samples)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "You can also run your model (as if it were a function) from the prior distribution, by calling the model without specifying inputs or a sampler.\nIn the below example, we specify a `gdemo` model which returns two variables, `x` and `y`.\nHere, including the `return` statement is necessary to retrieve the sampled `x` and `y` values.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "@model function gdemo(x, y)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    x ~ Normal(m, sqrt(s²))\n    y ~ Normal(m, sqrt(s²))\n    return x, y\nend",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "To produce a sample from the prior distribution, we instantiate the model with `missing` inputs:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# Samples from p(x,y)\ng_prior_sample = gdemo(missing, missing)\ng_prior_sample()\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Sampling from a Conditional Distribution (The Posterior)\n\n#### Treating observations as random variables\n\nInputs to the model that have a value `missing` are treated as parameters, aka random variables, to be estimated/sampled. This can be useful if you want to simulate draws for that parameter, or if you are sampling from a conditional distribution. Turing supports the following syntax:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n@model function gdemo(x, ::Type{T}=Float64) where {T}\n    if x === missing\n        # Initialise `x` if missing\n        x = Vector{T}(undef, 2)\n    end\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    for i in eachindex(x)\n        x[i] ~ Normal(m, sqrt(s²))\n    end\nend\n\n# Construct a model with x = missing\nmodel = gdemo(missing)\nc = sample(model, HMC(0.05, 20), 500)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Note the need to initialise `x` when missing since we are iterating over its elements later in the model.\nThe generated values for `x` can be extracted from the `Chains` object using `c[:x]`.\n\nTuring also supports mixed `missing` and non-`missing` values in `x`, where the missing ones will be treated as random variables to be sampled while the others get treated as observations.\nFor example:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n@model function gdemo(x)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    for i in eachindex(x)\n        x[i] ~ Normal(m, sqrt(s²))\n    end\nend\n\n# x[1] is a parameter, but x[2] is an observation\nmodel = gdemo([missing, 2.4])\nc = sample(model, HMC(0.01, 5), 500)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### Default Values\n\nArguments to Turing models can have default values much like how default values work in normal Julia functions.\nFor instance, the following will assign `missing` to `x` and treat it as a random variable.\nIf the default value is not `missing`, `x` will be assigned that value and will be treated as an observation instead.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\nusing Turing\n\n@model function generative(x=missing, ::Type{T}=Float64) where {T<:Real}\n    if x === missing\n        # Initialise x when missing\n        x = Vector{T}(undef, 10)\n    end\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    for i in 1:length(x)\n        x[i] ~ Normal(m, sqrt(s²))\n    end\n    return s², m\nend\n\nm = generative()\nchain = sample(m, HMC(0.01, 5), 1000)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### Access Values inside Chain\n\nYou can access the values inside a chain in several ways:\n\n 1. Turn them into a `DataFrame` object\n 2. Use their raw `AxisArray` form\n 3. Create a three-dimensional `Array` object\n\nFor example, let `c` be a `Chain`:\n\n 1. `DataFrame(c)` converts `c` to a `DataFrame`,\n 2. `c.value` retrieves the values inside `c` as an `AxisArray`, and\n 3. `c.value.data` retrieves the values inside `c` as a 3D `Array`.\n\n#### Variable Types and Type Parameters\n\nThe element type of a vector (or matrix) of random variables should match the `eltype` of its prior distribution, i.e., `<: Integer` for discrete distributions and `<: AbstractFloat` for continuous distributions.\n\nSome automatic differentiation backends (used in conjunction with Hamiltonian samplers such as `HMC` or `NUTS`) further require that the vector's element type needs to either be:\n\n1. `Real` to enable auto-differentiation through the model which uses special number types that are sub-types of `Real`, or\n\n2. Some type parameter `T` defined in the model header using the type parameter syntax, e.g. `function gdemo(x, ::Type{T} = Float64) where {T}`.\n\nSimilarly, when using a particle sampler, the Julia variable used should either be:\n\n1. An `Array`, or\n\n2. An instance of some type parameter `T` defined in the model header using the type parameter syntax, e.g. `function gdemo(x, ::Type{T} = Vector{Float64}) where {T}`.\n\n### Querying Probabilities from Model or Chain\n\nTuring offers three functions: [`loglikelihood`](https://turinglang.org/DynamicPPL.jl/dev/api/#StatsAPI.loglikelihood), [`logprior`](https://turinglang.org/DynamicPPL.jl/dev/api/#DynamicPPL.logprior), and [`logjoint`](https://turinglang.org/DynamicPPL.jl/dev/api/#DynamicPPL.logjoint) to query the log-likelihood, log-prior, and log-joint probabilities of a model, respectively.\n\nLet's look at a simple model called `gdemo`:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "@model function gdemo0()\n    s ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s))\n    return x ~ Normal(m, sqrt(s))\nend",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "If we observe x to be 1.0, we can condition the model on this datum using the [`condition`](https://turinglang.org/DynamicPPL.jl/dev/api/#AbstractPPL.condition) syntax:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "model = gdemo0() | (x=1.0,)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Now, let's compute the log-likelihood of the observation given specific values of the model parameters, `s` and `m`:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "loglikelihood(model, (s=1.0, m=1.0))",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We can easily verify that value in this case:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "logpdf(Normal(1.0, 1.0), 1.0)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We can also compute the log-prior probability of the model for the same values of s and m:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "logprior(model, (s=1.0, m=1.0))",
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "logpdf(InverseGamma(2, 3), 1.0) + logpdf(Normal(0, sqrt(1.0)), 1.0)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Finally, we can compute the log-joint probability of the model parameters and data:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "logjoint(model, (s=1.0, m=1.0))",
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "logpdf(Normal(1.0, 1.0), 1.0) +\nlogpdf(InverseGamma(2, 3), 1.0) +\nlogpdf(Normal(0, sqrt(1.0)), 1.0)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Querying with `Chains` object is easy as well:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "chn = sample(model, Prior(), 10)",
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "loglikelihood(model, chn)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Maximum likelihood and maximum a posteriori estimates\n\nTuring also has functions for estimating the maximum a posteriori and maximum likelihood parameters of a model. This can be done with",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "mle_estimate = maximum_likelihood(model)\nmap_estimate = maximum_a_posteriori(model)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "For more details see the [mode estimation page]({{<meta usage-mode-estimation>}}).\n\n## Beyond the Basics\n\n### Compositional Sampling Using Gibbs\n\nTuring.jl provides a Gibbs interface to combine different samplers. For example, one can combine an `HMC` sampler with a `PG` sampler to run inference for different parameters in a single model as below.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "@model function simple_choice(xs)\n    p ~ Beta(2, 2)\n    z ~ Bernoulli(p)\n    for i in 1:length(xs)\n        if z == 1\n            xs[i] ~ Normal(0, 1)\n        else\n            xs[i] ~ Normal(2, 1)\n        end\n    end\nend\n\nsimple_choice_f = simple_choice([1.5, 2.0, 0.3])\n\nchn = sample(simple_choice_f, Gibbs(:p => HMC(0.2, 3), :z => PG(20)), 1000)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The `Gibbs` sampler can be used to specify unique automatic differentiation backends for different variable spaces. Please see the [Automatic Differentiation]({{<meta usage-automatic-differentiation>}}) article for more.\n\nFor more details of compositional sampling in Turing.jl, please check the corresponding [paper](https://proceedings.mlr.press/v84/ge18b.html).\n\n### Working with `filldist` and `arraydist`\n\nTuring provides `filldist(dist::Distribution, n::Int)` and `arraydist(dists::AbstractVector{<:Distribution})` as a simplified interface to construct product distributions, e.g., to model a set of variables that share the same structure but vary by group.\n\n#### Constructing product distributions with filldist\n\nThe function `filldist` provides a general interface to construct product distributions over distributions of the same type and parameterisation.\nNote that, in contrast to the product distribution interface provided by Distributions.jl (`Product`), `filldist` supports product distributions over univariate or multivariate distributions.\n\nExample usage:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n@model function demo(x, g)\n    k = length(unique(g))\n    a ~ filldist(Exponential(), k) # = Product(fill(Exponential(), k))\n    mu = a[g]\n    for i in eachindex(x)\n        x[i] ~ Normal(mu[i])\n    end\n    return mu\nend\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### Constructing product distributions with `arraydist`\n\nThe function `arraydist` provides a general interface to construct product distributions over distributions of varying type and parameterisation.\nNote that in contrast to the product distribution interface provided by Distributions.jl (`Product`), `arraydist` supports product distributions over univariate or multivariate distributions.\n\nExample usage:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "@model function demo(x, g)\n    k = length(unique(g))\n    a ~ arraydist([Exponential(i) for i in 1:k])\n    mu = a[g]\n    for i in eachindex(x)\n        x[i] ~ Normal(mu[i])\n    end\n    return mu\nend",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Working with MCMCChains.jl\n\nTuring.jl wraps its samples using `MCMCChains.Chain` so that all the functions working for `MCMCChains.Chain` can be re-used in Turing.jl. Two typical functions are `MCMCChains.describe` and `MCMCChains.plot`, which can be used as follows for an obtained chain `chn`. For more information on `MCMCChains`, please see the [GitHub repository](https://github.com/TuringLang/MCMCChains.jl).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\ndescribe(chn) # Lists statistics of the samples.\nplot(chn) # Plots statistics of the samples.\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "There are numerous functions in addition to `describe` and `plot` in the `MCMCChains` package, such as those used in convergence diagnostics. For more information on the package, please see the [GitHub repository](https://github.com/TuringLang/MCMCChains.jl).\n\n### Changing Default Settings\n\nSome of Turing.jl's default settings can be changed for better usage.\n\n#### AD Backend\n\nTuring is thoroughly tested with three automatic differentiation (AD) backend packages.\nThe default AD backend is [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl), which uses forward-mode AD.\nTwo reverse-mode AD backends are also supported, namely [Mooncake](https://github.com/compintell/Mooncake.jl) and [ReverseDiff](https://github.com/JuliaDiff/ReverseDiff.jl).\n`Mooncake` and `ReverseDiff` also require the user to explicitly load them using `import Mooncake` or `import ReverseDiff` next to `using Turing`.\n\nFor more information on Turing's automatic differentiation backend, please see the [Automatic Differentiation]({{<meta usage-automatic-differentiation>}}) article as well as the [ADTests website](https://turinglang.org/ADTests/), where a number of AD backends (not just those above) are tested against Turing.jl.\n\n#### Progress Logging\n\n`Turing.jl` uses ProgressLogging.jl to log the sampling progress. Progress\nlogging is enabled as default but might slow down inference. It can be turned on\nor off by setting the keyword argument `progress` of `sample` to `true` or `false`.\nMoreover, you can enable or disable progress logging globally by calling `setprogress!(true)` or `setprogress!(false)`, respectively.\n\nTuring uses heuristics to select an appropriate visualisation backend. If you\nuse Jupyter notebooks, the default backend is\n[ConsoleProgressMonitor.jl](https://github.com/tkf/ConsoleProgressMonitor.jl).\nIn all other cases, progress logs are displayed with\n[TerminalLoggers.jl](https://github.com/c42f/TerminalLoggers.jl). Alternatively,\nif you provide a custom visualisation backend, Turing uses it instead of the\ndefault backend.",
      "metadata": {}
    }
  ],
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia",
      "language": "julia"
    }
  },
  "nbformat": 4
}
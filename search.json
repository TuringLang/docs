[
  {
    "objectID": "changelog.html",
    "href": "changelog.html",
    "title": "Changelog",
    "section": "",
    "text": "Turing.jl v0.42 brings with it all the underlying changes in DynamicPPL 0.39. Please see the DynamicPPL changelog for full details; in here we summarise only the changes that are most pertinent to end-users of Turing.jl.\n\n\nTuring.jl has supported threaded tilde-statements for a while now, as long as said tilde-statements are observations (i.e., likelihood terms). For example:\n@model function f(y)\n    x ~ Normal()\n    Threads.@threads for i in eachindex(y)\n        y[i] ~ Normal(x)\n    end\nend\nModels where tilde-statements or @addlogprob! are used in parallel require what we call ‘threadsafe evaluation’. In previous releases of Turing.jl, threadsafe evaluation was enabled whenever Julia was launched with more than one thread. However, this is an imprecise way of determining whether threadsafe evaluation is really needed. It causes performance degradation for models that do not actually need threadsafe evaluation, and generally led to ill-defined behaviour in various parts of the Turing codebase.\nIn Turing.jl v0.42, threadsafe evaluation is now opt-in. To enable threadsafe evaluation, after defining a model, you now need to call setthreadsafe(model, true) (note that this is not a mutating function, it returns a new model):\ny = randn(100)\nmodel = f(y)\nmodel = setthreadsafe(model, true)\nYou only need to do this if your model uses tilde-statements or @addlogprob! in parallel. You do not need to do this if:\n\nyour model has other kinds of parallelism but does not include tilde-statements inside;\nor you are using MCMCThreads() or MCMCDistributed() to sample multiple chains in parallel, but your model itself does not use parallelism.\n\nIf your model does include parallelised tilde-statements or @addlogprob! calls, and you evaluate it/sample from it without setting setthreadsafe(model, true), then you may get statistically incorrect results without any warnings or errors.\n\n\n\nMany operations in DynamicPPL have been substantially sped up. You should find that anything that uses LogDensityFunction (i.e., HMC/NUTS samplers, optimisation) is faster in this release. Prior sampling should also be much faster than before.\n\n\n\nIf you have a model that requires threadsafe evaluation (i.e., parallel observations), you can now use this with predict. Carrying on from the previous example, you can do:\nmodel = setthreadsafe(f(y), true)\nchain = sample(model, NUTS(), 1000)\n\npdn_model = f(fill(missing, length(y)))\npdn_model = setthreadsafe(pdn_model, true)  # set threadsafe\npredictions = predict(pdn_model, chain) # generate new predictions in parallel\n\n\n\nWhen sampling from a Turing model, the resulting MCMCChains.Chains object now contains the log-joint, log-prior, and log-likelihood under the names :logjoint, :logprior, and :loglikelihood respectively. Previously, :logjoint would be stored under the name :lp.\n\n\n\nWhen sampling using MCMCChains, the chain object will no longer have its chain.logevidence field set. Instead, you can calculate this yourself from the log-likelihoods stored in the chain. For SMC samplers, the log-evidence of the entire trajectory is stored in chain[:logevidence] (which is the same for every particle in the ‘chain’).\n\n\n\nTuring.Inference.Transition(model, vi[, stats]) has been removed; you can directly replace this with DynamicPPL.ParamsWithStats(vi, model[, stats]).\n\n\n\n\nTuring.jl v0.42 updates AdvancedVI.jl compatibility to 0.6 (we skipped the breaking 0.5 update as it does not introduce new features). AdvancedVI.jl@0.6 introduces major structural changes including breaking changes to the interface and multiple new features. The summary of the changes below are the things that affect the end-users of Turing. For a more comprehensive list of changes, please refer to the changelogs in AdvancedVI.\n\n\nA new level of interface for defining different variational algorithms has been introduced in AdvancedVI v0.5. As a result, the function Turing.vi now receives a keyword argument algorithm. The object algorithm &lt;: AdvancedVI.AbstractVariationalAlgorithm should now contain all the algorithm-specific configurations. Therefore, keyword arguments of vi that were algorithm-specific such as objective, operator, averager and so on, have been moved as fields of the relevant &lt;: AdvancedVI.AbstractVariationalAlgorithm structs.\nIn addition, the outputs also changed. Previously, vi returned both the last-iterate of the algorithm q and the iterate average q_avg. Now, for the algorithms running parameter averaging, only q_avg is returned. As a result, the number of returned values reduced from 4 to 3.\nFor example,\nq, q_avg, info, state = vi(\n    model, q, n_iters; objective=RepGradELBO(10), operator=AdvancedVI.ClipScale()\n)\nis now\nq_avg, info, state = vi(\n    model,\n    q,\n    n_iters;\n    algorithm=KLMinRepGradDescent(adtype; n_samples=10, operator=AdvancedVI.ClipScale()),\n)\nSimilarly,\nvi(\n    model,\n    q,\n    n_iters;\n    objective=RepGradELBO(10; entropy=AdvancedVI.ClosedFormEntropyZeroGradient()),\n    operator=AdvancedVI.ProximalLocationScaleEntropy(),\n)\nis now\nvi(model, q, n_iters; algorithm=KLMinRepGradProxDescent(adtype; n_samples=10))\nLastly, to obtain the last-iterate q of KLMinRepGradDescent, which is not returned in the new interface, simply select the averaging strategy to be AdvancedVI.NoAveraging(). That is,\nq, info, state = vi(\n    model,\n    q,\n    n_iters;\n    algorithm=KLMinRepGradDescent(\n        adtype;\n        n_samples=10,\n        operator=AdvancedVI.ClipScale(),\n        averager=AdvancedVI.NoAveraging(),\n    ),\n)\nAdditionally,\n\nThe default hyperparameters of DoGand DoWG have been altered.\nThe deprecated AdvancedVI@0.2-era interface is now removed.\nestimate_objective now always returns the value to be minimized by the optimization algorithm. For example, for ELBO maximization algorithms, estimate_objective will return the negative ELBO. This is breaking change from the previous behavior where the ELBO was returned.\nThe initial value for the q_meanfield_gaussian, q_fullrank_gaussian, and q_locationscale have changed. Specificially, the default initial value for the scale matrix has been changed from I to 0.6*I.\nWhen using algorithms that expect to operate in unconstrained spaces, the user is now explicitly expected to provide a Bijectors.TransformedDistribution wrapping an unconstrained distribution. (Refer to the docstring of vi.)\n\n\n\n\nAdvancedVI@0.6 adds numerous new features including the following new VI algorithms:\n\nKLMinWassFwdBwd: Also known as “Wasserstein variational inference,” this algorithm minimizes the KL divergence under the Wasserstein-2 metric.\nKLMinNaturalGradDescent: This algorithm, also known as “online variational Newton,” is the canonical “black-box” natural gradient variational inference algorithm, which minimizes the KL divergence via mirror descent under the KL divergence as the Bregman divergence.\nKLMinSqrtNaturalGradDescent: This is a recent variant of KLMinNaturalGradDescent that operates in the Cholesky-factor parameterization of Gaussians instead of precision matrices.\nFisherMinBatchMatch: This algorithm called “batch-and-match,” minimizes the variation of the 2nd order Fisher divergence via a proximal point-type algorithm.\n\nAny of the new algorithms above can readily be used by simply swappin the algorithm keyword argument of vi. For example, to use batch-and-match:\nvi(model, q, n_iters; algorithm=FisherMinBatchMatch())\n\n\n\n\nThe interface for defining an external sampler has been reworked. In general, implementations of external samplers should now no longer need to depend on Turing. This is because the interface functions required have been shifted upstream to AbstractMCMC.jl.\nIn particular, you now only need to define the following functions:\n\nAbstractMCMC.step(rng::Random.AbstractRNG, model::AbstractMCMC.LogDensityModel, ::MySampler; kwargs...) (and also a method with state, and the corresponding step_warmup methods if needed)\nAbstractMCMC.getparams(::MySamplerState) -&gt; Vector{&lt;:Real}\nAbstractMCMC.getstats(::MySamplerState) -&gt; NamedTuple\nAbstractMCMC.requires_unconstrained_space(::MySampler) -&gt; Bool (default true)\n\nThis means that you only need to depend on AbstractMCMC.jl. As long as the above functions are defined correctly, Turing will be able to use your external sampler.\nThe Turing.Inference.isgibbscomponent(::MySampler) interface function still exists, but in this version the default has been changed to true, so you should not need to overload this.\n\n\n\nThe Optim.jl interface has been removed (so you cannot call Optim.optimize directly on Turing models). You can use the maximum_likelihood or maximum_a_posteriori functions with an Optim.jl solver instead (via Optimization.jl: see https://docs.sciml.ai/Optimization/stable/optimization_packages/optim/ for documentation of the available solvers).\n\n\n\nThe constructors of OptimLogDensity have been replaced with a single constructor, OptimLogDensity(::DynamicPPL.LogDensityFunction).",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#dynamicppl-0.39",
    "href": "changelog.html#dynamicppl-0.39",
    "title": "Changelog",
    "section": "",
    "text": "Turing.jl v0.42 brings with it all the underlying changes in DynamicPPL 0.39. Please see the DynamicPPL changelog for full details; in here we summarise only the changes that are most pertinent to end-users of Turing.jl.\n\n\nTuring.jl has supported threaded tilde-statements for a while now, as long as said tilde-statements are observations (i.e., likelihood terms). For example:\n@model function f(y)\n    x ~ Normal()\n    Threads.@threads for i in eachindex(y)\n        y[i] ~ Normal(x)\n    end\nend\nModels where tilde-statements or @addlogprob! are used in parallel require what we call ‘threadsafe evaluation’. In previous releases of Turing.jl, threadsafe evaluation was enabled whenever Julia was launched with more than one thread. However, this is an imprecise way of determining whether threadsafe evaluation is really needed. It causes performance degradation for models that do not actually need threadsafe evaluation, and generally led to ill-defined behaviour in various parts of the Turing codebase.\nIn Turing.jl v0.42, threadsafe evaluation is now opt-in. To enable threadsafe evaluation, after defining a model, you now need to call setthreadsafe(model, true) (note that this is not a mutating function, it returns a new model):\ny = randn(100)\nmodel = f(y)\nmodel = setthreadsafe(model, true)\nYou only need to do this if your model uses tilde-statements or @addlogprob! in parallel. You do not need to do this if:\n\nyour model has other kinds of parallelism but does not include tilde-statements inside;\nor you are using MCMCThreads() or MCMCDistributed() to sample multiple chains in parallel, but your model itself does not use parallelism.\n\nIf your model does include parallelised tilde-statements or @addlogprob! calls, and you evaluate it/sample from it without setting setthreadsafe(model, true), then you may get statistically incorrect results without any warnings or errors.\n\n\n\nMany operations in DynamicPPL have been substantially sped up. You should find that anything that uses LogDensityFunction (i.e., HMC/NUTS samplers, optimisation) is faster in this release. Prior sampling should also be much faster than before.\n\n\n\nIf you have a model that requires threadsafe evaluation (i.e., parallel observations), you can now use this with predict. Carrying on from the previous example, you can do:\nmodel = setthreadsafe(f(y), true)\nchain = sample(model, NUTS(), 1000)\n\npdn_model = f(fill(missing, length(y)))\npdn_model = setthreadsafe(pdn_model, true)  # set threadsafe\npredictions = predict(pdn_model, chain) # generate new predictions in parallel\n\n\n\nWhen sampling from a Turing model, the resulting MCMCChains.Chains object now contains the log-joint, log-prior, and log-likelihood under the names :logjoint, :logprior, and :loglikelihood respectively. Previously, :logjoint would be stored under the name :lp.\n\n\n\nWhen sampling using MCMCChains, the chain object will no longer have its chain.logevidence field set. Instead, you can calculate this yourself from the log-likelihoods stored in the chain. For SMC samplers, the log-evidence of the entire trajectory is stored in chain[:logevidence] (which is the same for every particle in the ‘chain’).\n\n\n\nTuring.Inference.Transition(model, vi[, stats]) has been removed; you can directly replace this with DynamicPPL.ParamsWithStats(vi, model[, stats]).",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#advancedvi-0.6",
    "href": "changelog.html#advancedvi-0.6",
    "title": "Changelog",
    "section": "",
    "text": "Turing.jl v0.42 updates AdvancedVI.jl compatibility to 0.6 (we skipped the breaking 0.5 update as it does not introduce new features). AdvancedVI.jl@0.6 introduces major structural changes including breaking changes to the interface and multiple new features. The summary of the changes below are the things that affect the end-users of Turing. For a more comprehensive list of changes, please refer to the changelogs in AdvancedVI.\n\n\nA new level of interface for defining different variational algorithms has been introduced in AdvancedVI v0.5. As a result, the function Turing.vi now receives a keyword argument algorithm. The object algorithm &lt;: AdvancedVI.AbstractVariationalAlgorithm should now contain all the algorithm-specific configurations. Therefore, keyword arguments of vi that were algorithm-specific such as objective, operator, averager and so on, have been moved as fields of the relevant &lt;: AdvancedVI.AbstractVariationalAlgorithm structs.\nIn addition, the outputs also changed. Previously, vi returned both the last-iterate of the algorithm q and the iterate average q_avg. Now, for the algorithms running parameter averaging, only q_avg is returned. As a result, the number of returned values reduced from 4 to 3.\nFor example,\nq, q_avg, info, state = vi(\n    model, q, n_iters; objective=RepGradELBO(10), operator=AdvancedVI.ClipScale()\n)\nis now\nq_avg, info, state = vi(\n    model,\n    q,\n    n_iters;\n    algorithm=KLMinRepGradDescent(adtype; n_samples=10, operator=AdvancedVI.ClipScale()),\n)\nSimilarly,\nvi(\n    model,\n    q,\n    n_iters;\n    objective=RepGradELBO(10; entropy=AdvancedVI.ClosedFormEntropyZeroGradient()),\n    operator=AdvancedVI.ProximalLocationScaleEntropy(),\n)\nis now\nvi(model, q, n_iters; algorithm=KLMinRepGradProxDescent(adtype; n_samples=10))\nLastly, to obtain the last-iterate q of KLMinRepGradDescent, which is not returned in the new interface, simply select the averaging strategy to be AdvancedVI.NoAveraging(). That is,\nq, info, state = vi(\n    model,\n    q,\n    n_iters;\n    algorithm=KLMinRepGradDescent(\n        adtype;\n        n_samples=10,\n        operator=AdvancedVI.ClipScale(),\n        averager=AdvancedVI.NoAveraging(),\n    ),\n)\nAdditionally,\n\nThe default hyperparameters of DoGand DoWG have been altered.\nThe deprecated AdvancedVI@0.2-era interface is now removed.\nestimate_objective now always returns the value to be minimized by the optimization algorithm. For example, for ELBO maximization algorithms, estimate_objective will return the negative ELBO. This is breaking change from the previous behavior where the ELBO was returned.\nThe initial value for the q_meanfield_gaussian, q_fullrank_gaussian, and q_locationscale have changed. Specificially, the default initial value for the scale matrix has been changed from I to 0.6*I.\nWhen using algorithms that expect to operate in unconstrained spaces, the user is now explicitly expected to provide a Bijectors.TransformedDistribution wrapping an unconstrained distribution. (Refer to the docstring of vi.)\n\n\n\n\nAdvancedVI@0.6 adds numerous new features including the following new VI algorithms:\n\nKLMinWassFwdBwd: Also known as “Wasserstein variational inference,” this algorithm minimizes the KL divergence under the Wasserstein-2 metric.\nKLMinNaturalGradDescent: This algorithm, also known as “online variational Newton,” is the canonical “black-box” natural gradient variational inference algorithm, which minimizes the KL divergence via mirror descent under the KL divergence as the Bregman divergence.\nKLMinSqrtNaturalGradDescent: This is a recent variant of KLMinNaturalGradDescent that operates in the Cholesky-factor parameterization of Gaussians instead of precision matrices.\nFisherMinBatchMatch: This algorithm called “batch-and-match,” minimizes the variation of the 2nd order Fisher divergence via a proximal point-type algorithm.\n\nAny of the new algorithms above can readily be used by simply swappin the algorithm keyword argument of vi. For example, to use batch-and-match:\nvi(model, q, n_iters; algorithm=FisherMinBatchMatch())",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#external-sampler-interface",
    "href": "changelog.html#external-sampler-interface",
    "title": "Changelog",
    "section": "",
    "text": "The interface for defining an external sampler has been reworked. In general, implementations of external samplers should now no longer need to depend on Turing. This is because the interface functions required have been shifted upstream to AbstractMCMC.jl.\nIn particular, you now only need to define the following functions:\n\nAbstractMCMC.step(rng::Random.AbstractRNG, model::AbstractMCMC.LogDensityModel, ::MySampler; kwargs...) (and also a method with state, and the corresponding step_warmup methods if needed)\nAbstractMCMC.getparams(::MySamplerState) -&gt; Vector{&lt;:Real}\nAbstractMCMC.getstats(::MySamplerState) -&gt; NamedTuple\nAbstractMCMC.requires_unconstrained_space(::MySampler) -&gt; Bool (default true)\n\nThis means that you only need to depend on AbstractMCMC.jl. As long as the above functions are defined correctly, Turing will be able to use your external sampler.\nThe Turing.Inference.isgibbscomponent(::MySampler) interface function still exists, but in this version the default has been changed to true, so you should not need to overload this.",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#optimisation-interface",
    "href": "changelog.html#optimisation-interface",
    "title": "Changelog",
    "section": "",
    "text": "The Optim.jl interface has been removed (so you cannot call Optim.optimize directly on Turing models). You can use the maximum_likelihood or maximum_a_posteriori functions with an Optim.jl solver instead (via Optimization.jl: see https://docs.sciml.ai/Optimization/stable/optimization_packages/optim/ for documentation of the available solvers).",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#internal-changes",
    "href": "changelog.html#internal-changes",
    "title": "Changelog",
    "section": "",
    "text": "The constructors of OptimLogDensity have been replaced with a single constructor, OptimLogDensity(::DynamicPPL.LogDensityFunction).",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#dynamicppl-0.38",
    "href": "changelog.html#dynamicppl-0.38",
    "title": "Changelog",
    "section": "DynamicPPL 0.38",
    "text": "DynamicPPL 0.38\nTuring.jl v0.41 brings with it all the underlying changes in DynamicPPL 0.38. Please see the DynamicPPL changelog for full details: in this section we only describe the changes that will directly affect end-users of Turing.jl.\n\nPerformance\nA number of functions such as returned and predict will have substantially better performance in this release.\n\n\nProductNamedTupleDistribution\nDistributions.ProductNamedTupleDistribution can now be used on the right-hand side of ~ in Turing models.\n\n\nInitial parameters\nInitial parameters for MCMC sampling must now be specified in a different form. You still need to use the initial_params keyword argument to sample, but the allowed values are different. For almost all samplers in Turing.jl (except Emcee) this should now be a DynamicPPL.AbstractInitStrategy.\nThere are three kinds of initialisation strategies provided out of the box with Turing.jl (they are exported so you can use these directly with using Turing):\n\nInitFromPrior(): Sample from the prior distribution. This is the default for most samplers in Turing.jl (if you don’t specify initial_params).\nInitFromUniform(a, b): Sample uniformly from [a, b] in linked space. This is the default for Hamiltonian samplers. If a and b are not specified it defaults to [-2, 2], which preserves the behaviour in previous versions (and mimics that of Stan).\nInitFromParams(p): Explicitly provide a set of initial parameters. Note: p must be either a NamedTuple or an AbstractDict{&lt;:VarName}; it can no longer be a Vector. Parameters must be provided in unlinked space, even if the sampler later performs linking.\n\nFor this release of Turing.jl, you can also provide a NamedTuple or AbstractDict{&lt;:VarName} and this will be automatically wrapped in InitFromParams for you. This is an intermediate measure for backwards compatibility, and will eventually be removed.\n\n\nThis change is made because Vectors are semantically ambiguous. It is not clear which element of the vector corresponds to which variable in the model, nor is it clear whether the parameters are in linked or unlinked space. Previously, both of these would depend on the internal structure of the VarInfo, which is an implementation detail. In contrast, the behaviour of AbstractDicts and NamedTuples is invariant to the ordering of variables and it is also easier for readers to understand which variable is being set to which value.\nIf you were previously using varinfo[:] to extract a vector of initial parameters, you can now use Dict(k =&gt; varinfo[k] for k in keys(varinfo) to extract a Dict of initial parameters.\nFor more details about initialisation you can also refer to the main TuringLang docs, and/or the DynamicPPL API docs.\n\n\nresume_from and loadstate\nThe resume_from keyword argument to sample is now removed. Instead of sample(...; resume_from=chain) you can use sample(...; initial_state=loadstate(chain)) which is entirely equivalent. loadstate is exported from Turing now instead of in DynamicPPL.\nNote that loadstate only works for MCMCChains.Chains. For FlexiChains users please consult the FlexiChains docs directly where this functionality is described in detail.\n\n\npointwise_logdensities\npointwise_logdensities(model, chn), pointwise_loglikelihoods(...), and pointwise_prior_logdensities(...) now return an MCMCChains.Chains object if chn is itself an MCMCChains.Chains object. The old behaviour of returning an OrderedDict is still available: you just need to pass OrderedDict as the third argument, i.e., pointwise_logdensities(model, chn, OrderedDict).",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#initial-step-in-mcmc-sampling",
    "href": "changelog.html#initial-step-in-mcmc-sampling",
    "title": "Changelog",
    "section": "Initial step in MCMC sampling",
    "text": "Initial step in MCMC sampling\nHMC and NUTS samplers no longer take an extra single step before starting the chain. This means that if you do not discard any samples at the start, the first sample will be the initial parameters (which may be user-provided).\nNote that if the initial sample is included, the corresponding sampler statistics will be missing. Due to a technical limitation of MCMCChains.jl, this causes all indexing into MCMCChains to return Union{Float64, Missing} or similar. If you want the old behaviour, you can discard the first sample (e.g. using discard_initial=1).",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#breaking-changes-1",
    "href": "changelog.html#breaking-changes-1",
    "title": "Changelog",
    "section": "Breaking changes",
    "text": "Breaking changes\nDynamicPPL 0.37\nTuring.jl v0.40 updates DynamicPPL compatibility to 0.37. The summary of the changes provided here is intended for end-users of Turing. If you are a package developer, or would otherwise like to understand these changes in-depth, please see the DynamicPPL changelog.\n\n@submodel is now completely removed; please use to_submodel.\nPrior and likelihood calculations are now completely separated in Turing. Previously, the log-density used to be accumulated in a single field and thus there was no clear way to separate prior and likelihood components.\n\n@addlogprob! f, where f is a float, now adds to the likelihood by default.\nYou can instead use @addlogprob! (; logprior=x, loglikelihood=y) to control which log-density component to add to.\nThis means that usage of PriorContext and LikelihoodContext is no longer needed, and these have now been removed.\n\nThe special __context__ variable has been removed. If you still need to access the evaluation context, it is now available as __model__.context.\n\nLog-density in chains\nWhen sampling from a Turing model, the resulting MCMCChains.Chains object now contains not only the log-joint (accessible via chain[:lp]) but also the log-prior and log-likelihood (chain[:logprior] and chain[:loglikelihood] respectively).\nThese values now correspond to the log density of the sampled variables exactly as per the model definition / user parameterisation and thus will ignore any linking (transformation to unconstrained space). For example, if the model is @model f() = x ~ LogNormal(), chain[:lp] would always contain the value of logpdf(LogNormal(), x) for each sampled value of x. Previously these values could be incorrect if linking had occurred: some samplers would return logpdf(Normal(), log(x)) i.e. the log-density with respect to the transformed distribution.\nGibbs sampler\nWhen using Turing’s Gibbs sampler, e.g. Gibbs(:x =&gt; MH(), :y =&gt; HMC(0.1, 20)), the conditioned variables (for example y during the MH step, or x during the HMC step) are treated as true observations. Thus the log-density associated with them is added to the likelihood. Previously these would effectively be added to the prior (in the sense that if LikelihoodContext was used they would be ignored). This is unlikely to affect users but we mention it here to be explicit. This change only affects the log probabilities as the Gibbs component samplers see them; the resulting chain will include the usual log prior, likelihood, and joint, as described above.\nParticle Gibbs\nPreviously, only ‘true’ observations (i.e., x ~ dist where x is a model argument or conditioned upon) would trigger resampling of particles. Specifically, there were two cases where resampling would not be triggered:\n\nCalls to @addlogprob!\nGibbs-conditioned variables: e.g. y in Gibbs(:x =&gt; PG(20), :y =&gt; MH())\n\nTuring 0.40 changes this such that both of the above cause resampling. (The second case follows from the changes to the Gibbs sampler, see above.)\nThis release also fixes a bug where, if the model ended with one of these statements, their contribution to the particle weight would be ignored, leading to incorrect results.\nThe changes above also mean that certain models that previously worked with PG-within-Gibbs may now error. Specifically this is likely to happen when the dimension of the model is variable. For example:\n@model function f()\n    x ~ Bernoulli()\n    if x\n        y1 ~ Normal()\n    else\n        y1 ~ Normal()\n        y2 ~ Normal()\n    end\n    # (some likelihood term...)\nend\nsample(f(), Gibbs(:x =&gt; PG(20), (:y1, :y2) =&gt; MH()), 100)\nThis sampler now cannot be used for this model because depending on which branch is taken, the number of observations will be different. To use PG-within-Gibbs, the number of observations that the PG component sampler sees must be constant. Thus, for example, this will still work if x, y1, and y2 are grouped together under the PG component sampler.\nIf you absolutely require the old behaviour, we recommend using Turing.jl v0.39, but also thinking very carefully about what the expected behaviour of the model is, and checking that Turing is sampling from it correctly (note that the behaviour on v0.39 may in general be incorrect because of the fact that Gibbs-conditioned variables did not trigger resampling). We would also welcome any GitHub issues highlighting such problems. Our support for dynamic models is incomplete and is liable to undergo further changes.",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#other-changes",
    "href": "changelog.html#other-changes",
    "title": "Changelog",
    "section": "Other changes",
    "text": "Other changes\n\nSampling using Prior() should now be about twice as fast because we now avoid evaluating the model twice on every iteration.\nTuring.Inference.Transition now has different fields. If t isa Turing.Inference.Transition, t.stat is always a NamedTuple, not nothing (if it genuinely has no information then it’s an empty NamedTuple). Furthermore, t.lp has now been split up into t.logprior and t.loglikelihood (see also ‘Log-density in chains’ section above).",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#update-to-the-advancedvi-interface",
    "href": "changelog.html#update-to-the-advancedvi-interface",
    "title": "Changelog",
    "section": "Update to the AdvancedVI interface",
    "text": "Update to the AdvancedVI interface\nTuring’s variational inference interface was updated to match version 0.4 version of AdvancedVI.jl.\nAdvancedVI v0.4 introduces various new features:\n\nlocation-scale families with dense scale matrices,\nparameter-free stochastic optimization algorithms like DoG and DoWG,\nproximal operators for stable optimization,\nthe sticking-the-landing control variate for faster convergence, and\nthe score gradient estimator for non-differentiable targets.\n\nPlease see the Turing API documentation, and AdvancedVI’s documentation, for more details.",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#removal-of-turing.essential",
    "href": "changelog.html#removal-of-turing.essential",
    "title": "Changelog",
    "section": "Removal of Turing.Essential",
    "text": "Removal of Turing.Essential\nThe Turing.Essential module has been removed. Anything exported from there can be imported from either Turing or DynamicPPL.",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#addlogprob",
    "href": "changelog.html#addlogprob",
    "title": "Changelog",
    "section": "@addlogprob!",
    "text": "@addlogprob!\nThe @addlogprob! macro is now exported from Turing, making it officially part of the public interface.",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#dynamicppl-version",
    "href": "changelog.html#dynamicppl-version",
    "title": "Changelog",
    "section": "DynamicPPL version",
    "text": "DynamicPPL version\nDynamicPPL compatibility has been bumped to 0.36. This brings with it a number of changes: the ones most likely to affect you are submodel prefixing and conditioning. Variables in submodels are now represented correctly with field accessors. For example:\nusing Turing\n@model inner() = x ~ Normal()\n@model outer() = a ~ to_submodel(inner())\nkeys(VarInfo(outer())) now returns [@varname(a.x)] instead of [@varname(var\"a.x\")]\nFurthermore, you can now either condition on the outer model like outer() | (@varname(a.x) =&gt; 1.0), or the inner model like inner() | (@varname(x) =&gt; 1.0). If you use the conditioned inner model as a submodel, the conditioning will still apply correctly.\nPlease see the DynamicPPL release notes for fuller details.",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#gibbs-sampler",
    "href": "changelog.html#gibbs-sampler",
    "title": "Changelog",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\nTuring’s Gibbs sampler now allows for more complex VarNames, such as x[1] or x.a, to be used. For example, you can now do this:\n@model function f()\n    x = Vector{Float64}(undef, 2)\n    x[1] ~ Normal()\n    return x[2] ~ Normal()\nend\nsample(f(), Gibbs(@varname(x[1]) =&gt; MH(), @varname(x[2]) =&gt; MH()), 100)\nPerformance for the cases which used to previously work (i.e. VarNames like x which only consist of a single symbol) is unaffected, and VarNames with only field accessors (e.g. x.a) should be equally fast. It is possible that VarNames with indexing (e.g. x[1]) may be slower (although this is still an improvement over not working at all!). If you find any cases where you think the performance is worse than it should be, please do file an issue.",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#breaking-changes-2",
    "href": "changelog.html#breaking-changes-2",
    "title": "Changelog",
    "section": "Breaking changes",
    "text": "Breaking changes\n\nGibbs constructors\n0.37 removes the old Gibbs constructors deprecated in 0.36.\n\n\nRemove Zygote support\nZygote is no longer officially supported as an automatic differentiation backend, and AutoZygote is no longer exported. You can continue to use Zygote by importing AutoZygote from ADTypes and it may well continue to work, but it is no longer tested and no effort will be expended to fix it if something breaks.\nMooncake is the recommended replacement for Zygote.\n\n\nDynamicPPL 0.35\nTuring.jl v0.37 uses DynamicPPL v0.35, which brings with it several breaking changes:\n\nThe right hand side of .~ must from now on be a univariate distribution.\nIndexing VarInfo objects by samplers has been removed completely.\nThe order in which nested submodel prefixes are applied has been reversed.\nThe arguments for the constructor of LogDensityFunction have changed. LogDensityFunction also now satisfies the LogDensityProblems interface, without needing a wrapper object.\n\nFor more details about all of the above, see the changelog of DynamicPPL here.\n\n\nExport list\nTuring.jl’s export list has been cleaned up a fair bit. This affects what is imported into your namespace when you do an unqualified using Turing. You may need to import things more explicitly than before.\n\nThe DynamicPPL and AbstractMCMC modules are no longer exported. You will need to import DynamicPPL or using DynamicPPL: DynamicPPL (likewise AbstractMCMC) yourself, which in turn means that they have to be made available in your project environment.\n@logprob_str and @prob_str have been removed following a long deprecation period.\nWe no longer re-export everything from Bijectors and Libtask. To get around this, add using Bijectors or using Libtask at the top of your script (but we recommend using more selective imports).\n\nWe no longer export Bijectors.ordered. If you were using ordered, even Bijectors does not (currently) export this. You will have to manually import it with using Bijectors: ordered.\n\n\nOn the other hand, we have added a few more exports:\n\nDynamicPPL.returned and DynamicPPL.prefix are exported (for use with submodels).\nLinearAlgebra.I is exported for convenience.",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#breaking-changes-3",
    "href": "changelog.html#breaking-changes-3",
    "title": "Changelog",
    "section": "Breaking changes",
    "text": "Breaking changes\n0.36.0 introduces a new Gibbs sampler. It’s been included in several previous releases as Turing.Experimental.Gibbs, but now takes over the old Gibbs sampler, which gets removed completely.\nThe new Gibbs sampler currently supports the same user-facing interface as the old one, but the old constructors have been deprecated, and will be removed in the future. Also, given that the internals have been completely rewritten in a very different manner, there may be accidental breakage that we haven’t anticipated. Please report any you find.\nGibbsConditional has also been removed. It was never very user-facing, but it was exported, so technically this is breaking.\nThe old Gibbs constructor relied on being called with several subsamplers, and each of the constructors of the subsamplers would take as arguments the symbols for the variables that they are to sample, e.g. Gibbs(HMC(:x), MH(:y)). This constructor has been deprecated, and will be removed in the future. The new constructor works by mapping symbols, VarNames, or iterables thereof to samplers, e.g. Gibbs(x=&gt;HMC(), y=&gt;MH()), Gibbs(@varname(x) =&gt; HMC(), @varname(y) =&gt; MH()), Gibbs((:x, :y) =&gt; NUTS(), :z =&gt; MH()). This allows more granular specification of which sampler to use for which variable.\nLikewise, the old constructor for calling one subsampler more often than another, Gibbs((HMC(0.01, 4, :x), 2), (MH(:y), 1)) has been deprecated. The new way to do this is to use RepeatSampler, also introduced at this version: Gibbs(@varname(x) =&gt; RepeatSampler(HMC(0.01, 4), 2), @varname(y) =&gt; MH()).",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#breaking-changes-4",
    "href": "changelog.html#breaking-changes-4",
    "title": "Changelog",
    "section": "Breaking changes",
    "text": "Breaking changes\nJulia 1.10 is now the minimum required version for Turing.\nTapir.jl has been removed and replaced with its successor, Mooncake.jl. You can use Mooncake.jl by passing adbackend=AutoMooncake(; config=nothing) to the relevant samplers.\nSupport for Tracker.jl as an AD backend has been removed.",
    "crumbs": null
  },
  {
    "objectID": "changelog.html#breaking-changes-5",
    "href": "changelog.html#breaking-changes-5",
    "title": "Changelog",
    "section": "Breaking changes",
    "text": "Breaking changes\nThe following exported functions have been removed:\n\nconstrained_space\nget_parameter_bounds\noptim_objective\noptim_function\noptim_problem\n\nThe same functionality is now offered by the new exported functions\n\nmaximum_likelihood\nmaximum_a_posteriori",
    "crumbs": null
  },
  {
    "objectID": "tutorials/bayesian-neural-networks/index.html",
    "href": "tutorials/bayesian-neural-networks/index.html",
    "title": "Bayesian Neural Networks",
    "section": "",
    "text": "In this tutorial, we demonstrate how one can implement a Bayesian Neural Network using a combination of Turing and Lux, a suite of machine learning tools. We will use Lux to specify the neural network’s layers and Turing to implement the probabilistic inference, with the goal of implementing a classification algorithm.\nWe will begin with importing the relevant libraries.\nusing Turing\nusing FillArrays\nusing Lux\nusing Plots\nimport Mooncake\nusing Functors\n\nusing LinearAlgebra\nusing Random\nOur goal here is to use a Bayesian neural network to classify points in an artificial dataset. The code below generates data points arranged in a box-like pattern and displays a graph of the dataset we will be working with.\n# Number of points to generate\nN = 80\nM = round(Int, N / 4)\nrng = Random.default_rng()\nRandom.seed!(rng, 1234)\n\n# Generate artificial data\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nxt1s = Array([[x1s[i] + 0.5f0; x2s[i] + 0.5f0] for i in 1:M])\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nappend!(xt1s, Array([[x1s[i] - 5.0f0; x2s[i] - 5.0f0] for i in 1:M]))\n\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nxt0s = Array([[x1s[i] + 0.5f0; x2s[i] - 5.0f0] for i in 1:M])\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nappend!(xt0s, Array([[x1s[i] - 5.0f0; x2s[i] + 0.5f0] for i in 1:M]))\n\n# Store all the data for later\nxs = [xt1s; xt0s]\nts = [ones(2 * M); zeros(2 * M)]\n\n# Plot data points.\nfunction plot_data()\n    x1 = map(e -&gt; e[1], xt1s)\n    y1 = map(e -&gt; e[2], xt1s)\n    x2 = map(e -&gt; e[1], xt0s)\n    y2 = map(e -&gt; e[2], xt0s)\n\n    Plots.scatter(x1, y1; color=\"red\", clim=(0, 1))\n    return Plots.scatter!(x2, y2; color=\"blue\", clim=(0, 1))\nend\n\nplot_data()",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Neural Networks"
    ]
  },
  {
    "objectID": "tutorials/bayesian-neural-networks/index.html#building-a-neural-network",
    "href": "tutorials/bayesian-neural-networks/index.html#building-a-neural-network",
    "title": "Bayesian Neural Networks",
    "section": "Building a Neural Network",
    "text": "Building a Neural Network\nThe next step is to define a feedforward neural network where we express our parameters as distributions, and not single points as with traditional neural networks. For this we will use Dense to define linear layers and compose them via Chain, both are neural network primitives from Lux. The network nn_initial we created has two hidden layers with tanh activations and one output layer with sigmoid (σ) activation, as shown below.\n\n\n\n\n\n\n\nG\n\nInput layer                   Hidden layers                  Output layer\n\ncluster_hidden1\n\n\n\ncluster_hidden2\n\n\n\ncluster_output\n\n\n\ncluster_input\n\n\n\n\ninput1\n\n\n\n\nhidden11\n\n\n\n\ninput1--hidden11\n\n\n\n\nhidden12\n\n\n\n\ninput1--hidden12\n\n\n\n\nhidden13\n\n\n\n\ninput1--hidden13\n\n\n\n\ninput2\n\n\n\n\ninput2--hidden11\n\n\n\n\ninput2--hidden12\n\n\n\n\ninput2--hidden13\n\n\n\n\nhidden21\n\n\n\n\nhidden11--hidden21\n\n\n\n\nhidden22\n\n\n\n\nhidden11--hidden22\n\n\n\n\nhidden12--hidden21\n\n\n\n\nhidden12--hidden22\n\n\n\n\nhidden13--hidden21\n\n\n\n\nhidden13--hidden22\n\n\n\n\noutput1\n\n\n\n\nhidden21--output1\n\n\n\n\nhidden22--output1\n\n\n\n\n\n\n\n\n\nThe nn_initial is an instance that acts as a function and can take data as inputs and output predictions. We will define distributions on the neural network parameters.\n\n# Construct a neural network using Lux\nnn_initial = Chain(Dense(2 =&gt; 3, tanh), Dense(3 =&gt; 2, tanh), Dense(2 =&gt; 1, σ))\n\n# Initialize the model weights and state\nps, st = Lux.setup(rng, nn_initial)\n\nLux.parameterlength(nn_initial) # number of parameters in NN\n\n20\n\n\nThe probabilistic model specification below creates a parameters variable, which has IID normal variables. The parameters vector represents all parameters of our neural net (weights and biases).\n\n# Create a regularization term and a Gaussian prior variance term.\nalpha = 0.09\nsigma = sqrt(1.0 / alpha)\n\n3.3333333333333335\n\n\nWe also define a function to construct a named tuple from a vector of sampled parameters. (We could use ComponentArrays here and broadcast to avoid doing this, but this way avoids introducing an extra dependency.)\n\nfunction vector_to_parameters(ps_new::AbstractVector, ps::NamedTuple)\n    @assert length(ps_new) == Lux.parameterlength(ps)\n    i = 1\n    function get_ps(x)\n        z = reshape(view(ps_new, i:(i + length(x) - 1)), size(x))\n        i += length(x)\n        return z\n    end\n    return fmap(get_ps, ps)\nend\n\nvector_to_parameters (generic function with 1 method)\n\n\nTo interface with external libraries it is often desirable to use the StatefulLuxLayer to automatically handle the neural network states.\n\nconst nn = StatefulLuxLayer{true}(nn_initial, nothing, st)\n\n# Specify the probabilistic model.\n@model function bayes_nn(xs, ts; sigma = sigma, ps = ps, nn = nn)\n    # Sample the parameters\n    nparameters = Lux.parameterlength(nn_initial)\n    parameters ~ MvNormal(zeros(nparameters), Diagonal(abs2.(sigma .* ones(nparameters))))\n\n    # Forward NN to make predictions\n    preds = Lux.apply(nn, xs, f32(vector_to_parameters(parameters, ps)))\n\n    # Observe each prediction.\n    for i in eachindex(ts)\n        ts[i] ~ Bernoulli(preds[i])\n    end\nend\n\nbayes_nn (generic function with 2 methods)\n\n\nInference can now be performed by calling sample. We use the NUTS Hamiltonian Monte Carlo sampler here.\n\nsetprogress!(false)\n\n\n# Perform inference.\nn_iters = 2_000\nch = sample(bayes_nn(reduce(hcat, xs), ts), NUTS(; adtype=AutoMooncake()), n_iters);\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.8\n\n\n\n\nNow we extract the parameter samples from the sampled chain as θ (this is of size 5000 x 20 where 5000 is the number of iterations and 20 is the number of parameters). We’ll use these primarily to determine how good our model’s classifier is.\n\n# Extract all weight and bias parameters.\nθ = MCMCChains.group(ch, :parameters).value;",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Neural Networks"
    ]
  },
  {
    "objectID": "tutorials/bayesian-neural-networks/index.html#prediction-visualization",
    "href": "tutorials/bayesian-neural-networks/index.html#prediction-visualization",
    "title": "Bayesian Neural Networks",
    "section": "Prediction Visualization",
    "text": "Prediction Visualization\nWe can use MAP estimation to classify our population by using the set of weights that provided the highest log posterior.\n\n# A helper to run the nn through data `x` using parameters `θ`\nnn_forward(x, θ) = nn(x, vector_to_parameters(θ, ps))\n\n# Plot the data we have.\nfig = plot_data()\n\n# Find the index that provided the highest log posterior in the chain.\n_, i = findmax(ch[:logjoint])\n\n# Extract the max row value from i.\ni = i.I[1]\n\n# Plot the posterior distribution with a contour plot\nx1_range = collect(range(-6; stop=6, length=25))\nx2_range = collect(range(-6; stop=6, length=25))\nZ = [nn_forward([x1, x2], θ[i, :])[1] for x1 in x1_range, x2 in x2_range]\ncontour!(x1_range, x2_range, Z; linewidth=3, colormap=:seaborn_bright)\nfig\n\n\n\n\nThe contour plot above shows that the MAP method is not too bad at classifying our data.\nNow we can visualise our predictions.\n\\[\np(\\tilde{x} | X, \\alpha) = \\int_{\\theta} p(\\tilde{x} | \\theta) p(\\theta | X, \\alpha) \\approx \\sum_{\\theta \\sim p(\\theta | X, \\alpha)}f_{\\theta}(\\tilde{x})\n\\]\nThe nn_predict function takes the average predicted value from a network parameterized by weights drawn from the MCMC chain.\n\n# Return the average predicted value across\n# multiple weights.\nfunction nn_predict(x, θ, num)\n    num = min(num, size(θ, 1))  # make sure num does not exceed the number of samples\n    return mean([first(nn_forward(x, view(θ, i, :))) for i in 1:10:num])\nend\n\nnn_predict (generic function with 1 method)\n\n\nNext, we use the nn_predict function to predict the value at a sample of points where the x1 and x2 coordinates range between -6 and 6. As we can see below, we still have a satisfactory fit to our data, and more importantly, we can also see where the neural network is uncertain about its predictions much easier—those regions between cluster boundaries.\n\n# Plot the average prediction.\nfig = plot_data()\n\nn_end = 1500\nx1_range = collect(range(-6; stop=6, length=25))\nx2_range = collect(range(-6; stop=6, length=25))\nZ = [nn_predict([x1, x2], θ, n_end)[1] for x1 in x1_range, x2 in x2_range]\ncontour!(x1_range, x2_range, Z; linewidth=3, colormap=:seaborn_bright)\nfig\n\n\n\n\nSuppose we are interested in how the predictive power of our Bayesian neural network evolved between samples. In that case, the following graph displays an animation of the contour plot generated from the network weights in samples 1 to 1,000.\n\n# Number of iterations to plot.\nn_end = 500\n\nanim = @gif for i in 1:n_end\n    plot_data()\n    Z = [nn_forward([x1, x2], θ[i, :])[1] for x1 in x1_range, x2 in x2_range]\n    contour!(x1_range, x2_range, Z; title=\"Iteration $i\", clim=(0, 1))\nend every 5\n\n\n[ Info: Saved animation to /tmp/jl_K7sNeDUJCD.gif\n\n\n\n\n\n\n\nThis has been an introduction to the applications of Turing and Lux in defining Bayesian neural networks.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Neural Networks"
    ]
  },
  {
    "objectID": "tutorials/multinomial-logistic-regression/index.html",
    "href": "tutorials/multinomial-logistic-regression/index.html",
    "title": "Multinomial Logistic Regression",
    "section": "",
    "text": "Multinomial logistic regression is an extension of logistic regression. Logistic regression is used to model problems in which there are exactly two possible discrete outcomes. Multinomial logistic regression is used to model problems in which there are two or more possible discrete outcomes.\nIn our example, we’ll be using the iris dataset. The iris multiclass problem aims to predict the species of a flower given measurements (in centimetres) of sepal length and width and petal length and width. There are three possible species: Iris setosa, Iris versicolor, and Iris virginica.\nTo start, let’s import all the libraries we’ll need.\n# Load Turing.\nusing Turing\n\n# Load RDatasets.\nusing RDatasets\n\n# Load StatsPlots for visualisations and diagnostics.\nusing StatsPlots\n\n# Functionality for splitting and normalising the data.\nusing MLDataUtils: shuffleobs, splitobs, rescale!\n\n# We need a softmax function which is provided by NNlib.\nusing NNlib: softmax\n\n# Functionality for constructing arrays with identical elements efficiently.\nusing FillArrays\n\n# Functionality for working with scaled identity matrices.\nusing LinearAlgebra\n\n# Set a seed for reproducibility.\nusing Random\nRandom.seed!(0);",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Multinomial Logistic Regression"
    ]
  },
  {
    "objectID": "tutorials/multinomial-logistic-regression/index.html#data-cleaning-set-up",
    "href": "tutorials/multinomial-logistic-regression/index.html#data-cleaning-set-up",
    "title": "Multinomial Logistic Regression",
    "section": "Data Cleaning & Set Up",
    "text": "Data Cleaning & Set Up\nNow we’re going to import our dataset. Twenty rows of the dataset are shown below so you can get a good feel for what kind of data we have.\n\n# Import the \"iris\" dataset.\ndata = RDatasets.dataset(\"datasets\", \"iris\");\n\n# Show twenty random rows.\ndata[rand(1:size(data, 1), 20), :]\n\n20×5 DataFrame\n\n\n\nRow\nSepalLength\nSepalWidth\nPetalLength\nPetalWidth\nSpecies\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nCat…\n\n\n\n\n1\n5.0\n2.0\n3.5\n1.0\nversicolor\n\n\n2\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n3\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n4\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n5\n5.7\n2.8\n4.1\n1.3\nversicolor\n\n\n6\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n7\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n8\n7.6\n3.0\n6.6\n2.1\nvirginica\n\n\n9\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n10\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n11\n5.5\n2.4\n3.8\n1.1\nversicolor\n\n\n12\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n13\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n14\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n15\n6.1\n2.9\n4.7\n1.4\nversicolor\n\n\n16\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n17\n6.4\n2.7\n5.3\n1.9\nvirginica\n\n\n18\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n19\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n20\n6.2\n2.2\n4.5\n1.5\nversicolor\n\n\n\n\n\n\nIn this data set, the outcome Species is currently coded as a string. We convert it to a numerical value by using indices 1, 2, and 3 to indicate species setosa, versicolor, and virginica, respectively.\n\n# Recode the `Species` column.\nspecies = [\"setosa\", \"versicolor\", \"virginica\"]\ndata[!, :Species_index] = indexin(data[!, :Species], species)\n\n# Show twenty random rows of the new species columns\ndata[rand(1:size(data, 1), 20), [:Species, :Species_index]]\n\n20×2 DataFrame\n\n\n\nRow\nSpecies\nSpecies_index\n\n\n\nCat…\nUnion…\n\n\n\n\n1\nsetosa\n1\n\n\n2\nversicolor\n2\n\n\n3\nversicolor\n2\n\n\n4\nsetosa\n1\n\n\n5\nversicolor\n2\n\n\n6\nversicolor\n2\n\n\n7\nversicolor\n2\n\n\n8\nversicolor\n2\n\n\n9\nvirginica\n3\n\n\n10\nversicolor\n2\n\n\n11\nvirginica\n3\n\n\n12\nsetosa\n1\n\n\n13\nsetosa\n1\n\n\n14\nversicolor\n2\n\n\n15\nsetosa\n1\n\n\n16\nsetosa\n1\n\n\n17\nvirginica\n3\n\n\n18\nversicolor\n2\n\n\n19\nvirginica\n3\n\n\n20\nvirginica\n3\n\n\n\n\n\n\nAfter we’ve done that tidying, it’s time to split our dataset into training and testing sets, and separate the features and target from the data. Additionally, we must rescale our feature variables so that they are centred around zero by subtracting each column by the mean and dividing it by the standard deviation. This standardisation improves sampler efficiency by ensuring all features are on comparable scales.\n\n# Split our dataset 50%/50% into training/test sets.\ntrainset, testset = splitobs(shuffleobs(data), 0.5)\n\n# Define features and target.\nfeatures = [:SepalLength, :SepalWidth, :PetalLength, :PetalWidth]\ntarget = :Species_index\n\n# Turing requires data in matrix and vector form.\ntrain_features = Matrix(trainset[!, features])\ntest_features = Matrix(testset[!, features])\ntrain_target = trainset[!, target]\ntest_target = testset[!, target]\n\n# Standardise the features.\nμ, σ = rescale!(train_features; obsdim=1)\nrescale!(test_features, μ, σ; obsdim=1);",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Multinomial Logistic Regression"
    ]
  },
  {
    "objectID": "tutorials/multinomial-logistic-regression/index.html#model-declaration",
    "href": "tutorials/multinomial-logistic-regression/index.html#model-declaration",
    "title": "Multinomial Logistic Regression",
    "section": "Model Declaration",
    "text": "Model Declaration\nFinally, we can define our model logistic_regression. It is a function that takes three arguments where\n\nx is our set of independent variables;\ny is the element we want to predict;\nσ is the standard deviation we want to assume for our priors.\n\nWe select the setosa species as the baseline class (the choice does not matter). Then we create the intercepts and vectors of coefficients for the other classes against that baseline. More concretely, we create scalar intercepts intercept_versicolor and intersept_virginica and coefficient vectors coefficients_versicolor and coefficients_virginica with four coefficients each for the features SepalLength, SepalWidth, PetalLength and PetalWidth. We assume a normal distribution with mean zero and standard deviation σ as prior for each scalar parameter. We want to find the posterior distribution of these, in total ten, parameters to be able to predict the species for any given set of features.\n\n# Bayesian multinomial logistic regression\n@model function logistic_regression(x, y, σ)\n    n = size(x, 1)\n    length(y) == n ||\n        throw(DimensionMismatch(\"number of observations in `x` and `y` is not equal\"))\n\n    # Priors of intercepts and coefficients.\n    intercept_versicolor ~ Normal(0, σ)\n    intercept_virginica ~ Normal(0, σ)\n    coefficients_versicolor ~ MvNormal(Zeros(4), σ^2 * I)\n    coefficients_virginica ~ MvNormal(Zeros(4), σ^2 * I)\n\n    # Compute the likelihood of the observations.\n    values_versicolor = intercept_versicolor .+ x * coefficients_versicolor\n    values_virginica = intercept_virginica .+ x * coefficients_virginica\n    for i in 1:n\n        # the 0 corresponds to the base category `setosa`\n        v = softmax([0, values_versicolor[i], values_virginica[i]])\n        y[i] ~ Categorical(v)\n    end\nend;",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Multinomial Logistic Regression"
    ]
  },
  {
    "objectID": "tutorials/multinomial-logistic-regression/index.html#sampling",
    "href": "tutorials/multinomial-logistic-regression/index.html#sampling",
    "title": "Multinomial Logistic Regression",
    "section": "Sampling",
    "text": "Sampling\nNow we can run our sampler. This time we’ll use NUTS to sample from our posterior.\n\nsetprogress!(false)\n\n\nm = logistic_regression(train_features, train_target, 1)\nchain = sample(m, NUTS(), MCMCThreads(), 1_500, 3)\n\n\n\nChains MCMC chain (1500×24×3 Array{Float64, 3}):\n\nIterations        = 751:1:2250\nNumber of chains  = 3\nSamples per chain = 1500\nWall duration     = 17.94 seconds\nCompute duration  = 13.16 seconds\nparameters        = intercept_versicolor, intercept_virginica, coefficients_versicolor[1], coefficients_versicolor[2], coefficients_versicolor[3], coefficients_versicolor[4], coefficients_virginica[1], coefficients_virginica[2], coefficients_virginica[3], coefficients_virginica[4]\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n\n\n\n\n\n\nWarningSampling With Multiple Threads\n\n\n\n\n\nThe sample() call above assumes that you have at least nchains threads available in your Julia instance. If you do not, the multiple chains will run sequentially, and you may notice a warning. For more information, see the Turing documentation on sampling multiple chains.\n\n\n\nSince we ran multiple chains, we may as well do a spot check to make sure each chain converges around similar points.\n\nplot(chain)\n\n\n\n\nLooks good!\nWe can also use the corner function from MCMCChains to show the distributions of the various parameters of our multinomial logistic regression. The corner function requires MCMCChains and StatsPlots.\n\n# Only plotting the first 3 coefficients due to a bug in Plots.jl\ncorner(\n    chain,\n    MCMCChains.namesingroup(chain, :coefficients_versicolor)[1:3];\n)\n\n\n\n\n\n# Only plotting the first 3 coefficients due to a bug in Plots.jl\ncorner(\n    chain,\n    MCMCChains.namesingroup(chain, :coefficients_virginica)[1:3];\n)\n\n\n\n\nFortunately the corner plots appear to demonstrate unimodal distributions for each of our parameters, so it should be straightforward to take the means of each parameter’s sampled values to estimate our model to make predictions.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Multinomial Logistic Regression"
    ]
  },
  {
    "objectID": "tutorials/multinomial-logistic-regression/index.html#making-predictions",
    "href": "tutorials/multinomial-logistic-regression/index.html#making-predictions",
    "title": "Multinomial Logistic Regression",
    "section": "Making Predictions",
    "text": "Making Predictions\nHow do we test how well the model actually predicts which of the three classes an iris flower belongs to? We need to build a prediction function that takes the test dataset and runs it through the average parameter calculated during sampling.\nThe prediction function below takes a Matrix and a Chains object. It computes the mean of the sampled parameters and calculates the species with the highest probability for each observation. Note that we do not have to evaluate the softmax function since it does not affect the order of its inputs.\n\nfunction prediction(x::Matrix, chain)\n    # Pull the means from each parameter's sampled values in the chain.\n    intercept_versicolor = mean(chain, :intercept_versicolor)\n    intercept_virginica = mean(chain, :intercept_virginica)\n    coefficients_versicolor = [\n        mean(chain, k) for k in MCMCChains.namesingroup(chain, :coefficients_versicolor)\n    ]\n    coefficients_virginica = [\n        mean(chain, k) for k in MCMCChains.namesingroup(chain, :coefficients_virginica)\n    ]\n\n    # Compute the index of the species with the highest probability for each observation.\n    values_versicolor = intercept_versicolor .+ x * coefficients_versicolor\n    values_virginica = intercept_virginica .+ x * coefficients_virginica\n    species_indices = [\n        argmax((0, x, y)) for (x, y) in zip(values_versicolor, values_virginica)\n    ]\n\n    return species_indices\nend;\n\nLet’s see how we did! We run the test matrix through the prediction function, and compute the accuracy for our prediction.\n\n# Make the predictions.\npredictions = prediction(test_features, chain)\n\n# Calculate accuracy for our test set.\nmean(predictions .== testset[!, :Species_index])\n\n0.9066666666666666\n\n\nPerhaps more important is to see the accuracy per class.\n\nfor s in 1:3\n    rows = testset[!, :Species_index] .== s\n    println(\"Number of `\", species[s], \"`: \", count(rows))\n    println(\n        \"Percentage of `\",\n        species[s],\n        \"` predicted correctly: \",\n        mean(predictions[rows] .== testset[rows, :Species_index]),\n    )\nend\n\nNumber of `setosa`: 26\nPercentage of `setosa` predicted correctly: 1.0\nNumber of `versicolor`: 26\nPercentage of `versicolor` predicted correctly: 0.7692307692307693\nNumber of `virginica`: 23\nPercentage of `virginica` predicted correctly: 0.9565217391304348\n\n\nThis tutorial has demonstrated how to use Turing to perform Bayesian multinomial logistic regression.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Multinomial Logistic Regression"
    ]
  },
  {
    "objectID": "tutorials/gaussian-mixture-models/index.html",
    "href": "tutorials/gaussian-mixture-models/index.html",
    "title": "Gaussian Mixture Models",
    "section": "",
    "text": "The following tutorial illustrates the use of Turing for an unsupervised task, namely, clustering data using a Bayesian mixture model. The aim of this task is to infer a latent grouping (hidden structure) from unlabelled data.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "tutorials/gaussian-mixture-models/index.html#synthetic-data",
    "href": "tutorials/gaussian-mixture-models/index.html#synthetic-data",
    "title": "Gaussian Mixture Models",
    "section": "Synthetic Data",
    "text": "Synthetic Data\nWe generate a synthetic dataset of \\(N = 60\\) two-dimensional points \\(x_i \\in \\mathbb{R}^2\\) drawn from a Gaussian mixture model. For simplicity, we use \\(K = 2\\) clusters with\n\nequal weights, i.e., we use mixture weights \\(w = [0.5, 0.5]\\), and\nisotropic Gaussian distributions of the points in each cluster.\n\nMore concretely, we use the Gaussian distributions \\(\\mathcal{N}([\\mu_k, \\mu_k]^\\mathsf{T}, I)\\) with parameters \\(\\mu_1 = -3.5\\) and \\(\\mu_2 = 0.5\\).\n\nusing Distributions\nusing FillArrays\nusing StatsPlots\n\nusing LinearAlgebra\nusing Random\n\n# Set a random seed.\nRandom.seed!(3)\n\n# Define Gaussian mixture model.\nw = [0.5, 0.5]\nμ = [-2.0, 2.0]\nmixturemodel = MixtureModel([MvNormal(Fill(μₖ, 2), 0.2 * I) for μₖ in μ], w)\n\n# We draw the data points.\nN = 30\nx = rand(mixturemodel, N);\n\nThe following plot shows the dataset.\n\nscatter(x[1, :], x[2, :]; legend=false, title=\"Synthetic Dataset\")",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "tutorials/gaussian-mixture-models/index.html#gaussian-mixture-model-in-turing",
    "href": "tutorials/gaussian-mixture-models/index.html#gaussian-mixture-model-in-turing",
    "title": "Gaussian Mixture Models",
    "section": "Gaussian Mixture Model in Turing",
    "text": "Gaussian Mixture Model in Turing\nWe are interested in recovering the grouping from the dataset. More precisely, we want to infer the mixture weights, the parameters \\(\\mu_1\\) and \\(\\mu_2\\), and the assignment of each datum to a cluster for the generative Gaussian mixture model.\nIn a Bayesian Gaussian mixture model with \\(K\\) components, each data point \\(x_i\\) (\\(i = 1,\\ldots,N\\)) is generated according to the following generative process. First we draw the model parameters: the cluster means \\(\\mu_k\\) and the mixture weights \\(w\\) that determine the probability of each cluster. We use standard normal distributions as priors for \\(\\mu_k\\) and a Dirichlet distribution with parameters \\(\\alpha_1 = \\cdots = \\alpha_K = 1\\) as prior for \\(w\\): \\[\n\\begin{aligned}\n\\mu_k &\\sim \\mathcal{N}(0, 1) \\qquad (k = 1,\\ldots,K)\\\\\nw &\\sim \\operatorname{Dirichlet}(\\alpha_1, \\ldots, \\alpha_K)\n\\end{aligned}\n\\] After having constructed all the necessary model parameters, we can generate an observation by first selecting one of the clusters \\[\nz_i \\sim \\operatorname{Categorical}(w) \\qquad (i = 1,\\ldots,N),\n\\] and then drawing the datum accordingly, i.e., in our example drawing \\[\nx_i \\sim \\mathcal{N}([\\mu_{z_i}, \\mu_{z_i}]^\\mathsf{T}, I) \\qquad (i=1,\\ldots,N).\n\\] For more details on Gaussian mixture models, refer to Chapter 9 of Christopher M. Bishop, Pattern Recognition and Machine Learning.\nWe specify the model in Turing:\n\nusing Turing\n\n@model function gaussian_mixture_model(x)\n    # Draw the parameters for each of the K=2 clusters from a standard normal distribution.\n    K = 2\n    μ ~ MvNormal(Zeros(K), I)\n\n    # Draw the weights for the K clusters from a Dirichlet distribution with parameters αₖ = 1.\n    w ~ Dirichlet(K, 1.0)\n    # Alternatively, one could use a fixed set of weights.\n    # w = fill(1/K, K)\n\n    # Construct categorical distribution of assignments.\n    distribution_assignments = Categorical(w)\n\n    # Construct multivariate normal distributions of each cluster.\n    D, N = size(x)\n    distribution_clusters = [MvNormal(Fill(μₖ, D), I) for μₖ in μ]\n\n    # Draw assignments for each datum and generate it from the multivariate normal distribution.\n    k = Vector{Int}(undef, N)\n    for i in 1:N\n        k[i] ~ distribution_assignments\n        x[:, i] ~ distribution_clusters[k[i]]\n    end\n\n    return k\nend\n\nmodel = gaussian_mixture_model(x);\n\nWe run a MCMC simulation to obtain an approximation of the posterior distribution of the parameters \\(\\mu\\) and \\(w\\) and assignments \\(k\\). We use a Gibbs sampler that combines a particle Gibbs sampler for the discrete parameters (assignments \\(k\\)) and a Hamiltonian Monte Carlo sampler for the continuous parameters (\\(\\mu\\) and \\(w\\)). We generate multiple chains in parallel using multi-threading.\n\nsampler = Gibbs(:k =&gt; PG(100), (:μ, :w) =&gt; HMC(0.05, 10))\nnsamples = 150\nnchains = 4\nburn = 10\nchains = sample(model, sampler, MCMCThreads(), nsamples, nchains, discard_initial = burn);\n\n\n\n\n\n\n\nWarningSampling With Multiple Threads\n\n\n\nThe sample() call above assumes that you have at least two threads available in your Julia instance. If you do not, the multiple chains will run sequentially, and you may notice a warning. For more information, see the Turing documentation on sampling multiple chains.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "tutorials/gaussian-mixture-models/index.html#inferred-mixture-model",
    "href": "tutorials/gaussian-mixture-models/index.html#inferred-mixture-model",
    "title": "Gaussian Mixture Models",
    "section": "Inferred Mixture Model",
    "text": "Inferred Mixture Model\nAfter sampling we can visualise the trace and density of the parameters of interest.\nWe consider the samples of the location parameters \\(\\mu_1\\) and \\(\\mu_2\\) for the two clusters.\n\nplot(chains[[\"μ[1]\", \"μ[2]\"]]; legend=true)\n\n\n\n\nFrom the plots above, we can see that the chains have converged to seemingly different values for the parameters \\(\\mu_1\\) and \\(\\mu_2\\). However, these actually represent the same solution: it does not matter whether we assign \\(\\mu_1\\) to the first cluster and \\(\\mu_2\\) to the second, or vice versa, since the resulting sum is the same. (In principle it is also possible for the parameters to swap places within a single chain, although this does not happen in this example.) For more information see the Stan documentation, or Bishop’s book, where the concept of identifiability is discussed.\nHaving \\(\\mu_1\\) and \\(\\mu_2\\) swap can complicate the interpretation of the results, especially when different chains converge to different assignments. One solution here is to enforce an ordering on our \\(\\mu\\) vector, requiring \\(\\mu_k \\geq \\mu_{k-1}\\) for all \\(k\\). Bijectors.jl provides a convenient function, ordered(), which can be applied to a (continuous multivariate) distribution to enforce this:\n\nusing Bijectors: ordered\n\n@model function gaussian_mixture_model_ordered(x)\n    # Draw the parameters for each of the K=2 clusters from a standard normal distribution.\n    K = 2\n    μ ~ ordered(MvNormal(Zeros(K), I))\n    # Draw the weights for the K clusters from a Dirichlet distribution with parameters αₖ = 1.\n    w ~ Dirichlet(K, 1.0)\n    # Alternatively, one could use a fixed set of weights.\n    # w = fill(1/K, K)\n    # Construct categorical distribution of assignments.\n    distribution_assignments = Categorical(w)\n    # Construct multivariate normal distributions of each cluster.\n    D, N = size(x)\n    distribution_clusters = [MvNormal(Fill(μₖ, D), I) for μₖ in μ]\n    # Draw assignments for each datum and generate it from the multivariate normal distribution.\n    k = Vector{Int}(undef, N)\n    for i in 1:N\n        k[i] ~ distribution_assignments\n        x[:, i] ~ distribution_clusters[k[i]]\n    end\n    return k\nend\n\nmodel = gaussian_mixture_model_ordered(x);\n\nNow, re-running our model, we can see that the assigned means are consistent between chains:\n\nchains = sample(model, sampler, MCMCThreads(), nsamples, nchains, discard_initial = burn);\n\n\nplot(chains[[\"μ[1]\", \"μ[2]\"]]; legend=true)\n\n\n\n\nWe also inspect the samples of the mixture weights \\(w\\).\n\nplot(chains[[\"w[1]\", \"w[2]\"]]; legend=true)\n\n\n\n\nAs the distributions of the samples for the parameters \\(\\mu_1\\), \\(\\mu_2\\), \\(w_1\\), and \\(w_2\\) are unimodal, we can safely visualise the density region of our model using the average values.\n\n# Model with mean of samples as parameters.\nμ_mean = [mean(chains, \"μ[$i]\") for i in 1:2]\nw_mean = [mean(chains, \"w[$i]\") for i in 1:2]\nmixturemodel_mean = MixtureModel([MvNormal(Fill(μₖ, 2), I) for μₖ in μ_mean], w_mean)\ncontour(\n    range(-7.5, 3; length=1_000),\n    range(-6.5, 3; length=1_000),\n    (x, y) -&gt; logpdf(mixturemodel_mean, [x, y]);\n    widen=false,\n)\nscatter!(x[1, :], x[2, :]; legend=false, title=\"Synthetic Dataset\")",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "tutorials/gaussian-mixture-models/index.html#inferred-assignments",
    "href": "tutorials/gaussian-mixture-models/index.html#inferred-assignments",
    "title": "Gaussian Mixture Models",
    "section": "Inferred Assignments",
    "text": "Inferred Assignments\nFinally, we can inspect the assignments of the data points inferred using Turing. As we can see, the dataset is partitioned into two distinct groups.\n\nassignments = [mean(chains, \"k[$i]\") for i in 1:N]\nscatter(\n    x[1, :],\n    x[2, :];\n    legend=false,\n    title=\"Assignments on Synthetic Dataset\",\n    zcolor=assignments,\n)",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "tutorials/gaussian-mixture-models/index.html#marginalizing-out-the-assignments",
    "href": "tutorials/gaussian-mixture-models/index.html#marginalizing-out-the-assignments",
    "title": "Gaussian Mixture Models",
    "section": "Marginalizing Out The Assignments",
    "text": "Marginalizing Out The Assignments\nWe can write out the marginal posterior of (continuous) \\(w, \\mu\\) by summing out the influence of our (discrete) assignments \\(z_i\\) from our likelihood:\n\\[p(y \\mid w, \\mu ) = \\sum_{k=1}^K w_k p_k(y \\mid \\mu_k)\\]\nIn our case, this gives us:\n\\[p(y \\mid w, \\mu) = \\sum_{k=1}^K w_k \\cdot \\operatorname{MvNormal}(y \\mid \\mu_k, I)\\]\n\nMarginalizing By Hand\nWe could implement the above version of the Gaussian mixture model in Turing as follows.\nFirst, Turing uses log-probabilities, so the likelihood above must be converted into log-space:\n\\[\\log \\left( p(y \\mid w, \\mu) \\right) = \\text{logsumexp} \\left[\\log (w_k) + \\log(\\operatorname{MvNormal}(y \\mid \\mu_k, I)) \\right]\\]\nWhere we sum the components with logsumexp from the LogExpFunctions.jl package. The manually incremented likelihood can be added to the log-probability with @addlogprob!, giving us the following model:\n\nusing LogExpFunctions\n\n@model function gmm_marginalized(x)\n    K = 2\n    D, N = size(x)\n    μ ~ ordered(MvNormal(Zeros(K), I))\n    w ~ Dirichlet(K, 1.0)\n    dists = [MvNormal(Fill(μₖ, D), I) for μₖ in μ]\n    for i in 1:N\n        lvec = Vector(undef, K)\n        for k in 1:K\n            lvec[k] = (w[k] + logpdf(dists[k], x[:, i]))\n        end\n        @addlogprob! logsumexp(lvec)\n    end\nend\n\n\n\n\n\n\n\nWarningManually Incrementing Probablity\n\n\n\nWhen possible, use of @addlogprob! should be avoided, as it exists outside the usual structure of a Turing model. In most cases, a custom distribution should be used instead.\nThe next section demonstrates the preferred method: using the MixtureModel distribution we have seen already to perform the marginalization automatically.\n\n\n\n\nMarginalizing For Free With Distribution.jl’s MixtureModel Implementation\nWe can use Turing’s ~ syntax with anything that Distributions.jl provides logpdf and rand methods for. It turns out that the MixtureModel distribution it provides has, as its logpdf method, logpdf(MixtureModel([Component_Distributions], weight_vector), Y), where Y can be either a single observation or vector of observations.\nIn fact, Distributions.jl provides many convenient constructors for mixture models, allowing further simplification in common special cases.\nFor example, when mixtures distributions are of the same type, one can write: ~ MixtureModel(Normal, [(μ1, σ1), (μ2, σ2)], w), or when the weight vector is known to allocate probability equally, it can be ommited.\nThe logpdf implementation for a MixtureModel distribution is exactly the marginalization defined above, and so our model can be simplified to:\n\n@model function gmm_marginalized(x)\n    K = 2\n    D, _ = size(x)\n    μ ~ ordered(MvNormal(Zeros(K), I))\n    w ~ Dirichlet(K, 1.0)\n    x ~ MixtureModel([MvNormal(Fill(μₖ, D), I) for μₖ in μ], w)\nend\nmodel = gmm_marginalized(x);\n\nAs we have summed out the discrete components, we can perform inference using NUTS() alone.\n\nsampler = NUTS()\nchains = sample(model, sampler, MCMCThreads(), nsamples, nchains; discard_initial = burn);\n\nNUTS() significantly outperforms our compositional Gibbs sampler, in large part because our model is now Rao-Blackwellized thanks to the marginalization of our assignment parameter.\n\nplot(chains[[\"μ[1]\", \"μ[2]\"]], legend=true)",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "tutorials/gaussian-mixture-models/index.html#inferred-assignments-with-the-marginalized-model",
    "href": "tutorials/gaussian-mixture-models/index.html#inferred-assignments-with-the-marginalized-model",
    "title": "Gaussian Mixture Models",
    "section": "Inferred Assignments With The Marginalized Model",
    "text": "Inferred Assignments With The Marginalized Model\nAs we have summed over possible assignments, the latent parameter representing the assignments is no longer available in our chain. This is not a problem, however, as given any fixed sample \\((\\mu, w)\\), the assignment probability \\(p(z_i \\mid y_i)\\) can be recovered using Bayes’s theorme:\n\\[p(z_i \\mid y_i) = \\frac{p(y_i \\mid z_i) p(z_i)}{\\sum_{k = 1}^K \\left(p(y_i \\mid z_i) p(z_i) \\right)}\\]\nThis quantity can be computed for every \\(p(z = z_i \\mid y_i)\\), resulting in a probability vector, which is then used to sample posterior predictive assignments from a categorial distribution. For details on the mathematics here, see the Stan documentation on latent discrete parameters.\n\nfunction sample_class(xi, dists, w)\n    lvec = [(logpdf(d, xi) + log(w[i])) for (i, d) in enumerate(dists)]\n    rand(Categorical(softmax(lvec)))\nend\n\n@model function gmm_recover(x)\n    K = 2\n    D, N =  size(x)\n    μ ~ ordered(MvNormal(Zeros(K), I))\n    w ~ Dirichlet(K, 1.0)\n    dists = [MvNormal(Fill(μₖ, D), I) for μₖ in μ]\n    x ~ MixtureModel(dists, w)\n    # Return assignment draws for each datapoint.\n    return [sample_class(x[:, i], dists, w) for i in 1:N]\nend\n\nWe sample from this model as before:\n\nmodel = gmm_recover(x)\nchains = sample(model, sampler, MCMCThreads(), nsamples, nchains, discard_initial = burn);\n\nGiven a sample from the marginalized posterior, these assignments can be recovered with:\n\nassignments = mean(returned(gmm_recover(x), chains));\n\n\nscatter(\n    x[1, :],\n    x[2, :];\n    legend=false,\n    title=\"Assignments on Synthetic Dataset - Recovered\",\n    zcolor=assignments,\n)",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "tutorials/bayesian-time-series-analysis/index.html",
    "href": "tutorials/bayesian-time-series-analysis/index.html",
    "title": "Bayesian Time Series Analysis",
    "section": "",
    "text": "In time series analysis we are often interested in understanding how various real-life circumstances impact our quantity of interest. These can be, for instance, season, day of week, or time of day. To analyse this it is useful to decompose time series into simpler components (corresponding to relevant circumstances) and infer their relevance. In this tutorial we are going to use Turing for time series analysis and learn about useful ways to decompose time series.\n\nModelling time series\nBefore we start coding, let us talk about what exactly we mean with time series decomposition. In a nutshell, it is a divide-and-conquer approach where we express a time series as a sum or a product of simpler series. For instance, the time series \\(f(t)\\) can be decomposed into a sum of \\(n\\) components\n\\[f(t) = \\sum_{i=1}^n f_i(t),\\]\nor we can decompose \\(g(t)\\) into a product of \\(m\\) components\n\\[g(t) = \\prod_{i=1}^m g_i(t).\\]\nWe refer to this as additive or multiplicative decomposition respectively. This type of decomposition allows us to reason about individual components separately, which simplifies encoding prior information and interpreting model predictions. Two common components are trends, which represent the overall change of the time series (often assumed to be linear), and cyclic effects, which contribute periodic oscillations around the trend. Let us simulate some data with an additive linear trend and oscillating effects.\n\nusing Turing\nusing FillArrays\nusing StatsPlots\n\nusing LinearAlgebra\nusing Random\nusing Statistics\n\nRandom.seed!(12345)\n\ntrue_sin_freq = 2\ntrue_sin_amp = 5\ntrue_cos_freq = 7\ntrue_cos_amp = 2.5\ntmax = 10\nβ_true = 2\nα_true = -1\ntt = 0:0.05:tmax\nf₁(t) = α_true + β_true * t\nf₂(t) = true_sin_amp * sinpi(2 * t * true_sin_freq / tmax)\nf₃(t) = true_cos_amp * cospi(2 * t * true_cos_freq / tmax)\nf(t) = f₁(t) + f₂(t) + f₃(t)\n\nplot(f, tt; label=\"f(t)\", title=\"Observed time series\", legend=:topleft, linewidth=3)\nplot!(\n    [f₁, f₂, f₃],\n    tt;\n    label=[\"f₁(t)\" \"f₂(t)\" \"f₃(t)\"],\n    style=[:dot :dash :dashdot],\n    linewidth=1,\n)\n\n\n\n\nEven though we use simple components, combining them can give rise to fairly complex time series. In this time series, cyclic effects are just added on top of the trend. If we instead multiply the components the cyclic effects cause the series to oscillate between larger and larger values, since they get scaled by the trend.\n\ng(t) = f₁(t) * f₂(t) * f₃(t)\n\nplot(g, tt; label=\"f(t)\", title=\"Observed time series\", legend=:topleft, linewidth=3)\nplot!([f₁, f₂, f₃], tt; label=[\"f₁(t)\" \"f₂(t)\" \"f₃(t)\"], linewidth=1)\n\n\n\n\nUnlike \\(f\\), \\(g\\) oscillates around \\(0\\) since it is being multiplied with sines and cosines. To let a multiplicative decomposition oscillate around the trend we could define it as \\(\\tilde{g}(t) = f₁(t) * (1 + f₂(t)) * (1 + f₃(t)),\\) but for convenience we will leave it as is. The inference machinery is the same for both cases.\n\n\nModel fitting\nHaving discussed time series decomposition, let us fit a model to the time series above and recover the true parameters. Before building our model, we standardise the time axis to \\([0, 1]\\) and subtract the max of the time series. This helps convergence while maintaining interpretability and the correct scales for the cyclic components.\n\nσ_true = 0.35\nt = collect(tt[begin:3:end])\nt_min, t_max = extrema(t)\nx = (t .- t_min) ./ (t_max - t_min)\nyf = f.(t) .+ σ_true .* randn(size(t))\nyf_max = maximum(yf)\nyf = yf .- yf_max\n\nscatter(x, yf; title=\"Standardised data\", legend=false)\n\n\n\n\nLet us now build our model. We want to assume a linear trend, and cyclic effects. Encoding a linear trend is easy enough, but what about cyclical effects? We will take a scattergun approach, and create multiple cyclical features using both sine and cosine functions and let our inference machinery figure out which to keep. To do this, we define how long a one period should be, and create features in reference to said period. How long a period should be is problem dependent, but as an example let us say it is \\(1\\) year. If we then find evidence for a cyclic effect with a frequency of 2, that would mean a biannual effect. A frequency of 4 would mean quarterly etc. Since we are using synthetic data, we are simply going to let the period be 1, which is the entire length of the time series.\n\nfreqs = 1:10\nnum_freqs = length(freqs)\nperiod = 1\ncyclic_features = [sinpi.(2 .* freqs' .* x ./ period) cospi.(2 .* freqs' .* x ./ period)]\n\nplot_freqs = [1, 3, 5]\nfreq_ptl = plot(\n    cyclic_features[:, plot_freqs];\n    label=permutedims([\"sin(2π$(f)x)\" for f in plot_freqs]),\n    title=\"Cyclical features subset\",\n)\n\n\n\n\nHaving constructed the cyclical features, we can finally build our model. The model we will implement looks like this\n\\[\nf(t) = \\alpha + \\beta_t t + \\sum_{i=1}^F \\beta_{\\sin{},i} \\sin{}(2\\pi f_i t) + \\sum_{i=1}^F \\beta_{\\cos{},i} \\cos{}(2\\pi f_i t),\n\\]\nwith a Gaussian likelihood \\(y \\sim \\mathcal{N}(f(t), \\sigma^2)\\). For convenience we are treating the cyclical feature weights \\(\\beta_{\\sin{},i}\\) and \\(\\beta_{\\cos{},i}\\) the same in code and weight them with \\(\\beta_c\\). And just because it is so easy, we parameterise our model with the operation with which to apply the cyclic effects. This lets us use the exact same code for both additive and multiplicative models. Finally, we plot prior predictive samples to make sure our priors make sense.\n\n@model function decomp_model(t, c, op)\n    α ~ Normal(0, 10)\n    βt ~ Normal(0, 2)\n    βc ~ MvNormal(Zeros(size(c, 2)), I)\n    σ ~ truncated(Normal(0, 0.1); lower=0)\n\n    cyclic = c * βc\n    trend = α .+ βt .* t\n    μ = op(trend, cyclic)\n    y ~ MvNormal(μ, σ^2 * I)\n    return (; trend, cyclic)\nend\n\ny_prior_samples = mapreduce(hcat, 1:100) do _\n    rand(decomp_model(t, cyclic_features, +)).y\nend\nplot(t, y_prior_samples; linewidth=1, alpha=0.5, color=1, label=\"\", title=\"Prior samples\")\nscatter!(t, yf; color=2, label=\"Data\")\n\n\n\n\nWith the model specified and with a reasonable prior we can now let Turing decompose the time series for us!\n\nusing MCMCChains: get_sections\n\nfunction mean_ribbon(samples)\n    qs = quantile(samples)\n    low = qs[:, Symbol(\"2.5%\")]\n    up = qs[:, Symbol(\"97.5%\")]\n    m = mean(samples)[:, :mean]\n    return m, (m - low, up - m)\nend\n\nfunction plot_fit(x, y, decomp, ymax)\n    trend = mapreduce(x -&gt; x.trend, hcat, decomp)\n    cyclic = mapreduce(x -&gt; x.cyclic, hcat, decomp)\n\n    trend_plt = plot(\n        x,\n        trend .+ ymax;\n        color=1,\n        label=nothing,\n        alpha=0.2,\n        title=\"Trend\",\n        xlabel=\"Time\",\n        ylabel=\"f₁(t)\",\n    )\n    ls = [ones(length(t)) t] \\ y\n    α̂, β̂ = ls[1], ls[2:end]\n    plot!(\n        trend_plt,\n        t,\n        α̂ .+ t .* β̂ .+ ymax;\n        label=\"Least squares trend\",\n        color=5,\n        linewidth=4,\n    )\n\n    scatter!(trend_plt, x, y .+ ymax; label=nothing, color=2, legend=:topleft)\n    cyclic_plt = plot(\n        x,\n        cyclic;\n        color=1,\n        label=nothing,\n        alpha=0.2,\n        title=\"Cyclic effect\",\n        xlabel=\"Time\",\n        ylabel=\"f₂(t)\",\n    )\n    return trend_plt, cyclic_plt\nend\n\nmodel = decomp_model(x, cyclic_features, +) | (; y = yf)\nchain = sample(model, NUTS(), 2000, progress=false)\nyf_samples = predict(decondition(model), chain)\nm, conf = mean_ribbon(yf_samples)\npredictive_plt = plot(\n    t,\n    m .+ yf_max;\n    ribbon=conf,\n    label=\"Posterior density\",\n    title=\"Posterior decomposition\",\n    xlabel=\"Time\",\n    ylabel=\"f(t)\",\n)\nscatter!(predictive_plt, t, yf .+ yf_max; color=2, label=\"Data\", legend=:topleft)\n\ndecomp = returned(model, chain)\ndecomposed_plt = plot_fit(t, yf, decomp, yf_max)\nplot(predictive_plt, decomposed_plt...; layout=(3, 1), size=(700, 1000))\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.0125\n\n\n\n\n\n\n\nInference is successful and the posterior beautifully captures the data. We see that the least squares linear fit deviates somewhat from the posterior trend. Since our model takes cyclic effects into account separately, we get a better estimate of the true overall trend than if we would have just fitted a line. But what frequency content did the model identify?\n\nfunction plot_cyclic_features(βsin, βcos)\n    labels = reshape([\"freq = $i\" for i in freqs], 1, :)\n    colors = collect(freqs)'\n    style = reshape([i &lt;= 10 ? :solid : :dash for i in 1:length(labels)], 1, :)\n    sin_features_plt = density(\n        βsin[:, :, 1];\n        title=\"Sine features posterior\",\n        label=labels,\n        ylabel=\"Density\",\n        xlabel=\"Weight\",\n        color=colors,\n        linestyle=style,\n        legend=nothing,\n    )\n    cos_features_plt = density(\n        βcos[:, :, 1];\n        title=\"Cosine features posterior\",\n        ylabel=\"Density\",\n        xlabel=\"Weight\",\n        label=nothing,\n        color=colors,\n        linestyle=style,\n    )\n\n    return seasonal_features_plt = plot(\n        sin_features_plt,\n        cos_features_plt;\n        layout=(2, 1),\n        size=(800, 600),\n        legend=:outerright,\n    )\nend\n\nβc = Array(group(chain, :βc))\nplot_cyclic_features(βc[:, begin:num_freqs, :], βc[:, (num_freqs + 1):end, :])\n\n\n\n\nPlotting the posterior over the cyclic features reveals that the model managed to extract the true frequency content.\nSince we wrote our model to accept a combining operator, we can easily run the same analysis for a multiplicative model.\n\nyg = g.(t) .+ σ_true .* randn(size(t))\n\ny_prior_samples = mapreduce(hcat, 1:100) do _\n    rand(decomp_model(t, cyclic_features, .*)).y\nend\nplot(t, y_prior_samples; linewidth=1, alpha=0.5, color=1, label=\"\", title=\"Prior samples\")\nscatter!(t, yf; color=2, label=\"Data\")\n\n\n\n\n\nmodel = decomp_model(x, cyclic_features, .*) | (; y = yg)\nchain = sample(model, NUTS(), 2000, progress=false)\nyg_samples = predict(decondition(model), chain)\nm, conf = mean_ribbon(yg_samples)\npredictive_plt = plot(\n    t,\n    m;\n    ribbon=conf,\n    label=\"Posterior density\",\n    title=\"Posterior decomposition\",\n    xlabel=\"Time\",\n    ylabel=\"g(t)\",\n)\nscatter!(predictive_plt, t, yg; color=2, label=\"Data\", legend=:topleft)\n\ndecomp = returned(model, chain)\ndecomposed_plt = plot_fit(t, yg, decomp, 0)\nplot(predictive_plt, decomposed_plt...; layout=(3, 1), size=(700, 1000))\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.003125\n\n\n\n\n\n\n\nThe model fits! What about the inferred cyclic components?\n\nβc = Array(group(chain, :βc))\nplot_cyclic_features(βc[:, begin:num_freqs, :], βc[:, (num_freqs + 1):end, :])\n\n\n\n\nWhile a multiplicative model does manage to fit the data, it does not recover the true parameters for this dataset.\n\n\nWrapping up\nIn this tutorial we have seen how to implement and fit time series models using additive and multiplicative decomposition. We also saw how to visualise the model fit, and how to interpret learned cyclical components.\n\n\n\n\n Back to top",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Time Series Analysis"
    ]
  },
  {
    "objectID": "tutorials/variational-inference/index.html",
    "href": "tutorials/variational-inference/index.html",
    "title": "Variational Inference",
    "section": "",
    "text": "This post will look at variational inference (VI), an optimisation approach to approximate Bayesian inference, and how to use it in Turing.jl as an alternative to other approaches such as MCMC. This post will focus on the usage of VI in Turing rather than the principles and theory underlying VI. If you are interested in understanding the mathematics you can checkout our write-up or any other resource online (there are a lot of great ones).\nLet’s start with a minimal example. Consider a Turing.Model, which we denote as model. Approximating the posterior associated with model via VI is as simple as\nThus, it’s no more work than standard MCMC sampling in Turing. The default algorithm uses stochastic gradient descent to minimise the (exclusive) KL divergence. This approach is commonly referred to as automatic differentiation variational inference (ADVI)1, stochastic gradient VI2, and black-box variational inference3 with the reparameterization gradient456.\nTo get a bit more into what we can do with VI, let’s look at a more concrete example. We will reproduce the tutorial on Bayesian linear regression using VI instead of MCMC. After that, we will discuss how to customise the behaviour of vi for more advanced usage.\nLet’s first import the relevant packages:\nusing Random\nusing Turing\nusing Turing: Variational\nusing AdvancedVI\nusing Plots\n\nRandom.seed!(42);",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Variational Inference"
    ]
  },
  {
    "objectID": "tutorials/variational-inference/index.html#bayesian-linear-regression-example",
    "href": "tutorials/variational-inference/index.html#bayesian-linear-regression-example",
    "title": "Variational Inference",
    "section": "Bayesian Linear Regression Example",
    "text": "Bayesian Linear Regression Example\nLet’s start by setting up our example. We will re-use the Bayesian linear regression example. As we’ll see, there is really no additional work required to apply variational inference to a more complex Model.\n\nusing FillArrays\nusing RDatasets\n\nusing LinearAlgebra\n\n# Import the \"Default\" dataset.\ndata = RDatasets.dataset(\"datasets\", \"mtcars\");\n\n# Show the first six rows of the dataset.\nfirst(data, 6)\n\n6×12 DataFrame\n\n\n\nRow\nModel\nMPG\nCyl\nDisp\nHP\nDRat\nWT\nQSec\nVS\nAM\nGear\nCarb\n\n\n\nString31\nFloat64\nInt64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nInt64\nInt64\nInt64\nInt64\n\n\n\n\n1\nMazda RX4\n21.0\n6\n160.0\n110\n3.9\n2.62\n16.46\n0\n1\n4\n4\n\n\n2\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.9\n2.875\n17.02\n0\n1\n4\n4\n\n\n3\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.32\n18.61\n1\n1\n4\n1\n\n\n4\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n5\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\n6\nValiant\n18.1\n6\n225.0\n105\n2.76\n3.46\n20.22\n1\n0\n3\n1\n\n\n\n\n\n\n\n# Function to split samples.\nfunction split_data(df, at=0.70)\n    r = size(df, 1)\n    index = Int(round(r * at))\n    train = df[1:index, :]\n    test = df[(index + 1):end, :]\n    return train, test\nend\n\n# A handy helper function to rescale our dataset.\nfunction standardise(x)\n    return (x .- mean(x; dims=1)) ./ std(x; dims=1)\nend\n\nfunction standardise(x, orig)\n    return (x .- mean(orig; dims=1)) ./ std(orig; dims=1)\nend\n\n# Another helper function to unstandardize our datasets.\nfunction unstandardize(x, orig)\n    return x .* std(orig; dims=1) .+ mean(orig; dims=1)\nend\n\nfunction unstandardize(x, mean_train, std_train)\n    return x .* std_train .+ mean_train\nend\n\nunstandardize (generic function with 2 methods)\n\n\n\n# Remove the model column.\nselect!(data, Not(:Model))\n\n# Split our dataset 70%/30% into training/test sets.\ntrain, test = split_data(data, 0.7)\ntrain_unstandardized = copy(train)\n\n# Standardise both datasets.\nstd_train = standardise(Matrix(train))\nstd_test = standardise(Matrix(test), Matrix(train))\n\n# Save dataframe versions of our dataset.\ntrain_cut = DataFrame(std_train, names(data))\ntest_cut = DataFrame(std_test, names(data))\n\n# Create our labels. These are the values we are trying to predict.\ntrain_label = train_cut[:, :MPG]\ntest_label = test_cut[:, :MPG]\n\n# Get the list of columns to keep.\nremove_names = filter(x -&gt; !in(x, [\"MPG\"]), names(data))\n\n# Filter the test and train sets.\ntrain = Matrix(train_cut[:, remove_names]);\ntest = Matrix(test_cut[:, remove_names]);\n\n\n# Bayesian linear regression.\n@model function linear_regression(x, y, n_obs, n_vars, ::Type{T}=Vector{Float64}) where {T}\n    # Set variance prior.\n    σ² ~ truncated(Normal(0, 100); lower=0)\n\n    # Set intercept prior.\n    intercept ~ Normal(0, 3)\n\n    # Set the priors on our coefficients.\n    coefficients ~ MvNormal(Zeros(n_vars), 10.0 * I)\n\n    # Calculate all the mu terms.\n    mu = intercept .+ x * coefficients\n    return y ~ MvNormal(mu, σ² * I)\nend;\n\n\nn_obs, n_vars = size(train)\nm = linear_regression(train, train_label, n_obs, n_vars);",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Variational Inference"
    ]
  },
  {
    "objectID": "tutorials/variational-inference/index.html#basic-usage",
    "href": "tutorials/variational-inference/index.html#basic-usage",
    "title": "Variational Inference",
    "section": "Basic Usage",
    "text": "Basic Usage\nTo run VI, we must first set a variational family. For instance, the most commonly used family is the mean-field Gaussian family. For this, Turing provides functions that automatically construct the initialisation corresponding to the model m:\n\nq_init = q_meanfield_gaussian(m);\n\nvi will automatically recognise the variational family through the type of q_init. Here is a detailed documentation for the constructor:\n\n@doc(Variational.q_meanfield_gaussian)\n\nq_meanfield_gaussian(\n    [rng::Random.AbstractRNG,]\n    model::DynamicPPL.Model;\n    location::Union{Nothing,&lt;:AbstractVector} = nothing,\n    scale::Union{Nothing,&lt;:Diagonal} = nothing,\n    kwargs...\n)\nFind a numerically non-degenerate mean-field Gaussian q for approximating the  target model.\nIf the scale set as nothing, the default value will be a zero-mean Gaussian with a Diagonal scale matrix (the \"mean-field\" approximation) no larger than 0.6*I (covariance of 0.6^2*I). This guarantees that the samples from the initial variational approximation will fall in the range of (-2, 2) with 99.9% probability, which mimics the behavior of the Turing.InitFromUniform() strategy. Whether the default choice is used or not, the scale may be adjusted via q_initialize_scale so that the log-densities of model are finite over the samples from q.\nArguments\n\nmodel: The target DynamicPPL.Model.\n\n\nKeyword Arguments\n\nlocation: The location parameter of the initialization. If nothing, a vector of zeros is used.\n\nscale: The scale parameter of the initialization. If nothing, an identity matrix is used.\n\n\nThe remaining keyword arguments are passed to q_locationscale.\nReturns\n\nq::Bijectors.TransformedDistribution: A AdvancedVI.LocationScale distribution matching the support of model.\n\n\n\n\n\n\n\nAs we can see, the precise initialisation can be customized through the keyword arguments.\nLet’s run VI with the default setting:\n\nn_iters = 1000\nq_avg, info, state = vi(m, q_init, n_iters; show_progress=false);\n\n\n[ Info: The capability of the supplied target `LogDensityProblem` LogDensityProblems.LogDensityOrder{1}() is &gt;= `LogDensityProblems.LogDensityOrder{1}()`. To make use of this, the `adtype` argument for AdvancedVI must be one of `AutoReverseDiff`, `AutoZygote`, `AutoMooncake`, or `AutoEnzyme` in reverse mode.\n\n\n\n\nThe default setting uses the AdvancedVI.RepGradELBO objective, which corresponds to a variant of what is known as automatic differentiation VI7 or stochastic gradient VI8 or black-box VI9 with the reparameterization gradient101112. The default optimiser we use is AdvancedVI.DoWG13 combined with a proximal operator. (The use of proximal operators with VI on a location-scale family is discussed in detail by J. Domke1415 and others16.) We will take a deeper look into the returned values and the keyword arguments in the following subsections. First, here is the full documentation for vi:\n\n@doc(Variational.vi)\n\nvi(\n    [rng::Random.AbstractRNG,]\n    model::DynamicPPL.Model,\n    q,\n    max_iter::Int;\n    adtype::ADTypes.AbstractADType=DEFAULT_ADTYPE,\n    algorithm::AdvancedVI.AbstractVariationalAlgorithm = KLMinRepGradProxDescent(\n        adtype; n_samples=10\n    ),\n    show_progress::Bool = Turing.PROGRESS[],\n    kwargs...\n)\nApproximate the target model via the variational inference algorithm algorithm by starting from the initial variational approximation q. This is a thin wrapper around AdvancedVI.optimize.\nIf the chosen variational inference algorithm operates in an unconstrained space, then the provided initial variational approximation q must be a Bijectors.TransformedDistribution of an unconstrained distribution. For example, the initialization supplied by  q_meanfield_gaussian,q_fullrank_gaussian, q_locationscale.\nThe default algorithm, KLMinRepGradProxDescent (relevant docs), assumes q uses AdvancedVI.MvLocationScale, which can be constructed by invoking q_fullrank_gaussian or q_meanfield_gaussian. For other variational families, refer to the documentation of AdvancedVI to determine the best algorithm and other options.\nArguments\n\nmodel: The target DynamicPPL.Model.\n\nq: The initial variational approximation.\n\nmax_iter: Maximum number of steps.\n\nAny additional arguments are passed on to AdvancedVI.optimize.\n\n\nKeyword Arguments\n\nadtype: Automatic differentiation backend to be applied to the log-density. The default value for algorithm also uses this backend for differentiating the variational objective.\n\nalgorithm: Variational inference algorithm. The default is KLMinRepGradProxDescent, please refer to AdvancedVI docs for all the options.\n\nshow_progress: Whether to show the progress bar.\n\nunconstrained: Whether to transform the posterior to be unconstrained for running the variational inference algorithm. If true, then the output q will be wrapped into a Bijectors.TransformedDistribution with the transformation matching the support of the posterior. The default value depends on the chosen algorithm.\n\nAny additional keyword arguments are passed on to AdvancedVI.optimize.\n\n\nSee the docs of AdvancedVI.optimize for additional keyword arguments.\nReturns\n\nq: Output variational distribution of algorithm.\n\nstate: Collection of states used by algorithm. This can be used to resume from a past call to vi.\n\ninfo: Information generated while executing algorithm.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Variational Inference"
    ]
  },
  {
    "objectID": "tutorials/variational-inference/index.html#values-returned-by-vi",
    "href": "tutorials/variational-inference/index.html#values-returned-by-vi",
    "title": "Variational Inference",
    "section": "Values Returned by vi",
    "text": "Values Returned by vi\nThe main output of the algorithm is q_avg, the average of the parameters generated by the optimisation algorithm. For computing q_avg, the default setting uses what is known as polynomial averaging17. Usually, q_avg will perform better than the last-iterate q_last, which cana be obtained by disabling averaging:\n\nq_last, _, _ = vi(\n    m,\n    q_init,\n    n_iters;\n    show_progress=false,\n    algorithm=KLMinRepGradDescent(\n        AutoForwardDiff();\n        operator=AdvancedVI.ClipScale(),\n        averager=AdvancedVI.NoAveraging()\n    ),\n);\n\n\n[ Info: The capability of the supplied target `LogDensityProblem` LogDensityProblems.LogDensityOrder{1}() is &gt;= `LogDensityProblems.LogDensityOrder{1}()`. To make use of this, the `adtype` argument for AdvancedVI must be one of `AutoReverseDiff`, `AutoZygote`, `AutoMooncake`, or `AutoEnzyme` in reverse mode.\n\n\n\n\nFor instance, we can compare the ELBO of the two:\n\n@info(\"Objective of q_avg and q_last\",\n    ELBO_q_avg = estimate_objective(AdvancedVI.RepGradELBO(32), q_avg, LogDensityFunction(m)),\n    ELBO_q_last = estimate_objective(AdvancedVI.RepGradELBO(32), q_last, LogDensityFunction(m))\n)\n\n\n┌ Info: Objective of q_avg and q_last\n│   ELBO_q_avg = 52.798415041106054\n└   ELBO_q_last = 61.02384427285732\n\n\n\n\nWe can see that ELBO_q_avg is slightly more optimal.\nNow, info contains information generated during optimisation that could be useful for diagnostics. For the default setting, which is RepGradELBO, it contains the ELBO estimated at each step, which can be plotted as follows:\n\nPlots.plot([i.elbo for i in info], xlabel=\"Iterations\", ylabel=\"ELBO\", label=\"info\")\n\n\n\n\nSince the ELBO is estimated by a small number of samples, it appears noisy. Furthermore, at each step, the ELBO is evaluated on q_last, not q_avg, which is the actual output that we care about. To obtain more accurate ELBO estimates evaluated on q_avg, we have to define a custom callback function.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Variational Inference"
    ]
  },
  {
    "objectID": "tutorials/variational-inference/index.html#custom-callback-functions",
    "href": "tutorials/variational-inference/index.html#custom-callback-functions",
    "title": "Variational Inference",
    "section": "Custom Callback Functions",
    "text": "Custom Callback Functions\nTo inspect the progress of optimisation in more detail, one can define a custom callback function. For example, the following callback function estimates the ELBO on q_avg every 10 steps with a larger number of samples:\n\nusing DynamicPPL: DynamicPPL\nlinked_vi = DynamicPPL.link!!(DynamicPPL.VarInfo(m), m);\n\nfunction callback(; iteration, averaged_params, restructure, kwargs...)\n    if mod(iteration, 10) == 1\n        q_avg = restructure(averaged_params)\n        obj = AdvancedVI.RepGradELBO(128) # 128 samples for ELBO estimation\n        elbo_avg = -estimate_objective(obj, q_avg, LogDensityFunction(m, DynamicPPL.getlogjoint_internal, linked_vi))\n        (elbo_avg = elbo_avg,)\n    else\n        nothing\n    end\nend;\n\nThe NamedTuple returned by callback will be appended to the corresponding entry of info, and it will also be displayed on the progress meter if show_progress is set as true.\nThe custom callback can be supplied to vi as a keyword argument:\n\nq_mf, info_mf, _ = vi(m, q_init, n_iters; show_progress=false, callback=callback);\n\n\n[ Info: The capability of the supplied target `LogDensityProblem` LogDensityProblems.LogDensityOrder{1}() is &gt;= `LogDensityProblems.LogDensityOrder{1}()`. To make use of this, the `adtype` argument for AdvancedVI must be one of `AutoReverseDiff`, `AutoZygote`, `AutoMooncake`, or `AutoEnzyme` in reverse mode.\n\n\n\n\nLet’s plot the result:\n\niters = 1:10:length(info_mf)\nelbo_mf = [i.elbo_avg for i in info_mf[iters]]\nPlots.plot([i.elbo for i in info], xlabel=\"Iterations\", ylabel=\"ELBO\", label=\"info\", linewidth=0.4)\nPlots.plot!(iters, elbo_mf, xlabel=\"Iterations\", ylabel=\"ELBO\", label=\"callback\", ylims=(-200,Inf), linewidth=2)\n\n\n\n\nWe can see that the ELBO values are less noisy and progress more smoothly due to averaging.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Variational Inference"
    ]
  },
  {
    "objectID": "tutorials/variational-inference/index.html#using-different-optimisers",
    "href": "tutorials/variational-inference/index.html#using-different-optimisers",
    "title": "Variational Inference",
    "section": "Using Different Optimisers",
    "text": "Using Different Optimisers\nThe default optimiser we use is a proximal variant of DoWG18. For Gaussian variational families, this works well as a default option. Sometimes, the step size of AdvancedVI.DoWG could be too large, resulting in unstable behaviour. (In this case, we recommend trying AdvancedVI.DoG19) Or, for whatever reason, it might be desirable to use a different optimiser. Our implementation supports any optimiser that implements the Optimisers.jl interface.\nFor instance, let’s try using Optimisers.Adam20, which is a popular choice. Since AdvancedVI does not implement a proximal operator for Optimisers.Adam, we must use the AdvancedVI.ClipScale() projection operator, which ensures that the scale matrix of the variational approximation is positive definite. (See the paper by J. Domke 202021 for more detail about the use of a projection operator.)\n\nusing Optimisers\n\n_, info_adam, _ = vi(\n    m, q_init, n_iters;\n    show_progress=false,\n    callback=callback,\n    algorithm=KLMinRepGradDescent(AutoForwardDiff(); optimizer=Optimisers.Adam(3e-3), operator=ClipScale())\n);\n\n\n[ Info: The capability of the supplied target `LogDensityProblem` LogDensityProblems.LogDensityOrder{1}() is &gt;= `LogDensityProblems.LogDensityOrder{1}()`. To make use of this, the `adtype` argument for AdvancedVI must be one of `AutoReverseDiff`, `AutoZygote`, `AutoMooncake`, or `AutoEnzyme` in reverse mode.\n\n\n\n\n\niters = 1:10:length(info_mf)\nelbo_adam = [i.elbo_avg for i in info_adam[iters]]\nPlots.plot(iters, elbo_mf, xlabel=\"Iterations\", ylabel=\"ELBO\", label=\"DoWG\")\nPlots.plot!(iters, elbo_adam, xlabel=\"Iterations\", ylabel=\"ELBO\", label=\"Adam\")\n\n\n\n\nCompared to the default option AdvancedVI.DoWG(), we can see that Optimisers.Adam(3e-3) is converging more slowly. With more step size tuning, it is possible that Optimisers.Adam could perform better or equal. That is, most common optimisers require some degree of tuning to perform better or comparably to AdvancedVI.DoWG() or AdvancedVI.DoG(), which do not require much tuning at all. Due to this fact, they are referred to as parameter-free optimizers.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Variational Inference"
    ]
  },
  {
    "objectID": "tutorials/variational-inference/index.html#using-full-rank-variational-families",
    "href": "tutorials/variational-inference/index.html#using-full-rank-variational-families",
    "title": "Variational Inference",
    "section": "Using Full-Rank Variational Families",
    "text": "Using Full-Rank Variational Families\nSo far, we have only used the mean-field Gaussian family. This, however, approximates the posterior covariance with a diagonal matrix. To model the full covariance matrix, we can use the full-rank Gaussian family2223:\n\nq_init_fr = q_fullrank_gaussian(m);\n\n\n@doc(Variational.q_fullrank_gaussian)\n\nq_fullrank_gaussian(\n    [rng::Random.AbstractRNG,]\n    model::DynamicPPL.Model;\n    location::Union{Nothing,&lt;:AbstractVector} = nothing,\n    scale::Union{Nothing,&lt;:LowerTriangular} = nothing,\n    kwargs...\n)\nFind a numerically non-degenerate Gaussian q with a scale with full-rank factors (traditionally referred to as a \"full-rank family\") for approximating the target model.\nIf the scale set as nothing, the default value will be a zero-mean Gaussian with a LowerTriangular scale matrix (resulting in a covariance with \"full-rank\" factors) no larger than 0.6*I (covariance of 0.6^2*I). This guarantees that the samples from the initial variational approximation will fall in the range of (-2, 2) with 99.9% probability, which mimics the behavior of the Turing.InitFromUniform() strategy. Whether the default choice is used or not, the scale may be adjusted via q_initialize_scale so that the log-densities of model are finite over the samples from q.\nArguments\n\nmodel: The target DynamicPPL.Model.\n\n\nKeyword Arguments\n\nlocation: The location parameter of the initialization. If nothing, a vector of zeros is used.\n\nscale: The scale parameter of the initialization. If nothing, an identity matrix is used.\n\n\nThe remaining keyword arguments are passed to q_locationscale.\nReturns\n\nq::Bijectors.TransformedDistribution: A AdvancedVI.LocationScale distribution matching the support of model.\n\n\n\n\n\n\n\nThe term full-rank might seem a bit peculiar since covariance matrices are always full-rank. This term, however, traditionally comes from the fact that full-rank families use full-rank factors in addition to the diagonal of the covariance.\nIn contrast to the mean-field family, the full-rank family will often result in more computation per optimisation step and slower convergence, especially in high dimensions:\n\nq_fr, info_fr, _ = vi(m, q_init_fr, n_iters; show_progress=false, callback)\n\nPlots.plot(elbo_mf, xlabel=\"Iterations\", ylabel=\"ELBO\", label=\"Mean-Field\", ylims=(-200, Inf))\n\nelbo_fr = [i.elbo_avg for i in info_fr[iters]]\nPlots.plot!(elbo_fr, xlabel=\"Iterations\", ylabel=\"ELBO\", label=\"Full-Rank\", ylims=(-200, Inf))\n\n\n[ Info: The capability of the supplied target `LogDensityProblem` LogDensityProblems.LogDensityOrder{1}() is &gt;= `LogDensityProblems.LogDensityOrder{1}()`. To make use of this, the `adtype` argument for AdvancedVI must be one of `AutoReverseDiff`, `AutoZygote`, `AutoMooncake`, or `AutoEnzyme` in reverse mode.\n\n\n\n\n\n\n\nHowever, we can see that the full-rank families achieve a higher ELBO in the end. Due to the relationship between the ELBO and the Kullback–Leibler divergence, this indicates that the full-rank covariance is much more accurate. This trade-off between statistical accuracy and optimisation speed is often referred to as the statistical-computational trade-off. The fact that we can control this trade-off through the choice of variational family is a strength, rather than a limitation, of variational inference.\nWe can also visualise the covariance matrix.\n\nheatmap(cov(rand(q_fr, 100_000), dims=2))",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Variational Inference"
    ]
  },
  {
    "objectID": "tutorials/variational-inference/index.html#obtaining-summary-statistics",
    "href": "tutorials/variational-inference/index.html#obtaining-summary-statistics",
    "title": "Variational Inference",
    "section": "Obtaining Summary Statistics",
    "text": "Obtaining Summary Statistics\nLet’s inspect the resulting variational approximation in more detail and compare it against MCMC. To obtain summary statistics from VI, we can draw samples from the resulting variational approximation:\n\nz = rand(q_fr, 100_000);\n\nNow, we can, for example, look at expectations:\n\navg = vec(mean(z; dims=2))\n\n12-element Vector{Float64}:\n  0.29665249970140406\n -0.002014959072651458\n  0.37257594756248874\n -0.08706379452567733\n -0.09650423460261785\n  0.6029528091635894\n -0.01742810733672524\n  0.08540506979801704\n -0.06345151623877471\n  0.1334939457294853\n  0.1734491462355629\n -0.5947240224190139\n\n\nThe vector has the same ordering as the parameters in the model, e.g. in this case σ² has index 1, intercept has index 2 and coefficients has indices 3:12. If you forget or you might want to do something programmatically with the result, you can obtain the sym → indices mapping as follows:\n\nusing Bijectors: bijector\n\n_, sym2range = bijector(m, Val(true));\nsym2range\n\n(intercept = UnitRange{Int64}[2:2], σ² = UnitRange{Int64}[1:1], coefficients = UnitRange{Int64}[3:12])\n\n\nFor example, we can check the sample distribution and mean value of σ²:\n\nhistogram(z[1, :])\navg[union(sym2range[:σ²]...)]\n\n1-element Vector{Float64}:\n 0.29665249970140406\n\n\n\navg[union(sym2range[:intercept]...)]\n\n1-element Vector{Float64}:\n -0.002014959072651458\n\n\n\navg[union(sym2range[:coefficients]...)]\n\n10-element Vector{Float64}:\n  0.37257594756248874\n -0.08706379452567733\n -0.09650423460261785\n  0.6029528091635894\n -0.01742810733672524\n  0.08540506979801704\n -0.06345151623877471\n  0.1334939457294853\n  0.1734491462355629\n -0.5947240224190139\n\n\nFor further convenience, we can wrap the samples into a Chains object to summarise the results.\n\nvarinf = Turing.DynamicPPL.VarInfo(m)\nvns_and_values = Turing.DynamicPPL.varname_and_value_leaves(Turing.DynamicPPL.values_as(varinf, OrderedDict))\nvarnames = map(first, vns_and_values)\nvi_chain = Chains(reshape(z', (size(z,2), size(z,1), 1)), varnames)\n\nChains MCMC chain (100000×12×1 reshape(adjoint(::Matrix{Float64}), 100000, 12, 1) with eltype Float64):\n\nIterations        = 1:1:100000\nNumber of chains  = 1\nSamples per chain = 100000\nparameters        = σ², intercept, coefficients[1], coefficients[2], coefficients[3], coefficients[4], coefficients[5], coefficients[6], coefficients[7], coefficients[8], coefficients[9], coefficients[10]\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n(Since we’re drawing independent samples, we can simply ignore the ESS and Rhat metrics.) Unfortunately, extracting varnames is a bit verbose at the moment, but hopefully will become simpler in the near future.\nLet’s compare this against samples from NUTS:\n\nmcmc_chain = sample(m, NUTS(), 10_000; progress=false);\n\nvi_mean = mean(vi_chain)[:, 2]\nmcmc_mean = mean(mcmc_chain, names(mcmc_chain, :parameters))[:, 2]\n\nplot(mcmc_mean; xticks=1:1:length(mcmc_mean), label=\"mean of NUTS\")\nplot!(vi_mean; label=\"mean of VI\")\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.4\n\n\n\n\n\n\n\nThat looks pretty good! But let’s see how the predictive distributions looks for the two.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Variational Inference"
    ]
  },
  {
    "objectID": "tutorials/variational-inference/index.html#making-predictions",
    "href": "tutorials/variational-inference/index.html#making-predictions",
    "title": "Variational Inference",
    "section": "Making Predictions",
    "text": "Making Predictions\nSimilarily to the linear regression tutorial, we’re going to compare to multivariate ordinary linear regression using the GLM package:\n\n# Import the GLM package.\nusing GLM\n\n# Perform multivariate OLS.\nols = lm(\n    @formula(MPG ~ Cyl + Disp + HP + DRat + WT + QSec + VS + AM + Gear + Carb), train_cut\n)\n\n# Store our predictions in the original dataframe.\ntrain_cut.OLSPrediction = unstandardize(GLM.predict(ols), train_unstandardized.MPG)\ntest_cut.OLSPrediction = unstandardize(GLM.predict(ols, test_cut), train_unstandardized.MPG);\n\n\n# Make a prediction given an input vector, using mean parameter values from a chain.\nfunction prediction(chain, x)\n    p = get_params(chain)\n    α = mean(p.intercept)\n    β = collect(mean.(p.coefficients))\n    return α .+ x * β\nend\n\nprediction (generic function with 1 method)\n\n\n\n# Unstandardize the dependent variable.\ntrain_cut.MPG = unstandardize(train_cut.MPG, train_unstandardized.MPG)\ntest_cut.MPG = unstandardize(test_cut.MPG, train_unstandardized.MPG);\n\n\n# Show the first side rows of the modified dataframe.\nfirst(test_cut, 6)\n\n6×12 DataFrame\n\n\n\nRow\nMPG\nCyl\nDisp\nHP\nDRat\nWT\nQSec\nVS\nAM\nGear\nCarb\nOLSPrediction\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n15.2\n1.04746\n0.565102\n0.258882\n-0.652405\n0.0714991\n-0.716725\n-0.977008\n-0.598293\n-0.891883\n-0.469126\n19.8583\n\n\n2\n13.3\n1.04746\n0.929057\n1.90345\n0.380435\n0.465717\n-1.90403\n-0.977008\n-0.598293\n-0.891883\n1.11869\n16.0462\n\n\n3\n19.2\n1.04746\n1.32466\n0.691663\n-0.777058\n0.470584\n-0.873777\n-0.977008\n-0.598293\n-0.891883\n-0.469126\n18.5746\n\n\n4\n27.3\n-1.25696\n-1.21511\n-1.19526\n1.0037\n-1.38857\n0.288403\n0.977008\n1.59545\n1.07026\n-1.26303\n29.3233\n\n\n5\n26.0\n-1.25696\n-0.888346\n-0.762482\n1.62697\n-1.18903\n-1.09365\n-0.977008\n1.59545\n3.0324\n-0.469126\n30.7731\n\n\n6\n30.4\n-1.25696\n-1.08773\n-0.381634\n0.451665\n-1.79933\n-0.968007\n0.977008\n1.59545\n3.0324\n-0.469126\n25.2892\n\n\n\n\n\n\n\n# Construct the Chains from the Variational Approximations\nz_mf = rand(q_mf, 10_000);\nz_fr = rand(q_fr, 10_000);\n\nvi_mf_chain = Chains(reshape(z_mf', (size(z_mf,2), size(z_mf,1), 1)), varnames);\nvi_fr_chain = Chains(reshape(z_fr', (size(z_fr,2), size(z_fr,1), 1)), varnames);\n\n\n# Calculate the predictions for the training and testing sets using the samples `z` from variational posterior\ntrain_cut.VIMFPredictions = unstandardize(\n    prediction(vi_mf_chain, train), train_unstandardized.MPG\n)\ntest_cut.VIMFPredictions = unstandardize(\n    prediction(vi_mf_chain, test), train_unstandardized.MPG\n)\n\ntrain_cut.VIFRPredictions = unstandardize(\n    prediction(vi_fr_chain, train), train_unstandardized.MPG\n)\ntest_cut.VIFRPredictions = unstandardize(\n    prediction(vi_fr_chain, test), train_unstandardized.MPG\n)\n\ntrain_cut.BayesPredictions = unstandardize(\n    prediction(mcmc_chain, train), train_unstandardized.MPG\n)\ntest_cut.BayesPredictions = unstandardize(\n    prediction(mcmc_chain, test), train_unstandardized.MPG\n);\n\n\nvi_mf_loss1 = mean((train_cut.VIMFPredictions - train_cut.MPG) .^ 2)\nvi_fr_loss1 = mean((train_cut.VIFRPredictions - train_cut.MPG) .^ 2)\nbayes_loss1 = mean((train_cut.BayesPredictions - train_cut.MPG) .^ 2)\nols_loss1 = mean((train_cut.OLSPrediction - train_cut.MPG) .^ 2)\n\nvi_mf_loss2 = mean((test_cut.VIMFPredictions - test_cut.MPG) .^ 2)\nvi_fr_loss2 = mean((test_cut.VIFRPredictions - test_cut.MPG) .^ 2)\nbayes_loss2 = mean((test_cut.BayesPredictions - test_cut.MPG) .^ 2)\nols_loss2 = mean((test_cut.OLSPrediction - test_cut.MPG) .^ 2)\n\nprintln(\"Training set:\n    VI Mean-Field loss: $vi_mf_loss1\n    VI Full-Rank loss: $vi_fr_loss1\n    Bayes loss: $bayes_loss1\n    OLS loss: $ols_loss1\nTest set:\n    VI Mean-Field loss: $vi_mf_loss2\n    VI Full-Rank loss: $vi_fr_loss2\n    Bayes loss: $bayes_loss2\n    OLS loss: $ols_loss2\")\n\nTraining set:\n    VI Mean-Field loss: 3.0738046173238844\n    VI Full-Rank loss: 3.0770241899194968\n    Bayes loss: 3.0716275892504665\n    OLS loss: 3.0709261248930093\nTest set:\n    VI Mean-Field loss: 26.11379608195827\n    VI Full-Rank loss: 25.473131945954975\n    Bayes loss: 26.4747134257175\n    OLS loss: 27.09481307076057\n\n\nInterestingly the squared difference between true- and mean-prediction on the test-set is actually better for the full-rank variational posterior than for the “true” posterior obtained by MCMC sampling using NUTS. But, as Bayesians, we know that the mean doesn’t tell the entire story. One quick check is to look at the mean predictions ± standard deviation of the two different approaches:\n\npreds_vi_mf = mapreduce(hcat, 1:5:size(vi_mf_chain, 1)) do i\n    return unstandardize(prediction(vi_mf_chain[i], test), train_unstandardized.MPG)\nend\n\np1 = scatter(\n    1:size(test, 1),\n    mean(preds_vi_mf; dims=2);\n    yerr=std(preds_vi_mf; dims=2),\n    label=\"prediction (mean ± std)\",\n    size=(900, 500),\n    markersize=8,\n)\nscatter!(1:size(test, 1), unstandardize(test_label, train_unstandardized.MPG); label=\"true\")\nxaxis!(1:size(test, 1))\nylims!(10, 40)\ntitle!(\"VI Mean-Field\")\n\npreds_vi_fr = mapreduce(hcat, 1:5:size(vi_mf_chain, 1)) do i\n    return unstandardize(prediction(vi_fr_chain[i], test), train_unstandardized.MPG)\nend\n\np2 = scatter(\n    1:size(test, 1),\n    mean(preds_vi_fr; dims=2);\n    yerr=std(preds_vi_fr; dims=2),\n    label=\"prediction (mean ± std)\",\n    size=(900, 500),\n    markersize=8,\n)\nscatter!(1:size(test, 1), unstandardize(test_label, train_unstandardized.MPG); label=\"true\")\nxaxis!(1:size(test, 1))\nylims!(10, 40)\ntitle!(\"VI Full-Rank\")\n\npreds_mcmc = mapreduce(hcat, 1:5:size(mcmc_chain, 1)) do i\n    return unstandardize(prediction(mcmc_chain[i], test), train_unstandardized.MPG)\nend\n\np3 = scatter(\n    1:size(test, 1),\n    mean(preds_mcmc; dims=2);\n    yerr=std(preds_mcmc; dims=2),\n    label=\"prediction (mean ± std)\",\n    size=(900, 500),\n    markersize=8,\n)\nscatter!(1:size(test, 1), unstandardize(test_label, train_unstandardized.MPG); label=\"true\")\nxaxis!(1:size(test, 1))\nylims!(10, 40)\ntitle!(\"MCMC (NUTS)\")\n\nplot(p1, p2, p3; layout=(1, 3), size=(900, 250), label=\"\")\n\n\n\n\nWe can see that the full-rank VI approximation is very close to the predictions from MCMC samples. Also, the coverage of full-rank VI and MCMC is much better the crude mean-field approximation.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Variational Inference"
    ]
  },
  {
    "objectID": "tutorials/variational-inference/index.html#footnotes",
    "href": "tutorials/variational-inference/index.html#footnotes",
    "title": "Variational Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKucukelbir, A., Tran, D., Ranganath, R., Gelman, A., & Blei, D. M. (2017). Automatic differentiation variational inference. Journal of Machine Learning Research, 18(14).↩︎\nTitsias, M., & Lázaro-Gredilla, M. (2014). Doubly stochastic variational Bayes for non-conjugate inference. In Proceedings of the International Conference on Machine Learning. PMLR.↩︎\nRanganath, R., Gerrish, S., & Blei, D. (2014). Black box variational inference. In Proceedings of the International Conference on Artificial intelligence and statistics. PMLR.↩︎\nKingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. In Proceedings of the International Conference on Learning Representations.↩︎\nRezende, D. J., Mohamed, S., & Wierstra, D (2014). Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the International Conference on Machine Learning. PMLR.↩︎\nTitsias, M., & Lázaro-Gredilla, M. (2014). Doubly stochastic variational Bayes for non-conjugate inference. In Proceedings of the International Conference on Machine Learning. PMLR.↩︎\nKucukelbir, A., Tran, D., Ranganath, R., Gelman, A., & Blei, D. M. (2017). Automatic differentiation variational inference. Journal of Machine Learning Research, 18(14).↩︎\nTitsias, M., & Lázaro-Gredilla, M. (2014). Doubly stochastic variational Bayes for non-conjugate inference. In Proceedings of the International Conference on Machine Learning. PMLR.↩︎\nRanganath, R., Gerrish, S., & Blei, D. (2014). Black box variational inference. In Proceedings of the International Conference on Artificial intelligence and statistics. PMLR.↩︎\nKingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. In Proceedings of the International Conference on Learning Representations.↩︎\nRezende, D. J., Mohamed, S., & Wierstra, D (2014). Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the International Conference on Machine Learning. PMLR.↩︎\nTitsias, M., & Lázaro-Gredilla, M. (2014). Doubly stochastic variational Bayes for non-conjugate inference. In Proceedings of the International Conference on Machine Learning. PMLR.↩︎\nKhaled, A., Mishchenko, K., & Jin, C. (2023). DoWG unleashed: An efficient universal parameter-free gradient descent method. In Advances in Neural Information Processing Systems, 36.↩︎\nDomke, J. (2020). Provable smoothness guarantees for black-box variational inference. In Proceedings of the International Conference on Machine Learning. PMLR.↩︎\nDomke, J., Gower, R., & Garrigos, G. (2023). Provable convergence guarantees for black-box variational inference. In Advances in Neural Information Processing Systems, 36.↩︎\nKim, K., Oh, J., Wu, K., Ma, Y., & Gardner, J. (2023). On the convergence of black-box variational inference. In Advances in Neural Information Processing Systems, 36.↩︎\nShamir, O., & Zhang, T. (2013). Stochastic gradient descent for non-smooth optimisation: Convergence results and optimal averaging schemes. In Proceedings of the International Conference on Machine Learning. PMLR.↩︎\nKhaled, A., Mishchenko, K., & Jin, C. (2023). DoWG unleashed: An efficient universal parameter-free gradient descent method. In Advances in Neural Information Processing Systems, 36.↩︎\nIvgi, M., Hinder, O., & Carmon, Y. (2023). DoG is SGD’s best friend: A parameter-free dynamic step size schedule. In Proceedings of the International Conference on Machine Learning. PMLR.↩︎\nKingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimisation. In Proceedings of the International Conference on Learning Representations.↩︎\nDomke, J. (2020). Provable smoothness guarantees for black-box variational inference. In Proceedings of the International Conference on Machine Learning. PMLR.↩︎\nTitsias, M., & Lázaro-Gredilla, M. (2014). Doubly stochastic variational Bayes for non-conjugate inference. In Proceedings of the International Conference on Machine Learning. PMLR.↩︎\nKucukelbir, A., Tran, D., Ranganath, R., Gelman, A., & Blei, D. M. (2017). Automatic differentiation variational inference. Journal of Machine Learning Research, 18(14).↩︎",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Variational Inference"
    ]
  },
  {
    "objectID": "tutorials/infinite-mixture-models/index.html",
    "href": "tutorials/infinite-mixture-models/index.html",
    "title": "Infinite Mixture Models",
    "section": "",
    "text": "In many applications it is desirable to allow the model to adjust its complexity to the amount of data. Consider for example the task of assigning objects into clusters or groups. This task often involves the specification of the number of groups. However, often times it is not known beforehand how many groups exist. Moreover, in some applications, e.g. modelling topics in text documents or grouping species, the number of examples per group is heavy tailed. This makes it impossible to predefine the number of groups and requiring the model to form new groups when data points from previously unseen groups are observed.\nA natural approach for such applications is the use of non-parametric models, which can grow in complexity as more data are observed. This tutorial demonstrates how to use the Dirichlet process in a mixture of infinitely many Gaussians using Turing. For further information on Bayesian nonparametrics and the Dirichlet process, see the introduction by Zoubin Ghahramani and the book “Fundamentals of Nonparametric Bayesian Inference” by Subhashis Ghosal and Aad van der Vaart.\nusing Turing",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Infinite Mixture Models"
    ]
  },
  {
    "objectID": "tutorials/infinite-mixture-models/index.html#mixture-model",
    "href": "tutorials/infinite-mixture-models/index.html#mixture-model",
    "title": "Infinite Mixture Models",
    "section": "Mixture Model",
    "text": "Mixture Model\nBefore introducing infinite mixture models in Turing, we will briefly review the construction of finite mixture models. Subsequently, we will define how to use the Chinese restaurant process construction of a Dirichlet process for non-parametric clustering.\n\nTwo-Component Model\nFirst, consider the simple case of a mixture model with two Gaussian components with fixed covariance. The generative process of such a model can be written as:\n\\[\\begin{equation*}\n\\begin{aligned}\n\\pi_1 &\\sim \\mathrm{Beta}(a, b) \\\\\n\\pi_2 &= 1-\\pi_1 \\\\\n\\mu_1 &\\sim \\mathrm{Normal}(\\mu_0, \\Sigma_0) \\\\\n\\mu_2 &\\sim \\mathrm{Normal}(\\mu_0, \\Sigma_0) \\\\\nz_i &\\sim \\mathrm{Categorical}(\\pi_1, \\pi_2) \\\\\nx_i &\\sim \\mathrm{Normal}(\\mu_{z_i}, \\Sigma)\n\\end{aligned}\n\\end{equation*}\\]\nwhere \\(\\pi_1, \\pi_2\\) are the mixing weights of the mixture model, i.e. \\(\\pi_1 + \\pi_2 = 1\\), and \\(z_i\\) is a latent assignment of the observation \\(x_i\\) to a component (Gaussian).\nWe can implement this model in Turing for 1D data as follows:\n\n@model function two_model(x)\n    # Hyper-parameters\n    μ0 = 0.0\n    σ0 = 1.0\n\n    # Draw weights.\n    π1 ~ Beta(1, 1)\n    π2 = 1 - π1\n\n    # Draw locations of the components.\n    μ1 ~ Normal(μ0, σ0)\n    μ2 ~ Normal(μ0, σ0)\n\n    # Draw latent assignment.\n    z ~ Categorical([π1, π2])\n\n    # Draw observation from selected component.\n    if z == 1\n        x ~ Normal(μ1, 1.0)\n    else\n        x ~ Normal(μ2, 1.0)\n    end\nend\n\ntwo_model (generic function with 2 methods)\n\n\n\n\nFinite Mixture Model\nIf we have more than two components, this model can elegantly be extended using a Dirichlet distribution as prior for the mixing weights \\(\\pi_1, \\dots, \\pi_K\\). Note that the Dirichlet distribution is the multivariate generalization of the beta distribution. The resulting model can be written as:\n\\[\n\\begin{align}\n(\\pi_1, \\dots, \\pi_K) &\\sim Dirichlet(K, \\alpha) \\\\\n\\mu_k &\\sim \\mathrm{Normal}(\\mu_0, \\Sigma_0), \\;\\; \\forall k \\\\\nz &\\sim Categorical(\\pi_1, \\dots, \\pi_K) \\\\\nx &\\sim \\mathrm{Normal}(\\mu_z, \\Sigma)\n\\end{align}\n\\]\nwhich resembles the model in the Gaussian mixture model tutorial with a slightly different notation.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Infinite Mixture Models"
    ]
  },
  {
    "objectID": "tutorials/infinite-mixture-models/index.html#infinite-mixture-model",
    "href": "tutorials/infinite-mixture-models/index.html#infinite-mixture-model",
    "title": "Infinite Mixture Models",
    "section": "Infinite Mixture Model",
    "text": "Infinite Mixture Model\nThe question now arises, is there a generalization of a Dirichlet distribution for which the dimensionality \\(K\\) is infinite, i.e. \\(K = \\infty\\)?\nBut first, to implement an infinite Gaussian mixture model in Turing, we first need to load the Turing.RandomMeasures module. RandomMeasures contains a variety of tools useful in nonparametrics.\n\nusing Turing.RandomMeasures\n\nWe now will utilize the fact that one can integrate out the mixing weights in a Gaussian mixture model allowing us to arrive at the Chinese restaurant process construction. See Carl E. Rasmussen: The Infinite Gaussian Mixture Model, NIPS (2000) for details.\nIn fact, if the mixing weights are integrated out, the conditional prior for the latent variable \\(z\\) is given by:\n\\[\np(z_i = k \\mid z_{\\not i}, \\alpha) = \\frac{n_k + \\alpha K}{N - 1 + \\alpha}\n\\]\nwhere \\(z_{\\not i}\\) are the latent assignments of all observations except observation \\(i\\). Note that we use \\(n_k\\) to denote the number of observations at component \\(k\\) excluding observation \\(i\\). The parameter \\(\\alpha\\) is the concentration parameter of the Dirichlet distribution used as prior over the mixing weights.\n\nChinese Restaurant Process\nTo obtain the Chinese restaurant process construction, we can now derive the conditional prior if \\(K \\rightarrow \\infty\\).\nFor \\(n_k &gt; 0\\) we obtain:\n\\[\np(z_i = k \\mid z_{\\not i}, \\alpha) = \\frac{n_k}{N - 1 + \\alpha}\n\\]\nand for all infinitely many clusters that are empty (combined) we get:\n\\[\np(z_i = k \\mid z_{\\not i}, \\alpha) = \\frac{\\alpha}{N - 1 + \\alpha}\n\\]\nThose equations show that the conditional prior for component assignments is proportional to the number of such observations, meaning that the Chinese restaurant process has a rich get richer property.\nTo get a better understanding of this property, we can plot the cluster chosen by for each new observation drawn from the conditional prior.\n\n# Concentration parameter.\nα = 10.0\n\n# Random measure, e.g. Dirichlet process.\nrpm = DirichletProcess(α)\n\n# Cluster assignments for each observation.\nz = Vector{Int}()\n\n# Maximum number of observations we observe.\nNmax = 500\n\nfor i in 1:Nmax\n    # Number of observations per cluster.\n    K = isempty(z) ? 0 : maximum(z)\n    nk = Vector{Int}(map(k -&gt; sum(z .== k), 1:K))\n\n    # Draw new assignment.\n    push!(z, rand(ChineseRestaurantProcess(rpm, nk)))\nend\n\n\nusing Plots\n\n# Plot the cluster assignments over time\n@gif for i in 1:Nmax\n    scatter(\n        collect(1:i),\n        z[1:i];\n        markersize=2,\n        xlabel=\"observation (i)\",\n        ylabel=\"cluster (k)\",\n        legend=false,\n    )\nend\n\n\nGKS: cannot open display - headless operation mode active\n[ Info: Saved animation to /tmp/jl_05MBcjgtwz.gif\n\n\n\n\n\n\n\nFurther, we can see that the number of clusters is logarithmic in the number of observations and data points. This is a side-effect of the “rich-get-richer” phenomenon, i.e. we expect large clusters and thus the number of clusters has to be smaller than the number of observations.\n\\[\n\\mathbb{E}[K \\mid N] \\approx \\alpha \\cdot log \\big(1 + \\frac{N}{\\alpha}\\big)\n\\]\nWe can see from the equation that the concentration parameter \\(\\alpha\\) allows us to control the number of clusters formed a priori.\nIn Turing we can implement an infinite Gaussian mixture model using the Chinese restaurant process construction of a Dirichlet process as follows:\n\n@model function infiniteGMM(x)\n    # Hyper-parameters, i.e. concentration parameter and parameters of H.\n    α = 1.0\n    μ0 = 0.0\n    σ0 = 1.0\n\n    # Define random measure, e.g. Dirichlet process.\n    rpm = DirichletProcess(α)\n\n    # Define the base distribution, i.e. expected value of the Dirichlet process.\n    H = Normal(μ0, σ0)\n\n    # Latent assignment.\n    z = zeros(Int, length(x))\n\n    # Locations of the infinitely many clusters.\n    μ = zeros(Float64, 0)\n\n    for i in 1:length(x)\n\n        # Number of clusters.\n        K = maximum(z)\n        nk = Vector{Int}(map(k -&gt; sum(z .== k), 1:K))\n\n        # Draw the latent assignment.\n        z[i] ~ ChineseRestaurantProcess(rpm, nk)\n\n        # Create a new cluster?\n        if z[i] &gt; K\n            push!(μ, 0.0)\n\n            # Draw location of new cluster.\n            μ[z[i]] ~ H\n        end\n\n        # Draw observation.\n        x[i] ~ Normal(μ[z[i]], 1.0)\n    end\nend\n\ninfiniteGMM (generic function with 2 methods)\n\n\nWe can now use Turing to infer the assignments of some data points. First, we will create some random data that comes from three clusters, with means of 0, -5, and 10.\n\nusing Plots, Random\n\n# Generate some test data.\nRandom.seed!(1)\ndata = vcat(randn(10), randn(10) .- 5, randn(10) .+ 10)\ndata .-= mean(data)\ndata /= std(data);\n\nNext, we’ll sample from our posterior using SMC.\n\nsetprogress!(false)\n\n\n# MCMC sampling\nRandom.seed!(2)\niterations = 1000\nmodel_fun = infiniteGMM(data);\nchain = sample(model_fun, SMC(), iterations);\n\nFinally, we can plot the number of clusters in each sample.\n\n# Extract the number of clusters for each sample of the Markov chain.\nk = map(\n    t -&gt; length(unique(vec(chain[t, MCMCChains.namesingroup(chain, :z), :].value))),\n    1:iterations,\n);\n\n# Visualize the number of clusters.\nplot(k; xlabel=\"Iteration\", ylabel=\"Number of clusters\", label=\"Chain 1\")\n\n\n\n\nIf we visualise the histogram of the number of clusters sampled from our posterior, we observe that the model seems to prefer 3 clusters, which is the true number of clusters. Note that the number of clusters in a Dirichlet process mixture model is not limited a priori and will grow to infinity with probability one. However, if conditioned on data the posterior will concentrate on a finite number of clusters enforcing the resulting model to have a finite amount of clusters. It is, however, not given that the posterior of a Dirichlet process Gaussian mixture model converges to the true number of clusters, given that data comes from a finite mixture model. See Jeffrey Miller and Matthew Harrison: A simple example of Dirichlet process mixture inconsistency for the number of components for details.\n\nhistogram(k; xlabel=\"Number of clusters\", legend=false)\n\n\n\n\nOne issue with the Chinese restaurant process construction is that the number of latent parameters we need to sample scales with the number of observations. It may be desirable to use alternative constructions in certain cases. Alternative methods of constructing a Dirichlet process can be employed via the following representations:\nSize-Biased Sampling Process\n\\[\nj_k \\sim \\mathrm{Beta}(1, \\alpha) \\cdot \\mathrm{surplus}\n\\]\nStick-Breaking Process \\[\nv_k \\sim \\mathrm{Beta}(1, \\alpha)\n\\]\nChinese Restaurant Process \\[\np(z_n = k | z_{1:n-1}) \\propto \\begin{cases}\n\\frac{m_k}{n-1+\\alpha}, \\text{ if } m_k &gt; 0\\\\\\\n\\frac{\\alpha}{n-1+\\alpha}\n\\end{cases}\n\\]\nFor more details see this article.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Infinite Mixture Models"
    ]
  },
  {
    "objectID": "tutorials/bayesian-logistic-regression/index.html",
    "href": "tutorials/bayesian-logistic-regression/index.html",
    "title": "Bayesian Logistic Regression",
    "section": "",
    "text": "Bayesian logistic regression is the Bayesian counterpart to a common tool in machine learning, logistic regression. The goal of logistic regression is to predict a one or a zero for a given training item. An example might be predicting whether someone is sick or ill given their symptoms and personal information.\nIn our example, we’ll be working to predict whether someone is likely to default with a synthetic dataset found in the RDatasets package. This dataset, Defaults, comes from R’s ISLR package and contains information on borrowers.\nTo start, let’s import all the libraries we’ll need.\n# Import Turing and Distributions.\nusing Turing, Distributions\n\n# Import RDatasets.\nusing RDatasets\n\n# Import MCMCChains, Plots, and StatsPlots for visualisations and diagnostics.\nusing MCMCChains, Plots, StatsPlots\n\n# We need a logistic function, which is provided by StatsFuns.\nusing StatsFuns: logistic\n\n# Functionality for splitting and normalising the data\nusing MLDataUtils: shuffleobs, stratifiedobs, rescale!\n\n# Set a seed for reproducibility.\nusing Random\nRandom.seed!(0);",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Logistic Regression"
    ]
  },
  {
    "objectID": "tutorials/bayesian-logistic-regression/index.html#data-cleaning-set-up",
    "href": "tutorials/bayesian-logistic-regression/index.html#data-cleaning-set-up",
    "title": "Bayesian Logistic Regression",
    "section": "Data Cleaning & Set Up",
    "text": "Data Cleaning & Set Up\nNow we’re going to import our dataset. The first six rows of the dataset are shown below so you can get a good feel for what kind of data we have.\n\n# Import the \"Default\" dataset.\ndata = RDatasets.dataset(\"ISLR\", \"Default\");\n\n# Show the first six rows of the dataset.\nfirst(data, 6)\n\n6×4 DataFrame\n\n\n\nRow\nDefault\nStudent\nBalance\nIncome\n\n\n\nCat…\nCat…\nFloat64\nFloat64\n\n\n\n\n1\nNo\nNo\n729.526\n44361.6\n\n\n2\nNo\nYes\n817.18\n12106.1\n\n\n3\nNo\nNo\n1073.55\n31767.1\n\n\n4\nNo\nNo\n529.251\n35704.5\n\n\n5\nNo\nNo\n785.656\n38463.5\n\n\n6\nNo\nYes\n919.589\n7491.56\n\n\n\n\n\n\nMost machine learning processes require some effort to tidy up the data, and this is no different. We need to convert the Default and Student columns, which say “Yes” or “No” into 1s and 0s. Afterwards, we’ll get rid of the old words-based columns.\n\n# Convert \"Default\" and \"Student\" to numeric values.\ndata[!, :DefaultNum] = [r.Default == \"Yes\" ? 1.0 : 0.0 for r in eachrow(data)]\ndata[!, :StudentNum] = [r.Student == \"Yes\" ? 1.0 : 0.0 for r in eachrow(data)]\n\n# Delete the old columns which say \"Yes\" and \"No\".\nselect!(data, Not([:Default, :Student]))\n\n# Show the first six rows of our edited dataset.\nfirst(data, 6)\n\n6×4 DataFrame\n\n\n\nRow\nBalance\nIncome\nDefaultNum\nStudentNum\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n729.526\n44361.6\n0.0\n0.0\n\n\n2\n817.18\n12106.1\n0.0\n1.0\n\n\n3\n1073.55\n31767.1\n0.0\n0.0\n\n\n4\n529.251\n35704.5\n0.0\n0.0\n\n\n5\n785.656\n38463.5\n0.0\n0.0\n\n\n6\n919.589\n7491.56\n0.0\n1.0\n\n\n\n\n\n\nAfter we’ve done that tidying, it’s time to split our dataset into training and testing sets, and separate the labels from the data. We separate our data into two halves, train and test. You can use a higher percentage of splitting (or a lower one) by modifying the at = 0.05 argument. We have highlighted the use of only a 5% sample to show the power of Bayesian inference with small sample sizes.\nWe must rescale our variables so that they are centred around zero by subtracting each column by the mean and dividing it by the standard deviation. This rescaling ensures features are on comparable scales, which improves sampler initialisation and convergence. To do this we will leverage MLDataUtils, which also lets us effortlessly shuffle our observations and perform a stratified split to get a representative test set.\n\nfunction split_data(df, target; at=0.70)\n    shuffled = shuffleobs(df)\n    return trainset, testset = stratifiedobs(row -&gt; row[target], shuffled; p=at)\nend\n\nfeatures = [:StudentNum, :Balance, :Income]\nnumerics = [:Balance, :Income]\ntarget = :DefaultNum\n\ntrainset, testset = split_data(data, target; at=0.05)\nfor feature in numerics\n    μ, σ = rescale!(trainset[!, feature]; obsdim=1)\n    rescale!(testset[!, feature], μ, σ; obsdim=1)\nend\n\n# Turing requires data in matrix form, not dataframe\ntrain = Matrix(trainset[:, features])\ntest = Matrix(testset[:, features])\ntrain_label = trainset[:, target]\ntest_label = testset[:, target];",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Logistic Regression"
    ]
  },
  {
    "objectID": "tutorials/bayesian-logistic-regression/index.html#model-declaration",
    "href": "tutorials/bayesian-logistic-regression/index.html#model-declaration",
    "title": "Bayesian Logistic Regression",
    "section": "Model Declaration",
    "text": "Model Declaration\nFinally, we can define our model.\nlogistic_regression takes four arguments:\n\nx is our set of independent variables;\ny is the element we want to predict;\nn is the number of observations we have; and\nσ is the standard deviation we want to assume for our priors.\n\nWithin the model, we create four coefficients (intercept, student, balance, and income) and assign a prior of normally distributed with means of zero and standard deviations of σ. We want to find values of these four coefficients to predict any given y.\nThe for block creates a variable v which is the logistic function. We then observe the likelihood of calculating v given the actual label, y[i].\n\n# Bayesian logistic regression (LR)\n@model function logistic_regression(x, y, n, σ)\n    intercept ~ Normal(0, σ)\n\n    student ~ Normal(0, σ)\n    balance ~ Normal(0, σ)\n    income ~ Normal(0, σ)\n\n    for i in 1:n\n        v = logistic(intercept + student * x[i, 1] + balance * x[i, 2] + income * x[i, 3])\n        y[i] ~ Bernoulli(v)\n    end\nend;",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Logistic Regression"
    ]
  },
  {
    "objectID": "tutorials/bayesian-logistic-regression/index.html#sampling",
    "href": "tutorials/bayesian-logistic-regression/index.html#sampling",
    "title": "Bayesian Logistic Regression",
    "section": "Sampling",
    "text": "Sampling\nNow we can run our sampler. This time we’ll use NUTS to sample from our posterior.\n\nsetprogress!(false)\n\n\n# Retrieve the number of observations.\nn, _ = size(train)\n\n# Sample using NUTS.\nm = logistic_regression(train, train_label, n, 1)\nchain = sample(m, NUTS(), MCMCThreads(), 1_500, 3)\n\n\n\nChains MCMC chain (1500×18×3 Array{Float64, 3}):\n\nIterations        = 751:1:2250\nNumber of chains  = 3\nSamples per chain = 1500\nWall duration     = 12.25 seconds\nCompute duration  = 8.03 seconds\nparameters        = intercept, student, balance, income\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n\n\n\n\n\n\nWarningSampling With Multiple Threads\n\n\n\n\n\nThe sample() call above assumes that you have at least nchains threads available in your Julia instance. If you do not, the multiple chains will run sequentially, and you may notice a warning. For more information, see the Turing documentation on sampling multiple chains.\n\n\n\nSince we ran multiple chains, we may as well do a spot check to make sure each chain converges around similar points.\n\nplot(chain)\n\n\n\n\nLooks good!\nWe can also use the corner function from MCMCChains to show the distributions of the various parameters of our logistic regression.\n\n# The labels to use.\nl = [:student, :balance, :income]\n\n# Use the corner function. Requires StatsPlots and MCMCChains.\ncorner(chain, l)\n\n\n\n\nFortunately the corner plot appears to demonstrate unimodal distributions for each of our parameters, so it should be straightforward to take the means of each parameter’s sampled values to estimate our model to make predictions.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Logistic Regression"
    ]
  },
  {
    "objectID": "tutorials/bayesian-logistic-regression/index.html#making-predictions",
    "href": "tutorials/bayesian-logistic-regression/index.html#making-predictions",
    "title": "Bayesian Logistic Regression",
    "section": "Making Predictions",
    "text": "Making Predictions\nHow do we test how well the model actually predicts whether someone is likely to default? We need to build a prediction function that takes the test object we made earlier and runs it through the average parameter calculated during sampling.\nThe prediction function below takes a Matrix and a Chain object. It takes the mean of each parameter’s sampled values and re-runs the logistic function using those mean values for every element in the test set.\n\nfunction prediction(x::Matrix, chain, threshold)\n    # Pull the means from each parameter's sampled values in the chain.\n    intercept = mean(chain[:intercept])\n    student = mean(chain[:student])\n    balance = mean(chain[:balance])\n    income = mean(chain[:income])\n\n    # Retrieve the number of rows.\n    n, _ = size(x)\n\n    # Generate a vector to store our predictions.\n    v = Vector{Float64}(undef, n)\n\n    # Calculate the logistic function for each element in the test set.\n    for i in 1:n\n        num = logistic(\n            intercept .+ student * x[i, 1] + balance * x[i, 2] + income * x[i, 3]\n        )\n        if num &gt;= threshold\n            v[i] = 1\n        else\n            v[i] = 0\n        end\n    end\n    return v\nend;\n\nLet’s see how we did! We run the test matrix through the prediction function, and compute the mean squared error (MSE) for our prediction. The threshold variable sets the decision boundary for classification. For example, a threshold of 0.07 will predict a default (value of 1) for any predicted probability greater than 0.07 and no default otherwise. Lower thresholds increase sensitivity but may increase false positives.\n\n# Set the prediction threshold.\nthreshold = 0.07\n\n# Make the predictions.\npredictions = prediction(test, chain, threshold)\n\n# Calculate MSE for our test set.\nloss = sum((predictions - test_label) .^ 2) / length(test_label)\n\n0.12936842105263158\n\n\nPerhaps more important is to see what percentage of defaults we correctly predicted. The code below simply counts defaults and predictions and presents the results.\n\ndefaults = sum(test_label)\nnot_defaults = length(test_label) - defaults\n\npredicted_defaults = sum(test_label .== predictions .== 1)\npredicted_not_defaults = sum(test_label .== predictions .== 0)\n\nprintln(\"Defaults: $defaults\n    Predictions: $predicted_defaults\n    Percentage defaults correct $(predicted_defaults/defaults)\")\n\nprintln(\"Not defaults: $not_defaults\n    Predictions: $predicted_not_defaults\n    Percentage non-defaults correct $(predicted_not_defaults/not_defaults)\")\n\nDefaults: 316.0\n    Predictions: 273\n    Percentage defaults correct 0.8639240506329114\nNot defaults: 9184.0\n    Predictions: 7998\n    Percentage non-defaults correct 0.8708623693379791\n\n\nThe above shows that with a threshold of 0.07, we correctly predict a respectable portion of the defaults, and correctly identify most non-defaults. This is fairly sensitive to a choice of threshold, and you may wish to experiment with it.\nThis tutorial has demonstrated how to use Turing to perform Bayesian logistic regression.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Logistic Regression"
    ]
  },
  {
    "objectID": "tutorials/bayesian-poisson-regression/index.html",
    "href": "tutorials/bayesian-poisson-regression/index.html",
    "title": "Bayesian Poisson Regression",
    "section": "",
    "text": "This notebook is ported from the example notebook of PyMC3 on Poisson Regression.\nPoisson Regression is a technique commonly used to model count data. Some of the applications include predicting the number of people defaulting on their loans or the number of cars running on a highway on a given day. This example describes a method to implement the Bayesian version of this technique using Turing.\nWe will generate the dataset that we will be working on which describes the relationship between number of times a person sneezes during the day with his alcohol consumption and medicinal intake.\nWe start by importing the required libraries.\n\n#Import Turing, Distributions and DataFrames\nusing Turing, Distributions, DataFrames, Distributed\n\n# Import MCMCChain, Plots, and StatsPlots for visualisations and diagnostics.\nusing MCMCChains, Plots, StatsPlots\n\n# Set a seed for reproducibility.\nusing Random\nRandom.seed!(12);\n\n\nGenerating data\nWe start off by creating a toy dataset. We take the case of a person who takes medicine to prevent excessive sneezing. Alcohol consumption increases the rate of sneezing for that person. Thus, the two factors affecting the number of sneezes in a given day are alcohol consumption and whether the person has taken his medicine. Both these variables are taken as boolean valued while the number of sneezes will be a count valued variable. We also take into consideration that the interaction between the two boolean variables will affect the number of sneezes\n5 random rows are printed from the generated data to get a gist of the data generated.\n\ntheta_noalcohol_meds = 1    # no alcohol, took medicine\ntheta_alcohol_meds = 3      # alcohol, took medicine\ntheta_noalcohol_nomeds = 6  # no alcohol, no medicine\ntheta_alcohol_nomeds = 36   # alcohol, no medicine\n\n# no of samples for each of the above cases\nq = 100\n\n#Generate data from different Poisson distributions\nnoalcohol_meds = Poisson(theta_noalcohol_meds)\nalcohol_meds = Poisson(theta_alcohol_meds)\nnoalcohol_nomeds = Poisson(theta_noalcohol_nomeds)\nalcohol_nomeds = Poisson(theta_alcohol_nomeds)\n\nnsneeze_data = vcat(\n    rand(noalcohol_meds, q),\n    rand(alcohol_meds, q),\n    rand(noalcohol_nomeds, q),\n    rand(alcohol_nomeds, q),\n)\nalcohol_data = vcat(zeros(q), ones(q), zeros(q), ones(q))\nmeds_data = vcat(zeros(q), zeros(q), ones(q), ones(q))\n\ndf = DataFrame(;\n    nsneeze=nsneeze_data,\n    alcohol_taken=alcohol_data,\n    nomeds_taken=meds_data,\n    product_alcohol_meds=meds_data .* alcohol_data,\n)\ndf[sample(1:nrow(df), 5; replace=false), :]\n\n5×4 DataFrame\n\n\n\nRow\nnsneeze\nalcohol_taken\nnomeds_taken\nproduct_alcohol_meds\n\n\n\nInt64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n2\n1.0\n0.0\n0.0\n\n\n2\n1\n0.0\n0.0\n0.0\n\n\n3\n2\n0.0\n0.0\n0.0\n\n\n4\n30\n1.0\n1.0\n1.0\n\n\n5\n0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nVisualisation of the dataset\nWe plot the distribution of the number of sneezes for the 4 different cases taken above. As expected, the person sneezes the most when he has taken alcohol and not taken his medicine. He sneezes the least when he doesn’t consume alcohol and takes his medicine.\n\n# Data Plotting\n\np1 = Plots.histogram(\n    df[(df[:, :alcohol_taken] .== 0) .& (df[:, :nomeds_taken] .== 0), 1];\n    title=\"no_alcohol+meds\",\n)\np2 = Plots.histogram(\n    (df[(df[:, :alcohol_taken] .== 1) .& (df[:, :nomeds_taken] .== 0), 1]);\n    title=\"alcohol+meds\",\n)\np3 = Plots.histogram(\n    (df[(df[:, :alcohol_taken] .== 0) .& (df[:, :nomeds_taken] .== 1), 1]);\n    title=\"no_alcohol+no_meds\",\n)\np4 = Plots.histogram(\n    (df[(df[:, :alcohol_taken] .== 1) .& (df[:, :nomeds_taken] .== 1), 1]);\n    title=\"alcohol+no_meds\",\n)\nplot(p1, p2, p3, p4; layout=(2, 2), legend=false)\n\n\n\n\nWe must convert our DataFrame data into the Matrix form as the manipulations that we are about are designed to work with Matrix data. We also separate the features from the labels which will be later used by the Turing sampler to generate samples from the posterior.\n\n# Convert the DataFrame object to matrices.\ndata = Matrix(df[:, [:alcohol_taken, :nomeds_taken, :product_alcohol_meds]])\ndata_labels = df[:, :nsneeze]\ndata\n\n400×3 Matrix{Float64}:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n ⋮         \n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n\n\nWe must standardise our data (centring about 0 with unit variance) to help the Turing sampler initialise parameter estimates effectively. We do this by subtracting the mean and dividing by the standard deviation for each column:\n\n# Rescale our matrices.\ndata = (data .- mean(data; dims=1)) ./ std(data; dims=1)\n\n400×3 Matrix{Float64}:\n -0.998749  -0.998749  -0.576628\n -0.998749  -0.998749  -0.576628\n -0.998749  -0.998749  -0.576628\n -0.998749  -0.998749  -0.576628\n -0.998749  -0.998749  -0.576628\n -0.998749  -0.998749  -0.576628\n -0.998749  -0.998749  -0.576628\n -0.998749  -0.998749  -0.576628\n -0.998749  -0.998749  -0.576628\n -0.998749  -0.998749  -0.576628\n  ⋮                    \n  0.998749   0.998749   1.72988\n  0.998749   0.998749   1.72988\n  0.998749   0.998749   1.72988\n  0.998749   0.998749   1.72988\n  0.998749   0.998749   1.72988\n  0.998749   0.998749   1.72988\n  0.998749   0.998749   1.72988\n  0.998749   0.998749   1.72988\n  0.998749   0.998749   1.72988\n\n\n\n\nDeclaring the Model: Poisson Regression\nOur model, poisson_regression takes four arguments:\n\nx is our set of independent variables;\ny is the element we want to predict;\nn is the number of observations we have; and\nσ² is the standard deviation we want to assume for our priors.\n\nWithin the model, we create four coefficients (b0, b1, b2, and b3) and assign a prior of normally distributed with means of zero and standard deviations of σ². We want to find values of these four coefficients to predict any given y.\nIntuitively, we can think of the coefficients as:\n\nb1 is the coefficient which represents the effect of taking alcohol on the number of sneezes;\nb2 is the coefficient which represents the effect of taking in no medicines on the number of sneezes;\nb3 is the coefficient which represents the effect of interaction between taking alcohol and no medicine on the number of sneezes;\n\nThe for block creates a variable theta which is the weighted combination of the input features. We have defined the priors on these weights above. We then observe the likelihood of calculating theta given the actual label, y[i].\n\n# Bayesian poisson regression (LR)\n@model function poisson_regression(x, y, n, σ²)\n    b0 ~ Normal(0, σ²)\n    b1 ~ Normal(0, σ²)\n    b2 ~ Normal(0, σ²)\n    b3 ~ Normal(0, σ²)\n    for i in 1:n\n        theta = b0 + b1 * x[i, 1] + b2 * x[i, 2] + b3 * x[i, 3]\n        y[i] ~ Poisson(exp(theta))\n    end\nend;\n\n\n\nSampling from the posterior\nWe use the NUTS sampler to sample values from the posterior. We run multiple chains using the MCMCThreads() function to nullify the effect of a problematic chain. We then use the Gelman, Rubin, and Brooks Diagnostic to check the convergence of these multiple chains.\n\n# Retrieve the number of observations.\nn, _ = size(data)\n\n# Sample using NUTS.\n\nnum_chains = 4\nm = poisson_regression(data, data_labels, n, 10)\nchain = sample(m, NUTS(), MCMCThreads(), 2_500, num_chains; discard_adapt=false, progress=false)\n\n\n\nChains MCMC chain (2500×18×4 Array{Union{Missing, Float64}, 3}):\n\nIterations        = 1:1:2500\nNumber of chains  = 4\nSamples per chain = 2500\nWall duration     = 17.13 seconds\nCompute duration  = 13.14 seconds\nparameters        = b0, b1, b2, b3\ninternals         = logprior, loglikelihood, logjoint, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n\n\n\n\n\n\nWarningSampling With Multiple Threads\n\n\n\n\n\nThe sample() call above assumes that you have at least nchains threads available in your Julia instance. If you do not, the multiple chains will run sequentially, and you may notice a warning. For more information, see the Turing documentation on sampling multiple chains.\n\n\n\n\n\nViewing the Diagnostics\nWe use the Gelman, Rubin, and Brooks Diagnostic to check whether our chains have converged. Note that we require multiple chains to use this diagnostic which analyses the difference between these multiple chains.\nWe expect the chains to have converged. This is because we have taken sufficient number of iterations (1500) for the NUTS sampler. However, in case the test fails, then we will have to take a larger number of iterations, resulting in longer computation time.\n\n# Because some of the sampler statistics are `missing`, we need to extract only\n# the parameters and then concretize the array so that `gelmandiag` can be computed.\nparameter_chain = MCMCChains.concretize(MCMCChains.get_sections(chain, :parameters))\ngelmandiag(parameter_chain)\n\n\nGelman, Rubin, and Brooks diagnostic\n\n  parameters      psrf    psrfci\n      Symbol   Float64   Float64\n\n          b0    1.1302    1.1556\n          b1    1.0091    1.0110\n          b2    1.0503    1.0767\n          b3    1.0130    1.0179\n\n\n\n\n\nFrom the above diagnostic, we can conclude that the chains have converged because the PSRF values of the coefficients are close to 1.\nSo, we have obtained the posterior distributions of the parameters. We transform the coefficients and recover theta values by taking the exponent of the meaned values of the coefficients b0, b1, b2 and b3. We take the exponent of the means to get a better comparison of the relative values of the coefficients. We then compare this with the intuitive meaning that was described earlier.\n\n# Taking the first chain\nc1 = chain[:, :, 1]\n\n# Calculating the exponentiated means\nb0_exp = exp(mean(c1[:b0]))\nb1_exp = exp(mean(c1[:b1]))\nb2_exp = exp(mean(c1[:b2]))\nb3_exp = exp(mean(c1[:b3]))\n\nprint(\"The exponent of the meaned values of the weights (or coefficients are): \\n\")\nprintln(\"b0: \", b0_exp)\nprintln(\"b1: \", b1_exp)\nprintln(\"b2: \", b2_exp)\nprintln(\"b3: \", b3_exp)\nprint(\"The posterior distributions obtained after sampling can be visualised as :\\n\")\n\nThe exponent of the meaned values of the weights (or coefficients are): \nb0: 4.82742260537924\nb1: 1.9295756838005809\nb2: 2.7071596587575177\nb3: 1.2199712022407607\nThe posterior distributions obtained after sampling can be visualised as :\n\n\nVisualising the posterior by plotting it:\n\nplot(chain)\n\n\n\n\n\n\nInterpreting the Obtained Mean Values\nThe exponentiated mean of the coefficient b1 is roughly half of that of b2. This makes sense because in the data that we generated, the number of sneezes was more sensitive to the medicinal intake as compared to the alcohol consumption. We also get a weaker dependence on the interaction between the alcohol consumption and the medicinal intake as can be seen from the value of b3.\n\n\nRemoving the Warmup Samples\nAs can be seen from the plots above, the parameters converge to their final distributions after a few iterations. The initial samples during the warmup phase show higher variability and should be discarded before computing posterior statistics. Thus, we remove these warmup samples and view the diagnostics again. We discard the first 200 samples, corresponding to the adaptation phase used by the NUTS sampler (which we explicitly chose not to discard earlier with the discard_adapt=false argument).\n\nchains_new = chain[201:end, :, :]\n\nChains MCMC chain (2300×18×4 Array{Union{Missing, Float64}, 3}):\n\nIterations        = 201:1:2500\nNumber of chains  = 4\nSamples per chain = 2300\nWall duration     = 17.13 seconds\nCompute duration  = 13.14 seconds\nparameters        = b0, b1, b2, b3\ninternals         = logprior, loglikelihood, logjoint, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n\nplot(chains_new)\n\n\n\n\nAs can be seen from the numeric values and the plots above, the standard deviation values have decreased and all the plotted values are from the estimated posteriors. The exponentiated mean values, with the warmup samples removed, have not changed by much and they are still in accordance with their intuitive meanings as described earlier.\n\n\n\n\n Back to top",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Poisson Regression"
    ]
  },
  {
    "objectID": "uri/troubleshooting.html",
    "href": "uri/troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "If you are not redirected, please click here.\n\nNo matching items\n Back to top",
    "crumbs": null
  },
  {
    "objectID": "developers/compiler/minituring-contexts/index.html",
    "href": "developers/compiler/minituring-contexts/index.html",
    "title": "A Mini Turing Implementation II: Contexts",
    "section": "",
    "text": "In the Mini Turing tutorial we developed a miniature version of the Turing language, to illustrate its core design. A passing mention was made of contexts. In this tutorial we develop that aspect of our mini Turing language further to demonstrate how and why contexts are an important part of Turing’s design.",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL's Compiler",
      "A Mini Turing Implementation II: Contexts"
    ]
  },
  {
    "objectID": "developers/compiler/minituring-contexts/index.html#contexts-within-contexts",
    "href": "developers/compiler/minituring-contexts/index.html#contexts-within-contexts",
    "title": "A Mini Turing Implementation II: Contexts",
    "section": "Contexts within contexts",
    "text": "Contexts within contexts\nLet’s use the above two contexts to provide a slightly more general definition of the SamplingContext and the Metropolis-Hastings sampler we wrote in the mini Turing tutorial.\n\nstruct SamplingContext{S&lt;:AbstractMCMC.AbstractSampler,R&lt;:Random.AbstractRNG}\n    rng::R\n    sampler::S\n    subcontext::Union{PriorContext, JointContext}\nend\n\nThe new aspect here is the subcontext field. Note that this is a context within a context! The idea is that we don’t need to hard code how the MCMC sampler evaluates the log probability, but rather can pass that work onto the subcontext. This way the same sampler can be used to sample from either the joint or the prior distribution.\nThe methods for SamplingContext are largely as in the our earlier mini Turing case, except they now pass some of the work onto the subcontext:\n\nfunction observe(context::SamplingContext, args...)\n    # Sampling doesn't affect the observed values, so nothing to do here other than pass to\n    # the subcontext.\n    return observe(context.subcontext, args...)\nend\n\nstruct PriorSampler &lt;: AbstractMCMC.AbstractSampler end\n\nfunction assume(context::SamplingContext{PriorSampler}, varinfo, dist, var_id)\n    sample = Random.rand(context.rng, dist)\n    varinfo[var_id] = (sample, NaN)\n    # Once the value has been sampled, let the subcontext handle evaluating the log\n    # probability.\n    return assume(context.subcontext, varinfo, dist, var_id)\nend;\n\n# The subcontext field of the MHSampler determines which distribution this sampler\n# samples from.\nstruct MHSampler{D, T&lt;:Real} &lt;: AbstractMCMC.AbstractSampler\n    sigma::T\n    subcontext::D\nend\n\nMHSampler(subcontext) = MHSampler(1, subcontext)\n\nfunction assume(context::SamplingContext{&lt;:MHSampler}, varinfo, dist, var_id)\n    sampler = context.sampler\n    old_value = varinfo.values[var_id]\n\n    # propose a random-walk step, i.e, add the current value to a random\n    # value sampled from a Normal distribution centred at 0\n    value = rand(context.rng, Normal(old_value, sampler.sigma))\n    varinfo[var_id] = (value, NaN)\n    # Once the value has been sampled, let the subcontext handle evaluating the log\n    # probability.\n    return assume(context.subcontext, varinfo, dist, var_id)\nend;\n\n# The following three methods are identical to before, except for passing\n# `sampler.subcontext` to the context SamplingContext.\nfunction AbstractMCMC.step(\n    rng::Random.AbstractRNG, model::MiniModel, sampler::MHSampler; kwargs...\n)\n    vi = VarInfo()\n    ctx = SamplingContext(rng, PriorSampler(), sampler.subcontext)\n    model.f(vi, ctx, values(model.data)...)\n    return vi, vi\nend\n\nfunction AbstractMCMC.step(\n    rng::Random.AbstractRNG,\n    model::MiniModel,\n    sampler::MHSampler,\n    prev_state::VarInfo; # is just the old trace\n    kwargs...,\n)\n    vi = prev_state\n    new_vi = deepcopy(vi)\n    ctx = SamplingContext(rng, sampler, sampler.subcontext)\n    model.f(new_vi, ctx, values(model.data)...)\n\n    # Compute log acceptance probability\n    # Since the proposal is symmetric the computation can be simplified\n    logα = sum(values(new_vi.logps)) - sum(values(vi.logps))\n\n    # Accept proposal with computed acceptance probability\n    if -Random.randexp(rng) &lt; logα\n        return new_vi, new_vi\n    else\n        return prev_state, prev_state\n    end\nend;\n\nfunction AbstractMCMC.bundle_samples(\n    samples, model::MiniModel, ::MHSampler, ::Any, ::Type{Chains}; kwargs...\n)\n    # We get a vector of traces\n    values = [sample.values for sample in samples]\n    params = [key for key in keys(values[1]) if key ∉ keys(model.data)]\n    vals = reduce(hcat, [value[p] for value in values] for p in params)\n    # Composing the `Chains` data-structure, of which analysing infrastructure is provided\n    chains = Chains(vals, params)\n    return chains\nend;\n\nWe can use this to sample from the joint distribution just like before:\n\nsample(MiniModel(m, (x=3.0,)), MHSampler(JointContext()), 1_000_000; chain_type=Chains, progress=false)\n\nChains MCMC chain (1000000×2×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000000\nNumber of chains  = 1\nSamples per chain = 1000000\nparameters        = a, b\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nor we can choose to sample from the prior instead\n\nsample(MiniModel(m, (x=3.0,)), MHSampler(PriorContext()), 1_000_000; chain_type=Chains, progress=false)\n\nChains MCMC chain (1000000×2×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000000\nNumber of chains  = 1\nSamples per chain = 1000000\nparameters        = a, b\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nOf course, using an MCMC algorithm to sample from the prior is unnecessary and silly (PriorSampler exists, after all), but the point is to illustrate the flexibility of the context system. We could, for instance, use the same setup to implement an Approximate Bayesian Computation (ABC) algorithm.\nThe use of contexts also goes far beyond just evaluating log probabilities and sampling. Some examples from Turing are\n\nFixedContext, which fixes some variables to given values and removes them completely from the evaluation of any log probabilities. They power the Turing.fix and Turing.unfix functions.\nConditionContext conditions the model on fixed values for some parameters. They are used by Turing.condition and Turing.decondition, i.e. the model | (parameter=value,) syntax. The difference between fix and condition is whether the log probability for the corresponding variable is included in the overall log density.\nPriorExtractorContext collects information about what the prior distribution of each variable is.\nPrefixContext adds prefixes to variable names, allowing models to be used within other models without variable name collisions.\nPointwiseLikelihoodContext records the log likelihood of each individual variable.\nDebugContext collects useful debugging information while executing the model.\n\nAll of the above are what Turing calls parent contexts, which is to say that they all keep a subcontext just like our above SamplingContext did. Their implementations of assume and observe call the implementation of the subcontext once they are done doing their own work of fixing/conditioning/prefixing/etc. Contexts are often chained, so that e.g. a DebugContext may wrap within it a PrefixContext, which may in turn wrap a ConditionContext, etc. The only contexts that don’t have a subcontext in the Turing are the ones for evaluating the prior, likelihood, and joint distributions. These are called leaf contexts.\nThe above version of mini Turing is still much simpler than the full Turing language, but the principles of how contexts are used are the same.",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL's Compiler",
      "A Mini Turing Implementation II: Contexts"
    ]
  },
  {
    "objectID": "developers/compiler/model-manual/index.html",
    "href": "developers/compiler/model-manual/index.html",
    "title": "Manually Defining a Model",
    "section": "",
    "text": "Traditionally, models in Turing are defined using the @model macro:\n\nusing Turing\n\n@model function gdemo(x)\n    # Set priors.\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n\n    # Observe each value of x.\n    x .~ Normal(m, sqrt(s²))\n\n    return nothing\nend\n\nmodel = gdemo([1.5, 2.0])\n\nDynamicPPL.Model{typeof(gdemo), (:x,), (), (), Tuple{Vector{Float64}}, Tuple{}, DynamicPPL.DefaultContext, false}(gdemo, (x = [1.5, 2.0],), NamedTuple(), DynamicPPL.DefaultContext())\n\n\nThe @model macro accepts a function definition and rewrites it such that call of the function generates a Model struct for use by the sampler.\nHowever, models can be constructed by hand without the use of a macro. Taking the gdemo model above as an example, the macro-based definition can be implemented as well (a bit less generally) with the macro-free version\n\nusing DynamicPPL\n\n# Create the model function.\nfunction gdemo2(model, varinfo, x)\n    # Assume s² has an InverseGamma distribution.\n    s², varinfo = DynamicPPL.tilde_assume!!(\n        model.context, InverseGamma(2, 3), @varname(s²), varinfo\n    )\n\n    # Assume m has a Normal distribution.\n    m, varinfo = DynamicPPL.tilde_assume!!(\n        model.context, Normal(0, sqrt(s²)), @varname(m), varinfo\n    )\n\n    # Observe each value of x[i] according to a Normal distribution.\n    for i in eachindex(x)\n        _retval, varinfo = DynamicPPL.tilde_observe!!(\n            model.context, Normal(m, sqrt(s²)), x[i], @varname(x[i]), varinfo\n        )\n    end\n\n    # The final return statement should comprise both the original return\n    # value and the updated varinfo.\n    return nothing, varinfo\nend\n\n# The `false` type parameter here indicates that this model does not need\n# threadsafe evaluation (see the threadsafe evaluation page for details)\ngdemo2(x) = DynamicPPL.Model{false}(gdemo2, (; x))\n\n# Instantiate a Model object with our data variables.\nmodel2 = gdemo2([1.5, 2.0])\n\nModel{typeof(gdemo2), (:x,), (), (), Tuple{Vector{Float64}}, Tuple{}, DefaultContext, false}(gdemo2, (x = [1.5, 2.0],), NamedTuple(), DefaultContext())\n\n\nWe can sample from this model in the same way:\n\nchain = sample(model2, NUTS(), 1000; progress=false)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.8\n\n\n\n\nChains MCMC chain (1000×16×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 6.57 seconds\nCompute duration  = 6.57 seconds\nparameters        = s², m\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nThe subsequent pages in this section will show how the @model macro does this behind-the-scenes.\n\n\n\n Back to top",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL's Compiler",
      "Manually Defining a Model"
    ]
  },
  {
    "objectID": "developers/contexts/submodel-condition/index.html",
    "href": "developers/contexts/submodel-condition/index.html",
    "title": "Conditioning and fixing in submodels",
    "section": "",
    "text": "This page is a technical explanation of how contexts are managed for submodels.\nA user-facing guide to submodels is available on this page.",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Contexts",
      "Conditioning and fixing in submodels"
    ]
  },
  {
    "objectID": "developers/contexts/submodel-condition/index.html#prefixcontext",
    "href": "developers/contexts/submodel-condition/index.html#prefixcontext",
    "title": "Conditioning and fixing in submodels",
    "section": "PrefixContext",
    "text": "PrefixContext\nSubmodels in DynamicPPL come with the notion of prefixing variables: under the hood, this is implemented by adding a PrefixContext to the context stack.\nPrefixContext is a context that, as the name suggests, prefixes all variables inside a model with a given symbol. Thus, for example:\n\nusing DynamicPPL, Distributions\n\n@model function f()\n    x ~ Normal()\n    return y ~ Normal()\nend\n\n@model function g()\n    return a ~ to_submodel(f())\nend\n\ng (generic function with 2 methods)\n\n\ninside the submodel f, the variables x and y become a.x and a.y respectively. This is easiest to observe by running the model:\n\nvi = VarInfo(g())\nkeys(vi)\n\n2-element Vector{VarName{:a}}:\n a.x\n a.y\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn this case, where to_submodel is called without any other arguments, the prefix to be used is automatically inferred from the name of the variable on the left-hand side of the tilde. We will return to the ‘manual prefixing’ case later.\n\n\nThe phrase ‘becoming’ a different variable is a little underspecified: it is useful to pinpoint the exact location where the prefixing occurs, which is tilde_assume. The method responsible for it is tilde_assume(::PrefixContext, right, vn, vi): this attaches the prefix in the context to the VarName argument, before recursively calling tilde_assume with the new prefixed VarName. This means that even though a statement x ~ dist still enters the tilde pipeline at the top level as x, if the model evaluation context contains a PrefixContext, any function after tilde_assume(::PrefixContext, ...) will see a.x instead.",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Contexts",
      "Conditioning and fixing in submodels"
    ]
  },
  {
    "objectID": "developers/contexts/submodel-condition/index.html#conditioncontext",
    "href": "developers/contexts/submodel-condition/index.html#conditioncontext",
    "title": "Conditioning and fixing in submodels",
    "section": "ConditionContext",
    "text": "ConditionContext\nConditionContext is a context which stores values of variables that are to be conditioned on. These values may be stored as a Dict which maps VarNames to values, or alternatively as a NamedTuple. The latter only works correctly if all VarNames are ‘basic’, in that they have an identity optic (i.e., something like a.x or a[1] is forbidden). Because of this limitation, we will only use Dict in this example.\n\n\n\n\n\n\nNote\n\n\n\nIf a ConditionContext with a NamedTuple encounters anything to do with a prefix, its internal NamedTuple is converted to a Dict anyway, so it is quite reasonable to ignore the NamedTuple case in this exposition.\n\n\nOne can inspect the conditioning values with, for example:\n\n@model function d()\n    x ~ Normal()\n    return y ~ Normal()\nend\n\ncond_model = d() | (@varname(x) =&gt; 1.0)\ncond_ctx = cond_model.context\n\nConditionContext(Dict(x =&gt; 1.0), DynamicPPL.DefaultContext())\n\n\nThere are several internal functions that are used to determine whether a variable is conditioned, and if so, what its value is.\n\nDynamicPPL.hasconditioned_nested(cond_ctx, @varname(x))\n\ntrue\n\n\n\nDynamicPPL.getconditioned_nested(cond_ctx, @varname(x))\n\n1.0\n\n\nThese functions are in turn used by the function DynamicPPL.contextual_isassumption, which is largely the same as hasconditioned_nested, but also checks whether the value is missing (in which case it isn’t really conditioned).\n\nDynamicPPL.contextual_isassumption(cond_ctx, @varname(x))\n\nfalse\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that (neglecting missing values) the return value of contextual_isassumption is the opposite of hasconditioned_nested, i.e. for a variable that is conditioned on, contextual_isassumption returns false.\n\n\nIf a variable x is conditioned on, then the effect of this is to set the value of x to the given value (while still including its contribution to the log probability density). Since x is no longer a random variable, if we were to evaluate the model, we would find only one key in the VarInfo:\n\nkeys(VarInfo(cond_model))\n\n1-element Vector{VarName{:y, typeof(identity)}}:\n y",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Contexts",
      "Conditioning and fixing in submodels"
    ]
  },
  {
    "objectID": "developers/contexts/submodel-condition/index.html#joint-behaviour-desiderata-at-the-model-level",
    "href": "developers/contexts/submodel-condition/index.html#joint-behaviour-desiderata-at-the-model-level",
    "title": "Conditioning and fixing in submodels",
    "section": "Joint behaviour: desiderata at the model level",
    "text": "Joint behaviour: desiderata at the model level\nWhen paired together, these two contexts have the potential to cause substantial confusion: PrefixContext modifies the variable names that are seen, which may cause them to be out of sync with the values contained inside the ConditionContext.\nWe begin by mentioning some high-level desiderata for their joint behaviour. Take these models, for example:\n\n@model function inner()\n    println(\"inner context: $(__model__.context)\")\n    x ~ Normal()\n    return y ~ Normal()\nend\n\n@model function outer()\n    println(\"outer context: $(__model__.context)\")\n    return a ~ to_submodel(inner())\nend\n\n# 'Outer conditioning'\nwith_outer_cond = outer() | (@varname(a.x) =&gt; 1.0)\n\n# 'Inner conditioning'\ninner_cond = inner() | (@varname(x) =&gt; 1.0)\n@model function outer2()\n    println(\"outer context: $(__model__.context)\")\n    return a ~ to_submodel(inner_cond)\nend\nwith_inner_cond = outer2()\n\nModel{typeof(outer2), (), (), (), Tuple{}, Tuple{}, DefaultContext, false}(outer2, NamedTuple(), NamedTuple(), DefaultContext())\n\n\nWe want that:\n\nkeys(VarInfo(outer())) should return [a.x, a.y];\nkeys(VarInfo(with_outer_cond)) should return [a.y];\nkeys(VarInfo(with_inner_cond)) should return [a.y],\n\nIn other words, we can condition submodels either from the outside (point (2)) or from the inside (point (3)), and the variable name we use to specify the conditioning should match the level at which we perform the conditioning.\nThis is an incredibly salient point because it means that submodels can be treated as individual, opaque objects, and we can condition them without needing to know what it will be prefixed with, or the context in which that submodel is being used. For example, this means we can reuse inner_cond in another model with a different prefix, and it will still have its inner x value be conditioned, despite the prefix differing.\n\n\n\n\n\n\nNote\n\n\n\nIn the current version of DynamicPPL, these criteria are all fulfilled. However, this was not the case in the past: in particular, point (3) was not fulfilled, and users had to condition the internal submodel with the prefixes that were used outside. (See this GitHub issue for more information; this issue was the direct motivation for this documentation page.)",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Contexts",
      "Conditioning and fixing in submodels"
    ]
  },
  {
    "objectID": "developers/contexts/submodel-condition/index.html#desiderata-at-the-context-level",
    "href": "developers/contexts/submodel-condition/index.html#desiderata-at-the-context-level",
    "title": "Conditioning and fixing in submodels",
    "section": "Desiderata at the context level",
    "text": "Desiderata at the context level\nThe above section describes how we expect conditioning and prefixing to behave from a user’s perpective. We now turn to the question of how we implement this in terms of DynamicPPL contexts. We do not specify the implementation details here, but we will sketch out something resembling an API that will allow us to achieve the target behaviour.\nPoint (1) does not involve any conditioning, only prefixing; it is therefore already satisfied by virtue of the tilde_assume method shown above.\nPoints (2) and (3) are more tricky. As the reader may surmise, the difference between them is the order in which the contexts are stacked.\nFor the outer conditioning case (point (2)), the ConditionContext will contain a VarName that is already prefixed. When we enter the inner submodel, this ConditionContext has to be passed down and somehow combined with the PrefixContext that is created when we enter the submodel. We make the claim here that the best way to do this is to nest the PrefixContext inside the ConditionContext. This is indeed what happens, as can be demonstrated by running the model.\n\nwith_outer_cond()\n\nouter context: ConditionContext(Dict(a.x =&gt; 1.0), DynamicPPL.InitContext{Random.TaskLocalRNG, DynamicPPL.InitFromPrior}(Random.TaskLocalRNG(), DynamicPPL.InitFromPrior()))\ninner context: ConditionContext(Dict(a.x =&gt; 1.0), DynamicPPL.PrefixContext{AbstractPPL.VarName{:a, typeof(identity)}, DynamicPPL.InitContext{Random.TaskLocalRNG, DynamicPPL.InitFromPrior}}(a, DynamicPPL.InitContext{Random.TaskLocalRNG, DynamicPPL.InitFromPrior}(Random.TaskLocalRNG(), DynamicPPL.InitFromPrior())))\n\n\n1.5870428317619876\n\n\nFor the inner conditioning case (point (3)), the outer model is not run with any special context. The inner model will itself contain a ConditionContext will contain a VarName that is not prefixed. When we run the model, this ConditionContext should be then nested inside a PrefixContext to form the final evaluation context. Again, we can run the model to see this in action:\n\nwith_inner_cond()\n\nouter context: DynamicPPL.InitContext{Random.TaskLocalRNG, DynamicPPL.InitFromPrior}(Random.TaskLocalRNG(), DynamicPPL.InitFromPrior())\ninner context: DynamicPPL.PrefixContext{AbstractPPL.VarName{:a, typeof(identity)}, DynamicPPL.ConditionContext{Dict{AbstractPPL.VarName{:x, typeof(identity)}, Float64}, DynamicPPL.InitContext{Random.TaskLocalRNG, DynamicPPL.InitFromPrior}}}(a, ConditionContext(Dict(x =&gt; 1.0), DynamicPPL.InitContext{Random.TaskLocalRNG, DynamicPPL.InitFromPrior}(Random.TaskLocalRNG(), DynamicPPL.InitFromPrior())))\n\n\n0.6573752977829312\n\n\nPutting all of the information so far together, what it means is that if we have these two inner contexts (taken from above):\n\nusing DynamicPPL: PrefixContext, ConditionContext, DefaultContext\n\ninner_ctx_with_outer_cond = ConditionContext(\n    Dict(@varname(a.x) =&gt; 1.0), PrefixContext(@varname(a))\n)\ninner_ctx_with_inner_cond = PrefixContext(\n    @varname(a), ConditionContext(Dict(@varname(x) =&gt; 1.0))\n)\n\nPrefixContext{VarName{:a, typeof(identity)}, ConditionContext{Dict{VarName{:x, typeof(identity)}, Float64}, DefaultContext}}(a, ConditionContext(Dict(x =&gt; 1.0), DynamicPPL.DefaultContext()))\n\n\nthen we want both of these to be true (and thankfully, they are!):\n\nDynamicPPL.hasconditioned_nested(inner_ctx_with_outer_cond, @varname(a.x))\n\ntrue\n\n\n\nDynamicPPL.hasconditioned_nested(inner_ctx_with_inner_cond, @varname(a.x))\n\ntrue\n\n\nThis allows us to finally specify our task as follows:\n\nGiven the correct arguments, we need to make sure that hasconditioned_nested and getconditioned_nested behave correctly.\nWe need to make sure that both the correct arguments are supplied. In order to do so:\n\n\n(2a) We need to make sure that when evaluating a submodel, the context stack is arranged such that PrefixContext is applied inside the parent model’s context, but outside the submodel’s own context.\n(2b) We also need to make sure that the VarName passed to it is prefixed correctly.",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Contexts",
      "Conditioning and fixing in submodels"
    ]
  },
  {
    "objectID": "developers/contexts/submodel-condition/index.html#how-do-we-do-it",
    "href": "developers/contexts/submodel-condition/index.html#how-do-we-do-it",
    "title": "Conditioning and fixing in submodels",
    "section": "How do we do it?",
    "text": "How do we do it?\n\nhasconditioned_nested and getconditioned_nested accomplish this by first ‘collapsing’ the context stack, i.e. they go through the context stack, remove all PrefixContexts, and apply those prefixes to any conditioned variables below it in the stack. Once the PrefixContexts have been removed, one can then iterate through the context stack and check if any of the ConditionContexts contain the variable, or get the value itself. For more details the reader is encouraged to read the source code.\n\n(2a) We ensure that the context stack is correctly arranged by relying on the behaviour of make_evaluate_args_and_kwargs. This function is called whenever a model (which itself contains a context) is evaluated with a separate (‘external’) context, and makes sure to arrange both of these contexts such that the model’s context is nested inside the external context. Thus, as long as prefixing is implemented by applying a PrefixContext on the outermost layer of the inner model context, this will be correctly combined with an external context to give the behaviour seen above.\n(2b) At first glance, it seems like tilde_assume can take care of the VarName prefixing for us (as described in the first section). However, this is not actually the case: contextual_isassumption, which is the function that calls hasconditioned_nested, is much higher in the call stack than tilde_assume is. So, we need to explicitly prefix it before passing it to contextual_isassumption. This is done inside the @model macro, or technically, its subsidiary function isassumption.",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Contexts",
      "Conditioning and fixing in submodels"
    ]
  },
  {
    "objectID": "developers/contexts/submodel-condition/index.html#nested-submodels",
    "href": "developers/contexts/submodel-condition/index.html#nested-submodels",
    "title": "Conditioning and fixing in submodels",
    "section": "Nested submodels",
    "text": "Nested submodels\nJust in case the above wasn’t complicated enough, we need to also be very careful when dealing with nested submodels, which have multiple layers of PrefixContexts which may be interspersed with ConditionContexts. For example, in this series of nested submodels,\n\n@model function charlie()\n    x ~ Normal()\n    y ~ Normal()\n    return z ~ Normal()\nend\n@model function bravo()\n    return b ~ to_submodel(charlie() | (@varname(x) =&gt; 1.0))\nend\n@model function alpha()\n    return a ~ to_submodel(bravo() | (@varname(b.y) =&gt; 1.0))\nend\n\nalpha (generic function with 2 methods)\n\n\nwe expect that the only variable to be sampled should be z inside charlie, or rather, a.b.z once it has been through the prefixes.\n\nkeys(VarInfo(alpha()))\n\n1-element Vector{VarName{:a, ComposedFunction{Accessors.PropertyLens{:z}, Accessors.PropertyLens{:b}}}}:\n a.b.z\n\n\nThe general strategy that we adopt is similar to above. Following the principle that PrefixContext should be nested inside the outer context, but outside the inner submodel’s context, we can infer that the correct context inside charlie should be:\n\nbig_ctx = PrefixContext(\n    @varname(a),\n    ConditionContext(\n        Dict(@varname(b.y) =&gt; 1.0),\n        PrefixContext(@varname(b), ConditionContext(Dict(@varname(x) =&gt; 1.0))),\n    ),\n)\n\nPrefixContext{VarName{:a, typeof(identity)}, ConditionContext{Dict{VarName{:b, Accessors.PropertyLens{:y}}, Float64}, PrefixContext{VarName{:b, typeof(identity)}, ConditionContext{Dict{VarName{:x, typeof(identity)}, Float64}, DefaultContext}}}}(a, ConditionContext(Dict(b.y =&gt; 1.0), DynamicPPL.PrefixContext{AbstractPPL.VarName{:b, typeof(identity)}, DynamicPPL.ConditionContext{Dict{AbstractPPL.VarName{:x, typeof(identity)}, Float64}, DynamicPPL.DefaultContext}}(b, ConditionContext(Dict(x =&gt; 1.0), DynamicPPL.DefaultContext()))))\n\n\nWe need several things to work correctly here: we need the VarName prefixing to behave correctly, and then we need to implement hasconditioned_nested and getconditioned_nested on the resulting prefixed VarName. It turns out that the prefixing itself is enough to illustrate the most important point in this section, namely, the need to traverse the context stack in a different direction to what most of DynamicPPL does.\nLet’s work with a function called myprefix(::AbstractContext, ::VarName) (to avoid confusion with any existing DynamicPPL function). We should like myprefix(big_ctx, @varname(x)) to return @varname(a.b.x). Consider the following naive implementation, which mirrors a lot of code in the tilde-pipeline:\n\nusing DynamicPPL: childcontext, AbstractContext, AbstractParentContext\nusing AbstractPPL: AbstractPPL\n\nmyprefix(::AbstractContext, vn::VarName) = vn\nmyprefix(ctx::AbstractParentContext, vn::VarName) = myprefix(childcontext(ctx), vn)\nfunction myprefix(ctx::DynamicPPL.PrefixContext, vn::VarName)\n    # The functionality to actually manipulate the VarNames is in AbstractPPL\n    new_vn = AbstractPPL.prefix(vn, ctx.vn_prefix)\n    # Then pass to the child context\n    return myprefix(childcontext(ctx), new_vn)\nend\n\nmyprefix(big_ctx, @varname(x))\n\nb.a.x\n\n\nThis implementation clearly is not correct, because it applies the inner PrefixContext before the outer one.\nThe right way to implement myprefix is to, essentially, reverse the order of two lines above:\n\nfunction myprefix(ctx::DynamicPPL.PrefixContext, vn::VarName)\n    # Pass to the child context first\n    new_vn = myprefix(childcontext(ctx), vn)\n    # Then apply this context's prefix\n    return AbstractPPL.prefix(new_vn, ctx.vn_prefix)\nend\n\nmyprefix(big_ctx, @varname(x))\n\na.b.x\n\n\nThis is a much better result! The implementation of related functions such as hasconditioned_nested and getconditioned_nested, under the hood, use a similar recursion scheme, so you will find that this is a common pattern when reading the source code of various prefixing-related functions. When editing this code, it is worth being mindful of this as a potential source of incorrectness.\n\n\n\n\n\n\nNote\n\n\n\nIf you have encountered left and right folds, the above discussion illustrates the difference between them: the wrong implementation of myprefix uses a left fold (which collects prefixes in the opposite order from which they are encountered), while the correct implementation uses a right fold.",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Contexts",
      "Conditioning and fixing in submodels"
    ]
  },
  {
    "objectID": "developers/contexts/submodel-condition/index.html#loose-ends-1-manual-prefixing",
    "href": "developers/contexts/submodel-condition/index.html#loose-ends-1-manual-prefixing",
    "title": "Conditioning and fixing in submodels",
    "section": "Loose ends 1: Manual prefixing",
    "text": "Loose ends 1: Manual prefixing\nSometimes users may want to manually prefix a model, for example:\n\n@model function inner_manual()\n    x ~ Normal()\n    return y ~ Normal()\nend\n\n@model function outer_manual()\n    return _unused ~ to_submodel(prefix(inner_manual(), :a), false)\nend\n\nouter_manual (generic function with 2 methods)\n\n\nIn this case, the VarName on the left-hand side of the tilde is not used, and the prefix is instead specified using the prefix function.\nThe way to deal with this follows on from the previous discussion. Specifically, we said that:\n\n[…] as long as prefixing is implemented by applying a PrefixContext on the outermost layer of the inner model context, this will be correctly combined […]\n\nWhen automatic prefixing is used, this application of PrefixContext occurs inside the tilde_assume!! method. In the manual prefixing case, we need to make sure that prefix(submodel::Model, ::Symbol) does the same thing, i.e. it inserts a PrefixContext at the outermost layer of submodel’s context. We can see that this is precisely what happens:\n\n@model f() = x ~ Normal()\n\nmodel = f()\nprefixed_model = prefix(model, :a)\n\n(model.context, prefixed_model.context)\n\n(DefaultContext(), PrefixContext{VarName{:a, typeof(identity)}, DefaultContext}(a, DefaultContext()))",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Contexts",
      "Conditioning and fixing in submodels"
    ]
  },
  {
    "objectID": "developers/contexts/submodel-condition/index.html#loose-ends-2-fixedcontext",
    "href": "developers/contexts/submodel-condition/index.html#loose-ends-2-fixedcontext",
    "title": "Conditioning and fixing in submodels",
    "section": "Loose ends 2: FixedContext",
    "text": "Loose ends 2: FixedContext\nFinally, note that all of the above also applies to the interaction between PrefixContext and FixedContext, except that the functions have different names. (FixedContext behaves the same way as ConditionContext, except that unlike conditioned variables, fixed variables do not contribute to the log probability density.) This generally results in a large amount of code duplication, but the concepts that underlie both contexts are exactly the same.",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Contexts",
      "Conditioning and fixing in submodels"
    ]
  },
  {
    "objectID": "developers/transforms/dynamicppl/index.html",
    "href": "developers/transforms/dynamicppl/index.html",
    "title": "Variable transformations in DynamicPPL",
    "section": "",
    "text": "In the final part of this chapter, we will discuss the higher-level implications of constrained distributions in the Turing.jl framework.",
    "crumbs": [
      "Get Started",
      "Developers",
      "Variable Transformations",
      "Variable transformations in DynamicPPL"
    ]
  },
  {
    "objectID": "developers/transforms/dynamicppl/index.html#linked-and-unlinked-varinfos-in-dynamicppl",
    "href": "developers/transforms/dynamicppl/index.html#linked-and-unlinked-varinfos-in-dynamicppl",
    "title": "Variable transformations in DynamicPPL",
    "section": "Linked and unlinked VarInfos in DynamicPPL",
    "text": "Linked and unlinked VarInfos in DynamicPPL\n\nimport Random\nRandom.seed!(468);\n\n# Turing re-exports the entirety of Distributions\nusing Turing\n\nWhen we are performing Bayesian inference, we are trying to sample from a joint probability distribution, which isn’t usually a single, well-defined distribution like in the rather simplified example above. However, each random variable in the model will have its own distribution, and often some of these will be constrained. For example, if b ~ LogNormal() is a random variable in a model, then \\(p(b)\\) will be zero for any \\(b \\leq 0\\). Consequently, any joint probability \\(p(b, c, \\ldots)\\) will also be zero for any combination of parameters where \\(b \\leq 0\\), and so that joint distribution is itself constrained.\nTo get around this, DynamicPPL allows the variables to be transformed in exactly the same way as above. For simplicity, consider the following model:\n\nusing DynamicPPL\n\n@model function demo()\n    x ~ LogNormal()\nend\n\nmodel = demo()\nvi = VarInfo(model)\nvn_x = @varname(x)\n# Retrieve the 'internal' representation of x – we'll explain this later\nDynamicPPL.getindex_internal(vi, vn_x)\n\n1-element Vector{Float64}:\n 1.0746648736094493\n\n\nThe call to VarInfo executes the model once and stores the sampled value inside vi. By default, VarInfo itself stores un-transformed values. We can see this by comparing the value of the logpdf stored inside the VarInfo:\n\nDynamicPPL.getlogp(vi)\n\n(logprior = -0.9935400392011169, logjac = 0.0, loglikelihood = 0.0)\n\n\nwith a manual calculation:\n\nlogpdf(LogNormal(), DynamicPPL.getindex_internal(vi, vn_x))\n\n1-element Vector{Float64}:\n -0.9935400392011169\n\n\nIn DynamicPPL, the link function can be used to transform the variables. This function does three things: first, it transforms the variables; secondly, it updates the value of logp (by adding the Jacobian term); and thirdly, it sets a flag on the variables to indicate that it has been transformed. Note that this acts on all variables in the model, including unconstrained ones. (Unconstrained variables just have an identity transformation.)\n\nvi_linked = DynamicPPL.link(vi, model)\nprintln(\"Transformed value: $(DynamicPPL.getindex_internal(vi_linked, vn_x))\")\nprintln(\"Transformed logp: $(DynamicPPL.getlogp(vi_linked))\")\nprintln(\"Transformed flag: $(DynamicPPL.is_transformed(vi_linked, vn_x))\")\n\nTransformed value: [0.07200886749732066]\nTransformed logp: (logprior = -0.9935400392011169, logjac = -0.07200886749732066, loglikelihood = 0.0)\nTransformed flag: true\n\n\nIndeed, we can see that the new logp value matches with\n\nlogpdf(Normal(), DynamicPPL.getindex_internal(vi_linked, vn_x))\n\n1-element Vector{Float64}:\n -0.9215311717037962\n\n\nThe reverse transformation, invlink, reverts all of the above steps:\n\nvi = DynamicPPL.invlink(vi_linked, model)  # Same as the previous vi\nprintln(\"Un-transformed value: $(DynamicPPL.getindex_internal(vi, vn_x))\")\nprintln(\"Un-transformed logp: $(DynamicPPL.getlogp(vi))\")\nprintln(\"Un-transformed flag: $(DynamicPPL.is_transformed(vi, vn_x))\")\n\nUn-transformed value: [1.0746648736094493]\nUn-transformed logp: (logprior = -0.9935400392011169, logjac = 0.0, loglikelihood = 0.0)\nUn-transformed flag: false\n\n\n\nModel and internal representations\nIn DynamicPPL, there is a difference between the value of a random variable and its ‘internal’ value. This is most easily seen by first transforming, and then comparing the output of getindex and getindex_internal. The former extracts the regular value, which we call the model representation (because it is consistent with the distribution specified in the model). The latter, as the name suggests, gets the internal representation of the variable, which is how it is actually stored in the VarInfo object.\n\nprintln(\"   Model representation: $(getindex(vi_linked, vn_x))\")\nprintln(\"Internal representation: $(DynamicPPL.getindex_internal(vi_linked, vn_x))\")\n\n   Model representation: 1.0746648736094493\nInternal representation: [0.07200886749732066]\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that vi_linked[vn_x] can also be used as shorthand for getindex(vi_linked, vn_x); this usage is common in the DynamicPPL/Turing codebase.\n\n\nWe can see (for this linked varinfo) that there are two differences between these outputs:\n\nThe internal representation has been transformed using the bijector (in this case, the log function). This means that the is_transformed() flag which we used above doesn’t modify the model representation: it only tells us whether the internal representation has been transformed or not.\nThe internal representation is a vector, whereas the model representation is a scalar. This is because in DynamicPPL, all internal values are vectorised (i.e. converted into some vector), regardless of distribution. On the other hand, since the model specifies a univariate distribution, the model representation is a scalar.\n\nOne might also ask, what is the internal representation for an unlinked varinfo?\n\nprintln(\"   Model representation: $(getindex(vi, vn_x))\")\nprintln(\"Internal representation: $(DynamicPPL.getindex_internal(vi, vn_x))\")\n\n   Model representation: 1.0746648736094493\nInternal representation: [1.0746648736094493]\n\n\nFor an unlinked VarInfo, the internal representation is vectorised, but not transformed. We call this an unlinked internal representation; conversely, when the VarInfo has been linked, each variable will have a corresponding linked internal representation.\nThis sequence of events is summed up in the following diagram, where f(..., args) indicates that the ... is to be replaced with the object at the beginning of the arrow:\n\n\n\nFunctions related to variable transforms in DynamicPPL\n\n\nIn the final part of this article, we will take a more in-depth look at the internal DynamicPPL machinery that allows us to convert between representations and obtain the correct probability densities. Before that, though, we will take a quick high-level look at how the HMC sampler in Turing.jl uses the functions introduced so far.",
    "crumbs": [
      "Get Started",
      "Developers",
      "Variable Transformations",
      "Variable transformations in DynamicPPL"
    ]
  },
  {
    "objectID": "developers/transforms/dynamicppl/index.html#hmc-in-turing.jl",
    "href": "developers/transforms/dynamicppl/index.html#hmc-in-turing.jl",
    "title": "Variable transformations in DynamicPPL",
    "section": "HMC in Turing.jl",
    "text": "HMC in Turing.jl\nWhile DynamicPPL provides the functionality for transforming variables, the transformation itself happens at an even higher level, i.e. in the sampler itself. The HMC sampler in Turing.jl is in this file.\nIn the first step of sampling, it calls link on the sampler. What this means is that from the perspective of the HMC sampler, it never sees the constrained variable: it always thinks that it is sampling from an unconstrained distribution.\nThe biggest prerequisite for this to work correctly is that the potential energy term in the Hamiltonian—or in other words, the model log density—must be programmed correctly to include the Jacobian term. This is exactly the same as how we had to make sure to define logq(y) correctly in the toy HMC example above.\nWithin Turing.jl, this is correctly handled by calling\nx, inv_logjac = with_logabsdet_jacobian(y, inverse_transform)\nand then passing inv_logjac to DynamicPPL’s LogJacobianAccumulator.",
    "crumbs": [
      "Get Started",
      "Developers",
      "Variable Transformations",
      "Variable transformations in DynamicPPL"
    ]
  },
  {
    "objectID": "developers/transforms/dynamicppl/index.html#a-deeper-dive-into-dynamicppls-internal-machinery",
    "href": "developers/transforms/dynamicppl/index.html#a-deeper-dive-into-dynamicppls-internal-machinery",
    "title": "Variable transformations in DynamicPPL",
    "section": "A deeper dive into DynamicPPL’s internal machinery",
    "text": "A deeper dive into DynamicPPL’s internal machinery\nAs described above, DynamicPPL stores a (possibly linked) internal representation which is accessible via getindex_internal, but can also provide users with the original, untransformed, model representation via getindex. This abstraction allows the user to obtain samples from constrained distributions without having to perform the transformation themselves.\n\n\n\nMore functions related to variable transforms in DynamicPPL\n\n\nThe conversion between these representations is done using several internal functions in DynamicPPL, as depicted in the above diagram. The following operations are labelled:\n\nThis is linking, i.e. transforming a constrained variable to an unconstrained one.\nThis is vectorisation: for example, converting a scalar value to a 1-element vector.\nThis arrow brings us from the model representation to the linked internal representation. This is the composition of (1) and (2): linking and then vectorising.\nThis arrow brings us from the model representation to the unlinked internal representation. This only requires a single step, vectorisation.\n\nEach of these steps can be accomplished using the following functions.\n\n\n\n\n\n\n\n\n\nTo get the function\nTo get the inverse function\n\n\n\n\n(1)\nlink_transform(dist)\ninvlink_transform(dist)\n\n\n(2)\nto_vec_transform(dist)\nfrom_vec_transform(dist)\n\n\n(3)\nto_linked_internal_transform(vi, vn[, dist])\nfrom_linked_internal_transform(vi, vn[, dist])\n\n\n(4)\nto_internal_transform(vi, vn[, dist])\nfrom_internal_transform(vi, vn[, dist])\n\n\n\nNote that these functions do not perform the actual transformation; rather, they return the transformation function itself. For example, let’s take a look at the VarInfo from the previous section, which contains a single variable x ~ LogNormal().\n\nmodel_repn = vi[vn_x]\n\n1.0746648736094493\n\n\n\n# (1) Get the link function\nf_link = DynamicPPL.link_transform(LogNormal())\n# (2) Get the vectorise function\nf_vec = DynamicPPL.to_vec_transform(LogNormal())\n\n# Apply it to the model representation\nlinked_internal_repn = f_vec(f_link(model_repn))\n\n1-element Vector{Float64}:\n 0.07200886749732066\n\n\nEquivalently, we could have done:\n\n# (3) Get the linked internal transform function\nf_linked_internal = DynamicPPL.to_linked_internal_transform(vi, vn_x, LogNormal())\n\n# Apply it to the model representation\nlinked_internal_repn = f_linked_internal(model_repn)\n\n1-element Vector{Float64}:\n 0.07200886749732066\n\n\nAnd let’s confirm that this is the same as the linked internal representation, using the VarInfo that we linked earlier:\n\nDynamicPPL.getindex_internal(vi_linked, vn_x)\n\n1-element Vector{Float64}:\n 0.07200886749732066\n\n\nThe purpose of having all of these machinery is to allow other parts of DynamicPPL, such as the tilde pipeline, to handle transformed variables correctly. The following diagram shows how assume first checks whether the variable is transformed (using is_transformed), and then applies the appropriate transformation function.\n\n\n\n\n\n\n%%{ init: { 'themeVariables': { 'lineColor': '#000000' } } }%%\n%%{ init: { 'flowchart': { 'curve': 'linear', 'wrappingWidth': -1 } } }%%\ngraph TD\n    A[\"x ~ LogNormal()\"]:::boxStyle\n    B[\"vn = &lt;span style='color:#3B6EA8 !important;'&gt;@varname&lt;/span&gt;(x)&lt;br&gt;dist = LogNormal()&lt;br&gt;x, vi = ...\"]:::boxStyle\n    C[\"assume(vn, dist, vi)\"]:::boxStyle\n    D([\"&lt;span style='color:#3B6EA8 !important;'&gt;if&lt;/span&gt; is_transformed(vi, vn)\"]):::boxStyle\n    E[\"f = from_internal_transform(vi, vn, dist)\"]:::boxStyle\n    F[\"f = from_linked_internal_transform(vi, vn, dist)\"]:::boxStyle\n    G[\"&lt;span style='color:#3B6EA8 !important;'&gt;x, logjac = with_logabsdet_jacobian(f, getindex_internal(vi, vn, dist))&lt;/span&gt;\"]:::boxStyle\n    H[\"&lt;span style='color:#3B6EA8 !important;'&gt;return&lt;/span&gt; x, logpdf(dist, x) - logjac, vi\"]:::boxStyle\n    \n    A -.-&gt;|&lt;span style='color:#3B6EA8 ; background-color:#ffffff;'&gt;@model&lt;/span&gt;| B\n    B -.-&gt;|&lt;span style='color:#000000 ; background-color:#ffffff;'&gt;tilde-pipeline&lt;/span&gt;| C\n    C --&gt; D\n    D --&gt;|&lt;span style='color:#97365B ; background-color:#ffffff;'&gt;false&lt;/span&gt;| E\n    D --&gt;|&lt;span style='color:#97365B ; background-color:#ffffff;'&gt;true&lt;/span&gt;| F\n    E --&gt; G\n    F --&gt; G\n    G --&gt; H\n\n    classDef boxStyle fill:#ffffff,stroke:#000000,font-family:Courier,color:#000000\n    linkStyle default stroke:#000000,stroke-width:1px,color:#000000\n\n\n\n\n\n\nHere, with_logabsdet_jacobian is defined in the ChangesOfVariables.jl package, and returns both the effect of the transformation f as well as the log Jacobian term.\nBecause we chose f appropriately, we find here that x is always the model representation; furthermore, if the variable was not linked (i.e. is_transformed was false), the log Jacobian term will be zero. However, if it was linked, then the Jacobian term would be appropriately included, making sure that sampling proceeds correctly.",
    "crumbs": [
      "Get Started",
      "Developers",
      "Variable Transformations",
      "Variable transformations in DynamicPPL"
    ]
  },
  {
    "objectID": "developers/transforms/dynamicppl/index.html#why-do-we-need-to-do-this-at-runtime",
    "href": "developers/transforms/dynamicppl/index.html#why-do-we-need-to-do-this-at-runtime",
    "title": "Variable transformations in DynamicPPL",
    "section": "Why do we need to do this at runtime?",
    "text": "Why do we need to do this at runtime?\nGiven that we know whether a VarInfo is linked or not, one might wonder why we need both from_internal_transform and from_linked_internal_transform at the point where the model is evaluated. Could we not, for example, store the required transformation inside the VarInfo when we link it, and simply reuse that each time?\nThat is, why can’t we just do\n\n\n\n\n\n%%{ init: { 'flowchart': { 'curve': 'linear', 'wrappingWidth': -1 } } }%%\n%%{ init: { 'themeVariables': { 'lineColor': '#000000' } } }%%\ngraph TD\n      A[\"assume(varinfo, &lt;span style='color:#3B6EA8 !important;'&gt;@varname&lt;/span&gt;(x), Normal())\"]:::boxStyle\n      B[\"f = from_internal_transform(varinfo, varname, dist)\"]:::boxStyle\n      C[\"x, logjac = with_logabsdet_jacobian(f, getindex_internal(varinfo, varname))\"]:::boxStyle\n      D[\"&lt;span style='color:#3B6EA8 !important;'&gt;return&lt;/span&gt; x, logpdf(dist, x) - logjac, varinfo\"]:::dashedBox\n      \n      A --&gt; B\n      B --&gt; C\n      C --&gt; D\n\n    classDef dashedBox fill:#ffffff,stroke:#000000,stroke-dasharray: 5 5,font-family:Courier,color:#000000\n    classDef boxStyle fill:#ffffff,stroke:#000000,font-family:Courier,color:#000000\n\n    linkStyle default stroke:#000000,stroke-width:1px,color:#000000\n\n\n\n\n\n\nwhere from_internal_transform here only looks up a stored transformation function?\nUnfortunately, this is not possible in general, because the transformation function might not be a constant between different model evaluations. Consider, for example, the following model:\n\n@model function demo_dynamic_constraint()\n    m ~ Normal()\n    x ~ truncated(Normal(); lower=m)\n    return (m=m, x=x)\nend\n\ndemo_dynamic_constraint (generic function with 2 methods)\n\n\nHere, m is distributed according to a plain Normal(), whereas the variable x is constrained to be in the domain (m, Inf). Because of this, we expect that any time we sample from the model, we should have that m &lt; x (in terms of their model representations):\n\nmodel = demo_dynamic_constraint()\nvi = VarInfo(model)\nvn_m, vn_x = @varname(m), @varname(x)\n\nvi[vn_m], vi[vn_x]\n\n(-0.0740437565595174, 0.6327762377562545)\n\n\n(Note that vi[vn] is a shorthand for getindex(vi, vn), so this retrieves the model representations of m and x.) So far, so good. Let’s now link this VarInfo so that we end up working in an ‘unconstrained’ space, where both m and x can take on any values in (-Inf, Inf). First, we should check that the model representations are unchanged when linking:\n\nvi_linked = link(vi, model)\nvi_linked[vn_m], vi_linked[vn_x]\n\n(-0.0740437565595174, 0.6327762377562545)\n\n\nBut if we change the value of m, to, say, a bit larger than x:\n\n# Update the model representation for `m` in `vi_linked`.\nvi_linked[vn_m] = vi_linked[vn_x] + 1\nvi_linked[vn_m], vi_linked[vn_x]\n\n(1.6327762377562545, 0.6327762377562545)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is just for demonstration purposes! You shouldn’t be directly setting variables in a linked varinfo like this unless you know for a fact that the value will be compatible with the constraints of the model.\n\n\nNow, we see that the constraint m &lt; x is no longer satisfied. Hence, one might expect that if we try to evaluate the model using this VarInfo, we should obtain an error. Here, evaluate!! returns two things: the model’s return value itself (which we defined above to be a NamedTuple), and the resulting VarInfo post-evaluation.\n\nretval, ret_varinfo = DynamicPPL.evaluate!!(model, vi_linked)\ngetlogp(ret_varinfo)\n\n(logprior = -2.9368285247373587, logjac = 0.34697925043108835, loglikelihood = 0.0)\n\n\nBut we don’t get any errors! Indeed, we could even calculate the ‘log probability density’ for this evaluation.\nTo understand this, we need to look at the actual value which was used during the model evaluation. We can glean this from the return value (or from the returned VarInfo, but the former is easier):\n\nretval\n\n(m = 1.6327762377562545, x = 2.3395962320720263)\n\n\nWe can see here that the model evaluation used the value of m that we provided, but the value of x was ‘updated’.\nThe reason for this is that internally in a model evaluation, we construct the transformation function from the internal to the model representation based on the current realizations in the model! That is, we take the dist in a x ~ dist expression at model evaluation time and use that to construct the transformation, thus allowing it to change between model evaluations without invalidating the transformation.\nKnowing that the distribution of x depends on the value of m, we can now understand how the model representation of x got updated. The linked VarInfo does not store the model representation of x, but only its linked internal representation. So, what happened during the model evaluation above was that the linked internal representation of x – which was constructed using the original value of m – was transformed back into a new model representation using a different value of m.\nWe can reproduce the ‘new’ value of x by performing these transformations manually:\n\n# Generate a fresh linked VarInfo (without the new / 'corrupted' values)\nvi_linked = link(vi, model)\n# See the linked internal representations\nDynamicPPL.getindex_internal(vi_linked, vn_m), DynamicPPL.getindex_internal(vi_linked, vn_x)\n\n([-0.0740437565595174], [-0.34697925043108835])\n\n\nNow we update the value of m like we did before:\n\nvi_linked[vn_m] = vi_linked[vn_x] + 1\nvi_linked[vn_m]\n\n1.6327762377562545\n\n\nWhen evaluating the model, the distribution of x is now changed, and so is the corresponding inverse bijector:\n\nnew_dist_x = truncated(Normal(); lower=vi_linked[vn_m])\nnew_f_inv = DynamicPPL.invlink_transform(new_dist_x)\n\nBijectors.Inverse{Bijectors.TruncatedBijector{Float64, Float64}}(Bijectors.TruncatedBijector{Float64, Float64}(1.6327762377562545, Inf))\n\n\nand if we apply this to the internal representation of x:\n\nnew_f_inv(DynamicPPL.getindex_internal(vi_linked, vn_x))\n\n1-element Vector{Float64}:\n 2.3395962320720263\n\n\nwhich is the same value as we got above in retval.",
    "crumbs": [
      "Get Started",
      "Developers",
      "Variable Transformations",
      "Variable transformations in DynamicPPL"
    ]
  },
  {
    "objectID": "developers/transforms/dynamicppl/index.html#conclusion",
    "href": "developers/transforms/dynamicppl/index.html#conclusion",
    "title": "Variable transformations in DynamicPPL",
    "section": "Conclusion",
    "text": "Conclusion\nIn this chapter of the Turing docs, we’ve looked at:\n\nwhy variables might need to be transformed;\nhow this is accounted for mathematically with the Jacobian term;\nthe basic API and functionality of Bijectors.jl; and\nthe higher-level usage of transforms in DynamicPPL and Turing.\n\nThis will hopefully have equipped you with a better understanding of how constrained variables are handled in the Turing framework. With this knowledge, you should especially find it easier to navigate DynamicPPL’s VarInfo type, which forms the backbone of model evaluation.",
    "crumbs": [
      "Get Started",
      "Developers",
      "Variable Transformations",
      "Variable transformations in DynamicPPL"
    ]
  },
  {
    "objectID": "developers/contributing/index.html",
    "href": "developers/contributing/index.html",
    "title": "Contributing",
    "section": "",
    "text": "Turing is an open-source project and is hosted on GitHub. We welcome contributions from the community in all forms, large or small: bug reports, feature implementations, code contributions, or improvements to documentation or infrastructure are all extremely valuable. We would also very much appreciate examples of models written using Turing.\n\nHow to get involved\nOur outstanding issues are tabulated on our issue tracker. Closing one of these may involve implementing new features, fixing bugs, or writing example models.\nYou can also join the #turing channel on the Julia Slack and say hello!\nIf you are new to open source software, please see GitHub’s introduction or Julia’s contribution guide on using version control for collaboration.\n\n\nDocumentation\nEach of the packages in the Turing ecosystem (see Libraries) has its own documentation, which is typically found in the docs folder of the corresponding package. For example, the source code for DynamicPPL’s documentation can be found in its repository.\nThe documentation for Turing.jl itself consists of the tutorials that you see on this website, and is built from the separate docs repository. None of the documentation is generated from the main Turing.jl repository; in particular, the API that Turing exports does not currently form part of the documentation.\nOther sections of the website (anything that isn’t a package, or a tutorial) – for example, the list of libraries – is built from the turinglang.github.io repository.\n\n\nTests\nTuring, like most software libraries, has a test suite. You can run the whole suite by running julia --project=. from the root of the Turing repository, and then running\nimport Pkg; Pkg.test(\"Turing\")\nThe test suite subdivides into files in the test folder, and you can run only some of them using commands like\nimport Pkg; Pkg.test(\"Turing\"; test_args=[\"optim\", \"hmc\", \"--skip\", \"ext\"])\nThis one would run all files with “optim” or “hmc” in their path, such as test/optimisation/Optimisation.jl, but not files with “ext” in their path. Alternatively, you can set these arguments as command line arguments when you run Julia\njulia --project=. -e 'import Pkg; Pkg.test(; test_args=ARGS)' -- optim hmc --skip ext\nAlternatively, set the global ARGS variable, and call include(\"test/runtests.jl\").\n\n\nPull requests, versions, and releases\nWe merge all code changes through pull requests on GitHub. To make a contribution to one of the Turing packages, fork it on GitHub, start a new branch on your fork, and add commits to it. Once you’re done, open a pull request to the main repository under TuringLang. Someone from the dev team will review your code (if they don’t, ping @TuringLang/maintainers in a comment to get their attention) and check that the continuous integration tests pass (with some allowed exceptions, see below). If all looks good, we’ll merge your PR with gratitude. If not, we’ll help you fix it and then merge it with gratitude.\nEverything in this section about pull requests and branches applies to the Turing.jl and DynamicPPL.jl repositories. Most of it also applies to other repositories under the TuringLang ecosystem, though some do not bother with the main/breaking distinction or with a HISTORY.md. As at August 2025 we are slowly moving towards having all repos do the full process, so a new HISTORY.md in a repo that doesn’t yet have one is always welcome.\n\nBranches\nLike Julia packages generally, Turing.jl follows semantic versioning. Because of this, we have two persistently alive branches in our repository: main and breaking. All code that gets released as a new version of Turing gets merged into main, and a release is made from there. However, any breaking changes should first be merged into breaking. breaking will then periodically be merged into main.\nThe idea is that breaking always contains commits that build towards the next breaking release in the semantic versioning sense. That is, if the changes you make might break or change the behaviour of correctly written code that uses Turing.jl, your PR should target the breaking branch, and your code should be merged into breaking. If your changes cause no such breakage for users, your PR should target main. Notably, any bug fixes should merge directly into main.\nThis way we can frequently release new patch version from main, while developing breaking changes in parallel on breaking. E.g. if the current version is 0.19.3, and someone fixes a bug, we can merge the fix into main and release it as 0.19.4. Meanwhile, breaking changes can be developed and merged into breaking, which is building towards a release of 0.20.0. Multiple breaking changes may be accumulated into breaking, before finally the breaking-to-main merge is done, and 0.20.0 is released. On breaking the version number should then immediately be bumped to 0.21.\nWe do not generally backport bug fixes, although we may consider doing so in special circumstances.\n\n\nChange history\nWe keep a cumulative changelog in a file called HISTORY.md at the root of the repository. It should have an entry for every new breaking release, explaining everything our users need to know about the changes, such as what may have broken and how to fix things to work with the new version. Any major new features should also be described in HISTORY.md, as may any other changes that are useful for users to know about. Bug fixes generally don’t need an entry in HISTORY.md. Any new breaking release must have an entry in HISTORY.md, entries for non-breaking releases are optional.\n\n\nContinuous integration (CI) tests\nWe generally run the whole test suite of each repository in a GitHub action, typically for a few different versions of Julia, including the earliest supported version and the latest stable release. On some repositories we also run a few other checks in CI, such as code formatting and simple benchmarks. Generally all tests except those run on a prerelease version of Julia (e.g. a release candidate of an upcoming Julia release), and all code formatting checks, should pass before merging a PR. Exceptions can be made if the cause of the failure is known and unrelated to the PR. CI checks other than tests and formatting serve various purposes, and some of them can be allowed to fail. Some examples are\n\nAnything running on a prerelease of Julia. These inform us of trouble ahead when that prerelease becomes an actual release, but don’t require fixing for a PR to be merged.\nAny code coverage checks. Code coverage numbers can be helpful in catching missing tests or cases where the tests don’t test what they are intended to. However, we do not insist on any particular coverage figures, since they are not a very good metric of a test suite’s extensiveness.\nThe benchmarks on DynamicPPL repo. These should be investigated to understand why they fail. If the reason is a bug in the PR, an actual test should be added to the test suite to catch it. However, sometimes they fail for unrelated reasons.\nOccasionally CI failures are caused by bugs that require upstream fixes (such as for AD backends, or base Julia). Please ping a maintainer if you are unsure if this is the case. A good indicator for this is if the same test is failing on the base branch of your pull request.\nThe CI check in the docs repo for whether the docs are built with the latest Turing.jl release. This test failing is a reminder that we should make a PR to update to the latest version, but does not need fixing when working on a PR that makes unrelated changes to the documentation.\n\nIf you are ever unsure whether some CI check needs to pass, or if the reason why one is failing is mysterious or seems unrelated to the PR, ask a maintainer and they’ll help you out.\n\n\nPlease make mistakes\nGetting pull requests from outside the core developer team is one of the greatest joys of open source maintenance, and Turing’s community of contributors is its greatest asset. If you are thinking of contributing, please do open a pull request, even an imperfect or half-finished one, or an issue to discuss it first if you prefer. You don’t need to nail all of the above details on the first go, the dev team is very happy to help you figure out how to bump version numbers or whether you need to target main or breaking.\n\n\nFor Turing.jl core developers\nIf you are a core developer of TuringLang, two notes, in addition to the above, apply:\n\nYou don’t need to make your own fork of the package you are editing. Just make a new branch on the main repository, usually named your-username/change-you-are-making (we don’t strictly enforce this convention though). You should definitely still make a branch and a PR, and never push directly to main or breaking.\nYou can make a release of the package after your work is merged into main. This is done by leaving a comment on the latest commit on main, saying\n\n@JuliaRegistrator register\n\nRelease notes:\n[YOUR RELEASE NOTES HERE]\nIf you are making a breaking release, your release notes must also contain the string Breaking changes somewhere in them (this is mandated by the @JuliaRegistrator bot, described below).\nThe @JuliaRegistrator bot will handle creating a pull request into the Julia central package repository and tagging a new release in the repository. The release notes should be a copy-paste of the notes written in HISTORY.md if such an entry exists, or otherwise (for a patch release) a short summary of changes.\nEven core devs should always merge all their code through pull requests into main or breaking. All code should generally be reviewed by another core developer and pass continuous integration (CI) checks. Exceptions can be made in some cases though, such as ignoring failing CI checks where the cause is known and not due to the current pull request, or skipping code review when the pull request author is an experienced developer of the package and the changes are trivial.\n\n\n\nCode Formatting\nTuring uses JuliaFormatter.jl to ensure consistent code style across the codebase. All code must be formatted before submitting a pull request, and ideally with every commit.\n\nInstalling JuliaFormatter\nWe use version 1 of JuliaFormatter. Install it in your global Julia environment (not the project environment, as adding it to the Project.toml would make it an invalid dependency of the project):\njulia -e 'using Pkg; Pkg.add(name=\"JuliaFormatter\", version=\"1\"); Pkg.pin(\"JuliaFormatter\")'\n\n\nFormatting Code\nTo format all Julia files in the current directory and subdirectories:\njulia -e 'using JuliaFormatter; format(\".\")'\nRun this command from the root of the repository before committing your changes. This ensures your code follows the project’s formatting standards and maintains consistency across the codebase.\n\n\n\nStyle Guide\nTuring has a style guide, described below. Reviewing it before making a pull request is not strictly necessary, but you may be asked to change portions of your code to conform with the style guide before it is merged.\nMost Turing code follows Blue: a Style Guide for Julia. These conventions were created from a variety of sources including Python’s PEP8, Julia’s Notes for Contributors, and Julia’s Style Guide.\n\nSynopsis\n\nUse 4 spaces per indentation level, no tabs.\nTry to adhere to a 92 character line length limit.\nUse upper camel case convention for modules and types.\nUse lower case with underscores for method names (note: Julia code likes to use lower case without underscores).\nComments are good, try to explain the intentions of the code.\nUse whitespace to make the code more readable.\nNo whitespace at the end of a line (trailing whitespace).\nAvoid padding brackets with spaces. ex. Int64(value) preferred over Int64( value ).\n\n\n\nA Word on Consistency\nWhen adhering to the Blue style, it’s important to realise that these are guidelines, not rules. This is stated best in the PEP8:\n\nA style guide is about consistency. Consistency with this style guide is important. Consistency within a project is more important. Consistency within one module or function is most important.\n\n\nBut most importantly: know when to be inconsistent – sometimes the style guide just doesn’t apply. When in doubt, use your best judgment. Look at other examples and decide what looks best. And don’t hesitate to ask!\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Get Started",
      "Developers",
      "Contributing"
    ]
  },
  {
    "objectID": "developers/inference/abstractmcmc-turing/index.html",
    "href": "developers/inference/abstractmcmc-turing/index.html",
    "title": "How Turing Implements AbstractMCMC",
    "section": "",
    "text": "Prerequisite: Interface guide.",
    "crumbs": null
  },
  {
    "objectID": "developers/inference/abstractmcmc-turing/index.html#introduction",
    "href": "developers/inference/abstractmcmc-turing/index.html#introduction",
    "title": "How Turing Implements AbstractMCMC",
    "section": "Introduction",
    "text": "Introduction\nConsider the following Turing, code block:\n\nusing Turing\n\n@model function gdemo(x, y)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    x ~ Normal(m, sqrt(s²))\n    return y ~ Normal(m, sqrt(s²))\nend\n\nmod = gdemo(1.5, 2)\nalg = IS()\nn_samples = 1000\n\nchn = sample(mod, alg, n_samples, progress=false)\n\nChains MCMC chain (1000×5×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 2.61 seconds\nCompute duration  = 2.61 seconds\nparameters        = s², m\ninternals         = logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nThe function sample is part of the AbstractMCMC interface. As explained in the interface guide, building a sampling method that can be used by sample consists of overloading the structs and functions in AbstractMCMC. The interface guide also gives a standalone example of their implementation, AdvancedMH.jl.\nTuring sampling methods (most of which are written here) also implement AbstractMCMC. Turing defines a particular architecture for AbstractMCMC implementations, which enables working with models defined by the @model macro, and uses DynamicPPL as a backend. The goal of this page is to describe this architecture, and how you would go about implementing your own sampling method in Turing, using Importance Sampling as an example. I don’t go into all the details: for instance, I don’t address selectors or parallelism.\nFirst, we explain how Importance Sampling works in the abstract. Consider the model defined in the first code block. Mathematically, it can be written:\n\\[\n\\begin{align*}\ns &\\sim \\text{InverseGamma}(2, 3), \\\\\nm &\\sim \\text{Normal}(0, \\sqrt{s}), \\\\\nx &\\sim \\text{Normal}(m, \\sqrt{s}), \\\\\ny &\\sim \\text{Normal}(m, \\sqrt{s}).\n\\end{align*}\n\\]\nThe latent variables are \\(s\\) and \\(m\\), the observed variables are \\(x\\) and \\(y\\). The model joint distribution \\(p(s,m,x,y)\\) decomposes into the prior \\(p(s,m)\\) and the likelihood \\(p(x,y \\mid s,m).\\) Since \\(x = 1.5\\) and \\(y = 2\\) are observed, the goal is to infer the posterior distribution \\(p(s,m \\mid x,y).\\)\nImportance Sampling produces independent samples \\((s_i, m_i)\\) from the prior distribution. It also outputs unnormalized weights\n\\[\nw_i = \\frac {p(x,y,s_i,m_i)} {p(s_i, m_i)} = p(x,y \\mid s_i, m_i)\n\\]\nsuch that the empirical distribution\n\\[\n\\frac{1}{N} \\sum_{i =1}^N \\frac {w_i} {\\sum_{j=1}^N w_j} \\delta_{(s_i, m_i)}\n\\]\nis a good approximation of the posterior.",
    "crumbs": null
  },
  {
    "objectID": "developers/inference/abstractmcmc-turing/index.html#define-a-sampler",
    "href": "developers/inference/abstractmcmc-turing/index.html#define-a-sampler",
    "title": "How Turing Implements AbstractMCMC",
    "section": "1. Define a Sampler",
    "text": "1. Define a Sampler\nRecall the last line of the above code block:\n\nchn = sample(mod, alg, n_samples, progress=false)\n\nChains MCMC chain (1000×5×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 0.04 seconds\nCompute duration  = 0.04 seconds\nparameters        = s², m\ninternals         = logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nHere sample takes as arguments a model mod, an algorithm alg, and a number of samples n_samples, and returns an instance chn of Chains which can be analysed using the functions in MCMCChains.\n\nModels\nTo define a model, you declare a joint distribution on variables in the @model macro, and specify which variables are observed and which should be inferred, as well as the value of the observed variables. Thus, when implementing Importance Sampling,\n\nmod = gdemo(1.5, 2)\n\nDynamicPPL.Model{typeof(gdemo), (:x, :y), (), (), Tuple{Float64, Int64}, Tuple{}, DynamicPPL.DefaultContext, false}(gdemo, (x = 1.5, y = 2), NamedTuple(), DynamicPPL.DefaultContext())\n\n\ncreates an instance mod of the struct Model, which corresponds to the observations of a value of 1.5 for x, and a value of 2 for y.\nThis is all handled by DynamicPPL, more specifically here. I will return to how models are used to inform sampling algorithms below.\n\n\nAlgorithms\nAn algorithm is just a sampling method: in Turing, it is a subtype of the abstract type InferenceAlgorithm. Defining an algorithm may require specifying a few high-level parameters. For example, “Hamiltonian Monte-Carlo” may be too vague, but “Hamiltonian Monte Carlo with 10 leapfrog steps per proposal and a stepsize of 0.01” is an algorithm. “Metropolis-Hastings” may be too vague, but “Metropolis-Hastings with proposal distribution p” is an algorithm. Thus\n\nstepsize = 0.01\nL = 10\nalg = HMC(stepsize, L)\n\nHMC{AutoForwardDiff{nothing, Nothing}, AdvancedHMC.UnitEuclideanMetric}(0.01, 10, AutoForwardDiff())\n\n\ndefines a Hamiltonian Monte-Carlo algorithm, an instance of HMC, which is a subtype of InferenceAlgorithm.\nIn the case of Importance Sampling, there is no need to specify additional parameters:\n\nalg = IS()\n\nIS()\n\n\ndefines an Importance Sampling algorithm, an instance of IS, a subtype of InferenceAlgorithm.\nWhen creating your own Turing sampling method, you must, therefore, build a subtype of InferenceAlgorithm corresponding to your method.\n\n\nSamplers\nSamplers are not the same as algorithms. An algorithm is a generic sampling method, a sampler is an object that stores information about how algorithm and model interact during sampling, and is modified as sampling progresses. The Sampler struct is defined in DynamicPPL.\nTuring implements AbstractMCMC’s AbstractSampler with the Sampler struct defined in DynamicPPL. The most important attributes of an instance spl of Sampler are:\n\nspl.alg: the sampling method used, an instance of a subtype of InferenceAlgorithm\nspl.state: information about the sampling process, see below\n\nWhen you call sample(mod, alg, n_samples), Turing first uses model and alg to build an instance spl of Sampler , then calls the native AbstractMCMC function sample(mod, spl, n_samples).\nWhen you define your own Turing sampling method, you must therefore build:\n\na sampler constructor that uses a model and an algorithm to initialise an instance of Sampler. For Importance Sampling:\n\n\nfunction Sampler(alg::IS, model::Model, s::Selector)\n    info = Dict{Symbol,Any}()\n    state = ISState(model)\n    return Sampler(alg, info, s, state)\nend\n\n\na state struct implementing AbstractSamplerState corresponding to your method: we cover this in the following paragraph.\n\n\n\nStates\nThe vi field contains all the important information about sampling: first and foremost, the values of all the samples, but also the distributions from which they are sampled, the names of model parameters, and other metadata. As we will see below, many important steps during sampling correspond to queries or updates to spl.state.vi.\nBy default, you can use SamplerState, a concrete type defined in inference/Inference.jl, which extends AbstractSamplerState and has no field except for vi:\n\nmutable struct SamplerState{VIType&lt;:VarInfo} &lt;: AbstractSamplerState\n    vi::VIType\nend\n\nWhen doing Importance Sampling, we care not only about the values of the samples but also their weights. We will see below that the weight of each sample is also added to spl.state.vi. Moreover, the average\n\\[\n\\frac 1 N \\sum_{j=1}^N w_i = \\frac 1 N \\sum_{j=1}^N p(x,y \\mid s_i, m_i)\n\\]\nof the sample weights is a particularly important quantity:\n\nit is used to normalise the empirical approximation of the posterior distribution\nits logarithm is the importance sampling estimate of the log evidence \\(\\log p(x, y)\\)\n\nTo avoid having to compute it over and over again, is.jldefines an IS-specific concrete type ISState for sampler states, with an additional field final_logevidence containing\n\\[\n\\log \\frac 1 N \\sum_{j=1}^N w_i.\n\\]\n\nmutable struct ISState{V&lt;:VarInfo,F&lt;:AbstractFloat} &lt;: AbstractSamplerState\n    vi::V\n    final_logevidence::F\nend\n\n# additional constructor\nISState(model::Model) = ISState(VarInfo(model), 0.0)\n\nThe following diagram summarizes the hierarchy presented above.\n\n\n\n\n\n\n\nG\n\n\n\nspl\n\nspl\nSampler\n&lt;:AbstractSampler\n\n\n\nstate\n\nspl.state\nState\n&lt;:AbstractSamplerState\n\n\n\nspl-&gt;state\n\n\n\n\n\nalg\n\nspl.alg\nAlgorithm\n&lt;:InferenceAlgorithm\n\n\n\nspl-&gt;alg\n\n\n\n\n\nplaceholder1\n\n...\n\n\n\nspl-&gt;placeholder1\n\n\n\n\n\nvi\n\nspl.state.vi\nVarInfo\n&lt;:AbstractVarInfo\n\n\n\nstate-&gt;vi\n\n\n\n\n\nplaceholder2\n\n...\n\n\n\nstate-&gt;placeholder2\n\n\n\n\n\nplaceholder3\n\n...\n\n\n\nalg-&gt;placeholder3\n\n\n\n\n\nplaceholder4\n\n...\n\n\n\nplaceholder1-&gt;placeholder4",
    "crumbs": null
  },
  {
    "objectID": "developers/inference/abstractmcmc-turing/index.html#overload-the-functions-used-inside-mcmcsample",
    "href": "developers/inference/abstractmcmc-turing/index.html#overload-the-functions-used-inside-mcmcsample",
    "title": "How Turing Implements AbstractMCMC",
    "section": "2. Overload the functions used inside mcmcsample",
    "text": "2. Overload the functions used inside mcmcsample\nA lot of the things here are method-specific. However, Turing also has some functions that make it easier for you to implement these functions, for example.\n\nTransitions\nAbstractMCMC stores information corresponding to each individual sample in objects called transition, but does not specify what the structure of these objects could be. You could decide to implement a type MyTransition for transitions corresponding to the specifics of your methods. However, there are many situations in which the only information you need for each sample is:\n\nits value: \\(\\theta\\)\nlog of the joint probability of the observed data and this sample: lp\n\nInference.jl defines a struct Transition, which corresponds to this default situation\n\nstruct Transition{T,F&lt;:AbstractFloat}\n    θ::T\n    lp::F\nend\n\nIt also contains a constructor that builds an instance of Transition from an instance spl of Sampler: \\(\\theta\\) is spl.state.vi converted to a namedtuple, and lp is getlogp(spl.state.vi). is.jl uses this default constructor at the end of the step! function here.\n\n\nHow sample works\nA crude summary, which ignores things like parallelism, is the following:\nsample calls mcmcsample, which calls\n\nsample_init! to set things up\nstep! repeatedly to produce multiple new transitions\nsample_end! to perform operations once all samples have been obtained\nbundle_samples to convert a vector of transitions into a more palatable type, for instance a Chain.\n\nYou can, of course, implement all of these functions, but AbstractMCMC as well as Turing, also provide default implementations for simple cases. For instance, importance sampling uses the default implementations of sample_init! and bundle_samples, which is why you don’t see code for them inside is.jl.",
    "crumbs": null
  },
  {
    "objectID": "developers/inference/abstractmcmc-turing/index.html#overload-assume-and-observe",
    "href": "developers/inference/abstractmcmc-turing/index.html#overload-assume-and-observe",
    "title": "How Turing Implements AbstractMCMC",
    "section": "3. Overload assume and observe",
    "text": "3. Overload assume and observe\nThe functions mentioned above, such as sample_init!, step!, etc., must, of course, use information about the model in order to generate samples! In particular, these functions may need samples from distributions defined in the model or to evaluate the density of these distributions at some values of the corresponding parameters or observations.\nFor an example of the former, consider Importance Sampling as defined in is.jl. This implementation of Importance Sampling uses the model prior distribution as a proposal distribution, and therefore requires samples from the prior distribution of the model. Another example is Approximate Bayesian Computation, which requires multiple samples from the model prior and likelihood distributions in order to generate a single sample.\nAn example of the latter is the Metropolis-Hastings algorithm. At every step of sampling from a target posterior\n\\[\np(\\theta \\mid x_{\\text{obs}}),\n\\]\nin order to compute the acceptance ratio, you need to evaluate the model joint density\n\\[\np\\left(\\theta_{\\text{prop}}, x_{\\text{obs}}\\right)\n\\]\nwith \\(\\theta_{\\text{prop}}\\) a sample from the proposal and \\(x_{\\text{obs}}\\) the observed data.\nThis begs the question: how can these functions access model information during sampling? Recall that the model is stored as an instance m of Model. One of the attributes of m is the model evaluation function m.f, which is built by compiling the @model macro. Executing f runs the tilde statements of the model in order, and adds model information to the sampler (the instance of Sampler that stores information about the ongoing sampling process) at each step (see here for more information about how the @model macro is compiled). The DynamicPPL functions assume and observe determine what kind of information to add to the sampler for every tilde statement.\nConsider an instance m of Model and a sampler spl, with associated VarInfo vi = spl.state.vi. At some point during the sampling process, an AbstractMCMC function such as step! calls m(vi, ...), which calls the model evaluation function m.f(vi, ...).\n\nfor every tilde statement in the @model macro, m.f(vi, ...) returns model-related information (samples, value of the model density, etc.), and adds it to vi. How does it do that?\n\nrecall that the code for m.f(vi, ...) is automatically generated by compilation of the @model macro\nfor every tilde statement in the @model declaration, this code contains a call to assume(vi, ...) if the variable on the LHS of the tilde is a model parameter to infer, and observe(vi, ...) if the variable on the LHS of the tilde is an observation\nin the file corresponding to your sampling method (ie in Turing.jl/src/inference/&lt;your_method&gt;.jl), you have overloaded assume and observe, so that they can modify vi to include the information and samples that you care about!\nat a minimum, assume and observe return the log density lp of the sample or observation. the model evaluation function then immediately calls acclogp!!(vi, lp), which adds lp to the value of the log joint density stored in vi.\n\n\nHere’s what assume looks like for Importance Sampling:\n\nfunction DynamicPPL.assume(rng, spl::Sampler{&lt;:IS}, dist::Distribution, vn::VarName, vi)\n    r = rand(rng, dist)\n    push!(vi, vn, r, dist, spl)\n    return r, 0\nend\n\nThe function first generates a sample r from the distribution dist (the right hand side of the tilde statement). It then adds r to vi, and returns r and 0.\nThe observe function is even simpler:\n\nfunction DynamicPPL.observe(spl::Sampler{&lt;:IS}, dist::Distribution, value, vi)\n    return logpdf(dist, value)\nend\n\nIt simply returns the density (in the discrete case, the probability) of the observed value under the distribution dist.",
    "crumbs": null
  },
  {
    "objectID": "developers/inference/abstractmcmc-turing/index.html#summary-importance-sampling-step-by-step",
    "href": "developers/inference/abstractmcmc-turing/index.html#summary-importance-sampling-step-by-step",
    "title": "How Turing Implements AbstractMCMC",
    "section": "4. Summary: Importance Sampling step by step",
    "text": "4. Summary: Importance Sampling step by step\nWe focus on the AbstractMCMC functions that are overridden in is.jl and executed inside mcmcsample: step!, which is called n_samples times, and sample_end!, which is executed once after those n_samples iterations.\n\nDuring the \\(i\\)-th iteration, step! does 3 things:\n\nempty!!(spl.state.vi): remove information about the previous sample from the sampler’s VarInfo\nmodel(rng, spl.state.vi, spl): call the model evaluation function\n\ncalls to assume add the samples from the prior \\(s_i\\) and \\(m_i\\) to spl.state.vi\ncalls to assume or observe are followed by the line acclogp!!(vi, lp), where lp is an output of assume and observe\nlp is set to 0 after assume, and to the value of the density at the observation after observe\nWhen all the tilde statements have been covered, spl.state.vi.logp[] is the sum of the lp, i.e., the likelihood \\(\\log p(x, y \\mid s_i, m_i) = \\log p(x \\mid s_i, m_i) + \\log p(y \\mid s_i, m_i)\\) of the observations given the latent variable samples \\(s_i\\) and \\(m_i\\).\n\nreturn Transition(spl): build a transition from the sampler, and return that transition\n\nthe transition’s vi field is simply spl.state.vi\nthe lp field contains the likelihood spl.state.vi.logp[]\n\n\nWhen the n_samples iterations are completed, sample_end! fills the final_logevidence field of spl.state\n\nIt simply takes the logarithm of the average of the sample weights, using the log weights for numerical stability",
    "crumbs": null
  },
  {
    "objectID": "developers/inference/implementing-samplers/index.html",
    "href": "developers/inference/implementing-samplers/index.html",
    "title": "Implementing Samplers",
    "section": "",
    "text": "In this tutorial, we’ll go through step-by-step how to implement a “simple” sampler in AbstractMCMC.jl in such a way that it can be easily applied to Turing.jl models.\nIn particular, we’re going to implement a version of Metropolis-adjusted Langevin (MALA).\nNote that we will implement this sampler in the AbstractMCMC.jl framework, completely “ignoring” Turing.jl until the very end of the tutorial, at which point we’ll use a single line of code to make the resulting sampler available to Turing.jl. This is to really drive home the point that one can implement samplers in a way that is accessible to all of Turing.jl’s users without having to use Turing.jl yourself.",
    "crumbs": [
      "Get Started",
      "Developers",
      "Inference in Detail",
      "Implementing Samplers"
    ]
  },
  {
    "objectID": "developers/inference/implementing-samplers/index.html#quick-overview-of-mala",
    "href": "developers/inference/implementing-samplers/index.html#quick-overview-of-mala",
    "title": "Implementing Samplers",
    "section": "Quick overview of MALA",
    "text": "Quick overview of MALA\nWe can view MALA as a single step of the leapfrog integrator with resampling of momentum \\(p\\) at every step.1 To make that statement a bit more concrete, we first define the extended target \\(\\bar{\\gamma}(x, p)\\) as\n\\[\\begin{equation*}\n\\log \\bar{\\gamma}(x, p) \\propto \\log \\gamma(x) + \\log \\gamma_{\\mathcal{N}(0, M)}(p)\n\\end{equation*}\\]\nwhere \\(\\gamma_{\\mathcal{N}(0, M)}\\) denotes the density for a zero-centred Gaussian with covariance matrix \\(M\\). We then consider targeting this joint distribution over both \\(x\\) and \\(p\\) as follows. First we define the map\n\\[\\begin{equation*}\n\\begin{split}\n  L_{\\epsilon}: \\quad & \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}^d \\times \\mathbb{R}^d \\\\\n  & (x, p) \\mapsto (\\tilde{x}, \\tilde{p}) := L_{\\epsilon}(x, p)\n\\end{split}\n\\end{equation*}\\]\nas\n\\[\\begin{equation*}\n\\begin{split}\n  p_{1 / 2} &:= p + \\frac{\\epsilon}{2} \\nabla \\log \\gamma(x) \\\\\n  \\tilde{x} &:= x + \\epsilon M^{-1} p_{1 /2 } \\\\\n  p_1 &:= p_{1 / 2} + \\frac{\\epsilon}{2} \\nabla \\log \\gamma(\\tilde{x}) \\\\\n  \\tilde{p} &:= - p_1\n\\end{split}\n\\end{equation*}\\]\nThis might be familiar for some readers as a single step of the Leapfrog integrator. We then define the MALA kernel as follows: given the current iterate \\(x_i\\), we sample the next iterate \\(x_{i + 1}\\) as\n\\[\\begin{equation*}\n\\begin{split}\n  p &\\sim \\mathcal{N}(0, M) \\\\\n  (\\tilde{x}, \\tilde{p}) &:= L_{\\epsilon}(x_i, p) \\\\\n  \\alpha &:= \\min \\left\\{ 1, \\frac{\\bar{\\gamma}(\\tilde{x}, \\tilde{p})}{\\bar{\\gamma}(x_i, p)} \\right\\} \\\\\n  x_{i + 1} &:=\n  \\begin{cases}\n    \\tilde{x} \\quad & \\text{ with prob. } \\alpha \\\\\n    x_i       \\quad & \\text{ with prob. } 1 - \\alpha\n  \\end{cases}\n\\end{split}\n\\end{equation*}\\]\ni.e. we accept the proposal \\(\\tilde{x}\\) with probability \\(\\alpha\\) and reject it, thus sticking with our current iterate, with probability \\(1 - \\alpha\\).",
    "crumbs": [
      "Get Started",
      "Developers",
      "Inference in Detail",
      "Implementing Samplers"
    ]
  },
  {
    "objectID": "developers/inference/implementing-samplers/index.html#what-we-need-from-a-model-logdensityproblems.jl",
    "href": "developers/inference/implementing-samplers/index.html#what-we-need-from-a-model-logdensityproblems.jl",
    "title": "Implementing Samplers",
    "section": "What we need from a model: LogDensityProblems.jl",
    "text": "What we need from a model: LogDensityProblems.jl\nThere are a few things we need from the “target” / “model” / density that we want to sample from:\n\nWe need access to log-density evaluations \\(\\log \\gamma(x)\\) so we can compute the acceptance ratio involving \\(\\log \\bar{\\gamma}(x, p)\\).\nWe need access to log-density gradients \\(\\nabla \\log \\gamma(x)\\) so we can compute the Leapfrog steps \\(L_{\\epsilon}(x, p)\\).\nWe also need access to the “size” of the model so we can determine the size of \\(M\\).\n\nLuckily for us, there is a package called LogDensityProblems.jl which provides an interface for exactly this!\nTo demonstrate how one can implement the “LogDensityProblems.jl interface”2 we will use a simple Gaussian model as an example:\n\nusing LogDensityProblems: LogDensityProblems\n\n# Let's define some type that represents the model.\nstruct IsotropicNormalModel{M&lt;:AbstractVector{&lt;:Real}}\n    \"mean of the isotropic Gaussian\"\n    mean::M\nend\n\n# Specifies what input length the model expects.\nLogDensityProblems.dimension(model::IsotropicNormalModel) = length(model.mean)\n# Implementation of the log-density evaluation of the model.\nfunction LogDensityProblems.logdensity(model::IsotropicNormalModel, x::AbstractVector{&lt;:Real})\n    return - sum(abs2, x .- model.mean) / 2\nend\n\nThis gives us all of the properties we want for our MALA sampler with the exception of the computation of the gradient \\(\\nabla \\log \\gamma(x)\\). There is the method LogDensityProblems.logdensity_and_gradient which should return a 2-tuple where the first entry is the evaluation of the logdensity \\(\\log \\gamma(x)\\) and the second entry is the gradient \\(\\nabla \\log \\gamma(x)\\).\nThere are two ways to “implement” this method: 1) we implement it by hand, which is feasible in the case of our IsotropicNormalModel, or 2) we defer the implementation of this to an automatic differentiation backend.\nTo implement it by hand we can simply do\n\n# Tell LogDensityProblems.jl that first-order, i.e. gradient information, is available.\nLogDensityProblems.capabilities(::Type{&lt;:IsotropicNormalModel}) = LogDensityProblems.LogDensityOrder{1}()\n\n# Implement `logdensity_and_gradient`.\nfunction LogDensityProblems.logdensity_and_gradient(model::IsotropicNormalModel, x)\n    logγ_x = LogDensityProblems.logdensity(model, x)\n    ∇logγ_x = -x .* (x - model.mean)\n    return logγ_x, ∇logγ_x\nend\n\nLet’s just try it out:\n\n# Instantiate the problem.\nmodel = IsotropicNormalModel([-5., 0., 5.])\n# Create some example input that we can test on.\nx_example = randn(LogDensityProblems.dimension(model))\n# Evaluate!\nLogDensityProblems.logdensity(model, x_example)\n\n-30.418131722923178\n\n\nTo defer it to an automatic differentiation backend, we can do\n\n# Tell LogDensityProblems.jl we only have access to 0-th order information.\nLogDensityProblems.capabilities(::Type{&lt;:IsotropicNormalModel}) = LogDensityProblems.LogDensityOrder{0}()\n\n# Use `LogDensityProblemsAD`'s `ADgradient` in combination with some AD backend to implement `logdensity_and_gradient`.\nusing LogDensityProblemsAD, ADTypes, ForwardDiff\nmodel_with_grad = ADgradient(AutoForwardDiff(), model)\nLogDensityProblems.logdensity(model_with_grad, x_example)\n\n-30.418131722923178\n\n\nWe’ll continue with the second approach in this tutorial since this is typically what one does in practice, because there are better hobbies to spend time on than deriving gradients by hand.\nAt this point, one might wonder how we’re going to tie this back to Turing.jl in the end. Effectively, when working with inference methods that only require log-density evaluations and / or higher-order information of the log-density, Turing.jl actually converts the user-provided Model into an object implementing the above methods for LogDensityProblems.jl. As a result, most samplers provided by Turing.jl are actually implemented to work with LogDensityProblems.jl, enabling their use both within Turing.jl and outside of Turing.jl! Moreover, there exists similar conversions for Stan through BridgeStan and StanLogDensityProblems.jl, which means that a sampler supporting the LogDensityProblems.jl interface can easily be used on both Turing.jl and Stan models (in addition to user-provided models, as our IsotropicNormalModel above)!",
    "crumbs": [
      "Get Started",
      "Developers",
      "Inference in Detail",
      "Implementing Samplers"
    ]
  },
  {
    "objectID": "developers/inference/implementing-samplers/index.html#implementing-mala-in-abstractmcmc.jl",
    "href": "developers/inference/implementing-samplers/index.html#implementing-mala-in-abstractmcmc.jl",
    "title": "Implementing Samplers",
    "section": "Implementing MALA in AbstractMCMC.jl",
    "text": "Implementing MALA in AbstractMCMC.jl\nNow that we’ve established that a model implementing the LogDensityProblems.jl interface provides us with all the information we need from \\(\\log \\gamma(x)\\), we can address the question: given an object that implements the LogDensityProblems.jl interface, how can we define a sampler for it?\nWe’re going to do this by making our sampler a sub-type of AbstractMCMC.AbstractSampler in addition to implementing a few methods from AbstractMCMC.jl. Why? Because it gets us a lot of functionality for free, as we will see later.\nMoreover, AbstractMCMC.jl provides a very natural interface for MCMC algorithms.\nFirst, we’ll define our MALA type\n\nusing AbstractMCMC\n\nstruct MALA{T,A} &lt;: AbstractMCMC.AbstractSampler\n    \"stepsize used in the leapfrog step\"\n    ϵ_init::T\n    \"covariance matrix used for the momentum\"\n    M_init::A\nend\n\nNotice how we’ve added the suffix _init to both the stepsize and the covariance matrix. We’ve done this because a AbstractMCMC.AbstractSampler should be immutable. Of course there might be many scenarios where we want to allow something like the stepsize and / or the covariance matrix to vary between iterations, e.g. during the burn-in / adaptation phase of the sampling process we might want to adjust the parameters using statistics computed from these initial iterations. But information which can change between iterations should not go in the sampler itself! Instead, this information should go in the sampler state.\nThe sampler state should at the very least contain all the necessary information to perform the next MCMC iteration, but usually contains further information, e.g. quantities and statistics useful for evaluating whether the sampler has converged.\nWe will use the following sampler state for our MALA sampler:\n\nstruct MALAState{A&lt;:AbstractVector{&lt;:Real}}\n    \"current position\"\n    x::A\n    \"whether the proposal was accepted\"\n    accepted::Bool\nend\n\nIf we also wanted to adapt the parameters of our MALA, e.g. alter the stepsize depending on acceptance rates, we could also put ϵ in the state, but for now we’ll keep things simple.\nMoreover, we also want a sample type, which is a type meant for “public consumption”, i.e. the end-user. This is generally going to contain a subset of the information present in the state. But in such a simple scenario as this, we similarly only have a AbstractVector{&lt;:Real}:\n\nstruct MALASample{A&lt;:AbstractVector{&lt;:Real}}\n    \"current position\"\n    x::A\nend\n\nWe currently have three things:\n\nA AbstractMCMC.AbstractSampler implementation called MALA.\nA state MALAState for our sampler MALA.\nA sample MALASample for our sampler MALA.\n\nThat means that we’re ready to implement the only thing that really matters: AbstractMCMC.step.\nAbstractMCMC.step defines the MCMC iteration of our MALA given the current MALAState. Specifically, the signature of the function is as follows:\nfunction AbstractMCMC.step(\n    # The RNG to ensure reproducibility.\n    rng::Random.AbstractRNG,\n    # The model that defines our target.\n    model::AbstractMCMC.AbstractModel,\n    # The sampler for which we're taking a `step`.\n    sampler::AbstractMCMC.AbstractSampler,\n    # The current sampler `state`.\n    state;\n    # Additional keyword arguments that we may or may not need.\n    kwargs...\n)\nMoreover, there is a specific AbstractMCMC.AbstractModel which is used to indicate that the model that is provided implements the LogDensityProblems.jl interface: AbstractMCMC.LogDensityModel.\nSince, as we discussed earlier, in our case we’re indeed going to work with types that support the LogDensityProblems.jl interface, we’ll define AbstractMCMC.step for such a AbstractMCMC.LogDensityModel.\nNote that AbstractMCMC.LogDensityModel has no other purpose; it has a single field called logdensity, and it does nothing else. But by wrapping the model in AbstractMCMC.LogDensityModel, it allows samplers that want to work with LogDensityProblems.jl to define their AbstractMCMC.step on this type without running into method ambiguities.\nAll in all, that means that the signature for our AbstractMCMC.step is going to be the following:\nfunction AbstractMCMC.step(\n    rng::Random.AbstractRNG,\n    # `LogDensityModel` so we know we're working with LogDensityProblems.jl model.\n    model::AbstractMCMC.LogDensityModel,\n    # Our sampler.\n    sampler::MALA,\n    # Our sampler state.\n    state::MALAState;\n    kwargs...\n)\nGreat! Now let’s actually implement the full AbstractMCMC.step for our MALA.\nLet’s remind ourselves what we’re going to do:\n\nSample a new momentum \\(p\\).\nCompute the log-density of the extended target \\(\\log \\bar{\\gamma}(x, p)\\).\nTake a single leapfrog step \\((\\tilde{x}, \\tilde{p}) = L_{\\epsilon}(x, p)\\).\nAccept or reject the proposed \\((\\tilde{x}, \\tilde{p})\\).\n\nAll in all, this results in the following:\n\nusing Random: Random\nusing Distributions  # so we get the `MvNormal`\n\nfunction AbstractMCMC.step(\n    rng::Random.AbstractRNG,\n    model_wrapper::AbstractMCMC.LogDensityModel,\n    sampler::MALA,\n    state::MALAState;\n    kwargs...\n)\n    # Extract the wrapped model which implements LogDensityProblems.jl.\n    model = model_wrapper.logdensity\n    # Let's just extract the sampler parameters to make our lives easier.\n    ϵ = sampler.ϵ_init\n    M = sampler.M_init\n    # Extract the current parameters.\n    x = state.x\n    # Sample the momentum.\n    p_dist = MvNormal(zeros(LogDensityProblems.dimension(model)), M)\n    p = rand(rng, p_dist)\n    # Propose using a single leapfrog step.\n    x̃, p̃ = leapfrog_step(model, x, p, ϵ, M)\n    # Accept or reject proposal.\n    logp = LogDensityProblems.logdensity(model, x) + logpdf(p_dist, p)\n    logp̃ = LogDensityProblems.logdensity(model, x̃) + logpdf(p_dist, p̃)\n    logα = logp̃ - logp\n    state_new = if log(rand(rng)) &lt; logα\n        # Accept.\n        MALAState(x̃, true)\n    else\n        # Reject.\n        MALAState(x, false)\n    end\n    # Return the \"sample\" and the sampler state.\n    return MALASample(state_new.x), state_new\nend\n\nFairly straight-forward.\nOf course, we haven’t defined the leapfrog_step method yet, so let’s do that:\n\nfunction leapfrog_step(model, x, p, ϵ, M)\n    # Update momentum `p` using \"position\" `x`.\n    ∇logγ_x = last(LogDensityProblems.logdensity_and_gradient(model, x))\n    p1 = p + (ϵ / 2) .* ∇logγ_x\n    # Update the \"position\" `x` using momentum `p1`.\n    x̃ = x + ϵ .* (M \\ p1)\n    # Update momentum `p1` using position `x̃`\n    ∇logγ_x̃ = last(LogDensityProblems.logdensity_and_gradient(model, x̃))\n    p2 = p1 + (ϵ / 2) .* ∇logγ_x̃\n    # Flip momentum `p2`.\n    p̃ = -p2\n    return x̃, p̃\nend\n\nleapfrog_step (generic function with 1 method)\n\n\nWith all of this, we’re technically ready to sample!\n\nusing Random, LinearAlgebra\n\nrng = Random.default_rng()\nsampler = MALA(1, I)\nstate = MALAState(zeros(LogDensityProblems.dimension(model)), true)\n\nx_next, state_next = AbstractMCMC.step(\n    rng,\n    AbstractMCMC.LogDensityModel(model),\n    sampler,\n    state\n)\n\n(MALASample{Vector{Float64}}([0.0, 0.0, 0.0]), MALAState{Vector{Float64}}([0.0, 0.0, 0.0], false))\n\n\nGreat, it works!\nAnd I promised we would get quite some functionality for free if we implemented AbstractMCMC.step, and so we can now simply call sample to perform standard MCMC sampling:\n\n# Perform 1000 iterations with our `MALA` sampler.\nsamples = sample(model_with_grad, sampler, 10_000; initial_state=state, progress=false)\n# Concatenate into a matrix.\nsamples_matrix = stack(sample -&gt; sample.x, samples)\n\n3×10000 Matrix{Float64}:\n -1.30963   -3.23839   -3.23839   -5.29506   …  -4.75953  -6.87805  -5.40216\n  0.912364   0.232925   0.232925   0.556334      1.19432   4.14784   1.80041\n  4.49761    4.13746    4.13746    4.45066       4.72514   6.16506   4.18898\n\n\n\n# Compute the marginal means and standard deviations.\nhcat(mean(samples_matrix; dims=2), std(samples_matrix; dims=2))\n\n3×2 Matrix{Float64}:\n -4.98451    1.01421\n  0.0270054  1.01454\n  5.01257    1.0006\n\n\nLet’s visualise the samples\n\nusing StatsPlots\nplot(transpose(samples_matrix[:, 1:10:end]), alpha=0.5, legend=false)\n\n\n\n\nLook at that! Things are working; amazin’.\nWe can also exploit AbstractMCMC.jl’s parallel sampling capabilities:\n\n# Run separate 4 chains for 10 000 iterations using threads to parallelize.\nnum_chains = 4\nsamples = sample(\n    model_with_grad,\n    sampler,\n    MCMCThreads(),\n    10_000,\n    num_chains;\n    # Note we need to provide an initial state for every chain.\n    initial_state=fill(state, num_chains),\n    progress=false\n)\nsamples_array = stack(map(Base.Fix1(stack, sample -&gt; sample.x), samples))\n\n3×10000×4 Array{Float64, 3}:\n[:, :, 1] =\n -2.63734   -2.85045   -4.43973  -4.48691  …  -5.24659  -5.24659  -5.00873\n -0.621409  -0.973113  -2.85031  -1.78944     -1.02431  -1.02431  -0.581166\n  2.67406    3.98538    5.65047   4.52073      2.68679   2.68679   4.16458\n\n[:, :, 2] =\n -1.54995   -4.66355   -4.58451  -6.4518   …  -5.67564   -5.78986   -5.02517\n  0.449902   0.708761   1.54461   1.47411     -0.471126  -0.596071   1.42533\n  2.26963    4.19255    6.24551   7.44572      4.72048    5.93541    6.0763\n\n[:, :, 3] =\n -1.32946  -3.61154   -4.44622   -4.79259   …  -5.68799   -4.38469   -4.90971\n  0.70992   0.557278   0.287454  -0.300924      0.291689  -0.353335  -1.08355\n  2.42318   2.30773    1.70761    3.20167       2.851      2.90803    1.90414\n\n[:, :, 4] =\n -1.89252   -3.71206  -5.45521  -3.30143  …  -4.77195   -5.99827   -4.15068\n -0.900423   1.34743   2.60067   1.65335      0.361756   0.215757   1.17965\n  1.70317    3.97581   4.8279    4.45948      5.38473    6.35992    5.75117\n\n\nBut the fact that we have to provide the AbstractMCMC.sample call, etc. with an initial_state to get started is a bit annoying. We can avoid this by also defining a AbstractMCMC.step without the state argument:\n\nfunction AbstractMCMC.step(\n    rng::Random.AbstractRNG,\n    model_wrapper::AbstractMCMC.LogDensityModel,\n    ::MALA;\n    # NOTE: No state provided!\n    kwargs...\n)\n    model = model_wrapper.logdensity\n    # Let's just create the initial state by sampling using  a Gaussian.\n    x = randn(rng, LogDensityProblems.dimension(model))\n\n    return MALASample(x), MALAState(x, true)\nend\n\nEquipped with this, we no longer need to provide the initial_state everywhere:\n\nsamples = sample(model_with_grad, sampler, 10_000; progress=false)\nsamples_matrix = stack(sample -&gt; sample.x, samples)\nhcat(mean(samples_matrix; dims=2), std(samples_matrix; dims=2))\n\n3×2 Matrix{Float64}:\n -5.00525     1.00345\n  0.00221089  0.998493\n  5.00445     0.982784",
    "crumbs": [
      "Get Started",
      "Developers",
      "Inference in Detail",
      "Implementing Samplers"
    ]
  },
  {
    "objectID": "developers/inference/implementing-samplers/index.html#using-our-sampler-with-turing.jl",
    "href": "developers/inference/implementing-samplers/index.html#using-our-sampler-with-turing.jl",
    "title": "Implementing Samplers",
    "section": "Using our sampler with Turing.jl",
    "text": "Using our sampler with Turing.jl\nAs we promised, all of this hassle of implementing our MALA sampler in a way that uses LogDensityProblems.jl and AbstractMCMC.jl gets us something more than just an “automatic” implementation of AbstractMCMC.sample.\nIt also enables use with Turing.jl through the externalsampler, but we need to do one final thing first: we need to tell Turing.jl how to extract a vector of parameters from the state returned in our implementation of AbstractMCMC.step. In our case, the state is a MALAState, so we just need the following line:\n\n# Overload the `getparams` method for our \"state\" type.\nAbstractMCMC.getparams(state::MALAState) = state.x\n\nOptionally, we can also implement AbstractMCMC.getstats which instead returns a NamedTuple of statistics about the current state. When sampling with Turing, these statistics will be included in the output chain.\n\nAbstractMCMC.getstats(state::MALAState) = (accepted=state.accepted,)\n\nAnd with that, we’re good to go!\n\n\n\n\n\n\nNote\n\n\n\nUp until Turing.jl v0.41, you would have needed to define Turing.Inference.getparams(::MALASample). This has been changed in favour of the AbstractMCMC interface, which means that you no longer need to depend on Turing.jl to implement an external sampler.\n\n\n\nusing Turing\n\n# Our previous model defined as a Turing.jl model.\n@model mvnormal_model() = x ~ MvNormal([-5., 0., 5.], I)\n# Instantiate our model.\nturing_model = mvnormal_model()\n# Call `sample` but now we're passing in a Turing.jl `model` and wrapping\n# our `MALA` sampler in the `externalsampler` to tell Turing.jl that the sampler\n# expects something that implements LogDensityProblems.jl.\nchain = sample(turing_model, externalsampler(sampler), 10_000; progress=false)\n\nChains MCMC chain (10000×7×1 Array{Float64, 3}):\n\nIterations        = 1:1:10000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 2.56 seconds\nCompute duration  = 2.56 seconds\nparameters        = x[1], x[2], x[3]\ninternals         = accepted, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nPretty neat, eh?\n\nModels with constrained parameters\nOne thing we’ve sort of glossed over in all of the above is that MALA, at least how we’ve implemented it, requires \\(x\\) to live in \\(\\mathbb{R}^d\\) for some \\(d &gt; 0\\). If some of the parameters were in fact constrained, e.g. we were working with a Beta distribution which has support on the interval \\((0, 1)\\), not on \\(\\mathbb{R}^d\\), we could easily end up outside of the valid range \\((0, 1)\\).\n\n@model beta_model() = x ~ Beta(3, 3)\nturing_model = beta_model()\nchain = sample(turing_model, externalsampler(sampler), 10_000; progress=false)\n\nChains MCMC chain (10000×5×1 Array{Float64, 3}):\n\nIterations        = 1:1:10000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 1.29 seconds\nCompute duration  = 1.29 seconds\nparameters        = x\ninternals         = accepted, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nBy default, Turing.jl avoids such a situation from occurring by transforming the constrained parameters to an unconstrained space before passing them to the externalsampler. If this is undesirable, you can pass the unconstrained keyword argument to externalsampler:\n\nchain_constrained = sample(turing_model, externalsampler(sampler; unconstrained=false), 10_000; progress=false)\n\nChains MCMC chain (10000×5×1 Array{Float64, 3}):\n\nIterations        = 1:1:10000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 0.38 seconds\nCompute duration  = 0.38 seconds\nparameters        = x\ninternals         = accepted, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nIt turns out that this still sort of works because logpdf doesn’t error when evaluating outside of the support of the distribution, but instead returns -Inf:\n\nlogpdf(Beta(3, 3), 10.0)\n\n-Inf\n\n\nand so the samples that fall outside of the range are always rejected. But do notice how much worse all the diagnostics are, e.g. ess_tail is very poor compared to when we use unconstrained=true:\n\ness(chain)\n\n\nESS\n\n  parameters         ess   ess_per_sec\n      Symbol     Float64       Float64\n\n           x   4388.6835     3404.7196\n\n\n\n\n\n\ness(chain_constrained)\n\n\nESS\n\n  parameters         ess   ess_per_sec\n      Symbol     Float64       Float64\n\n           x   4774.5916    12498.9309\n\n\n\n\n\nMoreover, in more complex cases this won’t just result in a “nice” -Inf log-density value, but instead will error:\n\n@model function demo()\n    σ² ~ truncated(Normal(), lower=0)\n    # If we end up with negative values for `σ²`, the `Normal` will error.\n    x ~ Normal(0, σ²)\nend\nsample(demo(), externalsampler(sampler; unconstrained=false), 10_000; progress=false)\n\n\nDomainError with -0.25601123320280855:\nNormal: the condition σ &gt;= zero(σ) is not satisfied.\nStacktrace:\n  [1] #371\n    @ ~/.julia/packages/Distributions/psM3H/src/univariate/continuous/normal.jl:37 [inlined]\n  [2] check_args\n    @ ~/.julia/packages/Distributions/psM3H/src/utils.jl:89 [inlined]\n  [3] #Normal#370\n    @ ~/.julia/packages/Distributions/psM3H/src/univariate/continuous/normal.jl:37 [inlined]\n  [4] Normal\n    @ ~/.julia/packages/Distributions/psM3H/src/univariate/continuous/normal.jl:36 [inlined]\n  [5] Normal\n    @ ~/.julia/packages/Distributions/psM3H/src/univariate/continuous/normal.jl:42 [inlined]\n  [6] macro expansion\n    @ ~/.julia/packages/DynamicPPL/Hza15/src/compiler.jl:599 [inlined]\n  [7] demo\n    @ ~/work/docs/docs/developers/inference/implementing-samplers/index.qmd:491 [inlined]\n  [8] _evaluate!!\n    @ ~/.julia/packages/DynamicPPL/Hza15/src/model.jl:997 [inlined]\n  [9] evaluate!!\n    @ ~/.julia/packages/DynamicPPL/Hza15/src/model.jl:983 [inlined]\n [10] init!!\n    @ ~/.julia/packages/DynamicPPL/Hza15/src/model.jl:938 [inlined]\n [11] init!!\n    @ ~/.julia/packages/DynamicPPL/Hza15/src/model.jl:943 [inlined]\n [12] DynamicPPL.ParamsWithStats(param_vector::Vector{Float64}, ldf::LogDensityFunction{false, DynamicPPL.Model{typeof(demo), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, AutoForwardDiff{2, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{σ²::DynamicPPL.RangeAndLinked, x::DynamicPPL.RangeAndLinked}, DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{false, DynamicPPL.Model{typeof(demo), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{σ²::DynamicPPL.RangeAndLinked, x::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{2, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 2, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 2}}}, Tuple{}}, Vector{Float64}}, stats::@NamedTuple{accepted::Bool}; include_colon_eq::Bool, include_log_probs::Bool)\n    @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/chains.jl:153\n [13] DynamicPPL.ParamsWithStats(param_vector::Vector{Float64}, ldf::LogDensityFunction{false, DynamicPPL.Model{typeof(demo), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, AutoForwardDiff{2, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{σ²::DynamicPPL.RangeAndLinked, x::DynamicPPL.RangeAndLinked}, DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{false, DynamicPPL.Model{typeof(demo), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{σ²::DynamicPPL.RangeAndLinked, x::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{2, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 2, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 2}}}, Tuple{}}, Vector{Float64}}, stats::@NamedTuple{accepted::Bool})\n    @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/chains.jl:131\n [14] step(rng::TaskLocalRNG, model::DynamicPPL.Model{typeof(demo), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, sampler_wrapper::Turing.Inference.ExternalSampler{false, MALA{Int64, UniformScaling{Bool}}, AutoForwardDiff{nothing, Nothing}}; initial_state::Nothing, initial_params::InitFromPrior, kwargs::@Kwargs{})\n    @ Turing.Inference ~/.julia/packages/Turing/Ak3CD/src/mcmc/external_sampler.jl:192\n [15] step\n    @ ~/.julia/packages/Turing/Ak3CD/src/mcmc/external_sampler.jl:141 [inlined]\n [16] macro expansion\n    @ ~/.julia/packages/AbstractMCMC/mcqES/src/sample.jl:188 [inlined]\n [17] (::AbstractMCMC.var\"#29#30\"{Nothing, Int64, Int64, Int64, UnionAll, Nothing, @Kwargs{initial_params::InitFromPrior}, TaskLocalRNG, DynamicPPL.Model{typeof(demo), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, Turing.Inference.ExternalSampler{false, MALA{Int64, UniformScaling{Bool}}, AutoForwardDiff{nothing, Nothing}}, Int64, Float64, Int64, Int64})()\n    @ AbstractMCMC ~/.julia/packages/AbstractMCMC/mcqES/src/logging.jl:134\n [18] with_logstate(f::AbstractMCMC.var\"#29#30\"{Nothing, Int64, Int64, Int64, UnionAll, Nothing, @Kwargs{initial_params::InitFromPrior}, TaskLocalRNG, DynamicPPL.Model{typeof(demo), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, Turing.Inference.ExternalSampler{false, MALA{Int64, UniformScaling{Bool}}, AutoForwardDiff{nothing, Nothing}}, Int64, Float64, Int64, Int64}, logstate::Base.CoreLogging.LogState)\n    @ Base.CoreLogging ./logging/logging.jl:524\n [19] with_logger(f::Function, logger::LoggingExtras.TeeLogger{Tuple{LoggingExtras.EarlyFilteredLogger{TerminalLoggers.TerminalLogger, AbstractMCMC.var\"#1#3\"{Module}}, LoggingExtras.EarlyFilteredLogger{Base.CoreLogging.ConsoleLogger, AbstractMCMC.var\"#2#4\"{Module}}}})\n    @ Base.CoreLogging ./logging/logging.jl:635\n [20] with_progresslogger(f::Function, _module::Module, logger::Base.CoreLogging.ConsoleLogger)\n    @ AbstractMCMC ~/.julia/packages/AbstractMCMC/mcqES/src/logging.jl:157\n [21] macro expansion\n    @ ~/.julia/packages/AbstractMCMC/mcqES/src/logging.jl:133 [inlined]\n [22] mcmcsample(rng::TaskLocalRNG, model::DynamicPPL.Model{typeof(demo), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, sampler::Turing.Inference.ExternalSampler{false, MALA{Int64, UniformScaling{Bool}}, AutoForwardDiff{nothing, Nothing}}, N::Int64; progress::Bool, progressname::String, callback::Nothing, num_warmup::Int64, discard_initial::Int64, thinning::Int64, chain_type::Type, initial_state::Nothing, kwargs::@Kwargs{initial_params::InitFromPrior})\n    @ AbstractMCMC ~/.julia/packages/AbstractMCMC/mcqES/src/sample.jl:168\n [23] sample(rng::TaskLocalRNG, model::DynamicPPL.Model{typeof(demo), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, spl::Turing.Inference.ExternalSampler{false, MALA{Int64, UniformScaling{Bool}}, AutoForwardDiff{nothing, Nothing}}, N::Int64; initial_params::InitFromPrior, check_model::Bool, chain_type::Type, kwargs::@Kwargs{progress::Bool})\n    @ Turing.Inference ~/.julia/packages/Turing/Ak3CD/src/mcmc/abstractmcmc.jl:85\n [24] sample\n    @ ~/.julia/packages/Turing/Ak3CD/src/mcmc/abstractmcmc.jl:74 [inlined]\n [25] #sample#1\n    @ ~/.julia/packages/Turing/Ak3CD/src/mcmc/abstractmcmc.jl:71 [inlined]\n [26] top-level scope\n    @ ~/work/docs/docs/developers/inference/implementing-samplers/index.qmd:493\n\n\n\nAs expected, we run into a DomainError at some point. This would not have happened while if we set unconstrained=true, letting Turing.jl transform the model to an unconstrained form behind the scenes, everything works as expected:\n\nsample(demo(), externalsampler(sampler; unconstrained=true), 10_000; progress=false)\n\nChains MCMC chain (10000×6×1 Array{Float64, 3}):\n\nIterations        = 1:1:10000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 1.21 seconds\nCompute duration  = 1.21 seconds\nparameters        = σ², x\ninternals         = accepted, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nIf you have implemented a sampler and you know for sure that your sampler cannot work with unconstrained parameters, you can disable the default behaviour by overloading the following method:\n# This cell isn't actually run; it's just a demonstration.\nAbstractMCMC.requires_unconstrained_space(::MALA) = false\nSimilarly, which automatic differentiation backend one should use can be specified through the adtype keyword argument too. For example, if we want to use ReverseDiff.jl instead of the default ForwardDiff.jl:\n\nimport ReverseDiff\n# Specify that we want to use `AutoReverseDiff`.\nsample(\n    demo(),\n    externalsampler(sampler; unconstrained=true, adtype=AutoReverseDiff()),\n    10_000;\n    progress=false\n)\n\nChains MCMC chain (10000×6×1 Array{Float64, 3}):\n\nIterations        = 1:1:10000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 2.06 seconds\nCompute duration  = 2.06 seconds\nparameters        = σ², x\ninternals         = accepted, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.",
    "crumbs": [
      "Get Started",
      "Developers",
      "Inference in Detail",
      "Implementing Samplers"
    ]
  },
  {
    "objectID": "developers/inference/implementing-samplers/index.html#summary",
    "href": "developers/inference/implementing-samplers/index.html#summary",
    "title": "Implementing Samplers",
    "section": "Summary",
    "text": "Summary\nAt this point it’s worth maybe reminding ourselves what we did and also why we did it:\n\nWe define our models in the LogDensityProblems.jl interface because it makes the sampler agnostic to how the underlying model is implemented.\nWe implement our sampler in the AbstractMCMC.jl interface, which just means that our sampler is a subtype of AbstractMCMC.AbstractSampler and we implement the MCMC transition in AbstractMCMC.step.\nPoints 1 and 2 makes it so our sampler can be used with a wide range of model implementations, amongst them being models implemented in both Turing.jl and Stan. This gives you, the inference implementer, a large collection of models to test your inference method on, in addition to allowing users of Turing.jl and Stan to try out your inference method with minimal effort.",
    "crumbs": [
      "Get Started",
      "Developers",
      "Inference in Detail",
      "Implementing Samplers"
    ]
  },
  {
    "objectID": "developers/inference/implementing-samplers/index.html#footnotes",
    "href": "developers/inference/implementing-samplers/index.html#footnotes",
    "title": "Implementing Samplers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe’re going with the leapfrog formulation because in a future version of this tutorial we’ll add a section extending this simple “baseline” MALA sampler to more complex versions. See issue #479 for progress on this.↩︎\nThere is no such thing as a proper interface in Julia (at least not officially), and so we use the word “interface” here to mean a few minimal methods that needs to be implemented by any type that we treat as a target model.↩︎",
    "crumbs": [
      "Get Started",
      "Developers",
      "Inference in Detail",
      "Implementing Samplers"
    ]
  },
  {
    "objectID": "versions.html",
    "href": "versions.html",
    "title": "Latest Version",
    "section": "",
    "text": "Latest Version\n\n\n\nv0.42\nDocumentation\nChangelog\n\n\n\n\n\nPrevious Versions\n\n\n\nv0.41\nDocumentation\n\n\nv0.40\nDocumentation\n\n\nv0.39\nDocumentation\n\n\nv0.38\nDocumentation\n\n\n\n\n\nArchived Versions\nDocumentation for archived versions is available on our deprecated documentation site.\n\n\n\nv0.31\nDocumentation\n\n\nv0.30\nDocumentation\n\n\nv0.29\nDocumentation\n\n\nv0.28\nDocumentation\n\n\nv0.27\nDocumentation\n\n\nv0.26\nDocumentation\n\n\nv0.25\nDocumentation\n\n\nv0.24\nDocumentation\n\n\n\n\n\n\n\n Back to top",
    "crumbs": null
  },
  {
    "objectID": "usage/probability-interface/index.html",
    "href": "usage/probability-interface/index.html",
    "title": "Querying Model Probabilities",
    "section": "",
    "text": "The easiest way to manipulate and query Turing models is via the DynamicPPL probability interface.\nLet’s use a simple model of normally-distributed data as an example.\nusing Turing\nusing DynamicPPL\nusing Random\n\n@model function gdemo(n)\n    μ ~ Normal(0, 1)\n    x ~ MvNormal(fill(μ, n), I)\nend\n\ngdemo (generic function with 2 methods)\nWe generate some data using μ = 0:\nRandom.seed!(1776)\ndataset = randn(100)\ndataset[1:5]\n\n5-element Vector{Float64}:\n  0.8488780584442736\n -0.31936138249336765\n -1.3982098801744465\n -0.05198933163879332\n -1.1465116601038348",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Querying Model Probabilities"
    ]
  },
  {
    "objectID": "usage/probability-interface/index.html#conditioning-and-deconditioning",
    "href": "usage/probability-interface/index.html#conditioning-and-deconditioning",
    "title": "Querying Model Probabilities",
    "section": "Conditioning and Deconditioning",
    "text": "Conditioning and Deconditioning\nBayesian models can be transformed with two main operations, conditioning and deconditioning (also known as marginalisation). Conditioning takes a variable and fixes its value as known. We do this by passing a model and a collection of conditioned variables to |, or its alias, condition:\n\n# (equivalently)\n# conditioned_model = condition(gdemo(length(dataset)), (x=dataset, μ=0))\nconditioned_model = gdemo(length(dataset)) | (x=dataset, μ=0)\n\nModel{typeof(gdemo), (:n,), (), (), Tuple{Int64}, Tuple{}, DynamicPPL.ConditionContext{@NamedTuple{x::Vector{Float64}, μ::Int64}, DefaultContext}, false}(gdemo, (n = 100,), NamedTuple(), ConditionContext((x = [0.8488780584442736, -0.31936138249336765, -1.3982098801744465, -0.05198933163879332, -1.1465116601038348, -0.6306168227545849, 0.6862766694322289, -0.5485073478947856, -0.17212004616875684, 1.2883226251958486, -0.13661316034377538, 2.4316115122026973, 0.2251319215717449, -0.5115708179083417, -0.7810712258995324, -1.0191704692490737, 1.1210038448250719, -1.6944509713762377, -0.27314823183454695, 0.25273963222687423, 1.3914215917992434, 0.7525340831125464, 0.847154387311101, -0.7130402796655171, 0.2983575202861233, -0.1785631526879386, 0.08659477535701691, -0.5167265137098563, 2.111309740316035, 0.3957655443124509, -0.0804390853521051, 1.255042471667049, -0.07882822403959532, 1.2261373761992618, 0.43953618247769816, -0.40640013183427787, -0.6868635949523503, 1.7380713294668497, 0.13685965156352295, 0.1485185624825999, -0.7798816720822024, 2.2595105995080846, -0.13609014938597142, 0.22785777205259913, -2.1005250433485725, 0.44205288222935385, -1.238456637875994, -2.3727125492433427, -0.24406624959402184, -0.04488042525902438, 0.27510026183444175, 0.42472846594528796, 1.0337924022589282, 0.9126364433535069, -0.9006583845907805, 0.8665471057463393, 1.4924737539852484, 1.2886591566091432, 1.037264411147446, 1.4731954133339449, -0.31874662373651885, 1.2255399151799211, -1.6642044048811695, -0.5717328092786154, -1.2700237196779645, 0.5748199649058684, 0.16467729820692942, -1.195290550625328, -0.37133526877621703, -0.3018979982049836, -2.0183406292097397, -0.9588803575112745, 0.7177183994733006, -1.0133440177662316, -1.0881357990941283, 1.0487446580734279, 2.627227367991459, -1.59963908284846, -0.3122512299247273, -1.0265333654194488, 0.5557085182114885, -0.3206725445321106, -1.4314746067673778, 1.5740113510560039, -0.6566477752702335, 0.31342313477927125, 0.33135361418686027, -1.0489180508346863, -0.2670759024309527, 0.4683952221006179, 0.04918061587657951, 1.239814741442417, 2.2239462179369296, 1.8507671783064434, 1.756319462015174, -0.6577450354719728, 2.2795431083561626, -0.492273906928334, 0.7045614632761499, 0.11260553216111485], μ = 0), DynamicPPL.DefaultContext()))\n\n\nThis operation can be reversed by applying decondition:\n\noriginal_model = decondition(conditioned_model)\n\nModel{typeof(gdemo), (:n,), (), (), Tuple{Int64}, Tuple{}, DefaultContext, false}(gdemo, (n = 100,), NamedTuple(), DefaultContext())\n\n\nWe can also decondition only some of the variables:\n\npartially_conditioned = decondition(conditioned_model, :μ)\n\nModel{typeof(gdemo), (:n,), (), (), Tuple{Int64}, Tuple{}, DynamicPPL.ConditionContext{@NamedTuple{x::Vector{Float64}}, DefaultContext}, false}(gdemo, (n = 100,), NamedTuple(), ConditionContext((x = [0.8488780584442736, -0.31936138249336765, -1.3982098801744465, -0.05198933163879332, -1.1465116601038348, -0.6306168227545849, 0.6862766694322289, -0.5485073478947856, -0.17212004616875684, 1.2883226251958486, -0.13661316034377538, 2.4316115122026973, 0.2251319215717449, -0.5115708179083417, -0.7810712258995324, -1.0191704692490737, 1.1210038448250719, -1.6944509713762377, -0.27314823183454695, 0.25273963222687423, 1.3914215917992434, 0.7525340831125464, 0.847154387311101, -0.7130402796655171, 0.2983575202861233, -0.1785631526879386, 0.08659477535701691, -0.5167265137098563, 2.111309740316035, 0.3957655443124509, -0.0804390853521051, 1.255042471667049, -0.07882822403959532, 1.2261373761992618, 0.43953618247769816, -0.40640013183427787, -0.6868635949523503, 1.7380713294668497, 0.13685965156352295, 0.1485185624825999, -0.7798816720822024, 2.2595105995080846, -0.13609014938597142, 0.22785777205259913, -2.1005250433485725, 0.44205288222935385, -1.238456637875994, -2.3727125492433427, -0.24406624959402184, -0.04488042525902438, 0.27510026183444175, 0.42472846594528796, 1.0337924022589282, 0.9126364433535069, -0.9006583845907805, 0.8665471057463393, 1.4924737539852484, 1.2886591566091432, 1.037264411147446, 1.4731954133339449, -0.31874662373651885, 1.2255399151799211, -1.6642044048811695, -0.5717328092786154, -1.2700237196779645, 0.5748199649058684, 0.16467729820692942, -1.195290550625328, -0.37133526877621703, -0.3018979982049836, -2.0183406292097397, -0.9588803575112745, 0.7177183994733006, -1.0133440177662316, -1.0881357990941283, 1.0487446580734279, 2.627227367991459, -1.59963908284846, -0.3122512299247273, -1.0265333654194488, 0.5557085182114885, -0.3206725445321106, -1.4314746067673778, 1.5740113510560039, -0.6566477752702335, 0.31342313477927125, 0.33135361418686027, -1.0489180508346863, -0.2670759024309527, 0.4683952221006179, 0.04918061587657951, 1.239814741442417, 2.2239462179369296, 1.8507671783064434, 1.756319462015174, -0.6577450354719728, 2.2795431083561626, -0.492273906928334, 0.7045614632761499, 0.11260553216111485],), DynamicPPL.DefaultContext()))\n\n\nWe can see which of the variables in a model have been conditioned with DynamicPPL.conditioned:\n\nDynamicPPL.conditioned(partially_conditioned)\n\n(x = [0.8488780584442736, -0.31936138249336765, -1.3982098801744465, -0.05198933163879332, -1.1465116601038348, -0.6306168227545849, 0.6862766694322289, -0.5485073478947856, -0.17212004616875684, 1.2883226251958486  …  0.04918061587657951, 1.239814741442417, 2.2239462179369296, 1.8507671783064434, 1.756319462015174, -0.6577450354719728, 2.2795431083561626, -0.492273906928334, 0.7045614632761499, 0.11260553216111485],)\n\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes it is helpful to define convenience functions for conditioning on some variable(s). For instance, in this example we might want to define a version of gdemo that conditions on some observations of x:\ngdemo(x::AbstractVector{&lt;:Real}) = gdemo(length(x)) | (; x)\nFor illustrative purposes, however, we do not use this function in the examples below.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Querying Model Probabilities"
    ]
  },
  {
    "objectID": "usage/probability-interface/index.html#probabilities-and-densities",
    "href": "usage/probability-interface/index.html#probabilities-and-densities",
    "title": "Querying Model Probabilities",
    "section": "Probabilities and Densities",
    "text": "Probabilities and Densities\nWe often want to calculate the (unnormalised) probability density for an event. This probability might be a prior, a likelihood, or a posterior (joint) density. DynamicPPL provides convenient functions for this. To begin, let’s define a model gdemo, condition it on a dataset, and draw a sample. The returned sample only contains μ, since the value of x has already been fixed:\n\nmodel = gdemo(length(dataset)) | (x=dataset,)\n\nRandom.seed!(124)\nsample = rand(model)\n\n(μ = -0.6680014719649068,)\n\n\nWe can then calculate the joint probability of a set of samples (here drawn from the prior) with logjoint.\n\nlogjoint(model, sample)\n\n-181.7247437162069\n\n\nFor models with many variables rand(model) can be prohibitively slow since it returns a NamedTuple of samples from the prior distribution of the unconditioned variables. We recommend working with samples of type DataStructures.OrderedDict in this case (which Turing re-exports, so can be used directly):\n\nRandom.seed!(124)\nsample_dict = rand(OrderedDict, model)\n\nOrderedDict{VarName, Any} with 1 entry:\n  μ =&gt; -0.668001\n\n\nlogjoint can also be used on this sample:\n\nlogjoint(model, sample_dict)\n\n-181.7247437162069\n\n\nThe prior probability and the likelihood of a set of samples can be calculated with the functions logprior and loglikelihood respectively. The log joint probability is the sum of these two quantities:\n\nlogjoint(model, sample) ≈ loglikelihood(model, sample) + logprior(model, sample)\n\ntrue\n\n\n\nlogjoint(model, sample_dict) ≈ loglikelihood(model, sample_dict) + logprior(model, sample_dict)\n\ntrue",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Querying Model Probabilities"
    ]
  },
  {
    "objectID": "usage/probability-interface/index.html#example-cross-validation",
    "href": "usage/probability-interface/index.html#example-cross-validation",
    "title": "Querying Model Probabilities",
    "section": "Example: Cross-validation",
    "text": "Example: Cross-validation\nTo give an example of the probability interface in use, we can use it to estimate the performance of our model using cross-validation. In cross-validation, we split the dataset into several equal parts. Then, we choose one of these sets to serve as the validation set. Here, we measure fit using the cross entropy (Bayes loss).1 (For the sake of simplicity, in the following code, we enforce that nfolds must divide the number of data points. For a more competent implementation, see MLUtils.jl.)\n\n# Calculate the train/validation splits across `nfolds` partitions, assume `length(dataset)` divides `nfolds`\nfunction kfolds(dataset::Array{&lt;:Real}, nfolds::Int)\n    fold_size, remaining = divrem(length(dataset), nfolds)\n    if remaining != 0\n        error(\"The number of folds must divide the number of data points.\")\n    end\n    first_idx = firstindex(dataset)\n    last_idx = lastindex(dataset)\n    splits = map(0:(nfolds - 1)) do i\n        start_idx = first_idx + i * fold_size\n        end_idx = start_idx + fold_size\n        train_set_indices = [first_idx:(start_idx - 1); end_idx:last_idx]\n        return (view(dataset, train_set_indices), view(dataset, start_idx:(end_idx - 1)))\n    end\n    return splits\nend\n\nfunction cross_val(\n    dataset::Vector{&lt;:Real};\n    nfolds::Int=5,\n    nsamples::Int=1_000,\n    rng::Random.AbstractRNG=Random.default_rng(),\n)\n    # Initialize `loss` in a way such that the loop below does not change its type\n    model = gdemo(1) | (x=[first(dataset)],)\n    loss = zero(logjoint(model, rand(rng, model)))\n\n    for (train, validation) in kfolds(dataset, nfolds)\n        # First, we train the model on the training set, i.e., we obtain samples from the posterior.\n        # For normally-distributed data, the posterior can be computed in closed form.\n        # For general models, however, typically samples will be generated using MCMC with Turing.\n        posterior = Normal(mean(train), 1)\n        samples = rand(rng, posterior, nsamples)\n\n        # Evaluation on the validation set.\n        validation_model = gdemo(length(validation)) | (x=validation,)\n        loss += sum(samples) do sample\n            logjoint(validation_model, (μ=sample,))\n        end\n    end\n\n    return loss\nend\n\ncross_val(dataset)\n\n-212760.30282411768",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Querying Model Probabilities"
    ]
  },
  {
    "objectID": "usage/probability-interface/index.html#footnotes",
    "href": "usage/probability-interface/index.html#footnotes",
    "title": "Querying Model Probabilities",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee ParetoSmooth.jl for a faster and more accurate implementation of cross-validation than the one provided here.↩︎",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Querying Model Probabilities"
    ]
  },
  {
    "objectID": "usage/troubleshooting/index.html",
    "href": "usage/troubleshooting/index.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "This page collects a number of common error messages observed when using Turing, along with suggestions on how to fix them.\nIf the suggestions here do not resolve your problem, please do feel free to open an issue.\nusing Turing\nTuring.setprogress!(false)\n\n\n[ Info: [Turing]: progress logging is disabled globally\n\n\n\n\nfalse",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "usage/troubleshooting/index.html#initial-parameters",
    "href": "usage/troubleshooting/index.html#initial-parameters",
    "title": "Troubleshooting",
    "section": "Initial parameters",
    "text": "Initial parameters\n\nfailed to find valid initial parameters in {N} tries. This may indicate an error with the model or AD backend…\n\nThis error is seen when a Hamiltonian Monte Carlo sampler is unable to determine a valid set of initial parameters for the sampling. Here, ‘valid’ means that the log probability density of the model, as well as its gradient with respect to each parameter, is finite and not NaN.\n\nNaN gradient\nOne of the most common causes of this error is having a NaN gradient. To find out whether this is happening, you can evaluate the gradient manually. Here is an example with a model that is known to be problematic:\n\nusing Turing\nusing DynamicPPL.TestUtils.AD: run_ad\n\n@model function initial_bad()\n    a ~ Normal()\n    x ~ truncated(Normal(a), 0, Inf)\nend\n\nmodel = initial_bad()\nadtype = AutoForwardDiff()\nresult = run_ad(model, adtype; test=false, benchmark=false)\nresult.grad_actual\n\n\n[ Info: Running AD on initial_bad with ADTypes.AutoForwardDiff()\n       params : [0.08218824112899918, -1.5532879436739717]\n       actual : (-2.7732096439769327, [NaN, NaN])\n\n\n\n\n2-element Vector{Float64}:\n NaN\n NaN\n\n\n(See the DynamicPPL docs for more details on the run_ad function and its return type.)\nIn this case, the NaN gradient is caused by the Inf argument to truncated. (See, e.g., this issue on Distributions.jl.) Here, the upper bound of Inf is not needed, so it can be removed:\n\n@model function initial_good()\n    a ~ Normal()\n    x ~ truncated(Normal(a); lower=0)\nend\n\nmodel = initial_good()\nadtype = AutoForwardDiff()\nrun_ad(model, adtype; test=false, benchmark=false).grad_actual\n\n\n[ Info: Running AD on initial_good with ADTypes.AutoForwardDiff()\n       params : [-0.43229843185982686, -2.3160585619181653]\n       actual : (-3.2880089527292187, [-0.1286722057191132, 0.9476145775597784])\n\n\n\n\n2-element Vector{Float64}:\n -0.1286722057191132\n  0.9476145775597784\n\n\nMore generally, you could try using a different AD backend; if you don’t know why a model is returning NaN gradients, feel free to open an issue.\n\n\n-Inf log density\nAnother cause of this error is having models with very extreme parameters. This example is taken from this Turing.jl issue:\n\n@model function initial_bad2()\n    x ~ Exponential(100)\n    y ~ Uniform(0, x)\nend\nmodel = initial_bad2() | (y = 50.0,)\n\nDynamicPPL.Model{typeof(initial_bad2), (), (), (), Tuple{}, Tuple{}, DynamicPPL.ConditionContext{@NamedTuple{y::Float64}, DynamicPPL.DefaultContext}, false}(initial_bad2, NamedTuple(), NamedTuple(), ConditionContext((y = 50.0,), DynamicPPL.DefaultContext()))\n\n\nThe problem here is that HMC’s default initialisation strategy is InitFromUniform(-2, 2): in other words, it attempts to find initial values for transformed parameters inside the region of [-2, 2]. For a distribution of Exponential(100), the appropriate transformation is log(x) (see the variable transformation docs for more info).\nThus, HMC attempts to find initial values of log(x) in the region of [-2, 2], which corresponds to x in the region of [exp(-2), exp(2)] = [0.135, 7.39]. However, all of these values of x will give rise to a zero probability density for y because the value of y = 50.0 is outside the support of Uniform(0, x). Thus, the log density of the model is -Inf, as can be seen with logjoint:\n\nlogjoint(model, (x = exp(-2),))\n\n-Inf\n\n\n\nlogjoint(model, (x = exp(2),))\n\n-Inf\n\n\nYou can fix this by overriding the default initialisation strategy (discussed in more detail in the sampling options page).\n\n# Use initial parameters drawn from the model's prior.\nsample(model, NUTS(), 1000; initial_params=InitFromPrior())\n\n\n┌ Info: Found initial step size\n└   ϵ = 1.6\n\n\n\n\nChains MCMC chain (1000×15×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 3.21 seconds\nCompute duration  = 3.21 seconds\nparameters        = x\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n\n# Use manually specified initial parameters (always in untransformed space).\nsample(model, NUTS(), 1000; initial_params=InitFromParams((x = 60.0,)))\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.2\n\n\n\n\nChains MCMC chain (1000×15×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 0.29 seconds\nCompute duration  = 0.29 seconds\nparameters        = x\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nMore generally, you may also consider reparameterising the model to avoid such issues.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "usage/troubleshooting/index.html#forwarddiff-type-parameters",
    "href": "usage/troubleshooting/index.html#forwarddiff-type-parameters",
    "title": "Troubleshooting",
    "section": "ForwardDiff type parameters",
    "text": "ForwardDiff type parameters\n\nMethodError: no method matching Float64(::ForwardDiff.Dual{… The type Float64 exists, but no method is defined for this combination of argument types when trying to construct it.\n\nA common error with ForwardDiff looks like this:\n\n@model function forwarddiff_fail()\n    x = Float64[0.0, 1.0]\n    a ~ Normal()\n    @show typeof(a)\n    x[1] = a\n    b ~ MvNormal(x, I)\nend\nsample(forwarddiff_fail(), NUTS(; adtype=AutoForwardDiff()), 10)\n\ntypeof(a) = Float64\ntypeof(a) = Float64\ntypeof(a) = Float64\ntypeof(a) = ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}\n\n\n\nMethodError: no method matching Float64(::ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3})\nThe type `Float64` exists, but no method is defined for this combination of argument types when trying to construct it.\n\nClosest candidates are:\n  (::Type{T})(::Real, ::RoundingMode) where T&lt;:AbstractFloat\n   @ Base rounding.jl:265\n  (::Type{T})(::T) where T&lt;:Number\n   @ Core boot.jl:900\n  Float64(::IrrationalConstants.Inv4π)\n   @ IrrationalConstants ~/.julia/packages/IrrationalConstants/RokwY/src/macro.jl:111\n  ...\n\nStacktrace:\n  [1] convert(::Type{Float64}, x::ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3})\n    @ Base ./number.jl:7\n  [2] setindex!(A::Vector{Float64}, x::ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}, i::Int64)\n    @ Base ./array.jl:987\n  [3] forwarddiff_fail(__model__::DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.InitContext{Random.TaskLocalRNG, InitFromParams{DynamicPPL.VectorWithRanges{true, @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Nothing}}, false}, __varinfo__::DynamicPPL.OnlyAccsVarInfo{DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::DynamicPPL.LogPriorAccumulator{Float64}, LogJacobian::DynamicPPL.LogJacobianAccumulator{Float64}, LogLikelihood::DynamicPPL.LogLikelihoodAccumulator{Float64}}}})\n    @ Main.Notebook ~/work/docs/docs/usage/troubleshooting/index.qmd:123\n  [4] _evaluate!!(model::DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.InitContext{Random.TaskLocalRNG, InitFromParams{DynamicPPL.VectorWithRanges{true, @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Nothing}}, false}, varinfo::DynamicPPL.OnlyAccsVarInfo{DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::DynamicPPL.LogPriorAccumulator{Float64}, LogJacobian::DynamicPPL.LogJacobianAccumulator{Float64}, LogLikelihood::DynamicPPL.LogLikelihoodAccumulator{Float64}}}})\n    @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/model.jl:997\n  [5] evaluate!!(model::DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.InitContext{Random.TaskLocalRNG, InitFromParams{DynamicPPL.VectorWithRanges{true, @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Nothing}}, false}, varinfo::DynamicPPL.OnlyAccsVarInfo{DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::DynamicPPL.LogPriorAccumulator{Float64}, LogJacobian::DynamicPPL.LogJacobianAccumulator{Float64}, LogLikelihood::DynamicPPL.LogLikelihoodAccumulator{Float64}}}})\n    @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/model.jl:983\n  [6] init!!(rng::Random.TaskLocalRNG, model::DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, vi::DynamicPPL.OnlyAccsVarInfo{DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::DynamicPPL.LogPriorAccumulator{Float64}, LogJacobian::DynamicPPL.LogJacobianAccumulator{Float64}, LogLikelihood::DynamicPPL.LogLikelihoodAccumulator{Float64}}}}, strategy::InitFromParams{DynamicPPL.VectorWithRanges{true, @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Nothing})\n    @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/model.jl:938\n  [7] init!!(model::DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, vi::DynamicPPL.OnlyAccsVarInfo{DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::DynamicPPL.LogPriorAccumulator{Float64}, LogJacobian::DynamicPPL.LogJacobianAccumulator{Float64}, LogLikelihood::DynamicPPL.LogLikelihoodAccumulator{Float64}}}}, strategy::InitFromParams{DynamicPPL.VectorWithRanges{true, @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Nothing})\n    @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/model.jl:943\n  [8] (::DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}})(params::Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}})\n    @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/logdensityfunction.jl:258\n  [9] vector_mode_dual_eval!(f::DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, x::Vector{Float64})\n    @ ForwardDiff ~/.julia/packages/ForwardDiff/kQBw9/src/apiutils.jl:24\n [10] vector_mode_gradient!(result::DiffResults.MutableDiffResult{1, Float64, Tuple{Vector{Float64}}}, f::DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, x::Vector{Float64}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}})\n    @ ForwardDiff ~/.julia/packages/ForwardDiff/kQBw9/src/gradient.jl:105\n [11] gradient!(result::DiffResults.MutableDiffResult{1, Float64, Tuple{Vector{Float64}}}, f::DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, x::Vector{Float64}, cfg::ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, ::Val{false})\n    @ ForwardDiff ~/.julia/packages/ForwardDiff/kQBw9/src/gradient.jl:39\n [12] value_and_gradient(::DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, ::DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Tuple{}}, ::AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, ::Vector{Float64})\n    @ DifferentiationInterfaceForwardDiffExt ~/.julia/packages/DifferentiationInterface/4n6vR/ext/DifferentiationInterfaceForwardDiffExt/onearg.jl:419\n [13] logdensity_and_gradient(ldf::LogDensityFunction{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Tuple{}}, Vector{Float64}}, params::Vector{Float64})\n    @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/logdensityfunction.jl:276\n [14] (::Base.Fix1{typeof(LogDensityProblems.logdensity_and_gradient), LogDensityFunction{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Tuple{}}, Vector{Float64}}})(y::Vector{Float64})\n    @ Base ./operators.jl:1127\n [15] ∂H∂θ(h::AdvancedHMC.Hamiltonian{AdvancedHMC.DiagEuclideanMetric{Float64, Vector{Float64}}, AdvancedHMC.GaussianKinetic, Base.Fix1{typeof(LogDensityProblems.logdensity), LogDensityFunction{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Tuple{}}, Vector{Float64}}}, Base.Fix1{typeof(LogDensityProblems.logdensity_and_gradient), LogDensityFunction{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Tuple{}}, Vector{Float64}}}}, θ::Vector{Float64})\n    @ AdvancedHMC ~/.julia/packages/AdvancedHMC/kEVkt/src/hamiltonian.jl:46\n [16] phasepoint(h::AdvancedHMC.Hamiltonian{AdvancedHMC.DiagEuclideanMetric{Float64, Vector{Float64}}, AdvancedHMC.GaussianKinetic, Base.Fix1{typeof(LogDensityProblems.logdensity), LogDensityFunction{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Tuple{}}, Vector{Float64}}}, Base.Fix1{typeof(LogDensityProblems.logdensity_and_gradient), LogDensityFunction{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Tuple{}}, Vector{Float64}}}}, θ::Vector{Float64}, r::Vector{Float64})\n    @ AdvancedHMC ~/.julia/packages/AdvancedHMC/kEVkt/src/hamiltonian.jl:103\n [17] phasepoint(rng::Random.TaskLocalRNG, θ::Vector{Float64}, h::AdvancedHMC.Hamiltonian{AdvancedHMC.DiagEuclideanMetric{Float64, Vector{Float64}}, AdvancedHMC.GaussianKinetic, Base.Fix1{typeof(LogDensityProblems.logdensity), LogDensityFunction{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Tuple{}}, Vector{Float64}}}, Base.Fix1{typeof(LogDensityProblems.logdensity_and_gradient), LogDensityFunction{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Tuple{}}, Vector{Float64}}}})\n    @ AdvancedHMC ~/.julia/packages/AdvancedHMC/kEVkt/src/hamiltonian.jl:185\n [18] find_initial_params(rng::Random.TaskLocalRNG, model::DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, varinfo::DynamicPPL.VarInfo{@NamedTuple{a::DynamicPPL.Metadata{Dict{AbstractPPL.VarName{:a, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{AbstractPPL.VarName{:a, typeof(identity)}}, Vector{Float64}}, b::DynamicPPL.Metadata{Dict{AbstractPPL.VarName{:b, typeof(identity)}, Int64}, Vector{IsoNormal}, Vector{AbstractPPL.VarName{:b, typeof(identity)}}, Vector{Float64}}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::DynamicPPL.LogPriorAccumulator{Float64}, LogJacobian::DynamicPPL.LogJacobianAccumulator{Float64}, LogLikelihood::DynamicPPL.LogLikelihoodAccumulator{Float64}}}}, hamiltonian::AdvancedHMC.Hamiltonian{AdvancedHMC.DiagEuclideanMetric{Float64, Vector{Float64}}, AdvancedHMC.GaussianKinetic, Base.Fix1{typeof(LogDensityProblems.logdensity), LogDensityFunction{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Tuple{}}, Vector{Float64}}}, Base.Fix1{typeof(LogDensityProblems.logdensity_and_gradient), LogDensityFunction{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Tuple{}}, Vector{Float64}}}}, init_strategy::InitFromUniform{Float64}; max_attempts::Int64)\n    @ Turing.Inference ~/.julia/packages/Turing/Ak3CD/src/mcmc/hmc.jl:164\n [19] find_initial_params(rng::Random.TaskLocalRNG, model::DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, varinfo::DynamicPPL.VarInfo{@NamedTuple{a::DynamicPPL.Metadata{Dict{AbstractPPL.VarName{:a, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{AbstractPPL.VarName{:a, typeof(identity)}}, Vector{Float64}}, b::DynamicPPL.Metadata{Dict{AbstractPPL.VarName{:b, typeof(identity)}, Int64}, Vector{IsoNormal}, Vector{AbstractPPL.VarName{:b, typeof(identity)}}, Vector{Float64}}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::DynamicPPL.LogPriorAccumulator{Float64}, LogJacobian::DynamicPPL.LogJacobianAccumulator{Float64}, LogLikelihood::DynamicPPL.LogLikelihoodAccumulator{Float64}}}}, hamiltonian::AdvancedHMC.Hamiltonian{AdvancedHMC.DiagEuclideanMetric{Float64, Vector{Float64}}, AdvancedHMC.GaussianKinetic, Base.Fix1{typeof(LogDensityProblems.logdensity), LogDensityFunction{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Tuple{}}, Vector{Float64}}}, Base.Fix1{typeof(LogDensityProblems.logdensity_and_gradient), LogDensityFunction{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}, DifferentiationInterfaceForwardDiffExt.ForwardDiffGradientPrep{Tuple{DynamicPPL.LogDensityAt{true, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, typeof(DynamicPPL.getlogjoint_internal), @NamedTuple{a::DynamicPPL.RangeAndLinked, b::DynamicPPL.RangeAndLinked}}, AutoForwardDiff{3, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}}, Vector{Float64}, Tuple{}}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}, Tuple{}}, Vector{Float64}}}}, init_strategy::InitFromUniform{Float64})\n    @ Turing.Inference ~/.julia/packages/Turing/Ak3CD/src/mcmc/hmc.jl:152\n [20] initialstep(rng::Random.TaskLocalRNG, model::DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, spl::NUTS{AutoForwardDiff{nothing, Nothing}, AdvancedHMC.DiagEuclideanMetric}, vi_original::DynamicPPL.VarInfo{@NamedTuple{a::DynamicPPL.Metadata{Dict{AbstractPPL.VarName{:a, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{AbstractPPL.VarName{:a, typeof(identity)}}, Vector{Float64}}, b::DynamicPPL.Metadata{Dict{AbstractPPL.VarName{:b, typeof(identity)}, Int64}, Vector{IsoNormal}, Vector{AbstractPPL.VarName{:b, typeof(identity)}}, Vector{Float64}}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::DynamicPPL.LogPriorAccumulator{Float64}, LogJacobian::DynamicPPL.LogJacobianAccumulator{Float64}, LogLikelihood::DynamicPPL.LogLikelihoodAccumulator{Float64}}}}; initial_params::InitFromUniform{Float64}, nadapts::Int64, verbose::Bool, kwargs::@Kwargs{})\n    @ Turing.Inference ~/.julia/packages/Turing/Ak3CD/src/mcmc/hmc.jl:216\n [21] step(rng::Random.TaskLocalRNG, model::DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, spl::NUTS{AutoForwardDiff{nothing, Nothing}, AdvancedHMC.DiagEuclideanMetric}; initial_params::InitFromUniform{Float64}, kwargs::@Kwargs{nadapts::Int64})\n    @ Turing.Inference ~/.julia/packages/Turing/Ak3CD/src/mcmc/abstractmcmc.jl:180\n [22] step\n    @ ~/.julia/packages/Turing/Ak3CD/src/mcmc/abstractmcmc.jl:162 [inlined]\n [23] macro expansion\n    @ ~/.julia/packages/AbstractMCMC/mcqES/src/sample.jl:188 [inlined]\n [24] (::AbstractMCMC.var\"#29#30\"{Nothing, Int64, Int64, Int64, UnionAll, Nothing, @Kwargs{nadapts::Int64, initial_params::InitFromUniform{Float64}}, Random.TaskLocalRNG, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, NUTS{AutoForwardDiff{nothing, Nothing}, AdvancedHMC.DiagEuclideanMetric}, Int64, Float64, Int64, Int64})()\n    @ AbstractMCMC ~/.julia/packages/AbstractMCMC/mcqES/src/logging.jl:134\n [25] with_logstate(f::AbstractMCMC.var\"#29#30\"{Nothing, Int64, Int64, Int64, UnionAll, Nothing, @Kwargs{nadapts::Int64, initial_params::InitFromUniform{Float64}}, Random.TaskLocalRNG, DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, NUTS{AutoForwardDiff{nothing, Nothing}, AdvancedHMC.DiagEuclideanMetric}, Int64, Float64, Int64, Int64}, logstate::Base.CoreLogging.LogState)\n    @ Base.CoreLogging ./logging/logging.jl:524\n [26] with_logger(f::Function, logger::LoggingExtras.TeeLogger{Tuple{LoggingExtras.EarlyFilteredLogger{TerminalLoggers.TerminalLogger, AbstractMCMC.var\"#1#3\"{Module}}, LoggingExtras.EarlyFilteredLogger{Base.CoreLogging.ConsoleLogger, AbstractMCMC.var\"#2#4\"{Module}}}})\n    @ Base.CoreLogging ./logging/logging.jl:635\n [27] with_progresslogger(f::Function, _module::Module, logger::Base.CoreLogging.ConsoleLogger)\n    @ AbstractMCMC ~/.julia/packages/AbstractMCMC/mcqES/src/logging.jl:157\n [28] macro expansion\n    @ ~/.julia/packages/AbstractMCMC/mcqES/src/logging.jl:133 [inlined]\n [29] mcmcsample(rng::Random.TaskLocalRNG, model::DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, sampler::NUTS{AutoForwardDiff{nothing, Nothing}, AdvancedHMC.DiagEuclideanMetric}, N::Int64; progress::Bool, progressname::String, callback::Nothing, num_warmup::Int64, discard_initial::Int64, thinning::Int64, chain_type::Type, initial_state::Nothing, kwargs::@Kwargs{nadapts::Int64, initial_params::InitFromUniform{Float64}})\n    @ AbstractMCMC ~/.julia/packages/AbstractMCMC/mcqES/src/sample.jl:168\n [30] sample(rng::Random.TaskLocalRNG, model::DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, sampler::NUTS{AutoForwardDiff{nothing, Nothing}, AdvancedHMC.DiagEuclideanMetric}, N::Int64; check_model::Bool, chain_type::Type, initial_params::InitFromUniform{Float64}, initial_state::Nothing, progress::Bool, nadapts::Int64, discard_adapt::Bool, discard_initial::Int64, kwargs::@Kwargs{})\n    @ Turing.Inference ~/.julia/packages/Turing/Ak3CD/src/mcmc/hmc.jl:121\n [31] sample\n    @ ~/.julia/packages/Turing/Ak3CD/src/mcmc/hmc.jl:88 [inlined]\n [32] #sample#1\n    @ ~/.julia/packages/Turing/Ak3CD/src/mcmc/abstractmcmc.jl:71 [inlined]\n [33] sample(model::DynamicPPL.Model{typeof(forwarddiff_fail), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}, spl::NUTS{AutoForwardDiff{nothing, Nothing}, AdvancedHMC.DiagEuclideanMetric}, N::Int64)\n    @ Turing.Inference ~/.julia/packages/Turing/Ak3CD/src/mcmc/abstractmcmc.jl:68\n [34] top-level scope\n    @ ~/work/docs/docs/usage/troubleshooting/index.qmd:126\n\n\n\nThe problem here is the line x[1] = a. When the log probability density of the model is calculated, a is sampled from a normal distribution and is thus a Float64; however, when ForwardDiff calculates the gradient of the log density, a is a ForwardDiff.Dual object. However, x is always a Vector{Float64}, and the call x[1] = a attempts to insert a Dual object into a Vector{Float64}, which is not allowed.\n\n\n\n\n\n\nNote\n\n\n\nIn more detail: the basic premise of ForwardDiff is that functions have to accept Real parameters instead of Float64 (since Dual is a subtype of Real). Here, the line x[1] = a is equivalent to setindex!(x, a, 1), and although the method setindex!(::Vector{Float64}, ::Real, ...) does exist, it attempts to convert the Real into a Float64, which is where it fails.\n\n\nThere are two ways around this.\nFirstly, you could broaden the type of the container:\n\n@model function forwarddiff_working1()\n    x = Real[0.0, 1.0]\n    a ~ Normal()\n    x[1] = a\n    b ~ MvNormal(x, I)\nend\nsample(forwarddiff_working1(), NUTS(; adtype=AutoForwardDiff()), 10)\n\n\n┌ Info: Found initial step size\n└   ϵ = 1.6\n\n\n\n\nChains MCMC chain (10×17×1 Array{Float64, 3}):\n\nIterations        = 6:1:15\nNumber of chains  = 1\nSamples per chain = 10\nWall duration     = 2.36 seconds\nCompute duration  = 2.36 seconds\nparameters        = a, b[1], b[2]\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nThis is generally unfavourable because the Vector{Real} type contains an abstract type parameter. As a result, memory allocation is less efficient (because the compiler does not know the size of each vector’s elements). Furthermore, the compiler cannot infer the type of x[1], which can lead to type stability issues (to see this in action, run x = Real[0.0, 1.0]; @code_warntype x[1] in the Julia REPL).\nA better solution is to pass a type as a parameter to the model:\n\n@model function forwarddiff_working2(::Type{T}=Float64) where T\n    x = T[0.0, 1.0]\n    a ~ Normal()\n    x[1] = a\n    b ~ MvNormal(x, I)\nend\nsample(forwarddiff_working2(), NUTS(; adtype=AutoForwardDiff()), 10)\n\n\n┌ Info: Found initial step size\n└   ϵ = 1.6\n\n\n\n\nChains MCMC chain (10×17×1 Array{Float64, 3}):\n\nIterations        = 6:1:15\nNumber of chains  = 1\nSamples per chain = 10\nWall duration     = 1.35 seconds\nCompute duration  = 1.35 seconds\nparameters        = a, b[1], b[2]\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nAlternatively, you can use a different AD backend such as Mooncake.jl which does not rely on dual numbers.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "usage/dynamichmc/index.html",
    "href": "usage/dynamichmc/index.html",
    "title": "Using DynamicHMC",
    "section": "",
    "text": "Turing supports the use of DynamicHMC as a sampler through the DynamicNUTS function.\nTo use the DynamicNUTS function, you must import the DynamicHMC package as well as Turing. Turing does not formally require DynamicHMC but will include additional functionality if both packages are present.\nHere is a brief example:\n\nHow to apply DynamicNUTS:\n\n# Import Turing and DynamicHMC.\nusing DynamicHMC, Turing\n\n# Model definition.\n@model function gdemo(x, y)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    x ~ Normal(m, sqrt(s²))\n    return y ~ Normal(m, sqrt(s²))\nend\n\n# Pull 2,000 samples using DynamicNUTS.\ndynamic_nuts = externalsampler(DynamicHMC.NUTS())\nchn = sample(gdemo(1.5, 2.0), dynamic_nuts, 2000, progress=false)\n\nChains MCMC chain (2000×5×1 Array{Float64, 3}):\n\nIterations        = 1:1:2000\nNumber of chains  = 1\nSamples per chain = 2000\nWall duration     = 6.25 seconds\nCompute duration  = 6.25 seconds\nparameters        = s², m\ninternals         = logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Using DynamicHMC"
    ]
  },
  {
    "objectID": "usage/sampling-options/index.html",
    "href": "usage/sampling-options/index.html",
    "title": "MCMC Sampling Options",
    "section": "",
    "text": "Markov chain Monte Carlo sampling in Turing.jl is performed using the sample() function. As described on the Core Functionality page, single-chain and multiple-chain sampling can be done using, respectively,\nOn top of this, both methods also accept a number of keyword arguments that allow you to control the sampling process. This page will detail these options.\nTo begin, let’s create a simple model:\nusing Turing\n\n@model function demo_model()\n    x ~ Normal()\n    y ~ Normal(x)\n    4.0 ~ Normal(y)\n    return nothing\nend\n\ndemo_model (generic function with 2 methods)",
    "crumbs": [
      "Get Started",
      "User Guide",
      "MCMC Sampling Options"
    ]
  },
  {
    "objectID": "usage/sampling-options/index.html#controlling-logging",
    "href": "usage/sampling-options/index.html#controlling-logging",
    "title": "MCMC Sampling Options",
    "section": "Controlling logging",
    "text": "Controlling logging\nProgress bars can be controlled with the progress keyword argument. The exact values that can be used depend on whether you are using single-chain or multi-chain sampling.\nFor single-chain sampling, progress=true and progress=false enable and disable the progress bar, respectively.\nFor multi-chain sampling, progress can take the following values:\n\n:none or false: no progress bar\n(default) :overall or true: creates one overall progress bar for all chains\n:perchain: creates one overall progress bar, plus one extra progress bar per chain (note that this can lead to visual clutter if you have many chains)\n\nIf you want to globally enable or disable the progress bar, you can use:\n\nTuring.setprogress!(false);   # or true\n\n\n[ Info: [Turing]: progress logging is disabled globally\n\n\n\n\n(This handily also disables progress logging for the rest of this document.)\nFor NUTS in particular, you can also specify verbose=false to disable the “Found initial step size” info message.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "MCMC Sampling Options"
    ]
  },
  {
    "objectID": "usage/sampling-options/index.html#ensuring-sampling-reproducibility",
    "href": "usage/sampling-options/index.html#ensuring-sampling-reproducibility",
    "title": "MCMC Sampling Options",
    "section": "Ensuring sampling reproducibility",
    "text": "Ensuring sampling reproducibility\nLike many other Julia functions, a Random.AbstractRNG object can be passed as the first argument to sample() to ensure reproducibility of results.\n\nusing Random\nchn1 = sample(Xoshiro(468), demo_model(), MH(), 5);\nchn2 = sample(Xoshiro(468), demo_model(), MH(), 5);\n(chn1[:x] == chn2[:x], chn1[:y] == chn2[:y])\n\n(true, true)\n\n\nAlternatively, you can set the global RNG using Random.seed!(), although we recommend this less as it modifies global state.\n\nRandom.seed!(468)\nchn3 = sample(demo_model(), MH(), 5);\nRandom.seed!(468)\nchn4 = sample(demo_model(), MH(), 5);\n(chn3[:x] == chn4[:x], chn3[:y] == chn4[:y])\n\n(true, true)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe outputs of pseudorandom number generators in the standard Random library are not guaranteed to be the same across different Julia versions or platforms. If you require absolute reproducibility, you should use the StableRNGs.jl package.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "MCMC Sampling Options"
    ]
  },
  {
    "objectID": "usage/sampling-options/index.html#switching-the-output-type",
    "href": "usage/sampling-options/index.html#switching-the-output-type",
    "title": "MCMC Sampling Options",
    "section": "Switching the output type",
    "text": "Switching the output type\nBy default, the results of MCMC sampling are bundled up in an MCMCChains.Chains object.\n\nchn = sample(demo_model(), HMC(0.1, 20), 5)\ntypeof(chn)\n\nChains{Union{Missing, Float64}, AxisArrays.AxisArray{Union{Missing, Float64}, 3, Array{Union{Missing, Float64}, 3}, Tuple{AxisArrays.Axis{:iter, StepRange{Int64, Int64}}, AxisArrays.Axis{:var, Vector{Symbol}}, AxisArrays.Axis{:chain, UnitRange{Int64}}}}, Missing, @NamedTuple{parameters::Vector{Symbol}, internals::Vector{Symbol}}, @NamedTuple{varname_to_symbol::OrderedDict{AbstractPPL.VarName, Symbol}, start_time::Float64, stop_time::Float64}}\n\n\nIf you wish to use a different chain format provided in another package, you can specify the chain_type keyword argument. You should refer to the documentation of the respective package for exact details.\nAnother situation where specifying chain_type can be useful is when you want to obtain the raw MCMC outputs as a vector of transitions. This can be used for profiling or debugging purposes (often, chain construction can take a surprising amount of time compared to sampling, especially for very simple models). To do so, you can use chain_type=Any (i.e., do not convert the output to any specific chain format):\n\ntransitions = sample(demo_model(), MH(), 5; chain_type=Any)\ntypeof(transitions)\n\n\nVector{ParamsWithStats{OrderedDict{VarName, Any}, @NamedTuple{logprior::Float64, loglikelihood::Float64, logjoint::Float64}}} (alias for Array{DynamicPPL.ParamsWithStats{OrderedDict{AbstractPPL.VarName, Any}, @NamedTuple{logprior::Float64, loglikelihood::Float64, logjoint::Float64}}, 1})",
    "crumbs": [
      "Get Started",
      "User Guide",
      "MCMC Sampling Options"
    ]
  },
  {
    "objectID": "usage/sampling-options/index.html#specifying-initial-parameters",
    "href": "usage/sampling-options/index.html#specifying-initial-parameters",
    "title": "MCMC Sampling Options",
    "section": "Specifying initial parameters",
    "text": "Specifying initial parameters\nIn Turing.jl, initial parameters for MCMC sampling can be specified using the initial_params keyword argument.\n\n\n\n\n\n\nImportantNew initial_params in Turing v0.41\n\n\n\nIn Turing v0.41, the permitted values for initial_params are different. In particular, Vectors are no longer permitted, because they are semantically ambiguous (the way in which indices correspond to parameters relies on DynamicPPL internals). This page describes the new behaviour.\n\n\nFor single-chain sampling with Turing, the initial_params keyword argument should be a DynamicPPL.AbstractInitStrategy. There are several options; all the InitFrom... structs are re-exported by Turing.\n\nInitFromPrior(): generate initial parameters by sampling from the prior\nInitFromUniform(lower, upper): generate initial parameters by sampling uniformly from the given bounds in linked space\nInitFromParams(namedtuple_or_dict): use the provided initial parameters, supplied either as a NamedTuple or a Dict{&lt;:VarName}\nInitFromParams(mode_estimate): use the parameters in the optimisation result obtained via maximum_a_posteriori or maximum_likelihood\n\nIf initial_params is unspecified, each sampler will use its own default initialisation strategy: for most samplers this is InitFromPrior but for Hamiltonian samplers it is InitFromUniform(-2, 2) (which mimics the behaviour of Stan).\n\nchn = sample(demo_model(), MH(), 5; initial_params=InitFromParams((x = 1.0, y = -5.0)))\nchn[:x][1], chn[:y][1]\n\n(1.0, -5.0)\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that a number of samplers use warm-up steps by default (see the Thinning and Warmup section below), so chn[:param][1] may not correspond to the exact initial parameters you provided. MH() does not do this, which is why we use it here.\n\n\nThis approach scales for parameters with more complex types.\n\n@model function demo_complex()\n    x ~ LKJCholesky(3, 0.5)\n    y ~ MvNormal(zeros(3), I)\nend\ninit_x, init_y = rand(LKJCholesky(3, 0.5)), rand(MvNormal(zeros(3), I))\nchn = sample(demo_complex(), MH(), 5; initial_params=InitFromParams((x=init_x, y=init_y)));\n\nFor multiple-chain sampling, the initial_params keyword argument should be a vector with length equal to the number of chains being sampled. Each element of this vector should be the initial parameters for the corresponding chain, as described above. Thus, for example, you can supply a vector of AbstractInitStrategy objects. If you want to use the same initial parameters for all chains, you can use fill:\n\ninitial_params = fill(InitFromParams((x=1.0, y=-5.0)), 3)\nchn = sample(demo_model(), MH(), MCMCThreads(), 5, 3; initial_params=initial_params)\nchn[:x][1,:], chn[:y][1,:]\n\n\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/mcqES/src/sample.jl:432\n\n\n\n\n([1.0, 1.0, 1.0], [-5.0, -5.0, -5.0])\n\n\nIn Turing v0.41, initialisation with a raw NamedTuple is still supported (it will simply be wrapped in InitFromParams()); but we expect to remove this eventually, so it will likely be more future-proof to wrap this in InitFromParams() yourself.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "MCMC Sampling Options"
    ]
  },
  {
    "objectID": "usage/sampling-options/index.html#saving-and-resuming-sampling",
    "href": "usage/sampling-options/index.html#saving-and-resuming-sampling",
    "title": "MCMC Sampling Options",
    "section": "Saving and resuming sampling",
    "text": "Saving and resuming sampling\nBy default, MCMC sampling starts from scratch, using the initial parameters provided. You can, however, resume sampling from a previous chain. This is useful to, for example, perform sampling in batches, or to inspect intermediate results.\nFirstly, the previous chain must have been run using the save_state=true argument.\n\nrng = Xoshiro(468)\n\nchn1 = sample(rng, demo_model(), MH(), 5; save_state=true);\n\nFor MCMCChains.Chains, this results in the final sampler state being stored inside the chain metadata. You can access it using Turing.loadstate:\n\nsaved_state = Turing.loadstate(chn1)\ntypeof(saved_state)\n\nTuring.Inference.MHState{DynamicPPL.VarInfo{@NamedTuple{x::DynamicPPL.Metadata{Dict{AbstractPPL.VarName{:x, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{AbstractPPL.VarName{:x, typeof(identity)}}, Vector{Float64}}, y::DynamicPPL.Metadata{Dict{AbstractPPL.VarName{:y, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{AbstractPPL.VarName{:y, typeof(identity)}}, Vector{Float64}}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::DynamicPPL.LogPriorAccumulator{Float64}, LogJacobian::DynamicPPL.LogJacobianAccumulator{Float64}, LogLikelihood::DynamicPPL.LogLikelihoodAccumulator{Float64}}}}, Float64}\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can also directly access the saved sampler state with chn1.info.samplerstate, but we recommend not using this as it relies on the internal structure of MCMCChains.Chains.\n\n\nSampling can then be resumed from this state by providing it as the initial_state keyword argument.\n\nchn2 = sample(demo_model(), MH(), 5; initial_state=saved_state)\n\nChains MCMC chain (5×5×1 Array{Float64, 3}):\n\nIterations        = 1:1:5\nNumber of chains  = 1\nSamples per chain = 5\nWall duration     = 0.08 seconds\nCompute duration  = 0.08 seconds\nparameters        = x, y\ninternals         = logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nNote that the exact format saved in chn.info.samplerstate, and that expected by initial_state, depends on the invocation of sample used. For single-chain sampling, the saved state, and the required initial state, is just a single sampler state. For multiple-chain sampling, it is a vector of states, one per chain.\n\n\n\n\n\n\nWarningresume_from\n\n\n\nThe resume_from argument has been removed in Turing v0.41; please use initial_state=loadstate(chn) instead, as described here. In v0.41, loadstate is also exported from Turing rather than DynamicPPL.\n\n\nThis means that, for example, after sampling a single chain, you could sample three chains that branch off from that final state:\n\ninitial_states = fill(saved_state, 3)\nchn3 = sample(demo_model(), MH(), MCMCThreads(), 5, 3; initial_state=initial_states)\n\n\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/mcqES/src/sample.jl:432\n\n\n\n\nChains MCMC chain (5×5×3 Array{Float64, 3}):\n\nIterations        = 1:1:5\nNumber of chains  = 3\nSamples per chain = 5\nWall duration     = 0.04 seconds\nCompute duration  = 0.02 seconds\nparameters        = x, y\ninternals         = logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n\n\n\n\n\n\nNoteInitial states versus initial parameters\n\n\n\nThe initial_state and initial_params keyword arguments are mutually exclusive. If both are provided, initial_params will be silently ignored.\n\nchn2 = sample(rng, demo_model(), MH(), 5;\n    initial_state=saved_state, initial_params=InitFromParams((x=0.0, y=0.0))\n)\nchn2[:x][1], chn2[:y][1]\n\n(0.6039098670263787, 3.189622703262277)\n\n\nIn general, the saved state will contain a set of parameters (which will be the last parameters in the previous chain). However, the saved state not only specifies parameters but also other internal variables required by the sampler. For example, the MH state contains a cached log-density of the current parameters, which is later used for calculating the acceptance ratio.\nFinally, note that the first sample in the resumed chain will not be the same as the last sample in the previous chain; it will be the sample immediately after that.\n\n# In general these will not be the same (although it _could_ be if the MH step\n# was rejected -- that is why we seed the sampling in this section).\nchn1[:x][end], chn2[:x][1]\n\n(-0.3719817960998872, 0.6039098670263787)",
    "crumbs": [
      "Get Started",
      "User Guide",
      "MCMC Sampling Options"
    ]
  },
  {
    "objectID": "usage/sampling-options/index.html#thinning-and-warmup",
    "href": "usage/sampling-options/index.html#thinning-and-warmup",
    "title": "MCMC Sampling Options",
    "section": "Thinning and warmup",
    "text": "Thinning and warmup\nThe num_warmup and discard_initial keyword arguments can be used to control MCMC warmup. Both of these are integers, and respectively specify the number of warmup steps to perform, and the number of iterations at the start of the chain to discard. Note that the value of discard_initial should also include the num_warmup steps if you want the warmup steps to be discarded.\nHere are some examples of how these two keyword arguments interact:\n\n\n\n\n\n\n\n\nnum_warmup=\ndiscard_initial=\nDescription\n\n\n\n\n10\n10\nPerform 10 warmup steps, discard them; the chain starts from the first non-warmup step\n\n\n10\n15\nPerform 10 warmup steps, discard them and the next 5 steps; the chain starts from the 6th non-warmup step\n\n\n10\n5\nPerform 10 warmup steps, discard the first 5; the chain will contain 5 warmup steps followed by the rest of the chain\n\n\n0\n10\nNo warmup steps, discard the first 10 steps; the chain starts from the 11th step\n\n\n0\n0\nNo warmup steps, do not discard any steps; the chain starts from the 1st step (corresponding to the initial parameters)\n\n\n\nEach sampler has its own default value for num_warmup, but discard_initial always defaults to num_warmup.\nWarmup steps and ‘regular’ non-warmup steps differ in that warmup steps call AbstractMCMC.step_warmup, whereas regular steps call AbstractMCMC.step. For all the samplers defined in Turing, these two functions are identical; however, they may in general differ for other samplers. Please consult the documentation of the respective sampler for details.\nA thinning factor can be specified using the thinning keyword argument. For example, thinning=10 will keep every tenth sample, discarding the other nine.\nNote that thinning is not applied to the first discard_initial samples; it is only applied to the remaining samples. Thus, for example, if you use discard_initial=50 and thinning=10, the chain will contain samples 51, 61, 71, and so on.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "MCMC Sampling Options"
    ]
  },
  {
    "objectID": "usage/sampling-options/index.html#performing-model-checks",
    "href": "usage/sampling-options/index.html#performing-model-checks",
    "title": "MCMC Sampling Options",
    "section": "Performing model checks",
    "text": "Performing model checks\nDynamicPPL by default performs a number of checks on the model before any sampling is done. This catches a number of potential errors in a model, such as having repeated variables (see the DynamicPPL documentation for details).\nIf you wish to disable this you can pass check_model=false to sample().",
    "crumbs": [
      "Get Started",
      "User Guide",
      "MCMC Sampling Options"
    ]
  },
  {
    "objectID": "usage/sampling-options/index.html#callbacks",
    "href": "usage/sampling-options/index.html#callbacks",
    "title": "MCMC Sampling Options",
    "section": "Callbacks",
    "text": "Callbacks\nThe callback keyword argument can be used to specify a function that is called at the end of each sampler iteration. This function should have the signature callback(rng, model, sampler, sample, iteration::Int; kwargs...).\nIf you are performing multi-chain sampling, kwargs will additionally contain chain_number::Int, which ranges from 1 to the number of chains.\nThe TuringCallbacks.jl package contains a TensorBoardCallback, which can be used to obtain live progress visualisations using TensorBoard.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "MCMC Sampling Options"
    ]
  },
  {
    "objectID": "usage/sampling-options/index.html#automatic-differentiation",
    "href": "usage/sampling-options/index.html#automatic-differentiation",
    "title": "MCMC Sampling Options",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\nFinally, please note that for samplers which use automatic differentiation (e.g., HMC and NUTS), the AD type should be specified in the sampler constructor itself, rather than as a keyword argument to sample().\nIn other words, this is correct:\n\nspl = NUTS(; adtype=AutoForwardDiff())\nchn = sample(demo_model(), spl, 10);\n\n\n┌ Info: Found initial step size\n└   ϵ = 3.2\n\n\n\n\nand not this:\nspl = NUTS()\nchn = sample(demo_model(), spl, 10; adtype=AutoForwardDiff())",
    "crumbs": [
      "Get Started",
      "User Guide",
      "MCMC Sampling Options"
    ]
  },
  {
    "objectID": "usage/tracking-extra-quantities/index.html",
    "href": "usage/tracking-extra-quantities/index.html",
    "title": "Tracking Extra Quantities",
    "section": "",
    "text": "Often, there are quantities in models that we might be interested in viewing the values of, but which are not random variables in the model that are explicitly drawn from a distribution.\nAs a motivating example, the most natural parameterisation for a model might not be the most computationally feasible. Consider the following (efficiently reparametrized) implementation of Neal’s funnel (Neal, 2003):\nusing Turing\nsetprogress!(false)\n\n@model function Neal()\n    # Raw draws\n    y_raw ~ Normal(0, 1)\n    x_raw ~ arraydist([Normal(0, 1) for i in 1:9])\n\n    # Transform:\n    y = 3 * y_raw\n    x = exp.(y ./ 2) .* x_raw\n    return nothing\nend\n\n\n[ Info: [Turing]: progress logging is disabled globally\n\n\n\n\nNeal (generic function with 2 methods)\nIn this case, the random variables exposed in the chain (x_raw, y_raw) are not in a helpful form — what we’re after are the deterministically transformed variables x and y.\nThere are two ways to track these extra quantities in Turing.jl.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Tracking Extra Quantities"
    ]
  },
  {
    "objectID": "usage/tracking-extra-quantities/index.html#using-during-inference",
    "href": "usage/tracking-extra-quantities/index.html#using-during-inference",
    "title": "Tracking Extra Quantities",
    "section": "Using := (during inference)",
    "text": "Using := (during inference)\nThe first way is to use the := operator, which behaves exactly like = except that the values of the variables on its left-hand side are automatically added to the chain returned by the sampler. For example:\n\n@model function Neal_coloneq()\n    # Raw draws\n    y_raw ~ Normal(0, 1)\n    x_raw ~ arraydist([Normal(0, 1) for i in 1:9])\n\n    # Transform:\n    y := 3 * y_raw\n    x := exp.(y ./ 2) .* x_raw\nend\n\nsample(Neal_coloneq(), NUTS(), 1000)\n\n\n┌ Info: Found initial step size\n└   ϵ = 3.2\n\n\n\n\nChains MCMC chain (1000×34×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 6.79 seconds\nCompute duration  = 6.79 seconds\nparameters        = y_raw, x_raw[1], x_raw[2], x_raw[3], x_raw[4], x_raw[5], x_raw[6], x_raw[7], x_raw[8], x_raw[9], y, x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9]\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Tracking Extra Quantities"
    ]
  },
  {
    "objectID": "usage/tracking-extra-quantities/index.html#using-returned-post-inference",
    "href": "usage/tracking-extra-quantities/index.html#using-returned-post-inference",
    "title": "Tracking Extra Quantities",
    "section": "Using returned (post-inference)",
    "text": "Using returned (post-inference)\nAlternatively, one can specify the extra quantities as part of the model function’s return statement:\n\n@model function Neal_return()\n    # Raw draws\n    y_raw ~ Normal(0, 1)\n    x_raw ~ arraydist([Normal(0, 1) for i in 1:9])\n\n    # Transform and return as a NamedTuple\n    y = 3 * y_raw\n    x = exp.(y ./ 2) .* x_raw\n    return (x=x, y=y)\nend\n\nchain = sample(Neal_return(), NUTS(), 1000)\n\n\n┌ Info: Found initial step size\n└   ϵ = 3.2\n\n\n\n\nChains MCMC chain (1000×24×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 1.85 seconds\nCompute duration  = 1.85 seconds\nparameters        = y_raw, x_raw[1], x_raw[2], x_raw[3], x_raw[4], x_raw[5], x_raw[6], x_raw[7], x_raw[8], x_raw[9]\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nThe sampled chain does not contain x and y, but we can extract the values using the returned function. Calling this function outputs an array:\n\nnts = returned(Neal_return(), chain)\n\n1000×1 Matrix{@NamedTuple{x::Vector{Float64}, y::Float64}}:\n (x = [-18.393658972855906, -32.55935149378618, -65.34630964851002, -25.11410021321311, -100.40771931374127, 65.59506807338249, 3.534168586445667, 11.012669015224883, -27.46606162378522], y = 9.392944976734118)\n (x = [-0.01259394561164228, 0.0644116644115007, -0.0375121383786858, -0.10504186846314433, 0.1781744315872059, -0.04590039117258095, 0.0778930253495501, -0.11825042436682794, 0.141226681514477], y = -4.828104588464359)\n (x = [6.160872447422976, 2.5698522638137415, -0.4612099302901282, 0.7437507681052078, -3.4652259495649447, 3.117932050439933, -1.529527753545659, 1.4660017813748076, -4.0330999665166924], y = 2.446297329036774)\n (x = [2.313784329501971, 1.6861134894910854, -1.484485970895302, 0.4577013064457359, -0.053819601435108416, -1.1288930304062248, -1.6030884516474615, 0.9169906508355498, 0.6884272973961775], y = 0.8511576783908613)\n (x = [-0.5520697014698183, 0.7254620229707112, 0.23538723892800517, 0.17939759954464632, -0.47661131622778957, -0.1969871747875566, 0.3118306331739763, -0.5834730520162916, -0.36329914809717917], y = -0.603944129601315)\n (x = [-0.035067190700923734, 0.007753620033167731, 0.10034797266743725, -0.12377498395946662, -0.1903880520945286, -0.20522223485398233, -0.03924468922072636, -0.1443697216070945, 0.18092800151572255], y = -3.0008768170281543)\n (x = [-0.16727735135910912, 0.09105115469509001, 0.32967570519333966, -0.03853122406639907, 0.07307309885050832, -0.05037281226753975, -0.09973498641794698, -0.45061625743417355, 0.5814942394662992], y = -2.259067183149507)\n (x = [0.49542408664204374, 0.577602434993973, -0.9624095222454013, 1.8716073245232718, 0.08334021805651776, -0.7790608129590629, 0.5692618915927627, -0.5300648918228407, -0.09025720726779732], y = 0.4015026206842244)\n (x = [0.49542408664204374, 0.577602434993973, -0.9624095222454013, 1.8716073245232718, 0.08334021805651776, -0.7790608129590629, 0.5692618915927627, -0.5300648918228407, -0.09025720726779732], y = 0.4015026206842244)\n (x = [-14.489261404054107, 4.267384475624491, -3.5860225097027363, 4.02000469552619, -1.7545568409322274, 8.088104556435242, 1.5903335001456702, 0.7663268700976941, 6.200669381660623], y = 4.034039213697479)\n ⋮\n (x = [-1.2632994980648158, -0.34305054019586073, -0.42336277928914146, -0.013312669104229373, -0.028807649078594316, 0.9742914786754507, -0.8714120804488928, 0.5080185854608177, 0.11322708344313695], y = -0.9798525173516439)\n (x = [-3.953585147066779, 1.3314978802706627, 0.8646416548936735, -2.8327657826205783, 1.1804212664021116, 8.482955679567016, -1.8634048108754053, 10.810734151236632, 0.1031194391534212], y = 2.736296287934146)\n (x = [-2.108340051642967, 0.7055047486414738, -0.043456321066011945, -0.7008747013941252, -0.2291307836511678, 1.2866209863124747, -1.1186386653394216, 1.476979766516437, 1.4814381892658544], y = 0.0601345896887886)\n (x = [0.8100105933126217, 2.1405756485062164, -0.5945407680800021, -2.4064408987253296, -1.1909678442056544, 1.0745769307986321, 4.054820682666243, -5.187213873492703, -1.4341801262177478], y = 1.6573018529586991)\n (x = [-6.0353873524945945, 4.0655772934672125, -0.19638789068289805, -1.696774787414985, -0.832467097682913, 4.368641734792078, 3.5835579495842835, -1.256361542990508, -2.4500333129939547], y = 2.4691188087451197)\n (x = [-1.3160461926436637, 1.0840675533719428, -0.0852984292743251, 0.28234131053150396, -1.0221516318312258, 1.1037543531420277, 0.668350132512617, -1.5633492488105374, -1.6545293209401148], y = -0.047247683101878435)\n (x = [-0.03602928395556653, 0.7641586553472656, 0.10042224159145902, 0.4431105712510701, -0.7722574922238724, 0.814907101247099, -0.021328555373433065, -0.194020101837645, -0.2960199323424381], y = -1.3735203021753146)\n (x = [0.29274490692072447, 0.03145585585133981, 0.24491354603627782, -0.181508599242417, -0.20660266582406295, -0.13320969611643046, -0.03598474758259198, -0.18837047650351743, 0.10384648857401892], y = -3.293951503932695)\n (x = [0.5613616512316411, 0.3120642193900853, -1.8202866277018344, -1.3081470926357874, 0.026912989469064126, 2.1859842247549546, -0.05666248716795559, 0.10731298488896679, -0.06506631545283678], y = 1.113250417086351)\n\n\nwhere each element of which is a NamedTuple, as specified in the return statement of the model.\n\nnts[1]\n\n(x = [-18.393658972855906, -32.55935149378618, -65.34630964851002, -25.11410021321311, -100.40771931374127, 65.59506807338249, 3.534168586445667, 11.012669015224883, -27.46606162378522], y = 9.392944976734118)",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Tracking Extra Quantities"
    ]
  },
  {
    "objectID": "usage/tracking-extra-quantities/index.html#which-to-use",
    "href": "usage/tracking-extra-quantities/index.html#which-to-use",
    "title": "Tracking Extra Quantities",
    "section": "Which to use?",
    "text": "Which to use?\nThere are some pros and cons of using returned, as opposed to :=.\nFirstly, returned is more flexible, as it allows you to track any type of object; := only works with variables that can be inserted into an MCMCChains.Chains object. (Notice that x is a vector, and in the first case where we used :=, reconstructing the vector value of x can also be rather annoying as the chain stores each individual element of x separately.)\nA drawback is that naively using returned can lead to unnecessary computation during inference. This is because during the sampling process, the return values are also calculated (since they are part of the model function), but then thrown away. So, if the extra quantities are expensive to compute, this can be a problem.\nTo avoid this, you will essentially have to create two different models, one for inference and one for post-inference. The simplest way of doing this is to add a parameter to the model argument:\n\n@model function Neal_coloneq_optional(track::Bool)\n    # Raw draws\n    y_raw ~ Normal(0, 1)\n    x_raw ~ arraydist([Normal(0, 1) for i in 1:9])\n\n    if track\n        y = 3 * y_raw\n        x = exp.(y ./ 2) .* x_raw\n        return (x=x, y=y)\n    else\n        return nothing\n    end\nend\n\nchain = sample(Neal_coloneq_optional(false), NUTS(), 1000)\n\n\n┌ Info: Found initial step size\n└   ϵ = 1.6\n\n\n\n\nChains MCMC chain (1000×24×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 1.38 seconds\nCompute duration  = 1.38 seconds\nparameters        = y_raw, x_raw[1], x_raw[2], x_raw[3], x_raw[4], x_raw[5], x_raw[6], x_raw[7], x_raw[8], x_raw[9]\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nThe above ensures that x and y are not calculated during inference, but allows us to still use returned to extract them:\n\nreturned(Neal_coloneq_optional(true), chain)\n\n1000×1 Matrix{@NamedTuple{x::Vector{Float64}, y::Float64}}:\n (x = [0.15050669787199128, -0.20327824151135732, 0.018586565540666823, 0.05991359436992072, -0.1439934087963694, 0.1149363876021102, 0.10035621391866494, 0.06589212965621608, 0.1420401180003185], y = -4.041666558067558)\n (x = [4.211764243046747, 25.651935190273147, 4.292361430019657, -7.122526449604314, 16.89090863521272, -17.098567773759818, -23.751313904652697, -17.240960907508317, -31.205921721232375], y = 6.484306684218474)\n (x = [-0.0009696462089866348, -0.09887098846485268, -0.007604581359063145, 0.05663070329387826, 0.041533875523958985, 0.039231301330799245, -0.006937854236959694, 0.09604949266676548, 0.08920843987204459], y = -5.163539313092865)\n (x = [2.073256312173688, 1.779029985810005, -3.21952146127363, -14.039139539660937, 19.38821106704191, -27.763088536509404, 6.534290018921077, 2.432931781333112, 18.12164090135975], y = 4.827369438404224)\n (x = [-0.1160157082698639, 0.3822126361529404, 0.07717142246392915, -0.30998108323454265, -0.08283342018711967, 0.20812522268972503, 0.050164856312260336, 0.0024816200264621098, -0.15240639167886405], y = -2.688145551122947)\n (x = [-0.1160157082698639, 0.3822126361529404, 0.07717142246392915, -0.30998108323454265, -0.08283342018711967, 0.20812522268972503, 0.050164856312260336, 0.0024816200264621098, -0.15240639167886405], y = -2.688145551122947)\n (x = [-23.85495824762191, -21.393379121132604, 44.980403470050476, -7.615927944353342, -41.84841466999865, -41.51903538156703, 22.28270220455672, -45.9657947447297, 41.832288344331886], y = 8.035191142857329)\n (x = [-1.9500260941485061, 12.255030400286595, 11.212903160263199, 17.907530828570053, 7.223731335414224, -1.7881476141724317, 2.621273427274924, -5.2338511941288015, 11.10287715713849], y = 4.637554327414434)\n (x = [2.5788806600705994, 2.1514478170466327, 0.5484741925848523, 4.7642684287404835, 14.508350936871539, -2.8438247802878935, 6.090944586890596, 3.4947011350899873, -0.08249742600065989], y = 3.418003870177823)\n (x = [2.137655830551761, -13.731091880537111, -4.642447070026398, 28.594519778239565, 12.902733023348418, 7.248336502237038, -8.104463881792334, -0.17068453549180712, -2.4837518646395167], y = 4.568285305810903)\n ⋮\n (x = [0.8544305850304346, 2.9232104358988633, 1.4275745723944604, -1.4948649274124006, -3.42471123249116, 3.448438379747283, -2.603191835596334, 1.1819962355004587, -4.592878572902551], y = 1.9988760055596757)\n (x = [-0.18566529618799174, 1.8518265753584235, 0.31534186858775937, -0.7204015026257578, -2.562595085967615, 0.30583047956702886, 1.7306894462304163, -1.1659926947452808, 0.8170138276057584], y = -0.3252951295079525)\n (x = [9.272682075567245, 18.14027812751363, 4.9352539151973485, -5.421539624197198, 14.17971773293234, 9.778398762141222, -2.195124907396404, 14.109348940961842, -24.480149963566582], y = 5.479949837865291)\n (x = [-0.0007587706545891232, -0.07538235635829983, -0.00520026435587151, -0.010178253329794931, -0.053466048231234464, -0.027008154871080575, 0.0066972337288885965, -0.03194079484091301, 0.1590964391442477], y = -6.023925089880139)\n (x = [-0.28455861097773655, 14.18389453813947, 5.531183592885482, 11.697727782061708, 14.907336785578988, 16.084891511729225, -6.442661683852632, 17.221901983602425, -34.59059471194872], y = 6.29235401532417)\n (x = [0.4414320319280918, 0.6826171710456937, 1.451445914738376, -1.514187347917114, -1.8235247219379307, 1.3048732163921186, -0.5103277933703018, -0.1922820445413097, -0.9075443203075221], y = 0.3880897263767211)\n (x = [0.4021878865998852, 0.2322899713659788, 1.3582668282093613, 0.9380228882154782, 0.26945361290959746, 1.2578372900245194, -0.19925698680795986, -1.3499073104961123, 1.6309307234312533], y = 0.09970374515468401)\n (x = [-0.5043373221773658, -0.12068442907968598, -0.26780109565000965, -0.6671953399036292, 0.26642980911694875, 0.02575872921373505, -0.2729676363511692, -0.23738492341645495, 0.563820310975243], y = -1.486313127210567)\n (x = [5.322623961366033, -0.8801237120765388, -0.13278890984521144, 5.039282616906787, -2.2806198169929583, -1.1274540949483556, 1.1407880719026344, 3.955774033024096, -2.8241180710437024], y = 3.0628654853188766)\n\n\nAnother equivalent option is to use a submodel:\n\n@model function Neal()\n    y_raw ~ Normal(0, 1)\n    x_raw ~ arraydist([Normal(0, 1) for i in 1:9])\n    return (x_raw=x_raw, y_raw=y_raw)\nend\n\nchain = sample(Neal(), NUTS(), 1000)\n\n@model function Neal_with_extras()\n    neal ~ to_submodel(Neal(), false)\n    y = 3 * neal.y_raw\n    x = exp.(y ./ 2) .* neal.x_raw\n    return (x=x, y=y)\nend\n\nreturned(Neal_with_extras(), chain)\n\n\n┌ Info: Found initial step size\n└   ϵ = 1.6\n\n\n\n\n1000×1 Matrix{@NamedTuple{x::Vector{Float64}, y::Float64}}:\n (x = [0.10228663066499051, 0.12959499876499098, 0.6535073108060167, -0.1972079213347176, 0.04071284595949387, -0.7779699515076759, -0.2149662573660579, -0.03482295188967706, -0.1687773593887987], y = -1.5607112727847783)\n (x = [0.4018687879466435, 0.1185188557735874, -0.21716743250659304, -0.6580123918472941, -0.4109217787187522, -0.20823957870191875, -0.32956529654165234, -0.1153065593072803, 0.014154653220859494], y = -2.119638161423062)\n (x = [-0.39104274793534693, -0.327132822610663, 0.5985475684815226, 0.2204566184187692, 0.39130362318504014, 0.10159322237240431, -0.12133526642718005, 0.12368200396261866, 0.17396175177158146], y = -2.6119092728678854)\n (x = [0.051286574989444215, -0.08812329512286667, 0.02791448750154221, 0.04287603285131259, 0.0847229691477016, 0.09126298757926544, -0.056955920250293876, 0.013535871579442632, -0.031539178198142226], y = -5.288169624887465)\n (x = [-17.67795971964067, 22.60357744590523, -5.192844843687954, -8.775083287972693, -15.983156488775482, -16.768327139678888, 7.405512286490005, -7.069077634867597, -16.80937168010104], y = 5.244380132894694)\n (x = [0.011533017647201408, -0.009419019100524883, -0.006038782475875408, -0.009787755477224457, 0.0024734815614181637, 0.004793736780523902, -0.010186765793375978, 0.017473834392571632, 0.013276594769068256], y = -9.157684413758535)\n (x = [0.7182186387527972, 0.24520450753594467, -0.7059510484036445, -0.6279585364801024, 0.2912177216926441, 0.23848428774648744, 0.8974016282732769, 1.0672309632280434, -2.459196764969235], y = -0.3045901104872337)\n (x = [0.991424394916192, 0.3397968201081956, -0.6647555583468732, 0.33209411319455295, -0.16757561973255553, -0.1274353298601433, 0.36741085225703957, 0.640142572962359, -0.5542622938927406], y = -1.1945711081470234)\n (x = [-2.1890314511086486, -0.4505050913792349, 1.8605680597257181, -1.884801916549381, 3.7205738959555816, -0.4716830067319757, -0.8329120860756457, -2.635128636316592, 1.11127943162657], y = 1.0469687431669674)\n (x = [-0.6105100564303501, -0.4280612270130499, 1.9509927039652113, 0.13335113605583973, 0.6870952342366393, 0.27335301575380305, 0.3803009441874965, 0.20675334029206877, -0.5494322003581099], y = -0.7676932648403989)\n ⋮\n (x = [-0.10268513916277872, 3.19047105510486, -2.310117748184516, -1.572630132319082, -2.2708333175402573, 1.3251681439242402, -1.3133523438441115, 5.630526752261887, 4.459321778673798], y = 2.711171657118378)\n (x = [0.0693707004827799, 0.008178011146562624, 0.1617911036801347, -0.10192313941411095, -0.21313043555941383, 0.22107396857730163, 0.04043020634216654, -0.4827479457053514, -0.3166510286444241], y = -2.5454180626543925)\n (x = [1.56024542080265, -3.131856980112656, 4.244574804150247, -1.9240960668311178, -1.1699325365762805, 3.5744200309087764, -4.6411464051244815, -5.9807487716786065, 3.06233698264943], y = 2.08131065847382)\n (x = [-3.305249461034367, -1.2631730169015625, 1.602805946405829, 0.5701579498681033, 1.8644302518342033, 2.9303446307230683, 2.1104833630185267, 4.4644455903435505, -0.70900749388273], y = 1.136641469259319)\n (x = [3.471078934181806, -4.840101519761799, 9.73042682282952, 5.945023118407093, -2.432760474802048, 6.255838808954959, -11.680983710182682, 12.026271126482486, 8.452121085389699], y = 3.69959818673631)\n (x = [0.0013029560246142838, 0.025327130524027658, 0.006351181429290733, 0.019044197075516345, -0.03423203316949315, 0.02841596065091495, -0.022779820511191894, 0.024170320361762837, 0.07704476806224686], y = -6.090918921383419)\n (x = [0.02053996672677091, 0.08893290378154124, 0.023108634064590225, 0.04827981297886748, -0.07127559708948542, 0.044610231803209666, -0.056372598937420276, 0.031593107067870246, -0.012537887220689985], y = -5.539402769814741)\n (x = [-0.19489705282045658, 0.38240835455724187, 0.3791635855061397, 0.27558971726452447, 0.5298868037152697, -0.6714653859882957, -0.16757758894692204, 0.013490896307779646, 0.4485860350673346], y = -0.8814035251610866)\n (x = [6.241581315775735, 0.7909142476350833, -0.6140191878491875, -2.336258857401951, 11.248149042406498, -9.034421576175877, -3.440514023452552, -3.3534633962490705, -3.2947704020391235], y = 3.0253351828064674)\n\n\nNote that for the returned call to work, the Neal_with_extras() model must have the same variable names as stored in chain. This means the submodel Neal() must not be prefixed, i.e. to_submodel() must be passed a second parameter false.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Tracking Extra Quantities"
    ]
  },
  {
    "objectID": "usage/external-samplers/index.html",
    "href": "usage/external-samplers/index.html",
    "title": "Using External Samplers",
    "section": "",
    "text": "Turing provides several wrapped samplers from external sampling libraries, e.g., HMC samplers from AdvancedHMC. These wrappers allow new users to seamlessly sample statistical models without leaving Turing However, these wrappers might not always be complete, missing some functionality from the wrapped sampling library. Moreover, users might want to use samplers currently not wrapped within Turing.\nFor these reasons, Turing also makes running external samplers on Turing models easy without any necessary modifications or wrapping! Throughout, we will use a 10-dimensional Neal’s funnel as a running example::\n\n# Import libraries.\nusing Turing, Random, LinearAlgebra\n\nd = 10\n@model function funnel()\n    θ ~ Truncated(Normal(0, 3), -3, 3)\n    z ~ MvNormal(zeros(d - 1), exp(θ) * I)\n    return x ~ MvNormal(z, I)\nend\n\nfunnel (generic function with 2 methods)\n\n\nNow we sample the model to generate some observations, which we can then condition on.\n\n(; x) = rand(funnel() | (θ=0,))\nmodel = funnel() | (; x);\n\nUsers can use any sampler algorithm to sample this model if it follows the AbstractMCMC API. Before discussing how this is done in practice, giving a high-level description of the process is interesting. Imagine that we created an instance of an external sampler that we will call spl such that typeof(spl)&lt;:AbstractMCMC.AbstractSampler. In order to avoid type ambiguity within Turing, at the moment, it is necessary to declare spl as an external sampler to Turing espl = externalsampler(spl), where externalsampler(s::AbstractMCMC.AbstractSampler) is a Turing function that types our external sampler adequately.\nAn excellent point to start to show how this is done in practice is by looking at the sampling library AdvancedMH (AdvancedMH’s GitHub) for Metropolis-Hastings (MH) methods. Let’s say we want to use a random walk Metropolis-Hastings sampler without specifying the proposal distributions. The code below constructs an MH sampler using a multivariate Gaussian distribution with zero mean and unit variance in d dimensions as a random walk proposal.\n\n# Importing the sampling library\nusing AdvancedMH\nrwmh = AdvancedMH.RWMH(d)\n\nMetropolisHastings{RandomWalkProposal{false, ZeroMeanIsoNormal{Tuple{Base.OneTo{Int64}}}}}(RandomWalkProposal{false, ZeroMeanIsoNormal{Tuple{Base.OneTo{Int64}}}}(ZeroMeanIsoNormal(\ndim: 10\nμ: Zeros(10)\nΣ: [1.0 0.0 … 0.0 0.0; 0.0 1.0 … 0.0 0.0; … ; 0.0 0.0 … 1.0 0.0; 0.0 0.0 … 0.0 1.0]\n)\n))\n\n\n\nsetprogress!(false)\n\nSampling is then as easy as:\n\nchain = sample(model, externalsampler(rwmh), 10_000)\n\nChains MCMC chain (10000×14×1 Array{Float64, 3}):\n\nIterations        = 1:1:10000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 4.0 seconds\nCompute duration  = 4.0 seconds\nparameters        = θ, z[1], z[2], z[3], z[4], z[5], z[6], z[7], z[8], z[9]\ninternals         = accepted, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Using External Samplers"
    ]
  },
  {
    "objectID": "usage/external-samplers/index.html#using-external-samplers-on-turing-models",
    "href": "usage/external-samplers/index.html#using-external-samplers-on-turing-models",
    "title": "Using External Samplers",
    "section": "",
    "text": "Turing provides several wrapped samplers from external sampling libraries, e.g., HMC samplers from AdvancedHMC. These wrappers allow new users to seamlessly sample statistical models without leaving Turing However, these wrappers might not always be complete, missing some functionality from the wrapped sampling library. Moreover, users might want to use samplers currently not wrapped within Turing.\nFor these reasons, Turing also makes running external samplers on Turing models easy without any necessary modifications or wrapping! Throughout, we will use a 10-dimensional Neal’s funnel as a running example::\n\n# Import libraries.\nusing Turing, Random, LinearAlgebra\n\nd = 10\n@model function funnel()\n    θ ~ Truncated(Normal(0, 3), -3, 3)\n    z ~ MvNormal(zeros(d - 1), exp(θ) * I)\n    return x ~ MvNormal(z, I)\nend\n\nfunnel (generic function with 2 methods)\n\n\nNow we sample the model to generate some observations, which we can then condition on.\n\n(; x) = rand(funnel() | (θ=0,))\nmodel = funnel() | (; x);\n\nUsers can use any sampler algorithm to sample this model if it follows the AbstractMCMC API. Before discussing how this is done in practice, giving a high-level description of the process is interesting. Imagine that we created an instance of an external sampler that we will call spl such that typeof(spl)&lt;:AbstractMCMC.AbstractSampler. In order to avoid type ambiguity within Turing, at the moment, it is necessary to declare spl as an external sampler to Turing espl = externalsampler(spl), where externalsampler(s::AbstractMCMC.AbstractSampler) is a Turing function that types our external sampler adequately.\nAn excellent point to start to show how this is done in practice is by looking at the sampling library AdvancedMH (AdvancedMH’s GitHub) for Metropolis-Hastings (MH) methods. Let’s say we want to use a random walk Metropolis-Hastings sampler without specifying the proposal distributions. The code below constructs an MH sampler using a multivariate Gaussian distribution with zero mean and unit variance in d dimensions as a random walk proposal.\n\n# Importing the sampling library\nusing AdvancedMH\nrwmh = AdvancedMH.RWMH(d)\n\nMetropolisHastings{RandomWalkProposal{false, ZeroMeanIsoNormal{Tuple{Base.OneTo{Int64}}}}}(RandomWalkProposal{false, ZeroMeanIsoNormal{Tuple{Base.OneTo{Int64}}}}(ZeroMeanIsoNormal(\ndim: 10\nμ: Zeros(10)\nΣ: [1.0 0.0 … 0.0 0.0; 0.0 1.0 … 0.0 0.0; … ; 0.0 0.0 … 1.0 0.0; 0.0 0.0 … 0.0 1.0]\n)\n))\n\n\n\nsetprogress!(false)\n\nSampling is then as easy as:\n\nchain = sample(model, externalsampler(rwmh), 10_000)\n\nChains MCMC chain (10000×14×1 Array{Float64, 3}):\n\nIterations        = 1:1:10000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 4.0 seconds\nCompute duration  = 4.0 seconds\nparameters        = θ, z[1], z[2], z[3], z[4], z[5], z[6], z[7], z[8], z[9]\ninternals         = accepted, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Using External Samplers"
    ]
  },
  {
    "objectID": "usage/external-samplers/index.html#going-beyond-the-turing-api",
    "href": "usage/external-samplers/index.html#going-beyond-the-turing-api",
    "title": "Using External Samplers",
    "section": "Going beyond the Turing API",
    "text": "Going beyond the Turing API\nAs previously mentioned, the Turing wrappers can often limit the capabilities of the sampling libraries they wrap. AdvancedHMC1 (AdvancedHMC’s GitHub) is a clear example of this. A common practice when performing HMC is to provide an initial guess for the mass matrix. However, the native HMC sampler within Turing only allows the user to specify the type of the mass matrix despite the two options being possible within AdvancedHMC. Thankfully, we can use Turing’s support for external samplers to define an HMC sampler with a custom mass matrix in AdvancedHMC and then use it to sample our Turing model.\nWe can use the library Pathfinder2 (Pathfinder’s GitHub) to construct our estimate of mass matrix. Pathfinder is a variational inference algorithm that first finds the maximum a posteriori (MAP) estimate of a target posterior distribution and then uses the trace of the optimisation to construct a sequence of multivariate normal approximations to the target distribution. In this process, Pathfinder computes an estimate of the mass matrix the user can access. You can see an example of how to use Pathfinder with Turing in Pathfinder’s docs.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Using External Samplers"
    ]
  },
  {
    "objectID": "usage/external-samplers/index.html#using-new-inference-methods",
    "href": "usage/external-samplers/index.html#using-new-inference-methods",
    "title": "Using External Samplers",
    "section": "Using new inference methods",
    "text": "Using new inference methods\nSo far we have used Turing’s support for external samplers to go beyond the capabilities of the wrappers. This is made possible by an interface for external samplers, which is described in the Turing.jl documentation here: if you are implementing your own sampler and would like it to work with Turing.jl models, that link describes the methods that you need to overload.\nFor an example of an ‘external sampler’ that works in this way with Turing, we recommend the SliceSampling.jl library. Note that although this library is hosted under the TuringLang GitHub organisation, it is not a Turing.jl dependency, and thus from Turing’s perspective it is truly an ‘external’ sampler.\nIn this section, we will briefly go through the interface requirements for external samplers. First and foremost, the sampler MySampler should be a subtype of AbstractMCMC.AbstractSampler. Second, the stepping function of the MCMC algorithm must be defined as new methods of AbstractMCMC.step following the structure below:\n# First step\nfunction AbstractMCMC.step(\n    rng::Random.AbstractRNG,\n    model::AbstractMCMC.LogDensityModel,\n    spl::MySampler;\n    kwargs...,\n)\n    [...]\n    return transition, state\nend\n\n# N+1 step\nfunction AbstractMCMC.step(\n    rng::Random.AbstractRNG,\n    model::AbstractMCMC.LogDensityModel,\n    sampler::MySampler,\n    state;\n    kwargs...,\n) \n    [...]\n    return transition, state\nend\nNote that the model argument here must be an AbstractMCMC.LogDensityModel. This is a thin wrapper around an object which satisfies the LogDensityProblems.jl interface. Thus, in your external sampler, you can access the inner object with model.logdensity and call LogDensityProblems.logdensity(model.logdensity, params) to calculate the (unnormalised) log density of the model at params.\nAs shown above, there must be two step methods:\n\nA method that performs the first step, performing any initialisation it needs to; and\nA method that performs the following steps and takes an extra input, state, which carries the initialisation information.\n\nThe output of both of these methods must be a tuple containing: - a ‘transition’, which is essentially the ‘visible output’ of the sampler: this object is later used to construct an MCMCChains.Chains; - a ‘state’, representing the current state of the sampler, which is passed to the next step of the MCMC algorithm.\nApart from this, your sampler state should also implement Turing.Inference.getparams(model, transition) to return the parameters of the model as a vector. Here, transition represents the first output of the step function.\nfunction Turing.Inference.getparams(model::DynamicPPL.Model, state::MyTransition)\n    # Return a vector containing the parameters of the model.\nend\nThese functions are the bare minimum that your external sampler must implement to work with Turing models. There are other methods which can be overloaded to improve the performance or other features of the sampler; please refer to the documentation linked above for more details.\nIn general, we recommend that the AbstractMCMC interface is implemented directly in your library. However, any DynamicPPL- or Turing-specific functionality is best implemented in a MySamplerTuringExt extension.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Using External Samplers"
    ]
  },
  {
    "objectID": "usage/external-samplers/index.html#footnotes",
    "href": "usage/external-samplers/index.html#footnotes",
    "title": "Using External Samplers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nXu et al., AdvancedHMC.jl: A robust, modular and efficient implementation of advanced HMC algorithms, 2019↩︎\nZhang et al., Pathfinder: Parallel quasi-Newton variational inference, 2021↩︎",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Using External Samplers"
    ]
  },
  {
    "objectID": "usage/sampler-visualisation/index.html",
    "href": "usage/sampler-visualisation/index.html",
    "title": "Sampler Visualization",
    "section": "",
    "text": "For each sampler, we will use the same code to plot sampler paths. The block below loads the relevant libraries and defines a function for plotting the sampler’s trajectory across the posterior.\nThe Turing model definition used here is not especially practical, but it is designed in such a way as to produce visually interesting posterior surfaces to show how different samplers move along the distribution.\n\nENV[\"GKS_ENCODING\"] = \"utf-8\" # Allows the use of unicode characters in Plots.jl\nusing Plots\nusing StatsPlots\nusing Turing\nusing Random\nusing Bijectors\n\n# Set a seed.\nRandom.seed!(0)\n\n# Define a strange model.\n@model function gdemo(x)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    bumps = sin(m) + cos(m)\n    m = m + 5 * bumps\n    for i in eachindex(x)\n        x[i] ~ Normal(m, sqrt(s²))\n    end\n    return s², m\nend\n\n# Define our data points.\nx = [1.5, 2.0, 13.0, 2.1, 0.0]\n\n# Set up the model call, sample from the prior.\nmodel = gdemo(x)\n\n# Evaluate surface at coordinates.\nevaluate(m1, m2) = logjoint(model, (m=m2, s²=invlink.(Ref(InverseGamma(2, 3)), m1)))\n\nfunction plot_sampler(chain; label=\"\")\n    # Extract values from chain.\n    val = get(chain, [:s², :m, :logjoint])\n    ss = link.(Ref(InverseGamma(2, 3)), val.s²)\n    ms = val.m\n    lps = val.logjoint\n\n    # How many surface points to sample.\n    granularity = 100\n\n    # Range start/stop points.\n    spread = 0.5\n    σ_start = minimum(ss) - spread * std(ss)\n    σ_stop = maximum(ss) + spread * std(ss)\n    μ_start = minimum(ms) - spread * std(ms)\n    μ_stop = maximum(ms) + spread * std(ms)\n    σ_rng = collect(range(σ_start; stop=σ_stop, length=granularity))\n    μ_rng = collect(range(μ_start; stop=μ_stop, length=granularity))\n\n    # Make surface plot.\n    p = surface(\n        σ_rng,\n        μ_rng,\n        evaluate;\n        camera=(30, 65),\n        #   ticks=nothing,\n        colorbar=false,\n        color=:inferno,\n        title=label,\n    )\n\n    line_range = 1:length(ms)\n\n    scatter3d!(\n        ss[line_range],\n        ms[line_range],\n        lps[line_range];\n        mc=:viridis,\n        marker_z=collect(line_range),\n        msw=0,\n        legend=false,\n        colorbar=false,\n        alpha=0.5,\n        xlabel=\"σ\",\n        ylabel=\"μ\",\n        zlabel=\"Log probability\",\n        title=label,\n    )\n\n    return p\nend;\n\n\nsetprogress!(false)",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Sampler Visualization"
    ]
  },
  {
    "objectID": "usage/sampler-visualisation/index.html#introduction",
    "href": "usage/sampler-visualisation/index.html#introduction",
    "title": "Sampler Visualization",
    "section": "",
    "text": "For each sampler, we will use the same code to plot sampler paths. The block below loads the relevant libraries and defines a function for plotting the sampler’s trajectory across the posterior.\nThe Turing model definition used here is not especially practical, but it is designed in such a way as to produce visually interesting posterior surfaces to show how different samplers move along the distribution.\n\nENV[\"GKS_ENCODING\"] = \"utf-8\" # Allows the use of unicode characters in Plots.jl\nusing Plots\nusing StatsPlots\nusing Turing\nusing Random\nusing Bijectors\n\n# Set a seed.\nRandom.seed!(0)\n\n# Define a strange model.\n@model function gdemo(x)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    bumps = sin(m) + cos(m)\n    m = m + 5 * bumps\n    for i in eachindex(x)\n        x[i] ~ Normal(m, sqrt(s²))\n    end\n    return s², m\nend\n\n# Define our data points.\nx = [1.5, 2.0, 13.0, 2.1, 0.0]\n\n# Set up the model call, sample from the prior.\nmodel = gdemo(x)\n\n# Evaluate surface at coordinates.\nevaluate(m1, m2) = logjoint(model, (m=m2, s²=invlink.(Ref(InverseGamma(2, 3)), m1)))\n\nfunction plot_sampler(chain; label=\"\")\n    # Extract values from chain.\n    val = get(chain, [:s², :m, :logjoint])\n    ss = link.(Ref(InverseGamma(2, 3)), val.s²)\n    ms = val.m\n    lps = val.logjoint\n\n    # How many surface points to sample.\n    granularity = 100\n\n    # Range start/stop points.\n    spread = 0.5\n    σ_start = minimum(ss) - spread * std(ss)\n    σ_stop = maximum(ss) + spread * std(ss)\n    μ_start = minimum(ms) - spread * std(ms)\n    μ_stop = maximum(ms) + spread * std(ms)\n    σ_rng = collect(range(σ_start; stop=σ_stop, length=granularity))\n    μ_rng = collect(range(μ_start; stop=μ_stop, length=granularity))\n\n    # Make surface plot.\n    p = surface(\n        σ_rng,\n        μ_rng,\n        evaluate;\n        camera=(30, 65),\n        #   ticks=nothing,\n        colorbar=false,\n        color=:inferno,\n        title=label,\n    )\n\n    line_range = 1:length(ms)\n\n    scatter3d!(\n        ss[line_range],\n        ms[line_range],\n        lps[line_range];\n        mc=:viridis,\n        marker_z=collect(line_range),\n        msw=0,\n        legend=false,\n        colorbar=false,\n        alpha=0.5,\n        xlabel=\"σ\",\n        ylabel=\"μ\",\n        zlabel=\"Log probability\",\n        title=label,\n    )\n\n    return p\nend;\n\n\nsetprogress!(false)",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Sampler Visualization"
    ]
  },
  {
    "objectID": "usage/sampler-visualisation/index.html#samplers",
    "href": "usage/sampler-visualisation/index.html#samplers",
    "title": "Sampler Visualization",
    "section": "Samplers",
    "text": "Samplers\n\nGibbs\nGibbs sampling tends to exhibit a “jittery” trajectory. The example below combines HMC and PG sampling to traverse the posterior.\n\nc = sample(model, Gibbs(:s² =&gt; HMC(0.01, 5), :m =&gt; PG(20)), 1000)\nplot_sampler(c)\n\n\n\n\n\n\nHMC\nHamiltonian Monte Carlo (HMC) sampling is a typical sampler to use, as it tends to be fairly good at converging in an efficient manner. It can often be tricky to set the correct parameters for this sampler however, and the NUTS sampler is often easier to run if you don’t want to spend too much time fiddling with step size and the number of steps to take. Note however that HMC does not explore the positive values μ very well, likely due to the leapfrog and step size parameter settings.\n\nc = sample(model, HMC(0.01, 10), 1000)\nplot_sampler(c)\n\n\n\n\n\n\nHMCDA\nThe HMCDA sampler is an implementation of the Hamiltonian Monte Carlo with Dual Averaging algorithm found in the paper “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo” by Hoffman and Gelman (2011). The paper can be found on arXiv for the interested reader.\n\nc = sample(model, HMCDA(200, 0.65, 0.3), 1000)\nplot_sampler(c)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.4\n\n\n\n\n\n\n\n\n\nMH\nMetropolis-Hastings (MH) sampling is one of the earliest Markov Chain Monte Carlo methods. MH sampling does not “move” a lot, unlike many of the other samplers implemented in Turing. Typically a much longer chain is required to converge to an appropriate parameter estimate.\nThe plot below only uses 1,000 iterations of Metropolis-Hastings.\n\nc = sample(model, MH(), 1000)\nplot_sampler(c)\n\n\n\n\nAs you can see, the MH sampler doesn’t move parameter estimates very often.\n\n\nNUTS\nThe No U-Turn Sampler (NUTS) is an implementation of the algorithm found in the paper “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo” by Hoffman and Gelman (2011). The paper can be found on arXiv for the interested reader.\nNUTS tends to be very good at traversing complex posteriors quickly.\n\nc = sample(model, NUTS(0.65), 1000)\nplot_sampler(c)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.05\n\n\n\n\n\n\n\nThe only parameter that needs to be set other than the number of iterations to run is the target acceptance rate. In the Hoffman and Gelman paper, they note that a target acceptance rate of 0.65 is typical.\nHere is a plot showing a very high acceptance rate. Note that it appears to “stick” to a mode and is not particularly good at exploring the posterior as compared to the 0.65 target acceptance ratio case.\n\nc = sample(model, NUTS(0.95), 1000)\nplot_sampler(c)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.4\n\n\n\n\n\n\n\nAn exceptionally low acceptance rate will show very few moves on the posterior:\n\nc = sample(model, NUTS(0.2), 1000)\nplot_sampler(c)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.2\n\n\n\n\n\n\n\n\n\nPG\nThe Particle Gibbs (PG) sampler is an implementation of an algorithm from the paper “Particle Markov chain Monte Carlo methods” by Andrieu, Doucet, and Holenstein (2010). The interested reader can learn more here.\nThe two parameters are the number of particles, and the number of iterations. The plot below shows the use of 20 particles.\n\nc = sample(model, PG(20), 1000)\nplot_sampler(c)\n\n\n\n\nNext, we plot using 50 particles.\n\nc = sample(model, PG(50), 1000)\nplot_sampler(c)",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Sampler Visualization"
    ]
  },
  {
    "objectID": "faq/index.html",
    "href": "faq/index.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "This is a common source of confusion. In Turing.jl, you can only condition or fix expressions that explicitly appear on the left-hand side (LHS) of a ~ statement.\nFor example, if your model contains:\nx ~ filldist(Normal(), 2)\nYou cannot directly condition on x[2] using condition(model, @varname(x[2]) =&gt; 1.0) because x[2] never appears on the LHS of a ~ statement. Only x as a whole appears there.\nHowever, there is an important exception: when you use the broadcasting operator .~ with a univariate distribution, each element is treated as being separately drawn from that distribution, allowing you to condition on individual elements:\n@model function f1()\n    x = Vector{Float64}(undef, 3)\n    x .~ Normal()  # Each element is a separate draw\nend\n\nm1 = f1() | (@varname(x[1]) =&gt; 1.0)\nsample(m1, NUTS(), 100) # This works!\nIn contrast, you cannot condition on parts of a multivariate distribution because it represents a single distribution over the entire vector:\n@model function f2()\n    x = Vector{Float64}(undef, 3)\n    x ~ MvNormal(zeros(3), I)  # Single multivariate distribution\nend\n\nm2 = f2() | (@varname(x[1]) =&gt; 1.0)\nsample(m2, NUTS(), 100) # This doesn't work!\nThe key insight is that filldist creates a single distribution (not N independent distributions), which is why you cannot condition on individual elements. The distinction is not just about what appears on the LHS of ~, but whether you’re dealing with separate distributions (.~ with univariate) or a single distribution over multiple values (~ with multivariate or filldist).\nTo understand more about how Turing determines whether a variable is treated as random or observed, see:\n\nCore Functionality - basic explanation of the ~ notation and conditioning",
    "crumbs": [
      "Get Started",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "faq/index.html#why-is-this-variable-being-treated-as-random-instead-of-observed",
    "href": "faq/index.html#why-is-this-variable-being-treated-as-random-instead-of-observed",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "This is a common source of confusion. In Turing.jl, you can only condition or fix expressions that explicitly appear on the left-hand side (LHS) of a ~ statement.\nFor example, if your model contains:\nx ~ filldist(Normal(), 2)\nYou cannot directly condition on x[2] using condition(model, @varname(x[2]) =&gt; 1.0) because x[2] never appears on the LHS of a ~ statement. Only x as a whole appears there.\nHowever, there is an important exception: when you use the broadcasting operator .~ with a univariate distribution, each element is treated as being separately drawn from that distribution, allowing you to condition on individual elements:\n@model function f1()\n    x = Vector{Float64}(undef, 3)\n    x .~ Normal()  # Each element is a separate draw\nend\n\nm1 = f1() | (@varname(x[1]) =&gt; 1.0)\nsample(m1, NUTS(), 100) # This works!\nIn contrast, you cannot condition on parts of a multivariate distribution because it represents a single distribution over the entire vector:\n@model function f2()\n    x = Vector{Float64}(undef, 3)\n    x ~ MvNormal(zeros(3), I)  # Single multivariate distribution\nend\n\nm2 = f2() | (@varname(x[1]) =&gt; 1.0)\nsample(m2, NUTS(), 100) # This doesn't work!\nThe key insight is that filldist creates a single distribution (not N independent distributions), which is why you cannot condition on individual elements. The distinction is not just about what appears on the LHS of ~, but whether you’re dealing with separate distributions (.~ with univariate) or a single distribution over multiple values (~ with multivariate or filldist).\nTo understand more about how Turing determines whether a variable is treated as random or observed, see:\n\nCore Functionality - basic explanation of the ~ notation and conditioning",
    "crumbs": [
      "Get Started",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "faq/index.html#can-i-use-parallelism-threads-in-my-model",
    "href": "faq/index.html#can-i-use-parallelism-threads-in-my-model",
    "title": "Frequently Asked Questions",
    "section": "Can I use parallelism / threads in my model?",
    "text": "Can I use parallelism / threads in my model?\nYes, but with some important caveats:\n\n1. Parallel Sampling (Multiple Chains)\nTuring.jl fully supports sampling multiple chains in parallel:\n\nMultithreaded sampling: Use MCMCThreads() to run one chain per thread\nDistributed sampling: Use MCMCDistributed() for distributed computing\n\nSee the Core Functionality guide for examples.\n\n\n2. Threading Within Models\nUsing threads inside your model (e.g., Threads.@threads) requires more care. In particular, only threaded observe statements are safe to use; threaded assume statements can lead to crashes or incorrect results. Please see the Threadsafe Evaluation page for complete details.\n@model function f(y)\n    x = Vector{Float64}(undef, length(y))\n    Threads.@threads for i in eachindex(y)\n        # This would be unsafe!\n        # x[i] ~ Normal()\n        # This is safe:\n        y[i] ~ Normal()\n    end\nend\n# If you have parallel tilde-statements or `@addlogprob!` in a model, \n# you must mark the model as threadsafe:\nmodel = setthreadsafe(f(y), true)\nImportant limitations:\n\nObserve statements: Generally safe to use in threaded loops\nAssume statements (sampling statements): Often crash unpredictably or produce incorrect results\nAD backend compatibility: Many AD backends don’t support threading. Check the multithreaded column in ADTests for compatibility",
    "crumbs": [
      "Get Started",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "faq/index.html#how-do-i-check-the-type-stability-of-my-turing-model",
    "href": "faq/index.html#how-do-i-check-the-type-stability-of-my-turing-model",
    "title": "Frequently Asked Questions",
    "section": "How do I check the type stability of my Turing model?",
    "text": "How do I check the type stability of my Turing model?\nType stability is crucial for performance. Check out:\n\nPerformance Tips - includes specific advice on type stability\nUse DynamicPPL.DebugUtils.model_warntype to check type stability of your model",
    "crumbs": [
      "Get Started",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "faq/index.html#how-do-i-debug-my-turing-model",
    "href": "faq/index.html#how-do-i-debug-my-turing-model",
    "title": "Frequently Asked Questions",
    "section": "How do I debug my Turing model?",
    "text": "How do I debug my Turing model?\nFor debugging both statistical and syntactical issues:\n\nTroubleshooting Guide - common errors and their solutions\nFor more advanced debugging, DynamicPPL provides the DynamicPPL.DebugUtils module for inspecting model internals",
    "crumbs": [
      "Get Started",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "faq/index.html#what-are-the-main-differences-between-turing-bugs-and-stan-syntax",
    "href": "faq/index.html#what-are-the-main-differences-between-turing-bugs-and-stan-syntax",
    "title": "Frequently Asked Questions",
    "section": "What are the main differences between Turing, BUGS, and Stan syntax?",
    "text": "What are the main differences between Turing, BUGS, and Stan syntax?\nKey syntactic differences include:\n\nParameter blocks: Stan requires explicit data, parameters, and model blocks. In Turing, everything is defined within the @model macro\nVariable declarations: Stan requires upfront type declarations in parameter blocks. Turing infers types from the sampling statements\nTransformed data: Stan has a transformed data block for preprocessing. In Turing, data transformations should be done before defining the model\nGenerated quantities: Stan has a generated quantities block. In Turing, use the approach described in Tracking Extra Quantities\n\nExample comparison:\n// Stan\ndata {\n  real y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  mu ~ normal(0, 1);\n  sigma ~ normal(0, 1);\n  y ~ normal(mu, sigma);\n}\n# Turing\n@model function my_model(y)\n    mu ~ Normal(0, 1)\n    sigma ~ truncated(Normal(0, 1); lower=0)\n    y ~ Normal(mu, sigma)\nend",
    "crumbs": [
      "Get Started",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "faq/index.html#which-automatic-differentiation-backend-should-i-use",
    "href": "faq/index.html#which-automatic-differentiation-backend-should-i-use",
    "title": "Frequently Asked Questions",
    "section": "Which automatic differentiation backend should I use?",
    "text": "Which automatic differentiation backend should I use?\nThe choice of AD backend can significantly impact performance. See:\n\nAutomatic Differentiation Guide - comprehensive comparison of ForwardDiff, Mooncake, ReverseDiff, and other backends\nPerformance Tips - quick guide on choosing backends\nAD Backend Benchmarks - performance comparisons across various models",
    "crumbs": [
      "Get Started",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "faq/index.html#i-changed-one-line-of-my-model-and-now-its-so-much-slower-why",
    "href": "faq/index.html#i-changed-one-line-of-my-model-and-now-its-so-much-slower-why",
    "title": "Frequently Asked Questions",
    "section": "I changed one line of my model and now it’s so much slower; why?",
    "text": "I changed one line of my model and now it’s so much slower; why?\nSmall changes can have big performance impacts. Common culprits include:\n\nType instability introduced by the change\nSwitching from vectorised to scalar operations (or vice versa)\nInadvertently causing AD backend incompatibilities\nBreaking assumptions that allowed compiler optimizations\n\nSee our Performance Tips and Troubleshooting Guide for debugging performance regressions.",
    "crumbs": [
      "Get Started",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "usage/modifying-logprob/index.html",
    "href": "usage/modifying-logprob/index.html",
    "title": "Modifying the Log Probability",
    "section": "",
    "text": "Turing accumulates log probabilities internally in an internal data structure that is accessible through the internal variable __varinfo__ within the model definition. To avoid users having to deal with internal data structures, Turing provides the @addlogprob! macro which increases the accumulated log probability. For instance, this allows you to include arbitrary terms in the likelihood\n\nusing Turing\n\nmyloglikelihood(x, μ) = loglikelihood(Normal(μ, 1), x)\n\n@model function demo(x)\n    μ ~ Normal()\n    @addlogprob! myloglikelihood(x, μ)\nend\n\ndemo (generic function with 2 methods)\n\n\nand to force a sampler to reject a sample:\n\nusing Turing\nusing LinearAlgebra\n\n@model function demo(x)\n    m ~ MvNormal(zero(x), I)\n    if dot(m, x) &lt; 0\n        @addlogprob! -Inf\n        # Exit the model evaluation early\n        return nothing\n    end\n\n    x ~ MvNormal(m, I)\n    return nothing\nend\n\ndemo (generic function with 2 methods)\n\n\nNote that @addlogprob! (p::Float64) adds p to the log likelihood. If instead you want to add to the log prior, you can use\n\n@addlogprob! (; logprior=value_goes_here)\n\n\n\n\n Back to top",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Modifying the Log Probability"
    ]
  },
  {
    "objectID": "usage/automatic-differentiation/index.html",
    "href": "usage/automatic-differentiation/index.html",
    "title": "Automatic Differentiation",
    "section": "",
    "text": "Automatic differentiation (AD) is a technique used in Turing.jl to evaluate the gradient of a function at a given set of arguments. In the context of Turing.jl, the function being differentiated is the log probability density of a model, and the arguments are the parameters of the model (i.e. the values of the random variables). The gradient of the log probability density is used by various algorithms in Turing.jl, such as HMC (including NUTS), mode estimation (which uses gradient-based optimisation), and variational inference.\nThe Julia ecosystem has a number of AD libraries. You can switch between these using the unified ADTypes.jl interface, which for a given AD backend, provides types such as AutoBackend (see the documentation for more details). For example, to use the Mooncake.jl package for AD, you can run the following:\n\n# Turing re-exports AutoForwardDiff, AutoReverseDiff, and AutoMooncake.\n# Other ADTypes must be explicitly imported from ADTypes.jl or\n# DifferentiationInterface.jl.\nusing Turing\nsetprogress!(false)\n\n# Note that if you specify a custom AD backend, you must also import it.\nimport Mooncake\n\n@model function f()\n    x ~ Normal()\n    # Rest of your model here\nend\n\nsample(f(), HMC(0.1, 5; adtype=AutoMooncake()), 100)\n\n\n[ Info: [Turing]: progress logging is disabled globally\n\n\n\n\nChains MCMC chain (100×13×1 Array{Union{Missing, Float64}, 3}):\n\nIterations        = 1:1:100\nNumber of chains  = 1\nSamples per chain = 100\nWall duration     = 77.3 seconds\nCompute duration  = 77.3 seconds\nparameters        = x\ninternals         = logprior, loglikelihood, logjoint, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, numerical_error, step_size, nom_step_size\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nBy default, if you do not specify a backend, Turing will default to ForwardDiff.jl. In this case, you do not need to import ForwardDiff, as it is already a dependency of Turing.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Automatic Differentiation"
    ]
  },
  {
    "objectID": "usage/automatic-differentiation/index.html#what-is-automatic-differentiation",
    "href": "usage/automatic-differentiation/index.html#what-is-automatic-differentiation",
    "title": "Automatic Differentiation",
    "section": "",
    "text": "Automatic differentiation (AD) is a technique used in Turing.jl to evaluate the gradient of a function at a given set of arguments. In the context of Turing.jl, the function being differentiated is the log probability density of a model, and the arguments are the parameters of the model (i.e. the values of the random variables). The gradient of the log probability density is used by various algorithms in Turing.jl, such as HMC (including NUTS), mode estimation (which uses gradient-based optimisation), and variational inference.\nThe Julia ecosystem has a number of AD libraries. You can switch between these using the unified ADTypes.jl interface, which for a given AD backend, provides types such as AutoBackend (see the documentation for more details). For example, to use the Mooncake.jl package for AD, you can run the following:\n\n# Turing re-exports AutoForwardDiff, AutoReverseDiff, and AutoMooncake.\n# Other ADTypes must be explicitly imported from ADTypes.jl or\n# DifferentiationInterface.jl.\nusing Turing\nsetprogress!(false)\n\n# Note that if you specify a custom AD backend, you must also import it.\nimport Mooncake\n\n@model function f()\n    x ~ Normal()\n    # Rest of your model here\nend\n\nsample(f(), HMC(0.1, 5; adtype=AutoMooncake()), 100)\n\n\n[ Info: [Turing]: progress logging is disabled globally\n\n\n\n\nChains MCMC chain (100×13×1 Array{Union{Missing, Float64}, 3}):\n\nIterations        = 1:1:100\nNumber of chains  = 1\nSamples per chain = 100\nWall duration     = 77.3 seconds\nCompute duration  = 77.3 seconds\nparameters        = x\ninternals         = logprior, loglikelihood, logjoint, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, numerical_error, step_size, nom_step_size\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nBy default, if you do not specify a backend, Turing will default to ForwardDiff.jl. In this case, you do not need to import ForwardDiff, as it is already a dependency of Turing.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Automatic Differentiation"
    ]
  },
  {
    "objectID": "usage/automatic-differentiation/index.html#choosing-an-ad-backend",
    "href": "usage/automatic-differentiation/index.html#choosing-an-ad-backend",
    "title": "Automatic Differentiation",
    "section": "Choosing an AD Backend",
    "text": "Choosing an AD Backend\nThere are two aspects to choosing an AD backend: firstly, what backends are available; and secondly, which backend is best for your model.\n\nUsable AD Backends\nTuring.jl uses the functionality in DifferentiationInterface.jl (‘DI’) to interface with AD libraries in a unified way. In principle, any AD library that DI provides an interface for can be used with Turing; you should consult the DI documentation for an up-to-date list of compatible AD libraries.\nNote, however, that not all AD libraries in there are thoroughly tested on Turing models. Thus, it is possible that some of them will either error (because they don’t know how to differentiate through Turing’s code), or maybe even silently give incorrect results (if you are very unlucky). Turing is most extensively tested with ForwardDiff.jl (the default), ReverseDiff.jl, and Mooncake.jl. We also run a smaller set of tests with Enzyme.jl.\n\n\n\n\n\n\nNoteGradient preparation\n\n\n\nUsers of DifferentiationInterface.jl will have seen that it provides functions such as prepare_gradient, which allow you to perform a one-time setup to make subsequent gradient computations faster. Turing will automatically perform gradient preparation for you when calling functions such as sample or optimize, so you do not need to worry about this step.\n\n\n\n\nADTests\nBefore describing how to choose the best AD backend for your model, we should mention that we also publish a table of benchmarks for various models and AD backends in the ADTests website. These models aim to capture a variety of different features of Turing.jl and Julia in general, so that you can see which AD backends may be compatible with your model. Benchmarks are also included, although it should be noted that many of the models in ADTests are small and thus the timings may not be representative of larger, real-life models.\nIf you have suggestions for other models to include, please do let us know by creating an issue on GitHub!\n\n\nThe Best AD Backend for Your Model\nGiven the number of possible backends, how do you choose the best one for your model?\nA simple heuristic is to look at the number of parameters in your model. The log density of the model, i.e. the function being differentiated, is a function that goes from \\(\\mathbb{R}^n \\to \\mathbb{R}\\), where \\(n\\) is the number of parameters in your model. For models with a small number of parameters (say up to ~ 20), forward-mode AD (e.g. ForwardDiff) is generally faster due to a smaller overhead. On the other hand, for models with a large number of parameters, reverse-mode AD (e.g. ReverseDiff or Mooncake) is generally faster as it computes the gradients with respect to all parameters in a single pass.\nThe most exact way to ensure you are using the fastest AD that works for your problem is to benchmark them using the functionality in DynamicPPL (see the API documentation):\n\nusing ADTypes\nusing DynamicPPL.TestUtils.AD: run_ad, ADResult\nusing ForwardDiff, ReverseDiff\n\n@model function gdemo(x, y)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    x ~ Normal(m, sqrt(s²))\n    return y ~ Normal(m, sqrt(s²))\nend\nmodel = gdemo(1.5, 2)\n\nfor adtype in [AutoForwardDiff(), AutoReverseDiff()]\n    result = run_ad(model, adtype; benchmark=true)\n    @show result.grad_time / result.primal_time\nend\n\n\n[ Info: Running AD on gdemo with ADTypes.AutoForwardDiff()\n       params : [0.054372130266006784, 0.6576372764175252]\n       actual : (-4.985233195693965, [0.7353397174851433, 1.4462740599214583])\n     expected : (-4.985233195693965, [0.7353397174851433, 1.4462740599214583])\ngrad / primal : 1.815e-7/9.469e-8 = 1.917\nresult.grad_time / result.primal_time = 1.9171205218471734\n[ Info: Running AD on gdemo with ADTypes.AutoReverseDiff()\n       params : [1.8715929280399695, -5.992576062885155]\n       actual : (-19.568998844105796, [8.958832573688106, 3.3049573613944707])\n     expected : (-19.568998844105796, [8.958832573688108, 3.3049573613944707])\ngrad / primal : 1.5e-5/1.006e-7 = 149.1\nresult.grad_time / result.primal_time = 149.13370492705752\n\n\n\n\nIn this specific instance, ForwardDiff is clearly faster (due to the small size of the model).\n\n\n\n\n\n\nNoteA note about ReverseDiff’s compile argument\n\n\n\nThe additional keyword argument compile=true for AutoReverseDiff specifies whether to pre-record the tape only once and reuse it later. By default, this is set to false, which means no pre-recording. Setting compile=true can substantially improve performance, but risks silently incorrect results if not used with care. Pre-recorded tapes should only be used if you are absolutely certain that the sequence of operations performed in your code does not change between different executions of your model.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Automatic Differentiation"
    ]
  },
  {
    "objectID": "usage/automatic-differentiation/index.html#compositional-sampling-with-differing-ad-modes",
    "href": "usage/automatic-differentiation/index.html#compositional-sampling-with-differing-ad-modes",
    "title": "Automatic Differentiation",
    "section": "Compositional Sampling with Differing AD Modes",
    "text": "Compositional Sampling with Differing AD Modes\nWhen using Gibbs sampling, Turing also supports mixed automatic differentiation methods for different variable spaces. The following snippet shows how one can use ForwardDiff to sample the mean (m) parameter, and ReverseDiff for the variance (s) parameter:\n\nusing Turing\nusing ReverseDiff\n\n# Sample using Gibbs and varying autodiff backends.\nc = sample(\n    gdemo(1.5, 2),\n    Gibbs(\n        :m =&gt; HMC(0.1, 5; adtype=AutoForwardDiff()),\n        :s² =&gt; HMC(0.1, 5; adtype=AutoReverseDiff()),\n    ),\n    1000,\n    progress=false,\n)\n\nChains MCMC chain (1000×5×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 7.47 seconds\nCompute duration  = 7.47 seconds\nparameters        = s², m\ninternals         = logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Automatic Differentiation"
    ]
  },
  {
    "objectID": "usage/predictive-distributions/index.html",
    "href": "usage/predictive-distributions/index.html",
    "title": "Predictive Distributions",
    "section": "",
    "text": "Standard MCMC sampling methods return values of the parameters of the model. However, it is often also useful to generate new data points using the model, given a distribution of the parameters. Turing.jl allows you to do this using the predict function, along with conditioning syntax.\nConsider the following simple model, where we observe some normally-distributed data X and want to learn about its mean m.\nusing Turing\n@model function f(N)\n    m ~ Normal()\n    X ~ filldist(Normal(m), N)\nend\n\nf (generic function with 2 methods)\nNotice first how we have not specified X as an argument to the model. This allows us to use Turing’s conditioning syntax to specify whether we want to provide observed data or not.\n# Generate some synthetic data\nN = 5\ntrue_m = 3.0\nX = rand(Normal(true_m), N)\n\n# Instantiate the model with observed data\nmodel = f(N) | (; X = X)\n\n# Sample from the posterior\nchain = sample(model, NUTS(), 1_000; progress=false)\nmean(chain[:m])\n\n\n┌ Info: Found initial step size\n└   ϵ = 1.6\n\n\n\n\n2.8969375614131465",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Predictive Distributions"
    ]
  },
  {
    "objectID": "usage/predictive-distributions/index.html#posterior-predictive-distribution",
    "href": "usage/predictive-distributions/index.html#posterior-predictive-distribution",
    "title": "Predictive Distributions",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\nchain[:m] now contains samples from the posterior distribution of m. If we use these samples of the parameters to generate new data points, we obtain the posterior predictive distribution. Statistically, this is defined as\n\\[\np(\\tilde{x} | \\mathbf{X}) = \\int p(\\tilde{x} | \\theta) p(\\theta | \\mathbf{X}) d\\theta,\n\\]\nwhere \\(\\tilde{x}\\) are the new data which you wish to draw, \\(\\theta\\) are the model parameters, and \\(\\mathbf{X}\\) are the observed data. \\(p(\\tilde{x} | \\theta)\\) is the distribution of the new data given the parameters, which is specified in the Turing.jl model (the X ~ ... line); and \\(p(\\theta | \\mathbf{X})\\) is the posterior distribution, as given by the Markov chain.\nTo obtain samples of \\(\\tilde{x}\\), we need to first remove the observed data from the model (or ‘decondition’ the model). This means that when the model is evaluated, it will sample a new value for X. If you don’t decondition the model, then X will remain fixed to the observed data, and no new samples will be generated.\n\npredictive_model = decondition(model)\n\nDynamicPPL.Model{typeof(f), (:N,), (), (), Tuple{Int64}, Tuple{}, DynamicPPL.DefaultContext, false}(f, (N = 5,), NamedTuple(), DynamicPPL.DefaultContext())\n\n\n\n\n\n\n\n\nTipSelective deconditioning\n\n\n\nIf you only want to decondition a single variable X, you can use decondition(model, @varname(X)).\n\n\nTo demonstrate how this deconditioned model can generate new data, we can fix the value of m to be its mean and evaluate the model:\n\npredictive_model_with_mean_m = predictive_model | (; m = mean(chain[:m]))\nrand(predictive_model_with_mean_m)\n\n(X = [3.6502520730994967, 2.5387963666328335, 2.7694230773423794, 2.6723250765487947, 2.115889456578576],)\n\n\nThis has given us a single sample of X given the mean value of m. Of course, to take our Bayesian uncertainty into account, we want to use the full posterior distribution of m, not just its mean. To do so, we use predict, which effectively does the same as above but for every sample in the chain:\n\npredictive_samples = predict(predictive_model, chain)\n\nChains MCMC chain (1000×5×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nparameters        = X[1], X[2], X[3], X[4], X[5]\ninternals         = \n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n\n\n\n\n\n\nTipReproducibility\n\n\n\npredict, like many other Julia functions that involve randomness, takes an optional rng as its first argument. This controls the generation of new X samples, and makes your results reproducible.\n\n\n\n\n\n\n\n\nNote\n\n\n\npredict returns a Chains object itself, which will only contain the newly predicted variables. If you want to also retain the original parameters, you can use predict(rng, predictive_model, chain; include_all=true).\n\n\nWe can visualise the predictive distribution by combining all the samples and making a density plot:\n\nusing StatsPlots: density, density!, vline!\n\npredicted_X = vcat([predictive_samples[Symbol(\"X[$i]\")] for i in 1:N]...)\ndensity(predicted_X, label=\"Posterior predictive\")\n\n\n\n\nDepending on your data, you may naturally want to create different visualisations. For example, perhaps X contains some time-series data, in which case you can plot each prediction individually as a line against time.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Predictive Distributions"
    ]
  },
  {
    "objectID": "usage/predictive-distributions/index.html#prior-predictive-distribution",
    "href": "usage/predictive-distributions/index.html#prior-predictive-distribution",
    "title": "Predictive Distributions",
    "section": "Prior predictive distribution",
    "text": "Prior predictive distribution\nAlternatively, if we use the prior distribution of the parameters \\(p(\\theta)\\), we obtain the prior predictive distribution:\n\\[\np(\\tilde{x}) = \\int p(\\tilde{x} | \\theta) p(\\theta) d\\theta,\n\\]\nIn an exactly analogous fashion to above, you could sample from the prior distribution of the conditioned model, and then pass that to predict:\n\nprior_params = sample(model, Prior(), 1_000; progress=false)\nprior_predictive_samples = predict(predictive_model, prior_params)\n\nChains MCMC chain (1000×5×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nparameters        = X[1], X[2], X[3], X[4], X[5]\ninternals         = \n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nIn fact there is a simpler way: you can directly sample from the deconditioned model, using Turing’s Prior sampler. This will, in a single call, generate prior samples for both the parameters as well as the new data.\n\nprior_predictive_samples = sample(predictive_model, Prior(), 1_000; progress=false)\n\nChains MCMC chain (1000×9×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 0.08 seconds\nCompute duration  = 0.08 seconds\nparameters        = m, X[1], X[2], X[3], X[4], X[5]\ninternals         = logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nWe can visualise the prior predictive distribution in the same way as before. Let’s compare the two predictive distributions:\n\nprior_predicted_X = vcat([prior_predictive_samples[Symbol(\"X[$i]\")] for i in 1:N]...)\ndensity(prior_predicted_X, label=\"Prior predictive\")\ndensity!(predicted_X, label=\"Posterior predictive\")\nvline!([true_m], label=\"True mean\", linestyle=:dash, color=:black)\n\n\n\n\nWe can see here that the prior predictive distribution is:\n\nWider than the posterior predictive distribution;\nCentred on the prior mean of m (which is 0), rather than the posterior mean (which is close to the true mean of 3).\n\nBoth of these are because the posterior predictive distribution has been informed by the observed data.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Predictive Distributions"
    ]
  },
  {
    "objectID": "usage/threadsafe-evaluation/index.html",
    "href": "usage/threadsafe-evaluation/index.html",
    "title": "Threadsafe Evaluation",
    "section": "",
    "text": "A common technique to speed up Julia code is to use multiple threads to run computations in parallel. The Julia manual has a section on multithreading, which is a good introduction to the topic.\nWe assume that the reader is familiar with some threading constructs in Julia, and the general concept of data races. This page specificaly discusses Turing’s support for threadsafe model evaluation.\nprintln(\"This notebook is being run with $(Threads.nthreads()) threads.\")\n\nThis notebook is being run with 4 threads.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Threadsafe Evaluation"
    ]
  },
  {
    "objectID": "usage/threadsafe-evaluation/index.html#threading-in-turing-models",
    "href": "usage/threadsafe-evaluation/index.html#threading-in-turing-models",
    "title": "Threadsafe Evaluation",
    "section": "Threading in Turing models",
    "text": "Threading in Turing models\nGiven that Turing models mostly contain ‘plain’ Julia code, one might expect that all threading constructs such as Threads.@threads or Threads.@spawn can be used inside Turing models.\nThis is, to some extent, true: for example, you can use threading constructs to speed up deterministic computations. For example, here we use parallelism to speed up a transformation of x:\n\nusing Turing\n\n@model function parallel(y)\n    x ~ dist\n    x_transformed = similar(x)\n    Threads.@threads for i in eachindex(x)\n        x_transformed[i] = some_expensive_function(x[i])\n    end\n    y ~ some_likelihood(x_transformed)\nend\n\n\n┌ Warning: It looks like you are using `Threads.@threads` in your model definition.\n│ \n│ Note that since version 0.39 of DynamicPPL, threadsafe evaluation of models is disabled by default. If you need it, you will need to explicitly enable it by creating the model, and then running `model = setthreadsafe(model, true)`.\n│ \n│ Threadsafe model evaluation is only needed when parallelising tilde-statements (not arbitrary Julia code), and avoiding it can often lead to significant performance improvements.\n│ \n│ Please see https://turinglang.org/docs/usage/threadsafe-evaluation/ for more details of when threadsafe evaluation is actually required.\n└ @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/compiler.jl:383\n\n\n\n\nparallel (generic function with 2 methods)\n\n\nIn general, for code that does not involve tilde-statements (x ~ dist), threading works exactly as it does in regular Julia code.\nHowever, extra care must be taken when using tilde-statements (x ~ dist), or @addlogprob!, inside threaded blocks.\n\n\n\n\n\n\nNoteWhy are tilde-statements special?\n\n\n\nTilde-statements are expanded by the @model macro into something that modifies the internal VarInfo object used for model evaluation. Essentially, x ~ dist expands to something like\nx, __varinfo__ = DynamicPPL.tilde_assume!!(..., __varinfo__)\nand writing into __varinfo__ is, in general, not threadsafe. Thus, parallelising tilde-statements can lead to data races as described in the Julia manual.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Threadsafe Evaluation"
    ]
  },
  {
    "objectID": "usage/threadsafe-evaluation/index.html#threaded-observations",
    "href": "usage/threadsafe-evaluation/index.html#threaded-observations",
    "title": "Threadsafe Evaluation",
    "section": "Threaded observations",
    "text": "Threaded observations\nAs of version 0.42, Turing only supports the use of tilde-statements inside threaded blocks when these are observations (i.e., likelihood terms).\nHowever, such models must be marked by the user as requiring threadsafe evaluation, using setthreadsafe.\nThis means that the following code is safe to use:\n\n@model function threaded_obs(N)\n    x ~ Normal()\n    y = Vector{Float64}(undef, N)\n    Threads.@threads for i in 1:N\n        y[i] ~ Normal(x)\n    end\nend\n\nN = 100\ny = randn(N)\nthreadunsafe_model = threaded_obs(N) | (; y = y)\nthreadsafe_model = setthreadsafe(threadunsafe_model, true)\n\n\n┌ Warning: It looks like you are using `Threads.@threads` in your model definition.\n│ \n│ Note that since version 0.39 of DynamicPPL, threadsafe evaluation of models is disabled by default. If you need it, you will need to explicitly enable it by creating the model, and then running `model = setthreadsafe(model, true)`.\n│ \n│ Threadsafe model evaluation is only needed when parallelising tilde-statements (not arbitrary Julia code), and avoiding it can often lead to significant performance improvements.\n│ \n│ Please see https://turinglang.org/docs/usage/threadsafe-evaluation/ for more details of when threadsafe evaluation is actually required.\n└ @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/compiler.jl:383\n\n\n\n\nDynamicPPL.Model{typeof(threaded_obs), (:N,), (), (), Tuple{Int64}, Tuple{}, DynamicPPL.ConditionContext{@NamedTuple{y::Vector{Float64}}, DynamicPPL.DefaultContext}, true}(threaded_obs, (N = 100,), NamedTuple(), ConditionContext((y = [-0.6305515478827788, -2.0431104613771915, 1.9445839095581745, 0.04780388956641738, -0.545725588072871, -0.55661347012285, -0.687395596659439, -0.841269887392693, -0.8372076243076204, 0.729979726906271, -0.5360744310949656, 0.15762330603016506, -0.369288203230264, 1.05332009022481, 1.8331489236986818, -0.5914477639921777, -0.4869371060400832, 1.5237372052622504, -1.4908474490072465, 2.6255593481878376, 0.2596297211049135, -0.894540955253628, -0.5315735835850568, -1.5093766665812645, 0.6637359251332832, -0.2543833516965076, 2.0451410671500185, -1.0986099041705406, -0.25695740467998174, -0.1606868962495835, -1.3942099739039797, -0.8755130762814535, 1.098199829025723, 0.7710932086393351, 0.0031848019753931995, 0.45493718694648105, -0.18611697189380172, -0.49739474513489557, 0.18637704411470063, 1.0984817565504466, -0.7246819228227666, 0.7489481071172335, 0.013146200443190356, 0.10654142779974493, 0.14936743344297956, 0.5976022868233318, -0.5552424137834382, 0.6979360988011801, -1.3495535747982146, -3.2119074792843834, 0.26384216311139935, 1.372363586920074, -1.402089796258677, 1.6073558380556876, 1.5408726088217972, -0.14434779257981362, 1.4617522505853167, 2.0481917287642686, -0.819753772865383, 0.31944200006567214, 0.03774997895681625, -0.05478782422603415, -0.44000178911295373, -1.0997997627043508, 0.2087028428963356, 0.46918823227049594, 0.702647229764999, 0.5818067364851282, 0.9145478961543889, -2.3512142070931725, -1.387245687734575, -0.57783393316075, -0.04030016626758858, 0.9972914290887502, 0.006360566281137585, -1.0233658481350414, 1.3407328731901218, -1.4985691069702987, 0.008014754397554008, 1.4429084988995344, -0.4037552185890292, -1.9338816532000185, 1.3338777138319697, -0.9382108152162355, -0.5188668502835753, 2.4703453840835667, -0.5492595893189646, 0.9782478335583603, 1.2663514016431021, 0.1385989072000875, 0.44119789431843354, -0.7252530725292358, 0.5889188772873388, -0.006257276894078264, 0.9227572629681071, 1.7394978720738306, -0.5546256694066708, -1.0018481327602031, 1.273572336606085, 0.29667484872676764],), DynamicPPL.DefaultContext()))\n\n\nEvaluating this model is threadsafe, in that Turing guarantees to provide the correct result in functions such as:\n\nlogjoint(threadsafe_model, (; x = 0.0))\n\n-151.54668777757442\n\n\n(we can compare with the true value)\n\nlogpdf(Normal(), 0.0) + sum(logpdf.(Normal(0.0), y))\n\n-151.54668777757445\n\n\nNote that if you do not use setthreadsafe, the above code may give wrong results, or even error:\n\nlogjoint(threadunsafe_model, (; x = 0.0))\n\n-39.36964387089936\n\n\nYou can sample from this model and safely use functions such as predict or returned, as long as the model is always marked as threadsafe:\n\nmodel = setthreadsafe(threaded_obs(N) | (; y = y), true)\nchn = sample(model, NUTS(), 100; check_model=false, progress=false)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.2\n\n\n\n\nChains MCMC chain (100×15×1 Array{Float64, 3}):\n\nIterations        = 51:1:150\nNumber of chains  = 1\nSamples per chain = 100\nWall duration     = 6.14 seconds\nCompute duration  = 6.14 seconds\nparameters        = x\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n\npmodel = setthreadsafe(threaded_obs(N), true)  # don't condition on data\npredict(pmodel, chn)\n\nChains MCMC chain (100×100×1 Array{Float64, 3}):\n\nIterations        = 1:1:100\nNumber of chains  = 1\nSamples per chain = 100\nparameters        = y[1], y[2], y[3], y[4], y[5], y[6], y[7], y[8], y[9], y[10], y[11], y[12], y[13], y[14], y[15], y[16], y[17], y[18], y[19], y[20], y[21], y[22], y[23], y[24], y[25], y[76], y[77], y[78], y[79], y[80], y[81], y[82], y[83], y[84], y[85], y[86], y[87], y[88], y[89], y[90], y[91], y[92], y[93], y[94], y[95], y[96], y[97], y[98], y[99], y[100], y[26], y[27], y[28], y[29], y[30], y[31], y[32], y[33], y[34], y[35], y[36], y[37], y[38], y[39], y[40], y[41], y[42], y[43], y[44], y[45], y[46], y[47], y[48], y[49], y[50], y[51], y[52], y[53], y[54], y[55], y[56], y[57], y[58], y[59], y[60], y[61], y[62], y[63], y[64], y[65], y[66], y[67], y[68], y[69], y[70], y[71], y[72], y[73], y[74], y[75]\ninternals         = \n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n\n\n\n\n\n\nWarningPrevious versions\n\n\n\nUp until Turing v0.41, you did not need to use setthreadsafe to enable threadsafe evaluation, and it was automatically enabled whenever Julia was launched with more than one thread.\nThere were several reasons for changing this: one major one is because threadsafe evaluation comes with a performance cost, which can sometimes be substantial (see below).\nFurthermore, the number of threads is not an appropriate way to determine whether threadsafe evaluation is needed!",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Threadsafe Evaluation"
    ]
  },
  {
    "objectID": "usage/threadsafe-evaluation/index.html#threaded-assumptions-sampling-latent-values",
    "href": "usage/threadsafe-evaluation/index.html#threaded-assumptions-sampling-latent-values",
    "title": "Threadsafe Evaluation",
    "section": "Threaded assumptions / sampling latent values",
    "text": "Threaded assumptions / sampling latent values\nOn the other hand, parallelising the sampling of latent values is not supported. Attempting to do this will either error or give wrong results.\n\n@model function threaded_assume_bad(N)\n    x = Vector{Float64}(undef, N)\n    Threads.@threads for i in 1:N\n        x[i] ~ Normal()\n    end\n    return x\nend\n\nmodel = threaded_assume_bad(100)\n\n# This will throw an error (and probably a different error\n# each time it's run...)\nmodel()\n\n\n┌ Warning: It looks like you are using `Threads.@threads` in your model definition.\n│ \n│ Note that since version 0.39 of DynamicPPL, threadsafe evaluation of models is disabled by default. If you need it, you will need to explicitly enable it by creating the model, and then running `model = setthreadsafe(model, true)`.\n│ \n│ Threadsafe model evaluation is only needed when parallelising tilde-statements (not arbitrary Julia code), and avoiding it can often lead to significant performance improvements.\n│ \n│ Please see https://turinglang.org/docs/usage/threadsafe-evaluation/ for more details of when threadsafe evaluation is actually required.\n└ @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/compiler.jl:383\n\n\n\n\n\nTaskFailedException\n\n    nested task error: AssertionError: Multiple concurrent writes to Dict detected!\n    Stacktrace:\n      [1] rehash!(h::Dict{AbstractPPL.VarName, Int64}, newsz::Int64)\n        @ Base ./dict.jl:182\n      [2] _setindex!\n        @ ./dict.jl:337 [inlined]\n      [3] setindex!(h::Dict{AbstractPPL.VarName, Int64}, v0::Int64, key::AbstractPPL.VarName{:x, Accessors.IndexLens{Tuple{Int64}}})\n        @ Base ./dict.jl:363\n      [4] push!(meta::DynamicPPL.Metadata{Dict{AbstractPPL.VarName, Int64}, Vector{Distribution}, Vector{AbstractPPL.VarName}, Vector{Real}}, vn::AbstractPPL.VarName{:x, Accessors.IndexLens{Tuple{Int64}}}, r::Float64, dist::Normal{Float64})\n        @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/varinfo.jl:1709\n      [5] push!!\n        @ ~/.julia/packages/DynamicPPL/Hza15/src/varinfo.jl:1721 [inlined]\n      [6] push!!(vi::DynamicPPL.VarInfo{DynamicPPL.Metadata{Dict{AbstractPPL.VarName, Int64}, Vector{Distribution}, Vector{AbstractPPL.VarName}, Vector{Real}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::DynamicPPL.LogPriorAccumulator{Float64}, LogJacobian::DynamicPPL.LogJacobianAccumulator{Float64}, LogLikelihood::DynamicPPL.LogLikelihoodAccumulator{Float64}}}}, vn::AbstractPPL.VarName{:x, Accessors.IndexLens{Tuple{Int64}}}, val::Float64, dist::Normal{Float64})\n        @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/varinfo.jl:1657\n      [7] tilde_assume!!(ctx::DynamicPPL.InitContext{Random.TaskLocalRNG, InitFromPrior}, dist::Normal{Float64}, vn::AbstractPPL.VarName{:x, Accessors.IndexLens{Tuple{Int64}}}, vi::DynamicPPL.VarInfo{DynamicPPL.Metadata{Dict{AbstractPPL.VarName, Int64}, Vector{Distribution}, Vector{AbstractPPL.VarName}, Vector{Real}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::DynamicPPL.LogPriorAccumulator{Float64}, LogJacobian::DynamicPPL.LogJacobianAccumulator{Float64}, LogLikelihood::DynamicPPL.LogLikelihoodAccumulator{Float64}}}})\n        @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/contexts/init.jl:372\n      [8] (::var\"#88#threadsfor_fun#8\"{var\"#88#threadsfor_fun#7#9\"{DynamicPPL.Model{typeof(threaded_assume_bad), (:N,), (), (), Tuple{Int64}, Tuple{}, DynamicPPL.InitContext{Random.TaskLocalRNG, InitFromPrior}, false}, UnitRange{Int64}}})(tid::Int64; onethread::Bool)\n        @ Main.Notebook ./threadingconstructs.jl:253\n      [9] #88#threadsfor_fun\n        @ ./threadingconstructs.jl:220 [inlined]\n     [10] (::Base.Threads.var\"#1#2\"{var\"#88#threadsfor_fun#8\"{var\"#88#threadsfor_fun#7#9\"{DynamicPPL.Model{typeof(threaded_assume_bad), (:N,), (), (), Tuple{Int64}, Tuple{}, DynamicPPL.InitContext{Random.TaskLocalRNG, InitFromPrior}, false}, UnitRange{Int64}}}, Int64})()\n        @ Base.Threads ./threadingconstructs.jl:154\n\n...and 3 more exceptions.\n\nStacktrace:\n  [1] threading_run(fun::var\"#88#threadsfor_fun#8\"{var\"#88#threadsfor_fun#7#9\"{DynamicPPL.Model{typeof(threaded_assume_bad), (:N,), (), (), Tuple{Int64}, Tuple{}, DynamicPPL.InitContext{Random.TaskLocalRNG, InitFromPrior}, false}, UnitRange{Int64}}}, static::Bool)\n    @ Base.Threads ./threadingconstructs.jl:173\n  [2] macro expansion\n    @ ./threadingconstructs.jl:190 [inlined]\n  [3] threaded_assume_bad\n    @ ~/work/docs/docs/usage/threadsafe-evaluation/index.qmd:140 [inlined]\n  [4] _evaluate!!\n    @ ~/.julia/packages/DynamicPPL/Hza15/src/model.jl:997 [inlined]\n  [5] evaluate!!\n    @ ~/.julia/packages/DynamicPPL/Hza15/src/model.jl:983 [inlined]\n  [6] init!!\n    @ ~/.julia/packages/DynamicPPL/Hza15/src/model.jl:938 [inlined]\n  [7] init!!\n    @ ~/.julia/packages/DynamicPPL/Hza15/src/model.jl:936 [inlined]\n  [8] Model\n    @ ~/.julia/packages/DynamicPPL/Hza15/src/model.jl:911 [inlined]\n  [9] (::DynamicPPL.Model{typeof(threaded_assume_bad), (:N,), (), (), Tuple{Int64}, Tuple{}, DynamicPPL.DefaultContext, false})()\n    @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/model.jl:904\n [10] top-level scope\n    @ ~/work/docs/docs/usage/threadsafe-evaluation/index.qmd:150",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Threadsafe Evaluation"
    ]
  },
  {
    "objectID": "usage/threadsafe-evaluation/index.html#when-is-threadsafe-evaluation-really-needed",
    "href": "usage/threadsafe-evaluation/index.html#when-is-threadsafe-evaluation-really-needed",
    "title": "Threadsafe Evaluation",
    "section": "When is threadsafe evaluation really needed?",
    "text": "When is threadsafe evaluation really needed?\nYou only need to enable threadsafe evaluation if you are using tilde-statements or @addlogprob! inside threaded blocks.\nSpecifically, you do not need to enable threadsafe evaluation if:\n\nYou have parallelism inside the model, but it does not involve tilde-statements or @addlogprob!.\n@model function parallel_no_tilde(y)\n    x ~ Normal()\n    fy = similar(y)\n    Threads.@threads for i in eachindex(y)\n        fy[i] = some_expensive_function(x, y[i])\n    end\nend\n# This does not need setthreadsafe\nmodel = parallel_no_tilde(y)\nYou are sampling from a model using MCMCThreads(), but the model itself does not contain any parallel tilde-statements or @addlogprob!.\n@model function no_parallel(y)\n    x ~ Normal()\n    y ~ Normal(x)\nend\n\n# This does not need setthreadsafe\nmodel = no_parallel(1.0)\nchn = sample(model, NUTS(), MCMCThreads(), 100)",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Threadsafe Evaluation"
    ]
  },
  {
    "objectID": "usage/threadsafe-evaluation/index.html#performance-considerations",
    "href": "usage/threadsafe-evaluation/index.html#performance-considerations",
    "title": "Threadsafe Evaluation",
    "section": "Performance considerations",
    "text": "Performance considerations\nAs described above, one of the major considerations behind the introduction of setthreadsafe is that threadsafe evaluation comes with a performance cost.\nConsider a simple model that does not use threading:\n\n@model function gdemo()\n    s ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s))\n    1.5 ~ Normal(m, sqrt(s))\n    2.0 ~ Normal(m, sqrt(s))\nend\nmodel_no_threadsafe = gdemo()\nmodel_threadsafe = setthreadsafe(gdemo(), true)\n\nDynamicPPL.Model{typeof(gdemo), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, true}(gdemo, NamedTuple(), NamedTuple(), DynamicPPL.DefaultContext())\n\n\nOne can see that evaluation of the threadsafe model is substantially slower:\n\nusing Chairmarks, DynamicPPL\n\nfunction benchmark_eval(m)\n    vi = VarInfo(m)\n    display(median(@be DynamicPPL.evaluate!!($m, $vi)))\nend\n\nbenchmark_eval(model_no_threadsafe)\nbenchmark_eval(model_threadsafe)\n\n280.938 ns (8 allocs: 464 bytes)\n\n\n3.326 μs (49 allocs: 2.766 KiB)\n\n\nIn previous versions of Turing, this cost would always be incurred whenever Julia was launched with multiple threads, even if the model did not use any threading at all!",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Threadsafe Evaluation"
    ]
  },
  {
    "objectID": "usage/threadsafe-evaluation/index.html#alternatives-to-threaded-observation",
    "href": "usage/threadsafe-evaluation/index.html#alternatives-to-threaded-observation",
    "title": "Threadsafe Evaluation",
    "section": "Alternatives to threaded observation",
    "text": "Alternatives to threaded observation\nAn alternative to using threaded observations is to manually calculate the log-likelihood term (which can be parallelised using any of Julia’s standard mechanisms), and then outside of the threaded block, add it to the model using @addlogprob!.\nFor example:\n\n# Note that `y` has to be passed as an argument; you can't\n# condition on it because otherwise `y[i]` won't be defined.\n@model function threaded_obs_addlogprob(N, y)\n    x ~ Normal()\n\n    # Instead of this:\n    # Threads.@threads for i in 1:N\n    #     y[i] ~ Normal(x)\n    # end\n\n    # Do this instead:\n    lls = map(1:N) do i\n        Threads.@spawn begin\n            logpdf(Normal(x), y[i])\n        end\n    end\n    @addlogprob! sum(fetch.(lls))\nend\n\nthreaded_obs_addlogprob (generic function with 2 methods)\n\n\nIn a similar way, you can also use your favourite parallelism package, such as FLoops.jl or OhMyThreads.jl. See this Discourse post for some examples.\nWe make no promises about the use of tilde-statements with these packages (indeed it will most likely error), but as long as you use them to only parallelise regular Julia code (i.e., not tilde-statements), they will work as intended.\nThe main downside of this approach is:\n\nYou can’t use conditioning syntax to provide data; it has to be passed as an argument or otherwise included inside the model.\nYou can’t use predict to sample new data.\n\nOn the other hand, one benefit of rewriting the model this way is that sampling from this model with MCMCThreads() will always be reproducible.\n\nusing Random\nN = 100\ny = randn(N)\n# Note that since `@addlogprob!` is outside of the threaded block, we don't\n# need to use `setthreadsafe`.\nmodel = threaded_obs_addlogprob(N, y)\nnuts_kwargs = (progress=false, verbose=false)\n\nchain1 = sample(Xoshiro(468), model, NUTS(), MCMCThreads(), 1000, 4; nuts_kwargs...)\nchain2 = sample(Xoshiro(468), model, NUTS(), MCMCThreads(), 1000, 4; nuts_kwargs...)\nmean(chain1[:x]), mean(chain2[:x])  # should be identical\n\n(0.15297293739916906, 0.15297293739916906)\n\n\nIn contrast, the original threaded_obs (which used tilde inside Threads.@threads) is not reproducible when using MCMCThreads(). (In principle, we would like to fix this bug, but we haven’t yet investigated where it stems from.)\n\nmodel = setthreadsafe(threaded_obs(N) | (; y = y), true)\nnuts_kwargs = (progress=false, verbose=false)\nchain1 = sample(Xoshiro(468), model, NUTS(), MCMCThreads(), 1000, 4; nuts_kwargs...)\nchain2 = sample(Xoshiro(468), model, NUTS(), MCMCThreads(), 1000, 4; nuts_kwargs...)\nmean(chain1[:x]), mean(chain2[:x])  # oops!\n\n(0.1518861047621763, 0.15635838566436852)",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Threadsafe Evaluation"
    ]
  },
  {
    "objectID": "usage/threadsafe-evaluation/index.html#ad-support",
    "href": "usage/threadsafe-evaluation/index.html#ad-support",
    "title": "Threadsafe Evaluation",
    "section": "AD support",
    "text": "AD support\nFinally, if you are using Turing with automatic differentiation, you also need to keep track of which AD backends support threadsafe evaluation.\nForwardDiff is the only AD backend that we find to work reliably with threaded model evaluation.\nIn particular:\n\nReverseDiff sometimes gives right results, but quite often gives incorrect gradients.\nMooncake currently does not support multithreading at all.\nEnzyme mostly gives the right result, but sometimes gives incorrect gradients.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Threadsafe Evaluation"
    ]
  },
  {
    "objectID": "usage/threadsafe-evaluation/index.html#under-the-hood",
    "href": "usage/threadsafe-evaluation/index.html#under-the-hood",
    "title": "Threadsafe Evaluation",
    "section": "Under the hood",
    "text": "Under the hood\n\n\n\n\n\n\nNote\n\n\n\nThis part will likely only be of interest to DynamicPPL developers and the very curious user.\n\n\n\nWhy is VarInfo not threadsafe?\nAs alluded to above, the issue with threaded tilde-statements stems from the fact that these tilde-statements modify the VarInfo object used for model evaluation, leading to potential data races.\nTraditionally, VarInfo objects contain both metadata as well as accumulators. Metadata is where information about the random variables’ values are stored. It is a Dict-like structure, and pushing to it from multiple threads is therefore not threadsafe (Julia’s Dict has similar limitations).\nOn the other hand, accumulators are used to store outputs of the model, such as log-probabilities The way DynamicPPL’s threadsafe evaluation works is to create one set of accumulators per thread, and then combine the results at the end of model evaluation.\nIn this way, any function call that solely involving accumulators can be made threadsafe. For example, this is why observations are supported: there is no need to modify metadata, and only the log-likelihood accumulator needs to be updated.\nHowever, assume tilde-statements always modify the metadata, and thus cannot currently be made threadsafe.\n\n\nOnlyAccsVarInfo\nAs it happens, much of what is needed in DynamicPPL can be constructed such that they only rely on accumulators.\nFor example, as long as there is no need to sample new values of random variables, it is actually fine to completely omit the metadata object. This is the case for LogDensityFunction: since values are provided as the input vector, there is no need to store it in metadata. We need only calculate the associated log-prior probability, which is stored in an accumulator. Thus, since DynamicPPL v0.39, LogDensityFunction itself is completely threadsafe.\nTechnically speaking, this is achieved using OnlyAccsVarInfo, which is a subtype of VarInfo that only contains accumulators, and no metadata at all. It implements enough of the VarInfo interface to be used in model evaluation, but will error if any functions attempt to modify or read its metadata.\nThere is currently an ongoing push to use OnlyAccsVarInfo in as many settings as we possibly can. For example, this is why predict is threadsafe in DynamicPPL v0.39: instead of modifying metadata to store the predicted values, we store them inside a ValuesAsInModelAccumulator instead, and combine them at the end of evaluation.\nHowever, propagating these changes up to Turing will require a substantial amount of additional work, since there are many places in Turing which currently rely on a full VarInfo (with metadata). See, e.g., this PR for more information.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Threadsafe Evaluation"
    ]
  },
  {
    "objectID": "usage/submodels/index.html",
    "href": "usage/submodels/index.html",
    "title": "Submodels",
    "section": "",
    "text": "using Turing\nusing Random: Xoshiro, seed!\nseed!(468)\n\nRandom.TaskLocalRNG()\nIn Turing.jl, you can define models and use them as components of larger models (i.e., submodels), using the to_submodel function. In this way, you can (for example) define a model once and use it in multiple places:\n@model function inner()\n    a ~ Normal()\n    return a + 100\nend\n\n@model function outer()\n    # This line adds the variable `x.a` to the chain.\n    # The inner variable `a` is prefixed with the\n    # left-hand side of the `~` operator, i.e. `x`.\n    x ~ to_submodel(inner())\n    # Here, the value of x will be `a + 100` because\n    # that is the return value of the submodel.\n    b ~ Normal(x)\nend\n\nouter (generic function with 2 methods)\nIf we sample from this model, we would expect that x.a should be close to zero, and b close to 100:\nrand(outer())\n\n(var\"x.a\" = 0.07200886749732076, b = 99.9979651109378)",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Submodels"
    ]
  },
  {
    "objectID": "usage/submodels/index.html#manipulating-submodels",
    "href": "usage/submodels/index.html#manipulating-submodels",
    "title": "Submodels",
    "section": "Manipulating submodels",
    "text": "Manipulating submodels\n\nConditioning\nIn general, everything that can be done to a model ‘carries over’ to when it is used as a submodel. For example, you can condition a variable in a submodel in two ways:\n\n# From the outside; the prefix `x` must be applied because\n# from the perspective of `outer`, the variable is called\n# `x.a`.\nouter_conditioned1 = outer() | (@varname(x.a) =&gt; 1);\nrand(Xoshiro(468), outer_conditioned1)\n\n(b = 101.07200886749732,)\n\n\nOr equivalently, from the inside:\n\n@model function outer_conditioned2()\n    # The prefix doesn't need to be applied here because\n    # `inner` itself has no knowledge of the prefix.\n    x ~ to_submodel(inner() | (@varname(a) =&gt; 1))\n    b ~ Normal(x)\nend\nrand(Xoshiro(468), outer_conditioned2())\n\n(b = 101.07200886749732,)\n\n\nIn both cases the variable x.a does not appear.\nNote, however, that you cannot condition on the return value of a submodel. Thus, for example, if we had:\n\n@model function inner_sensible()\n    a ~ Normal()\n    return a\nend\n\n@model function outer()\n    x ~ to_submodel(inner())\n    b ~ Normal(x)\nend\n\nouter (generic function with 2 methods)\n\n\nand we tried to condition on x, it would be silently ignored, even though x is equal to a.\nThe reason for this is because it is entirely coincidental that the return value of the submodel is equal to a. In general, a return value can be anything, and conditioning on it is in general not a meaningful operation.\n\n\nPrefixing\nPrefixing is the only place where submodel behaviour is ‘special’ compared to that of ordinary models.\nBy default, all variables in a submodel are prefixed with the left-hand side of the tilde-statement. This is done to avoid clashes if the same submodel is used multiple times in a model.\nYou can disable this by passing false as the second argument to to_submodel:\n\n@model function outer_no_prefix()\n    x ~ to_submodel(inner(), false)\n    b ~ Normal(x)\nend\nrand(outer_no_prefix())\n\n(a = 0.6327762377562545, b = 99.65279863588333)",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Submodels"
    ]
  },
  {
    "objectID": "usage/submodels/index.html#accessing-submodel-variables",
    "href": "usage/submodels/index.html#accessing-submodel-variables",
    "title": "Submodels",
    "section": "Accessing submodel variables",
    "text": "Accessing submodel variables\nIn all of the examples above, x is equal to a + 100 because that is the return value of the submodel. To access the actual latent variables in the submodel itself, the simplest option is to include the variable in the return value of the submodel:\n\n@model function inner_with_retval()\n    a ~ Normal()\n    # You can return anything you like from the model,\n    # but if you want to access the latent variables, they\n    # should be included in the return value.\n    return (; a=a, a_plus_100=a + 100)\nend\n@model function outer_with_retval()\n    # The variable `x` will now contain the return value of the submodel,\n    # which is a named tuple with `a` and `a_plus_100`.\n    x ~ to_submodel(inner_with_retval())\n    # You can access the value of x.a directly, because\n    # x is a NamedTuple which contains `a`. Since `b` is\n    # centred on `x.a`, it should be close to 0, not 100.\n    b ~ Normal(x.a)\nend\nrand(Xoshiro(468), outer_with_retval())\n\n(var\"x.a\" = 0.07200886749732076, b = -0.0020348890621966487)\n\n\nYou can also manually access the value by looking inside the special __varinfo__ object.\n\n\n\n\n\n\nWarning\n\n\n\nThis relies on DynamicPPL internals and we do not recommend doing this unless you have no other option, e.g., if the submodel is defined in a different package which you do not control.\n\n\n\n@model function outer_with_varinfo()\n    x ~ to_submodel(inner())\n    # Access the value of x.a\n    a_value = __varinfo__[@varname(x.a)]\n    b ~ Normal(a_value)\nend\nrand(Xoshiro(468), outer_with_varinfo())\n\n(var\"x.a\" = 0.07200886749732076, b = -0.0020348890621966487)",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Submodels"
    ]
  },
  {
    "objectID": "usage/submodels/index.html#example-linear-models",
    "href": "usage/submodels/index.html#example-linear-models",
    "title": "Submodels",
    "section": "Example: linear models",
    "text": "Example: linear models\nHere is a motivating example for the use of submodels. Suppose we want to fit a (very simplified) regression model to some data \\(x\\) and \\(y\\), where\n\\[\\begin{align}\nc_0 &\\sim \\text{Normal}(0, 5) \\\\\nc_1 &\\sim \\text{Normal}(0, 5) \\\\\n\\mu &= c_0 + c_1x \\\\\ny &\\sim d\n\\end{align}\\]\nwhere \\(d\\) is some distribution parameterised by the value of \\(\\mu\\), which we don’t know the exact form of.\nIn practice, what we would do is to write several different models, one for each function \\(f\\):\n\n@model function normal(x, y)\n    c0 ~ Normal(0, 5)\n    c1 ~ Normal(0, 5)\n    mu = c0 .+ c1 .* x\n    # Assume that y = mu, and that the noise in `y` is\n    # normally distributed with standard deviation sigma\n    sigma ~ truncated(Cauchy(0, 3); lower=0)\n    for i in eachindex(y)\n        y[i] ~ Normal(mu[i], sigma)\n    end\nend\n\n@model function logpoisson(x, y)\n    c0 ~ Normal(0, 5)\n    c1 ~ Normal(0, 5)\n    mu = c0 .+ c1 .* x\n    # exponentiate mu because the rate parameter of\n    # a Poisson distribution must be positive\n    for i in eachindex(y)\n        y[i] ~ Poisson(exp(mu[i]))\n    end\nend\n\n# and so on...\n\nlogpoisson (generic function with 2 methods)\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou could use arraydist to avoid the loops: for example, in logpoisson, one could write y ~ arraydist(Poisson.(exp.(mu))), but for simplicity in this tutorial we spell it out fully.\n\n\nWe would then fit all of our models and use some criterion to test which model is most suitable (see e.g. Wikipedia, or section 3.4 of Bishop’s Pattern Recognition and Machine Learning).\nHowever, the code above is quite repetitive. For example, if we wanted to adjust the priors on c0 and c1, we would have to do it in each model separately. If this was any other kind of code, we would naturally think of extracting the common parts into a separate function. In this case we can do exactly that with a submodel:\n\n@model function priors(x)\n    c0 ~ Normal(0, 5)\n    c1 ~ Normal(0, 5)\n    mu = c0 .+ c1 .* x\n    return (; c0=c0, c1=c1, mu=mu)\nend\n\n@model function normal(x, y)\n    ps = to_submodel(priors(x))\n    sigma ~ truncated(Cauchy(0, 3); lower=0)\n    for i in eachindex(y)\n        y[i] ~ Normal(ps.mu[i], sigma)\n    end\nend\n\n@model function logpoisson(x, y)\n    ps = to_submodel(priors(x))\n    for i in eachindex(y)\n        y[i] ~ Poisson(exp(ps.mu[i]))\n    end\nend\n\nlogpoisson (generic function with 2 methods)\n\n\nOne could go even further and extract the y section into its own submodel as well, which would bring us to a generalised linear modelling interface that does not actually require the user to define their own Turing models at all:\n\n@model function normal_family(mu, y)\n    sigma ~ truncated(Cauchy(0, 3); lower=0)\n    for i in eachindex(y)\n        y[i] ~ Normal(mu[i], sigma)\n    end\n    return nothing\nend\n\n@model function logpoisson_family(mu, y)\n    for i in eachindex(y)\n        y[i] ~ Poisson(exp(mu[i]))\n    end\n    return nothing\nend\n\n# An end-user could just use this function. Of course,\n# a more thorough interface would also allow the user to\n# specify priors, etc.\nfunction make_model(x, y, family::Symbol)\n    if family == :normal\n        family_model = normal_family\n    elseif family == :logpoisson\n        family_model = logpoisson_family\n    else\n        error(\"unknown family: `$family`\")\n    end\n\n    @model function general(x, y)\n        ps ~ to_submodel(priors(x), false)\n        _n ~ to_submodel(family_model(ps.mu, y), false)\n    end\n    return general(x, y)\nend\n\nsample(make_model([1, 2, 3], [1, 2, 3], :normal), NUTS(), 1000; progress=false)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.8\n\n\n\n\nChains MCMC chain (1000×17×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 8.03 seconds\nCompute duration  = 8.03 seconds\nparameters        = c0, c1, sigma\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nWhile this final example really showcases the composability of submodels, it also illustrates a minor syntactic drawback. When we create a submodel from family_model(ps.mu, y), in principle, we do not really care about its return value because it is not used anywhere else in the model. Ideally, we should therefore not need to place anything on the left-hand side of to_submodel. However, because the special behaviour of to_submodel relies on the tilde operator, and the tilde operator requires a left-hand side, we have to use a dummy variable (here _n).\nFurthermore, because the left-hand side of a tilde-statement must be a valid variable name, we cannot use destructuring syntax on the left-hand side of to_submodel, even if the return value is a NamedTuple. Thus, for example, the following is not allowed:\n(; c0, c1, mu) ~ to_submodel(priors(x))\nTo use destructuring syntax, you would have to add a separate line:\nps = to_submodel(priors(x))\n(; c0, c1, mu) = ps",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Submodels"
    ]
  },
  {
    "objectID": "usage/submodels/index.html#submodels-versus-distributions",
    "href": "usage/submodels/index.html#submodels-versus-distributions",
    "title": "Submodels",
    "section": "Submodels versus distributions",
    "text": "Submodels versus distributions\nFinally, we end with a discussion of why some of the behaviour for submodels above has come about. This is slightly more behind-the-scenes and therefore will likely be of most interest to Turing developers.\nFundamentally, submodels are to be compared against distributions: both of them can appear on the right-hand side of a tilde statement. However, distributions only have one ‘output’, i.e., the value that is sampled from them:\n\ndist = Normal()\nrand(dist)\n\n0.38140870041648106\n\n\nAnother point to bear in mind is that, given a sample from dist, asking for its log-probability is a meaningful calculation.\n\nlogpdf(dist, rand(dist))\n\n-0.9501197128359701\n\n\nIn contrast, models (and hence submodels) have two different outputs: the latent variables, and the return value. These are accessed respectively using rand(model) and model():\n\n@model function f()\n    a ~ Normal()\n    return \"hello, world.\"\nend\n\nmodel = f()\n\nDynamicPPL.Model{typeof(f), (), (), (), Tuple{}, Tuple{}, DynamicPPL.DefaultContext, false}(f, NamedTuple(), NamedTuple(), DynamicPPL.DefaultContext())\n\n\n\n# Latent variables\nrand(model)\n\n(a = -0.9608421769747073,)\n\n\n\n# Return value\nmodel()\n\n\"hello, world.\"\n\n\nJust like for distributions, one can indeed ask for the log-probability of the latent variables (although we have to specify whether we want the joint, likelihood, or prior):\n\nlogjoint(model, rand(model))\n\n-2.5510092354699627\n\n\nBut it does not make sense to ask for the log-probability of the return value (which in this case is a string, and in general, could be literally any object).\nThe fact that we have what looks like a unified notation for these is a bit of a lie, since it hides this distinction. In particular, for x ~ distr, x is assigned the value of rand(distr); but for y ~ submodel, y is assigned the value of submodel(). This is why, for example, it is impossible to condition on y in y ~ ...; we can only condition on x in x ~ dist.\nEventually we would like to make this more logically consistent. In particular, it is clear that y ~ submodel should return not one but two objects: the latent variables and the return value. Furthermore, it should be possible to condition on the latent variables, but not on the return value. See this issue for an ongoing discussion of the best way to accomplish this.\nIt should be mentioned that extracting the latent variables from a submodel is not entirely trivial since the submodel is run using the same VarInfo as the parent model (i.e., we would have to do a before-and-after comparison to see which new variables were added by the submodel).\nAlso, we are still working out the exact data structure that should be used to represent the latent variables. In the examples above rand(model) returns a NamedTuple, but this actually causes loss of information because the keys of a NamedTuple are Symbols, whereas we really want to use VarNames. See this issue for a current proposal.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Submodels"
    ]
  },
  {
    "objectID": "usage/mode-estimation/index.html",
    "href": "usage/mode-estimation/index.html",
    "title": "Mode Estimation",
    "section": "",
    "text": "After defining a statistical model, in addition to sampling from its distributions, one may be interested in finding the parameter values that maximise for instance the posterior distribution density function or the likelihood. This is called mode estimation. Turing provides support for two mode estimation techniques, maximum likelihood estimation (MLE) and maximum a posteriori (MAP) estimation.\nTo demonstrate mode estimation, let us load Turing and declare a model:\nusing Turing\n\n@model function gdemo(x)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n\n    for i in eachindex(x)\n        x[i] ~ Normal(m, sqrt(s²))\n    end\nend\n\ngdemo (generic function with 2 methods)\nOnce the model is defined, we can construct a model instance as we normally would:\n# Instantiate the gdemo model with our data.\ndata = [1.5, 2.0]\nmodel = gdemo(data)\n\nDynamicPPL.Model{typeof(gdemo), (:x,), (), (), Tuple{Vector{Float64}}, Tuple{}, DynamicPPL.DefaultContext, false}(gdemo, (x = [1.5, 2.0],), NamedTuple(), DynamicPPL.DefaultContext())\nFinding the maximum a posteriori or maximum likelihood parameters is as simple as\n# Generate a MLE estimate.\nmle_estimate = maximum_likelihood(model)\n\n# Generate a MAP estimate.\nmap_estimate = maximum_a_posteriori(model)\n\nModeResult with maximized lp of -4.62\n[0.9074074074066046, 1.1666666666668017]\nThe estimates are returned as instances of the ModeResult type. It has the fields values for the parameter values found and lp for the log probability at the optimum, as well as f for the objective function and optim_result for more detailed results of the optimisation procedure.\n@show mle_estimate.values\n@show mle_estimate.lp;\n\nmle_estimate.values = [0.06249999999685591, 1.7500000000022418]\nmle_estimate.lp = -0.0652883441695642",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Mode Estimation"
    ]
  },
  {
    "objectID": "usage/mode-estimation/index.html#controlling-the-optimisation-process",
    "href": "usage/mode-estimation/index.html#controlling-the-optimisation-process",
    "title": "Mode Estimation",
    "section": "Controlling the optimisation process",
    "text": "Controlling the optimisation process\nUnder the hood maximum_likelihood and maximum_a_posteriori use the Optimisation.jl package, which provides a unified interface to many other optimisation packages. By default Turing typically uses the LBFGS method from Optim.jl to find the mode estimate, but we can easily change that:\n\nusing OptimizationOptimJL: NelderMead\n@show maximum_likelihood(model, NelderMead())\n\nusing OptimizationNLopt: NLopt.LD_TNEWTON_PRECOND_RESTART\n@show maximum_likelihood(model, LD_TNEWTON_PRECOND_RESTART());\n\n\nmaximum_likelihood(model, NelderMead()) = [0.062494086008820525, 1.7500103636565552]\n┌ Warning: The selected optimization algorithm requires second order derivatives, but `SecondOrder` ADtype was not provided.\n│         So a `SecondOrder` with ADTypes.AutoForwardDiff() for both inner and outer will be created, this can be suboptimal and not work in some cases so\n│         an explicit `SecondOrder` ADtype is recommended.\n└ @ OptimizationBase ~/.julia/packages/OptimizationBase/R2xIG/src/cache.jl:51\nmaximum_likelihood(model, LD_TNEWTON_PRECOND_RESTART()) = [0.06249999999999959, 1.7500000000000007]\n\n\n\n\nThe above are just two examples, Optimisation.jl supports many more.\nWe can also help the optimisation by giving it a starting point we know is close to the final solution, or by specifying an automatic differentiation method\n\nimport Mooncake\n\nmaximum_likelihood(\n    model, NelderMead(); initial_params=[0.1, 2], adtype=AutoMooncake()\n)\n\nModeResult with maximized lp of -0.07\n[0.062494553692639856, 1.7500042095865365]\n\n\nWhen providing values to arguments like initial_params the parameters are typically specified in the order in which they appear in the code of the model, so in this case first s² then m. More precisely it’s the order returned by Turing.Inference.getparams(model, DynamicPPL.VarInfo(model)).\n\n\n\n\n\n\nNoteInitialisation strategies and consistency with MCMC sampling\n\n\n\nSince Turing v0.41, for MCMC sampling, the initial_params argument must be a DynamicPPL.AbstractInitStrategy as described in the sampling options page). The optimisation interface has not yet been updated to use this; thus, initial parameters are still specified as Vectors. We expect that this will be changed in the near future.\n\n\nWe can also do constrained optimisation, by providing either intervals within which the parameters must stay, or constraint functions that they need to respect. For instance, here’s how one can find the MLE with the constraint that the variance must be less than 0.01 and the mean must be between -1 and 1.:\n\nmaximum_likelihood(model; lb=[0.0, -1.0], ub=[0.01, 1.0])\n\nModeResult with maximized lp of -59.73\n[0.009999999997180547, 0.9999999998844025]\n\n\nThe arguments for lower (lb) and upper (ub) bounds follow the arguments of Optimisation.OptimizationProblem, as do other parameters for providing constraints, such as cons. Any extraneous keyword arguments given to maximum_likelihood or maximum_a_posteriori are passed to Optimisation.solve. Some often useful ones are maxiters for controlling the maximum number of iterations and abstol and reltol for the absolute and relative convergence tolerances:\n\nbadly_converged_mle = maximum_likelihood(\n    model, NelderMead(); maxiters=10, reltol=1e-9\n)\n\nModeResult with maximized lp of -2.55\n[1.9634023598525399, 1.8167262287596853]\n\n\nWe can check whether the optimisation converged using the optim_result field of the result:\n\n@show badly_converged_mle.optim_result;\n\nbadly_converged_mle.optim_result = retcode: Failure\nu: [0.6746788661969938, 1.8167262287596853]\nFinal objective value:     2.546656120613406\n\n\n\nFor more details, such as a full list of possible arguments, we encourage the reader to read the docstring of the function Turing.Optimisation.estimate_mode, which is what maximum_likelihood and maximum_a_posteriori call, and the documentation of Optimisation.jl.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Mode Estimation"
    ]
  },
  {
    "objectID": "usage/mode-estimation/index.html#analyzing-your-mode-estimate",
    "href": "usage/mode-estimation/index.html#analyzing-your-mode-estimate",
    "title": "Mode Estimation",
    "section": "Analyzing your mode estimate",
    "text": "Analyzing your mode estimate\nTuring extends several methods from StatsBase that can be used to analyse your mode estimation results. Methods implemented include vcov, informationmatrix, coeftable, params, and coef, among others.\nFor example, let’s examine our ML estimate from above using coeftable:\n\nusing StatsBase: coeftable\ncoeftable(mle_estimate)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoef.\nStd. Error\nz\nPr(&gt;\nz\n)\n\n\n\n\ns²\n0.0625\n0.0625\n1.0\n0.317311\n-0.0599977\n0.184998\n\n\nm\n1.75\n0.176777\n9.89949\n4.18383e-23\n1.40352\n2.09648\n\n\n\n\n\nStandard errors are calculated from the Fisher information matrix (inverse Hessian of the log likelihood or log joint). Note that standard errors calculated in this way may not always be appropriate for MAP estimates, so please be cautious in interpreting them.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Mode Estimation"
    ]
  },
  {
    "objectID": "usage/mode-estimation/index.html#sampling-with-the-mapmle-as-initial-states",
    "href": "usage/mode-estimation/index.html#sampling-with-the-mapmle-as-initial-states",
    "title": "Mode Estimation",
    "section": "Sampling with the MAP/MLE as initial states",
    "text": "Sampling with the MAP/MLE as initial states\nYou can begin sampling your chain from an MLE/MAP estimate by wrapping it in InitFromParams and providing it to the sample function with the keyword initial_params. For example, here is how to sample from the full posterior using the MAP estimate as the starting point:\n\nmap_estimate = maximum_a_posteriori(model)\nchain = sample(model, NUTS(), 1_000; initial_params=InitFromParams(map_estimate))",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Mode Estimation"
    ]
  },
  {
    "objectID": "usage/custom-distribution/index.html",
    "href": "usage/custom-distribution/index.html",
    "title": "Custom Distributions",
    "section": "",
    "text": "Turing.jl supports the use of distributions from the Distributions.jl package. By extension, it also supports the use of customised distributions by defining them as subtypes of the Distribution type in the Distributions.jl package, as well as corresponding functions.\nThis page shows a workflow of how to define a customised distribution, using our own implementation of a simple Uniform distribution as a simple example.\nusing Distributions, Turing, Random, Bijectors",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Custom Distributions"
    ]
  },
  {
    "objectID": "usage/custom-distribution/index.html#define-the-distribution-type",
    "href": "usage/custom-distribution/index.html#define-the-distribution-type",
    "title": "Custom Distributions",
    "section": "Define the Distribution Type",
    "text": "Define the Distribution Type\nFirst, define a type of the distribution, as a subtype of a corresponding distribution type in the Distributions.jl package.\n\nstruct CustomUniform &lt;: ContinuousUnivariateDistribution end",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Custom Distributions"
    ]
  },
  {
    "objectID": "usage/custom-distribution/index.html#implement-sampling-and-evaluation-of-the-log-pdf",
    "href": "usage/custom-distribution/index.html#implement-sampling-and-evaluation-of-the-log-pdf",
    "title": "Custom Distributions",
    "section": "Implement Sampling and Evaluation of the log-pdf",
    "text": "Implement Sampling and Evaluation of the log-pdf\nSecond, implement the rand and logpdf functions for your new distribution, which will be used to run the model.\n\n# sample in [0, 1]\nDistributions.rand(rng::AbstractRNG, d::CustomUniform) = rand(rng)\n\n# p(x) = 1 → log[p(x)] = 0\nDistributions.logpdf(d::CustomUniform, x::Real) = zero(x)",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Custom Distributions"
    ]
  },
  {
    "objectID": "usage/custom-distribution/index.html#define-helper-functions",
    "href": "usage/custom-distribution/index.html#define-helper-functions",
    "title": "Custom Distributions",
    "section": "Define Helper Functions",
    "text": "Define Helper Functions\nIn most cases, it may be required to define some helper functions.\n\nDomain Transformation\nCertain samplers, such as HMC, require the domain of the priors to be unbounded. Therefore, to use our CustomUniform as a prior in a model we also need to define how to transform samples from [0, 1] to ℝ. To do this, we need to define the corresponding Bijector from Bijectors.jl, which is what Turing.jl uses internally to deal with constrained distributions.\nTo transform from [0, 1] to ℝ we can use the Logit bijector:\n\nBijectors.bijector(d::CustomUniform) = Logit(0.0, 1.0)\n\nIn the present example, CustomUniform is a subtype of ContinuousUnivariateDistribution. The procedure for subtypes of ContinuousMultivariateDistribution and ContinuousMatrixDistribution is exactly the same. For example, Wishart defines a distribution over positive-definite matrices and so the bijector returns a PDBijector when called with a Wishart distribution as an argument. For discrete distributions, there is no need to define a bijector; the Identity bijector is used by default.\nAs an alternative to the above, for UnivariateDistribution we could define the minimum and maximum of the distribution:\n\nDistributions.minimum(d::CustomUniform) = 0.0\nDistributions.maximum(d::CustomUniform) = 1.0\n\nand Bijectors.jl will return a default Bijector called TruncatedBijector which makes use of minimum and maximum to derive the correct transformation.\nInternally, Turing basically does the following when it needs to convert a constrained distribution to an unconstrained distribution, e.g. when sampling using HMC:\n\ndist = Gamma(2,3)\nb = bijector(dist)\ntransformed_dist = transformed(dist, b) # results in distribution with transformed support + correction for logpdf\n\nBijectors.UnivariateTransformed{Distributions.Gamma{Float64}, Base.Fix1{typeof(broadcast), typeof(log)}}(\ndist: Distributions.Gamma{Float64}(α=2.0, θ=3.0)\ntransform: Base.Fix1{typeof(broadcast), typeof(log)}(broadcast, log)\n)\n\n\nand then we can call rand and logpdf as usual, where - rand(transformed_dist) returns a sample in the unconstrained space, and - logpdf(transformed_dist, y) returns the log density of the original distribution, but with y living in the unconstrained space.\nTo read more about Bijectors.jl, check out its documentation.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Custom Distributions"
    ]
  },
  {
    "objectID": "usage/performance-tips/index.html",
    "href": "usage/performance-tips/index.html",
    "title": "Performance Tips",
    "section": "",
    "text": "This section briefly summarises a few common techniques to ensure good performance when using Turing. We refer to the Julia documentation for general techniques to ensure good performance of Julia programs.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Performance Tips"
    ]
  },
  {
    "objectID": "usage/performance-tips/index.html#use-multivariate-distributions",
    "href": "usage/performance-tips/index.html#use-multivariate-distributions",
    "title": "Performance Tips",
    "section": "Use multivariate distributions",
    "text": "Use multivariate distributions\nIt is generally preferable to use multivariate distributions if possible.\nThe following example:\n\nusing Turing\n@model function gmodel(x)\n    m ~ Normal()\n    for i in eachindex(x)\n        x[i] ~ Normal(m, 0.2)\n    end\nend\n\ngmodel (generic function with 2 methods)\n\n\ncan be directly expressed more efficiently with a simple transformation:\n\nusing FillArrays\n\n@model function gmodel(x)\n    m ~ Normal()\n    return x ~ MvNormal(Fill(m, length(x)), 0.04 * I)\nend\n\ngmodel (generic function with 2 methods)",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Performance Tips"
    ]
  },
  {
    "objectID": "usage/performance-tips/index.html#choose-your-ad-backend",
    "href": "usage/performance-tips/index.html#choose-your-ad-backend",
    "title": "Performance Tips",
    "section": "Choose your AD backend",
    "text": "Choose your AD backend\nAutomatic differentiation (AD) makes it possible to use modern, efficient gradient-based samplers like NUTS and HMC. This, however, also means that using a performant AD system is incredibly important. Turing currently supports several AD backends, including ForwardDiff (the default), Mooncake, and ReverseDiff.\nFor many common types of models, the default ForwardDiff backend performs well, and there is no need to worry about changing it. However, if you need more speed, you can try different backends via the standard ADTypes interface by passing an AbstractADType to the sampler with the optional adtype argument, e.g. NUTS(; adtype = AutoMooncake()).\nGenerally, adtype = AutoForwardDiff() is likely to be the fastest and most reliable for models with few parameters (say, less than 20 or so), while reverse-mode backends such as AutoMooncake() or AutoReverseDiff() will perform better for models with many parameters or linear algebra operations. If in doubt, you can benchmark your model with different backends to see which one performs best. See the Automatic Differentiation page for details.\n\nSpecial care for ReverseDiff with a compiled tape\nFor large models, the fastest option is often ReverseDiff with a compiled tape, specified as adtype=AutoReverseDiff(; compile=true). However, it is important to note that if your model contains any branching code, such as if-else statements, the gradients from a compiled tape may be inaccurate, leading to erroneous results. If you use this option for the (considerable) speedup it can provide, make sure to check your code for branching and ensure that it does not affect the gradients. It is also a good idea to verify your gradients with another backend.",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Performance Tips"
    ]
  },
  {
    "objectID": "usage/performance-tips/index.html#ensure-that-types-in-your-model-can-be-inferred",
    "href": "usage/performance-tips/index.html#ensure-that-types-in-your-model-can-be-inferred",
    "title": "Performance Tips",
    "section": "Ensure that types in your model can be inferred",
    "text": "Ensure that types in your model can be inferred\nFor efficient gradient-based inference, e.g. using HMC, NUTS or ADVI, it is important to ensure the types in your model can be inferred.\nThe following example with abstract types\n\n@model function tmodel(x, y)\n    p, n = size(x)\n    params = Vector{Real}(undef, n)\n    for i in 1:n\n        params[i] ~ truncated(Normal(); lower=0)\n    end\n\n    a = x * params\n    return y ~ MvNormal(a, I)\nend\n\ntmodel (generic function with 2 methods)\n\n\ncan be transformed into the following representation with concrete types:\n\n@model function tmodel(x, y, ::Type{T}=Float64) where {T}\n    p, n = size(x)\n    params = Vector{T}(undef, n)\n    for i in 1:n\n        params[i] ~ truncated(Normal(); lower=0)\n    end\n\n    a = x * params\n    return y ~ MvNormal(a, I)\nend\n\ntmodel (generic function with 4 methods)\n\n\nAlternatively, you could use filldist in this example:\n\n@model function tmodel(x, y)\n    params ~ filldist(truncated(Normal(); lower=0), size(x, 2))\n    a = x * params\n    return y ~ MvNormal(a, I)\nend\n\ntmodel (generic function with 4 methods)\n\n\nYou can use DynamicPPL’s debugging utilities to find types in your model definition that the compiler cannot infer. These will be marked in red in the Julia REPL (much like when using the @code_warntype macro).\nFor example, consider the following model:\n\n@model function tmodel(x)\n    p = Vector{Real}(undef, 1)\n    p[1] ~ Normal()\n    p = p .+ 1\n    return x ~ Normal(p[1])\nend\n\ntmodel (generic function with 6 methods)\n\n\nBecause the element type of p is an abstract type (Real), the compiler cannot infer a concrete type for p[1]. To detect this, we can use\n\nmodel = tmodel(1.0)\n\nusing DynamicPPL\nDynamicPPL.DebugUtils.model_warntype(model)\n\nIn this particular model, the following call to getindex should be highlighted in red (the exact numbers may vary):\n[...]\n│    %120 = p::AbstractVector\n│    %121 = Base.getindex(%120, 1)::Any\n[...]",
    "crumbs": [
      "Get Started",
      "User Guide",
      "Performance Tips"
    ]
  },
  {
    "objectID": "developers/models/varinfo-overview/index.html",
    "href": "developers/models/varinfo-overview/index.html",
    "title": "Evaluation of DynamicPPL Models with VarInfo",
    "section": "",
    "text": "Once you have defined a model using the @model macro, Turing.jl provides high-level interfaces for applying MCMC sampling, variational inference, optimisation, and other inference algorithms. Suppose, however, that you want to work more directly with the model. A common use case for this is if you are developing your own inference algorithm.\nThis page describes how you can evaluate DynamicPPL models and obtain information about variable values, log densities, and other quantities of interest. In particular, this provides a high-level overview of what we call VarInfo: this is a data structure that holds information about the execution state while traversing a model.\nTo begin, let’s define a simple model.\nusing DynamicPPL, Distributions\n\n@model function simple()\n    @info \" --- Executing model --- \"\n    x ~ Normal()            # Prior\n    2.0 ~ Normal(x)         # Likelihood\n    return (; xplus1 = x + 1)  # Return value\nend\n\nmodel = simple()\n\nModel{typeof(simple), (), (), (), Tuple{}, Tuple{}, DefaultContext, false}(simple, NamedTuple(), NamedTuple(), DefaultContext())",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Models",
      "Evaluation of DynamicPPL Models with VarInfo"
    ]
  },
  {
    "objectID": "developers/models/varinfo-overview/index.html#the-outputs-of-a-model",
    "href": "developers/models/varinfo-overview/index.html#the-outputs-of-a-model",
    "title": "Evaluation of DynamicPPL Models with VarInfo",
    "section": "The outputs of a model",
    "text": "The outputs of a model\nA DynamicPPL model has similar characteristics to Julia functions (which should not come as a surprise, since the @model macro is applied to a Julia function). However, an ordinary function only has a return value, whereas DynamicPPL models can have both return values as well as latent variables (i.e., the random variables in the model).\nIn general, both of these are of interest. We can obtain the return value by calling the model as if it were a function:\n\nretval = model()\n\n\n[ Info:  --- Executing model --- \n\n\n\n\n(xplus1 = 1.3441994346709203,)\n\n\nand the latent variables using rand():\n\nlatents = rand(Dict, model)\n\n\n[ Info:  --- Executing model --- \n\n\n\n\nDict{VarName, Any} with 1 entry:\n  x =&gt; -0.231571\n\n\n\n\n\n\n\n\nNoteWhy Dict?\n\n\n\nSimply calling rand(model), by default, returns a NamedTuple. This is fine for simple models where all variables on the left-hand side of tilde statements are standalone variables like x. However, if you have indices or fields such as x[1] or x.a on the left-hand side, then the NamedTuple will not be able to represent these variables properly. Feeding such a NamedTuple back into the model will lead to errors.\nIn general, Dict{VarName} will always avoid such correctness issues.\n\n\nBefore proceeding, it is worth mentioning that both of these calls generate values for random variables by sampling from their prior distributions. We will see how to use different sampling strategies later.",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Models",
      "Evaluation of DynamicPPL Models with VarInfo"
    ]
  },
  {
    "objectID": "developers/models/varinfo-overview/index.html#passing-latent-values-into-a-model",
    "href": "developers/models/varinfo-overview/index.html#passing-latent-values-into-a-model",
    "title": "Evaluation of DynamicPPL Models with VarInfo",
    "section": "Passing latent values into a model",
    "text": "Passing latent values into a model\nHaving considered what one can obtain from a model, we now turn to how we can use it.\nSuppose you now want to obtain the log probability (prior, likelihood, or joint) of a model, given certain parameters. For this purpose, DynamicPPL provides the logprior, loglikelihood, and logjoint functions:\n\nlogprior(model, latents)\n\n\n[ Info:  --- Executing model --- \n\n\n\n\n-0.9457510019785719\n\n\nOne can check this against the expected log prior:\n\nlogpdf(Normal(), latents[@varname(x)])\n\n-0.9457510019785719\n\n\nLikewise, you can evaluate the return value of the model given the latent variables:\n\nreturned(model, latents)\n\n\n[ Info:  --- Executing model --- \n\n\n\n\n(xplus1 = 0.768429411306621,)",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Models",
      "Evaluation of DynamicPPL Models with VarInfo"
    ]
  },
  {
    "objectID": "developers/models/varinfo-overview/index.html#varinfo",
    "href": "developers/models/varinfo-overview/index.html#varinfo",
    "title": "Evaluation of DynamicPPL Models with VarInfo",
    "section": "VarInfo",
    "text": "VarInfo\nThe above functions are convenient, but for many ‘serious’ applications they might not be flexible enough. For example, if you wanted to obtain the return value and the log joint, you would have to execute the model twice: once with returned and once with logjoint.\nIf you want to avoid this duplicate work, you need to use a lower-level interface, which is DynamicPPL.evaluate!!. At its core, evaluate!! takes a model and a VarInfo object, and returns a tuple of the return value and the new VarInfo. So, before we even get to evaluate!!, we need to understand what a VarInfo is.\nA VarInfo is a container that tracks the state of model execution, as well as any outputs related to its latent variables, such as log probabilities. DynamicPPL’s source code contains many different kinds of VarInfos, each with different trade-offs. The details of these are somewhat arcane and unfortunately cannot be fully abstracted away, mainly due to performance considerations.\nFor the vast majority of users, it suffices to know that you can generate one of them for a model with the constructor VarInfo([rng, ]model). Note that this construction executes the model once (sampling new parameter values from the prior in the process).\n\nv = VarInfo(model)\n\n\n[ Info:  --- Executing model --- \n\n\n\n\nVarInfo{@NamedTuple{x::DynamicPPL.Metadata{Dict{VarName{:x, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:x, typeof(identity)}}, Vector{Float64}}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::LogPriorAccumulator{Float64}, LogJacobian::LogJacobianAccumulator{Float64}, LogLikelihood::LogLikelihoodAccumulator{Float64}}}}((x = DynamicPPL.Metadata{Dict{VarName{:x, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:x, typeof(identity)}}, Vector{Float64}}(Dict(x =&gt; 1), [x], UnitRange{Int64}[1:1], [-0.6896099641868259], Normal{Float64}[Distributions.Normal{Float64}(μ=0.0, σ=1.0)], Bool[0]),), DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::LogPriorAccumulator{Float64}, LogJacobian::LogJacobianAccumulator{Float64}, LogLikelihood::LogLikelihoodAccumulator{Float64}}}((LogPrior = DynamicPPL.LogPriorAccumulator(-1.1567194845575504), LogJacobian = DynamicPPL.LogJacobianAccumulator(0.0), LogLikelihood = DynamicPPL.LogLikelihoodAccumulator(-4.535939412931202))))\n\n\n(Don’t worry about the printout of the VarInfo object: we won’t need to understand its internal structure.) We can index into a VarInfo:\n\nv[@varname(x)]\n\n-0.6896099641868259\n\n\nTo access the values of log-probabilities, DynamicPPL provides the getlogprior, getloglikelihood, and getlogjoint functions:\n\nDynamicPPL.getlogprior(v)\n\n-1.1567194845575504\n\n\nWhat about the return value? Well, the VarInfo does not store this directly: recall that evaluate!! gives us back the return value separately from the updated VarInfo. So, let’s try calling it to see what happens. The default behaviour of evaluate!! is to use the parameter values stored in the VarInfo during model execution. That is, when it sees x ~ Normal(), it will use the value of x stored in v. We will see later how to change this behaviour.\n\nretval, vout = DynamicPPL.evaluate!!(model, v)\n\n\n[ Info:  --- Executing model --- \n\n\n\n\n((xplus1 = 0.31039003581317415,), VarInfo{@NamedTuple{x::DynamicPPL.Metadata{Dict{VarName{:x, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:x, typeof(identity)}}, Vector{Float64}}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::LogPriorAccumulator{Float64}, LogJacobian::LogJacobianAccumulator{Float64}, LogLikelihood::LogLikelihoodAccumulator{Float64}}}}((x = DynamicPPL.Metadata{Dict{VarName{:x, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:x, typeof(identity)}}, Vector{Float64}}(Dict(x =&gt; 1), [x], UnitRange{Int64}[1:1], [-0.6896099641868259], Normal{Float64}[Distributions.Normal{Float64}(μ=0.0, σ=1.0)], Bool[0]),), DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::LogPriorAccumulator{Float64}, LogJacobian::LogJacobianAccumulator{Float64}, LogLikelihood::LogLikelihoodAccumulator{Float64}}}((LogPrior = DynamicPPL.LogPriorAccumulator(-1.1567194845575504), LogJacobian = DynamicPPL.LogJacobianAccumulator(0.0), LogLikelihood = DynamicPPL.LogLikelihoodAccumulator(-4.535939412931202)))))\n\n\nSo here in a single call we have obtained both the return value and an updated VarInfo vout, from which we can again extract log probabilities and variable values. We can see from this that the value of vout[@varname(x)] is the same as v[@varname(x)]:\n\nvout[@varname(x)] == v[@varname(x)]\n\ntrue\n\n\nwhich is in line with the statement above that by default evaluate!! uses the values stored in the VarInfo.\nAt this point, the keen reader will notice that we have not really solved the problem here. Although the call to DynamicPPL.evaluate!! does indeed only execute the model once, we also had to do this once more at the beginning when constructing the VarInfo.\nBesides, we don’t know how to control the parameter values used during model execution: they were simply whatever we got in the original VarInfo.",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Models",
      "Evaluation of DynamicPPL Models with VarInfo"
    ]
  },
  {
    "objectID": "developers/models/varinfo-overview/index.html#specifying-parameter-values",
    "href": "developers/models/varinfo-overview/index.html#specifying-parameter-values",
    "title": "Evaluation of DynamicPPL Models with VarInfo",
    "section": "Specifying parameter values",
    "text": "Specifying parameter values\nWe will first tackle the problem of specifying our own parameter values. To do this, we need to use DynamicPPL.init!! instead of DynamicPPL.evaluate!!.\nThe difference is that instead of using the values stored in the VarInfo (which evaluate!! does by default), init!! uses a strategy for generating new values, and overwrites the values in the VarInfo accordingly. For example, InitFromPrior() says that any time a tilde-statement x ~ dist is encountered, a new value for x should be sampled from dist:\n\nretval, v_new = DynamicPPL.init!!(model, v, InitFromPrior())\n\n\n[ Info:  --- Executing model --- \n\n\n\n\n((xplus1 = -0.5726425696768096,), VarInfo{@NamedTuple{x::DynamicPPL.Metadata{Dict{VarName{:x, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:x, typeof(identity)}}, Vector{Float64}}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::LogPriorAccumulator{Float64}, LogJacobian::LogJacobianAccumulator{Float64}, LogLikelihood::LogLikelihoodAccumulator{Float64}}}}((x = DynamicPPL.Metadata{Dict{VarName{:x, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:x, typeof(identity)}}, Vector{Float64}}(Dict(x =&gt; 1), [x], UnitRange{Int64}[1:1], [-1.5726425696768096], Normal{Float64}[Distributions.Normal{Float64}(μ=0.0, σ=1.0)], Bool[0]),), DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::LogPriorAccumulator{Float64}, LogJacobian::LogJacobianAccumulator{Float64}, LogLikelihood::LogLikelihoodAccumulator{Float64}}}((LogPrior = DynamicPPL.LogPriorAccumulator(-2.155540859184512), LogJacobian = DynamicPPL.LogJacobianAccumulator(0.0), LogLikelihood = DynamicPPL.LogLikelihoodAccumulator(-7.300825998538133)))))\n\n\nThis updates v_new with the new values that were sampled, and also means that log probabilities are computed using these new values.\n\n\n\n\n\n\nNoteRandom number generator\n\n\n\nYou can also provide an AbstractRNG as the first argument to init!! to control the reproducibility of the sampling: here we have omitted it.\n\n\nAlternatively, to provide specific sets of values, we can use InitFromParams(...) to specify them. InitFromParams can wrap either a NamedTuple or an AbstractDict{&lt;:VarName}, but Dict is generally much preferred as this guarantees correct behaviour even for complex variable names.\n\nretval, v_new = DynamicPPL.init!!(\n    model, v, InitFromParams(Dict(@varname(x) =&gt; 3.0))\n)\n\n\n[ Info:  --- Executing model --- \n\n\n\n\n((xplus1 = 4.0,), VarInfo{@NamedTuple{x::DynamicPPL.Metadata{Dict{VarName{:x, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:x, typeof(identity)}}, Vector{Float64}}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::LogPriorAccumulator{Float64}, LogJacobian::LogJacobianAccumulator{Float64}, LogLikelihood::LogLikelihoodAccumulator{Float64}}}}((x = DynamicPPL.Metadata{Dict{VarName{:x, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:x, typeof(identity)}}, Vector{Float64}}(Dict(x =&gt; 1), [x], UnitRange{Int64}[1:1], [3.0], Normal{Float64}[Distributions.Normal{Float64}(μ=0.0, σ=1.0)], Bool[0]),), DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::LogPriorAccumulator{Float64}, LogJacobian::LogJacobianAccumulator{Float64}, LogLikelihood::LogLikelihoodAccumulator{Float64}}}((LogPrior = DynamicPPL.LogPriorAccumulator(-5.418938533204673), LogJacobian = DynamicPPL.LogJacobianAccumulator(0.0), LogLikelihood = DynamicPPL.LogLikelihoodAccumulator(-1.4189385332046727)))))\n\n\nWe now find that if we look into v_new, the value of x is indeed 3.0:\n\nv_new[@varname(x)]\n\n3.0\n\n\nand we can extract the return value and log probabilities exactly as before.\nNote that init!! always ignores any values that are already present in the VarInfo, and overwrites them with new values according to the specified strategy.\nIf you have a loop in which you want to repeatedly evaluate a model with different parameter values, then the workflow shown here is recommended:\n\nFirst generate a VarInfo using VarInfo(model);\nThen call DynamicPPL.init!!(model, v, InitFromParams(...)) to evaluate the model using those parameters.\n\nThis requires you to pay a one-time cost at the very beginning to generate the VarInfo, but subsequent evaluations will be efficient. DynamicPPL uses this approach when implementing functions such as predict(model, chain).\n\n\n\n\n\n\nTip\n\n\n\nIf you want to avoid even the first model evaluation, you will need to read on to the ‘Advanced’ section below. However, for most applications this should not necessary.",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Models",
      "Evaluation of DynamicPPL Models with VarInfo"
    ]
  },
  {
    "objectID": "developers/models/varinfo-overview/index.html#parameters-in-the-form-of-vectors",
    "href": "developers/models/varinfo-overview/index.html#parameters-in-the-form-of-vectors",
    "title": "Evaluation of DynamicPPL Models with VarInfo",
    "section": "Parameters in the form of Vectors",
    "text": "Parameters in the form of Vectors\nIn general, one problem with init!! is that it is often slower than evaluate!!. This is primarily because it does more work: it has to not only read from the provided parameters, but also overwrite existing values in the VarInfo.\n\nusing Chairmarks, Logging\n# We need to silence the 'executing model' message, or else it will\n# fill up the entire screen!\nwith_logger(ConsoleLogger(stderr, Logging.Warn)) do\n    median(@be DynamicPPL.evaluate!!(model, v_new))\nend\n\n373.608 ns (7 allocs: 288 bytes)\n\n\n\nwith_logger(ConsoleLogger(stderr, Logging.Warn)) do\n    median(@be DynamicPPL.init!!(model, v_new, InitFromParams(Dict(@varname(x) =&gt; 3.0))))\nend\n\n477.383 ns (12 allocs: 624 bytes)\n\n\nWhen evaluating models in tight loops, as is often the case in inference algorithms, this overhead can be quite unwanted. DynamicPPL provides a rather dangerous, but powerful, way to get around this, which is the DynamicPPL.unflatten function. unflatten allows you to directly modify the internal storage of a VarInfo, without having to go through init!! and model evaluation. Its input is a vector of parameters.\n\nxs = [7.0]\nv_unflattened = DynamicPPL.unflatten(v_new, xs)\nv_unflattened[@varname(x)]\n\n7.0\n\n\nWe can then directly use v_new in evaluate!!, which will use the value 7.0 for x:\n\nretval, vout = DynamicPPL.evaluate!!(model, v_unflattened)\n\n\n[ Info:  --- Executing model --- \n\n\n\n\n((xplus1 = 8.0,), VarInfo{@NamedTuple{x::DynamicPPL.Metadata{Dict{VarName{:x, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:x, typeof(identity)}}, Vector{Float64}}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::LogPriorAccumulator{Float64}, LogJacobian::LogJacobianAccumulator{Float64}, LogLikelihood::LogLikelihoodAccumulator{Float64}}}}((x = DynamicPPL.Metadata{Dict{VarName{:x, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:x, typeof(identity)}}, Vector{Float64}}(Dict(x =&gt; 1), [x], UnitRange{Int64}[1:1], [7.0], Normal{Float64}[Distributions.Normal{Float64}(μ=0.0, σ=1.0)], Bool[0]),), DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::LogPriorAccumulator{Float64}, LogJacobian::LogJacobianAccumulator{Float64}, LogLikelihood::LogLikelihoodAccumulator{Float64}}}((LogPrior = DynamicPPL.LogPriorAccumulator(-25.418938533204674), LogJacobian = DynamicPPL.LogJacobianAccumulator(0.0), LogLikelihood = DynamicPPL.LogLikelihoodAccumulator(-13.418938533204672)))))\n\n\nEven the combination of unflatten and evaluate!! tends to be faster than a single call to init!!, especially for larger models.\nHowever, there are several reasons why this function is dangerous. If you use it, you must pay close attention to correctness:\n\nFor models with multiple variables, the order in which these variables occur in the vector is not obvious. The short answer is that it depends on the order in which the variables are added to the VarInfo during its initialisation. If you have models where the order of variables can vary from one execution to another, then unflatten can easily lead to incorrect results.\nThe meaning of the values passed in will generally depend on whether the VarInfo is linked or not (see the Variable Transformations page for more information about linked VarInfos). You must make sure that the values passed in are consistent with the link status of the VarInfo. In contrast, InitFromParams always uses unlinked values.\nWhile unflatten modifies the parameter values stored in the VarInfo, it does not modify any other information, such as log probabilities. Thus, after calling unflatten, your VarInfo will be in an inconsistent state, and you should not attempt to read any other information from it until you have called evaluate!! again (which recomputes e.g. log probabilities).\n\nThe inverse operation of unflatten is DynamicPPL.getindex_internal(v, :):\n\nDynamicPPL.getindex_internal(v_unflattened, :)\n\n1-element Vector{Float64}:\n 7.0",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Models",
      "Evaluation of DynamicPPL Models with VarInfo"
    ]
  },
  {
    "objectID": "developers/models/varinfo-overview/index.html#logdensityfunction",
    "href": "developers/models/varinfo-overview/index.html#logdensityfunction",
    "title": "Evaluation of DynamicPPL Models with VarInfo",
    "section": "LogDensityFunction",
    "text": "LogDensityFunction\nThere is one place where unflatten is (unfortunately) quite indispensable, namely, the implementation of the LogDensityProblems.jl interface for Turing models.\nThe LogDensityProblems interface defines interface functions such as\nLogDensityProblems.logdensity(f, x::AbstractVector)\nwhich evaluates the log density of a model f given a vector of parameters x.\nGiven what we have seen above, this can be done by wrapping a model and a VarInfo together inside a struct. Here is a rough sketch of how this can be implemented:\n\nusing LogDensityProblems\n\nstruct MyModelLogDensity{M&lt;:DynamicPPL.Model,V&lt;:DynamicPPL.VarInfo}\n    model::M\n    varinfo::V\nend\n\nfunction LogDensityProblems.logdensity(f::MyModelLogDensity, x::AbstractVector)\n    v_new = DynamicPPL.unflatten(f.varinfo, x)\n    _, vout = DynamicPPL.evaluate!!(f.model, v_new)\n    return DynamicPPL.getlogjoint(vout)\nend\n\n# Usage\nmy_ldf = MyModelLogDensity(model, VarInfo(model))\nLogDensityProblems.logdensity(my_ldf, [2.5])\n\n\n[ Info:  --- Executing model --- \n[ Info:  --- Executing model --- \n\n\n\n\n-5.087877066409346\n\n\nDynamicPPL contains a LogDensityFunction type that, at its core, is essentially the same as the above.\n\n# the varinfo object defaults to VarInfo(model)\nldf = DynamicPPL.LogDensityFunction(model)\nLogDensityProblems.logdensity(ldf, [2.5])\n\n\n[ Info:  --- Executing model --- \n[ Info:  --- Executing model --- \n\n\n\n\n-5.087877066409346\n\n\nThe real implementation is a bit more complicated as it provides more options, as well as support for gradients with automatic differentiation.\nIn this way, any Turing model can be converted into an object that you can use with LogDensityProblems-compatible optimisers, samplers, and other algorithms. This is very powerful as it allows the algorithms to completely ignore the internal structure of the model, and simply treat it as an opaque log-density function. For example, Turing’s external sampler interface makes heavy use of this.\nHowever, it should be noted that because this uses unflatten under the hood, it suffers from exactly the same limitations as described above. For example, models that do not have a fixed number or order of latent variables can lead to incorrect results or errors.",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Models",
      "Evaluation of DynamicPPL Models with VarInfo"
    ]
  },
  {
    "objectID": "developers/models/varinfo-overview/index.html#advanced-typed-and-untyped-varinfo",
    "href": "developers/models/varinfo-overview/index.html#advanced-typed-and-untyped-varinfo",
    "title": "Evaluation of DynamicPPL Models with VarInfo",
    "section": "Advanced: Typed and untyped VarInfo",
    "text": "Advanced: Typed and untyped VarInfo\nThe discussion above suffices for many applications of DynamicPPL, but one question remains: how to avoid the initial overhead of constructing a VarInfo object before we can do anything useful with it. This is important when implementing a function such as logjoint(model, params): in principle, only a single evaluation should be needed.\nTo tackle this, we need to understand a little bit more about two kinds of VarInfo. Conceptually, DynamicPPL has both typed and untyped VarInfos. This distinction is also described in section 4.2.4 of our recent Turing.jl paper.\nEvaluating a model with an existing typed VarInfo is generally much faster, and once you have a typed VarInfo it is a good idea to stick with it. However, when instantiating a new VarInfo, it is often better to start with an untyped VarInfo, fill in the values, and then convert it to a typed VarInfo.\n\n\n\n\n\n\nNoteWhy is untyped initialisation better?\n\n\n\nInitialising a fresh VarInfo requires adding variables to it as they are encountered during model execution. There are two main reasons for preferring untyped VarInfo: firstly, compilation time with typed VarInfo scales poorly with the number of variables; and secondly, typed VarInfos can error with certain kinds of models. See this issue for more information.\n\n\nTo see this in action, let’s begin by constructing an empty untyped VarInfo. This does not execute the model, and so the resulting object has no stored variable values. If we try to index into it, we will get an error:\n\nv_empty_untyped = VarInfo()\nv_empty_untyped[@varname(x)]\n\n\nKeyError: key x not found\nStacktrace:\n [1] getindex\n   @ ./dict.jl:477 [inlined]\n [2] getidx\n   @ ~/.julia/packages/DynamicPPL/Hza15/src/varinfo.jl:635 [inlined]\n [3] is_transformed\n   @ ~/.julia/packages/DynamicPPL/Hza15/src/varinfo.jl:885 [inlined]\n [4] is_transformed\n   @ ~/.julia/packages/DynamicPPL/Hza15/src/varinfo.jl:884 [inlined]\n [5] from_maybe_linked_internal_transform\n   @ ~/.julia/packages/DynamicPPL/Hza15/src/abstract_varinfo.jl:1095 [inlined]\n [6] getindex(vi::VarInfo{DynamicPPL.Metadata{Dict{VarName, Int64}, Vector{Distribution}, Vector{VarName}, Vector{Real}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::LogPriorAccumulator{Float64}, LogJacobian::LogJacobianAccumulator{Float64}, LogLikelihood::LogLikelihoodAccumulator{Float64}}}}, vn::VarName{:x, typeof(identity)})\n   @ DynamicPPL ~/.julia/packages/DynamicPPL/Hza15/src/varinfo.jl:1491\n [7] top-level scope\n   @ ~/work/docs/docs/developers/models/varinfo-overview/index.qmd:323\n\n\n\n\n\n\n\n\n\nNoteVarInfo(model) returns a typed VarInfo\n\n\n\nAlthough VarInfo() with no arguments returns an untyped VarInfo, note that calling VarInfo(model) returns a typed VarInfo. This is a slightly awkward aspect of DynamicPPL’s current API.\n\n\nTo generate new values for it, we will use DynamicPPL.init!! as before.\n\n_, v_filled_untyped = DynamicPPL.init!!(model, v_empty_untyped, InitFromParams(Dict(@varname(x) =&gt; 5.0)))\n\n\n[ Info:  --- Executing model --- \n\n\n\n\n((xplus1 = 6.0,), VarInfo (1 variable (x), dimension 1; accumulators: (LogPrior = DynamicPPL.LogPriorAccumulator(-13.418938533204672), LogJacobian = DynamicPPL.LogJacobianAccumulator(0.0), LogLikelihood = DynamicPPL.LogLikelihoodAccumulator(-5.418938533204673))))\n\n\nNow that we have filled in the untyped VarInfo, we can access parameter values, log probabilities, and so on:\n\nDynamicPPL.getlogprior(v_filled_untyped)\n\n-13.418938533204672\n\n\nSo, putting this all together, this is how an implementation of logprior(model, params) could look:\n\nfunction mylogprior(model, params)\n    # Create empty untyped VarInfo\n    v_empty_untyped = VarInfo()\n    # Fill in values from given params\n    _, v_filled_untyped = DynamicPPL.init!!(model, v_empty_untyped, InitFromParams(params))\n    # Extract log prior\n    return DynamicPPL.getlogprior(v_filled_untyped)\nend\n\nmylogprior(model, Dict(@varname(x) =&gt; 5.0))\n\n\n[ Info:  --- Executing model --- \n\n\n\n\n-13.418938533204672\n\n\nNotice that the above only required a single model evaluation.\nIf we later want to convert the untyped VarInfo into a typed VarInfo (for example, for later reuse), we can do so using DynamicPPL.typed_varinfo:\n\nv_filled_typed = DynamicPPL.typed_varinfo(v_filled_untyped)\n\nVarInfo{@NamedTuple{x::DynamicPPL.Metadata{Dict{VarName{:x, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:x, typeof(identity)}}, Vector{Float64}}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::LogPriorAccumulator{Float64}, LogJacobian::LogJacobianAccumulator{Float64}, LogLikelihood::LogLikelihoodAccumulator{Float64}}}}((x = DynamicPPL.Metadata{Dict{VarName{:x, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:x, typeof(identity)}}, Vector{Float64}}(Dict(x =&gt; 1), [x], UnitRange{Int64}[1:1], [5.0], Normal{Float64}[Distributions.Normal{Float64}(μ=0.0, σ=1.0)], Bool[0]),), DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::LogPriorAccumulator{Float64}, LogJacobian::LogJacobianAccumulator{Float64}, LogLikelihood::LogLikelihoodAccumulator{Float64}}}((LogPrior = DynamicPPL.LogPriorAccumulator(-13.418938533204672), LogJacobian = DynamicPPL.LogJacobianAccumulator(0.0), LogLikelihood = DynamicPPL.LogLikelihoodAccumulator(-5.418938533204673))))\n\n\nThis allows us to demonstrate how VarInfo(model) is implemented:\n\nfunction myvarinfo(model)\n    # Create empty untyped VarInfo\n    v_empty_untyped = VarInfo()\n    # Sample values from prior\n    _, v_filled_untyped = DynamicPPL.init!!(model, v_empty_untyped, InitFromPrior())\n    # Convert to typed VarInfo\n    return DynamicPPL.typed_varinfo(v_filled_untyped)\nend\n\nmyvarinfo (generic function with 1 method)\n\n\nNotice here that evaluate!! runs much faster with a typed VarInfo than with untyped: this is why generally for repeated evaluation you should use a typed VarInfo. The same is true of init!!.\n\nwith_logger(ConsoleLogger(stderr, Logging.Warn)) do\n    median(@be DynamicPPL.evaluate!!(model, v_filled_untyped))\nend\n\n2.057 μs (32 allocs: 1.328 KiB)\n\n\n\nwith_logger(ConsoleLogger(stderr, Logging.Warn)) do\n    median(@be DynamicPPL.evaluate!!(model, v_filled_typed))\nend\n\n376.519 ns (7 allocs: 288 bytes)",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL Models",
      "Evaluation of DynamicPPL Models with VarInfo"
    ]
  },
  {
    "objectID": "developers/inference/variational-inference/index.html",
    "href": "developers/inference/variational-inference/index.html",
    "title": "Variational Inference",
    "section": "",
    "text": "In this post, we’ll examine variational inference (VI), a family of approximate Bayesian inference methods. We will focus on one of the more standard VI methods, Automatic Differentiation Variational Inference (ADVI).\nHere, we’ll examine the theory behind VI, but if you’re interested in using ADVI in Turing, check out this tutorial.",
    "crumbs": [
      "Get Started",
      "Developers",
      "Inference in Detail",
      "Variational Inference"
    ]
  },
  {
    "objectID": "developers/inference/variational-inference/index.html#computing-kl-divergence-without-knowing-the-posterior",
    "href": "developers/inference/variational-inference/index.html#computing-kl-divergence-without-knowing-the-posterior",
    "title": "Variational Inference",
    "section": "Computing KL-divergence without knowing the posterior",
    "text": "Computing KL-divergence without knowing the posterior\nFirst off, recall that\n\n\\[\np(z \\mid x\\_i) = \\frac{p(x\\_i, z)}{p(x\\_i)}\n\\]\n\nso we can write\n\n\\[\n\\begin{align*}\n\\mathrm{D\\_{KL}} \\left( q(z), p(z \\mid \\\\{ x\\_i \\\\}\\_{i = 1}^n) \\right) &= \\mathbb{E}\\_{z \\sim q(z)} \\left[ \\log q(z) \\right] - \\sum\\_{i = 1}^n \\mathbb{E}\\_{z \\sim q(z)} \\left[ \\log p(x\\_i, z) - \\log p(x\\_i) \\right] \\\\\n    &= \\mathbb{E}\\_{z \\sim q(z)} \\left[ \\log q(z) \\right] - \\sum\\_{i = 1}^n \\mathbb{E}\\_{z \\sim q(z)} \\left[ \\log p(x\\_i, z) \\right] + \\sum\\_{i = 1}^n \\mathbb{E}\\_{z \\sim q(z)} \\left[ \\log p(x_i) \\right] \\\\\n    &= \\mathbb{E}\\_{z \\sim q(z)} \\left[ \\log q(z) \\right] - \\sum\\_{i = 1}^n \\mathbb{E}\\_{z \\sim q(z)} \\left[ \\log p(x\\_i, z) \\right] + \\sum\\_{i = 1}^n \\log p(x\\_i),\n\\end{align*}\n\\]\n\nwhere in the last equality we used the fact that \\(p(x_i)\\) is independent of \\(z\\).\nNow you’re probably thinking “Oh great! Now you’ve introduced \\(p(x_i)\\) which we also can’t compute (in general)!”. Woah. Calm down human. Let’s do some more algebra. The above expression can be rearranged to\n\n\\[\n\\mathrm{D\\_{KL}} \\left( q(z), p(z \\mid \\\\{ x\\_i \\\\}\\_{i = 1}^n) \\right) + \\underbrace{\\sum\\_{i = 1}^n \\mathbb{E}\\_{z \\sim q(z)} \\left[ \\log p(x\\_i, z) \\right] - \\mathbb{E}\\_{z \\sim q(z)} \\left[ \\log q(z) \\right]}\\_{=: \\mathrm{ELBO}(q)} = \\underbrace{\\sum\\_{i = 1}^n \\mathbb{E}\\_{z \\sim q(z)} \\left[ \\log p(x\\_i) \\right]}\\_{\\text{constant}}.\n\\]\n\nSee? The left-hand side is constant and, as we mentioned before, \\(\\mathrm{D_{KL}} \\ge 0\\). What happens if we try to maximize the term we just gave the completely arbitrary name \\(\\mathrm{ELBO}\\)? Well, if \\(\\mathrm{ELBO}\\) goes up while \\(p(x_i)\\) stays constant then \\(\\mathrm{D_{KL}}\\) has to go down! That is, the \\(q(z)\\) which minimizes the KL-divergence is the same \\(q(z)\\) which maximizes \\(\\mathrm{ELBO}(q)\\):\n\n\\[\n\\underset{q}{\\mathrm{argmin}} \\  \\mathrm{D\\_{KL}} \\left( q(z), p(z \\mid \\\\{ x\\_i \\\\}\\_{i = 1}^n) \\right) = \\underset{q}{\\mathrm{argmax}} \\ \\mathrm{ELBO}(q)\n\\]\n\nwhere\n\n\\[\n\\begin{align*}\n\\mathrm{ELBO}(q) &:= \\left( \\sum\\_{i = 1}^n \\mathbb{E}\\_{z \\sim q(z)} \\left[ \\log p(x\\_i, z) \\right]  \\right) - \\mathbb{E}\\_{z \\sim q(z)} \\left[ \\log q(z) \\right] \\\\\n    &= \\left( \\sum\\_{i = 1}^n \\mathbb{E}\\_{z \\sim q(z)} \\left[ \\log p(x\\_i, z) \\right] \\right) + \\mathbb{H}\\left( q(z) \\right)\n\\end{align*}\n\\]\n\nand \\(\\mathbb{H} \\left(q(z) \\right)\\) denotes the (differential) entropy of \\(q(z)\\).\nAssuming joint \\(p(x_i, z)\\) and the entropy \\(\\mathbb{H}\\left(q(z)\\right)\\) are both tractable, we can use a Monte-Carlo for the remaining expectation. This leaves us with the following tractable expression\n\n\\[\n\\underset{q}{\\mathrm{argmin}} \\ \\mathrm{D\\_{KL}} \\left( q(z), p(z \\mid \\\\{ x\\_i \\\\}\\_{i = 1}^n) \\right) \\approx \\underset{q}{\\mathrm{argmax}} \\ \\widehat{\\mathrm{ELBO}}(q)\n\\]\n\nwhere\n\n\\[\n\\widehat{\\mathrm{ELBO}}(q) = \\frac{1}{m} \\left( \\sum\\_{k = 1}^m \\sum\\_{i = 1}^n \\log p(x\\_i, z\\_k) \\right) + \\mathbb{H} \\left(q(z)\\right) \\quad \\text{where} \\quad z\\_k \\sim q(z) \\quad \\forall k = 1, \\dots, m.\n\\]\n\nHence, as long as we can sample from \\(q(z)\\) somewhat efficiently, we can indeed minimize the KL-divergence! Neat, eh?\nSidenote: in the case where \\(q(z)\\) is tractable but \\(\\mathbb{H} \\left(q(z) \\right)\\) is not , we can use a Monte-Carlo estimate for this term too but this generally results in a higher-variance estimate.\nAlso, I fooled you real good: the ELBO isn’t an arbitrary name, hah! In fact it’s an abbreviation for the expected lower bound (ELBO) because it, uhmm, well, it’s the expected lower bound (remember \\(\\mathrm{D_{KL}} \\ge 0\\)). Yup.",
    "crumbs": [
      "Get Started",
      "Developers",
      "Inference in Detail",
      "Variational Inference"
    ]
  },
  {
    "objectID": "developers/inference/variational-inference/index.html#maximizing-the-elbo",
    "href": "developers/inference/variational-inference/index.html#maximizing-the-elbo",
    "title": "Variational Inference",
    "section": "Maximizing the ELBO",
    "text": "Maximizing the ELBO\nFinding the optimal \\(q\\) over all possible densities of course isn’t feasible. Instead we consider a family of parameterized densities \\(\\mathscr{D}\\_{\\Theta}\\) where \\(\\Theta\\) denotes the space of possible parameters. Each density in this family \\(q\\_{\\theta} \\in \\mathscr{D}\\_{\\Theta}\\) is parameterized by a unique \\(\\theta \\in \\Theta\\). Moreover, we’ll assume\n\n\\(q\\_{\\theta}(z)\\), i.e. evaluating the probability density \\(q\\) at any point \\(z\\), is differentiable\n\\(z \\sim q\\_{\\theta}(z)\\), i.e. the process of sampling from \\(q\\_{\\theta}(z)\\), is differentiable\n\n\nis fairly straight-forward, but (2) is a bit tricky. What does it even mean for a sampling process to be differentiable? This is quite an interesting problem in its own right and would require something like a 50-page paper to properly review the different approaches (highly recommended read).\n\nWe’re going to make use of a particular such approach which goes under a bunch of different names: reparametrization trick, path derivative, etc. This refers to making the assumption that all elements \\(q\\_{\\theta} \\in \\mathscr{Q}\\_{\\Theta}\\) can be considered as reparameterizations of some base density, say \\(\\bar{q}(z)\\). That is, if \\(q\\_{\\theta} \\in \\mathscr{Q}\\_{\\Theta}\\) then\n\n\\[\nz \\sim q\\_{\\theta}(z) \\quad \\iff \\quad z := g\\_{\\theta}(\\tilde{z}) \\quad \\text{where} \\quad \\bar{z} \\sim \\bar{q}(z)\n\\]\n\nfor some function \\(g\\_{\\theta}\\) differentiable wrt. \\(\\theta\\). So all \\(q_{\\theta} \\in \\mathscr{Q}\\_{\\Theta}\\) are using the same reparameterization-function \\(g\\) but each \\(q\\_{\\theta}\\) correspond to different choices of \\(\\theta\\) for \\(f\\_{\\theta}\\).\nUnder this assumption we can differentiate the sampling process by taking the derivative of \\(g\\_{\\theta}\\) wrt. \\(\\theta\\), and thus we can differentiate the entire \\(\\widehat{\\mathrm{ELBO}}(q\\_{\\theta})\\) wrt. \\(\\theta\\)! With the gradient available we can either try to solve for optimality either by setting the gradient equal to zero or maximise \\(\\widehat{\\mathrm{ELBO}}(q\\_{\\theta})\\) stepwise by traversing \\(\\mathscr{Q}\\_{\\Theta}\\) in the direction of steepest ascent. For the sake of generality, we’re going to go with the stepwise approach.\nWith all this nailed down, we eventually reach the section on Automatic Differentiation Variational Inference (ADVI).",
    "crumbs": [
      "Get Started",
      "Developers",
      "Inference in Detail",
      "Variational Inference"
    ]
  },
  {
    "objectID": "developers/inference/variational-inference/index.html#automatic-differentiation-variational-inference-advi",
    "href": "developers/inference/variational-inference/index.html#automatic-differentiation-variational-inference-advi",
    "title": "Variational Inference",
    "section": "Automatic Differentiation Variational Inference (ADVI)",
    "text": "Automatic Differentiation Variational Inference (ADVI)\nSo let’s revisit the assumptions we’ve made at this point:\n\nThe variational posterior \\(q\\_{\\theta}\\) is in a parameterized family of densities denoted \\(\\mathscr{Q}\\_{\\Theta}\\), with \\(\\theta \\in \\Theta\\).\n\\(\\mathscr{Q}\\_{\\Theta}\\) is a space of reparameterizable densities with \\(\\bar{q}(z)\\) as the base-density.\nThe parameterisation function \\(g\\_{\\theta}\\) is differentiable wrt. \\(\\theta\\).\nEvaluation of the probability density \\(q\\_{\\theta}(z)\\) is differentiable wrt. \\(\\theta\\).\n\\(\\mathbb{H}\\left(q\\_{\\theta}(z)\\right)\\) is tractable.\nEvaluation of the joint density \\(p(x, z)\\) is tractable and differentiable wrt. \\(z\\)\nThe support of \\(q(z)\\) is a subspace of the support of \\(p(z \\mid x)\\) : \\(\\mathrm{supp}\\left(q(z)\\right) \\subseteq \\mathrm{supp}\\left(p(z \\mid x)\\right)\\).\n\nAll of these are not necessary to do VI, but they are very convenient and results in a fairly flexible approach. One distribution which has a density satisfying all of the above assumptions except (7) (we’ll get back to this in second) for any tractable and differentiable \\(p(z \\mid \\\\{ x\\_i \\\\}\\_{i = 1}^n)\\) is the good ole’ Gaussian/normal distribution:\n\n\\[\nz \\sim \\mathcal{N}(\\mu, \\Sigma) \\quad \\iff \\quad z = g\\_{\\mu, L}(\\bar{z}) := \\mu + L^T \\tilde{z} \\quad \\text{where} \\quad \\bar{z} \\sim \\bar{q}(z) := \\mathcal{N}(1\\_d, I\\_{d \\times d})\n\\]\n\nwhere \\(\\Sigma = L L^T,\\) with \\(L\\) obtained from the Cholesky-decomposition. Abusing notation a bit, we’re going to write\n\n\\[\n\\theta = (\\mu, \\Sigma) := (\\mu\\_1, \\dots, \\mu\\_d, L\\_{11}, \\dots, L\\_{1, d}, L\\_{2, 1}, \\dots, L\\_{2, d}, \\dots, L\\_{d, 1}, \\dots, L\\_{d, d}).\n\\]\n\nWith this assumption we finally have a tractable expression for \\(\\widehat{\\mathrm{ELBO}}(q_{\\mu, \\Sigma})\\)! Well, assuming (7) is holds. Since a Gaussian has non-zero probability on the entirety of \\(\\mathbb{R}^d\\), we also require \\(p(z \\mid \\\\{ x_i \\\\}_{i = 1}^n)\\) to have non-zero probability on all of \\(\\mathbb{R}^d\\).\nThough not necessary, we’ll often make a mean-field assumption for the variational posterior \\(q(z)\\), i.e. assume independence between the latent variables. In this case, we’ll write\n\n\\[\n\\theta = (\\mu, \\sigma^2) := (\\mu\\_1, \\dots, \\mu\\_d, \\sigma\\_1^2, \\dots, \\sigma\\_d^2).\n\\]\n\n\nExamples\nAs a (trivial) example we could apply the approach described above to is the following generative model for \\(p(z \\mid \\\\{ x_i \\\\}\\_{i = 1}^n)\\):\n\n\\[\n\\begin{align*}\n    m &\\sim \\mathcal{N}(0, 1) \\\\\n    x\\_i &\\overset{\\text{i.i.d.}}{=} \\mathcal{N}(m, 1), \\quad i = 1, \\dots, n.\n\\end{align*}\n\\]\n\nIn this case \\(z = m\\) and we have the posterior defined \\(p(m \\mid \\\\{ x\\_i \\\\}\\_{i = 1}^n) = p(m) \\prod\\_{i = 1}^n p(x\\_i \\mid m)\\). Then the variational posterior would be\n\n\\[\nq\\_{\\mu, \\sigma} = \\mathcal{N}(\\mu, \\sigma^2), \\quad \\text{where} \\quad \\mu \\in \\mathbb{R}, \\ \\sigma^2 \\in \\mathbb{R}^{ + }.\n\\]\n\nAnd since prior of \\(m\\), \\(\\mathcal{N}(0, 1)\\), has non-zero probability on the entirety of \\(\\mathbb{R}\\), same as \\(q(m)\\), i.e. assumption (7) above holds, everything is fine and life is good.\nBut what about this generative model for \\(p(z \\mid \\\\{ x_i \\\\}_{i = 1}^n)\\):\n\n\\[\n\\begin{align*}\n    s &\\sim \\mathrm{InverseGamma}(2, 3), \\\\\n    m &\\sim \\mathcal{N}(0, s), \\\\\n    x\\_i &\\overset{\\text{i.i.d.}}{=} \\mathcal{N}(m, s), \\quad i = 1, \\dots, n,\n\\end{align*}\n\\]\n\nwith posterior \\(p(s, m \\mid \\\\{ x\\_i \\\\}\\_{i = 1}^n) = p(s) p(m \\mid s) \\prod\\_{i = 1}^n p(x\\_i \\mid s, m)\\) and the mean-field variational posterior \\(q(s, m)\\) will be\n\n\\[\nq\\_{\\mu\\_1, \\mu\\_2, \\sigma\\_1^2, \\sigma\\_2^2}(s, m) = p\\_{\\mathcal{N}(\\mu\\_1, \\sigma\\_1^2)}(s)\\ p\\_{\\mathcal{N}(\\mu\\_2, \\sigma\\_2^2)}(m),\n\\]\n\nwhere we’ve denoted the evaluation of the probability density of a Gaussian as \\(p_{\\mathcal{N}(\\mu, \\sigma^2)}(x)\\).\nObserve that \\(\\mathrm{InverseGamma}(2, 3)\\) has non-zero probability only on \\(\\mathbb{R}^{ + } := (0, \\infty)\\) which is clearly not all of \\(\\mathbb{R}\\) like \\(q(s, m)\\) has, i.e.\n\n\\[\n\\mathrm{supp} \\left( q(s, m) \\right) \\not\\subseteq \\mathrm{supp} \\left( p(z \\mid \\\\{ x\\_i \\\\}\\_{i = 1}^n) \\right).\n\\]\n\nRecall from the definition of the KL-divergence that when this is the case, the KL-divergence isn’t well defined. This gets us to the automatic part of ADVI.\n\n\n“Automatic”? How?\nFor a lot of the standard (continuous) densities \\(p\\) we can actually construct a probability density \\(\\tilde{p}\\) with non-zero probability on all of \\(\\mathbb{R}\\) by transforming the “constrained” probability density \\(p\\) to \\(\\tilde{p}\\). In fact, in these cases this is a one-to-one relationship. As we’ll see, this helps solve the support-issue we’ve been going on and on about.\n\nTransforming densities using change of variables\nIf we want to compute the probability of \\(x\\) taking a value in some set \\(A \\subseteq \\mathrm{supp} \\left( p(x) \\right)\\), we have to integrate \\(p(x)\\) over \\(A\\), i.e.\n\n\\[\n\\mathbb{P}_p(x \\in A) = \\int_A p(x) \\mathrm{d}x.\n\\]\n\nThis means that if we have a differentiable bijection \\(f: \\mathrm{supp} \\left( q(x) \\right) \\to \\mathbb{R}^d\\) with differentiable inverse \\(f^{-1}: \\mathbb{R}^d \\to \\mathrm{supp} \\left( p(x) \\right)\\), we can perform a change of variables\n\n\\[\n\\mathbb{P}\\_p(x \\in A) = \\int\\_{f^{-1}(A)} p \\left(f^{-1}(y) \\right) \\ \\left| \\det \\mathcal{J}\\_{f^{-1}}(y) \\right| \\mathrm{d}y,\n\\]\n\nwhere \\(\\mathcal{J}_{f^{-1}}(x)\\) denotes the jacobian of \\(f^{-1}\\) evaluated at \\(x\\). Observe that this defines a probability distribution\n\n\\[\n\\mathbb{P}\\_{\\tilde{p}}\\left(y \\in f^{-1}(A) \\right) = \\int\\_{f^{-1}(A)} \\tilde{p}(y) \\mathrm{d}y,\n\\]\n\nsince \\(f^{-1}\\left(\\mathrm{supp} (p(x)) \\right) = \\mathbb{R}^d\\) which has probability 1. This probability distribution has density \\(\\tilde{p}(y)\\) with \\(\\mathrm{supp} \\left( \\tilde{p}(y) \\right) = \\mathbb{R}^d\\), defined\n\n\\[\n\\tilde{p}(y) = p \\left( f^{-1}(y) \\right) \\ \\left| \\det \\mathcal{J}\\_{f^{-1}}(y) \\right|\n\\]\n\nor equivalently\n\n\\[\n\\tilde{p} \\left( f(x) \\right) = \\frac{p(x)}{\\big| \\det \\mathcal{J}\\_{f}(x) \\big|}\n\\]\n\ndue to the fact that\n\n\\[\n\\big| \\det \\mathcal{J}\\_{f^{-1}}(y) \\big| = \\big| \\det \\mathcal{J}\\_{f}(x) \\big|^{-1}\n\\]\n\nNote: it’s also necessary that the log-abs-det-jacobian term is non-vanishing. This can for example be accomplished by assuming \\(f\\) to also be elementwise monotonic.\n\n\nBack to VI\nSo why is this is useful? Well, we’re looking to generalise our approach using a normal distribution to cases where the supports don’t match up. How about defining \\(q(z)\\) by\n\n\\[\n\\begin{align*}\n  \\eta &\\sim \\mathcal{N}(\\mu, \\Sigma), \\\\\\\\\n  z &= f^{-1}(\\eta),\n\\end{align*}\n\\]\n\nwhere \\(f^{-1}: \\mathbb{R}^d \\to \\mathrm{supp} \\left( p(z \\mid x) \\right)\\) is a differentiable bijection with differentiable inverse. Then \\(z \\sim q_{\\mu, \\Sigma}(z) \\implies z \\in \\mathrm{supp} \\left( p(z \\mid x) \\right)\\) as we wanted. The resulting variational density is\n\n\\[\nq\\_{\\mu, \\Sigma}(z) = p\\_{\\mathcal{N}(\\mu, \\Sigma)}\\left( f(z) \\right) \\ \\big| \\det \\mathcal{J}\\_{f}(z) \\big|.\n\\]\n\nNote that the way we’ve constructed \\(q(z)\\) here is basically a reverse of the approach we described above. Here we sample from a distribution with support on \\(\\mathbb{R}\\) and transform to \\(\\mathrm{supp} \\left( p(z \\mid x) \\right)\\).\nIf we want to write the ELBO explicitly in terms of \\(\\eta\\) rather than \\(z\\), the first term in the ELBO becomes\n\n\\[\n\\begin{align*}\n  \\mathbb{E}\\_{z \\sim q_{\\mu, \\Sigma}(z)} \\left[ \\log p(x\\_i, z) \\right] &= \\mathbb{E}\\_{\\eta \\sim \\mathcal{N}(\\mu, \\Sigma)} \\Bigg[ \\log \\frac{p\\left(x\\_i, f^{-1}(\\eta) \\right)}{\\big| \\det \\mathcal{J}_{f^{-1}}(\\eta) \\big|} \\Bigg] \\\\\n  &= \\mathbb{E}\\_{\\eta \\sim \\mathcal{N}(\\mu, \\Sigma)} \\left[ \\log p\\left(x\\_i, f^{-1}(\\eta) \\right) \\right] - \\mathbb{E}\\_{\\eta \\sim \\mathcal{N}(\\mu, \\Sigma)} \\left[ \\left| \\det \\mathcal{J}\\_{f^{-1}}(\\eta) \\right| \\right].\n\\end{align*}\n\\]\n\nThe entropy is invariant under change of variables, thus \\(\\mathbb{H} \\left(q\\_{\\mu, \\Sigma}(z)\\right)\\) is simply the entropy of the normal distribution which is known analytically.\nHence, the resulting empirical estimate of the ELBO is\n\n\\[\n\\begin{align*}\n\\widehat{\\mathrm{ELBO}}(q\\_{\\mu, \\Sigma}) &= \\frac{1}{m} \\left( \\sum\\_{k = 1}^m \\sum\\_{i = 1}^n \\left(\\log p\\left(x\\_i, f^{-1}(\\eta_k)\\right) - \\log \\big| \\det \\mathcal{J}\\_{f^{-1}}(\\eta\\_k) \\big| \\right) \\right) + \\mathbb{H} \\left(p\\_{\\mathcal{N}(\\mu, \\Sigma)}(z)\\right) \\\\\n& \\text{where} \\quad z\\_k  \\sim \\mathcal{N}(\\mu, \\Sigma) \\quad \\forall k = 1, \\dots, m\n\\end{align*}.\n\\]\n\nAnd maximizing this wrt. \\(\\mu\\) and \\(\\Sigma\\) is what’s referred to as Automatic Differentiation Variational Inference (ADVI)!\nNow if you want to try it out, check out the tutorial on how to use ADVI in Turing.jl!",
    "crumbs": [
      "Get Started",
      "Developers",
      "Inference in Detail",
      "Variational Inference"
    ]
  },
  {
    "objectID": "developers/inference/abstractmcmc-interface/index.html",
    "href": "developers/inference/abstractmcmc-interface/index.html",
    "title": "Interface Guide",
    "section": "",
    "text": "Turing implements a sampling interface (hosted at AbstractMCMC) that is intended to provide a common framework for Markov chain Monte Carlo samplers. The interface presents several structures and functions that one needs to overload in order to implement an interface-compatible sampler.\nThis guide will demonstrate how to implement the interface without Turing.\n\n\nAny implementation of an inference method that uses the AbstractMCMC interface should implement a subset of the following types and functions:\n\nA subtype of AbstractSampler, defined as a mutable struct containing state information or sampler parameters.\nA function sample_init! which performs any necessary set-up (default: do not perform any set-up).\nA function step! which returns a transition that represents a single draw from the sampler.\nA function transitions_init which returns a container for the transitions obtained from the sampler (default: return a Vector{T} of length N where T is the type of the transition obtained in the first step and N is the number of requested samples).\nA function transitions_save! which saves transitions to the container (default: save the transition of iteration i at position i in the vector of transitions).\nA function sample_end! which handles any sampler wrap-up (default: do not perform any wrap-up).\nA function bundle_samples which accepts the container of transitions and returns a collection of samples (default: return the vector of transitions).\n\nThe interface methods with exclamation points are those that are intended to allow for state mutation. Any mutating function is meant to allow mutation where needed – you might use:\n\nsample_init! to run some kind of sampler preparation, before sampling begins. This could mutate a sampler’s state.\nstep! might mutate a sampler flag after each sample.\nsample_end! contains any wrap-up you might need to do. If you were sampling in a transformed space, this might be where you convert everything back to a constrained space.\n\n\n\n\nThe motivation for the interface is to allow Julia’s fantastic probabilistic programming language community to have a set of standards and common implementations so we can all thrive together. Markov chain Monte Carlo methods tend to have a very similar framework to one another, and so a common interface should help more great inference methods built in single-purpose packages to experience more use among the community.\n\n\n\nMetropolis-Hastings is often the first sampling method that people are exposed to. It is a very straightforward algorithm and is accordingly the easiest to implement, so it makes for a good example. In this section, you will learn how to use the types and functions listed above to implement the Metropolis-Hastings sampler using the MCMC interface.\nThe full code for this implementation is housed in AdvancedMH.jl.\n\n\nLet’s begin by importing the relevant libraries. We’ll import AbstractMCMC, which contains the interface framework we’ll fill out. We also need Distributions and Random.\n\n# Import the relevant libraries.\nusing AbstractMCMC: AbstractMCMC\nusing Distributions\nusing Random\n\nAn interface extension (like the one we’re writing right now) typically requires that you overload or implement several functions. Specifically, you should import the functions you intend to overload. This next code block accomplishes that.\nFrom Distributions, we need Sampleable, VariateForm, and ValueSupport, three abstract types that define a distribution. Models in the interface are assumed to be subtypes of Sampleable{VariateForm, ValueSupport}. In this section our model is going to be extremely simple, so we will not end up using these except to make sure that the inference functions are dispatching correctly.\n\n\n\nLet’s begin our sampler definition by defining a sampler called MetropolisHastings which is a subtype of AbstractSampler. Correct typing is very important for proper interface implementation – if you are missing a subtype, your method may not be dispatched to when you call sample.\n\n# Define a sampler type.\nstruct MetropolisHastings{T,D} &lt;: AbstractMCMC.AbstractSampler\n    init_θ::T\n    proposal::D\nend\n\n# Default constructors.\nMetropolisHastings(init_θ::Real) = MetropolisHastings(init_θ, Normal(0, 1))\nfunction MetropolisHastings(init_θ::Vector{&lt;:Real})\n    return MetropolisHastings(init_θ, MvNormal(zero(init_θ), I))\nend\n\nMetropolisHastings\n\n\nAbove, we have defined a sampler that stores the initial parameterisation of the prior, and a distribution object from which proposals are drawn. You can have a struct that has no fields, and simply use it for dispatching onto the relevant functions, or you can store a large amount of state information in your sampler.\nThe general intuition for what to store in your sampler struct is that anything you may need to perform inference between samples but you don’t want to store in a transition should go into the sampler struct. It’s the only way you can carry non-sample related state information between step! calls.\n\n\n\nNext, we need to have a model of some kind. A model is a struct that’s a subtype of AbstractModel that contains whatever information is necessary to perform inference on your problem. In our case we want to know the mean and variance parameters for a standard Normal distribution, so we can keep our model to the log density of a Normal.\nNote that we only have to do this because we are not yet integrating the sampler with Turing – Turing has a very sophisticated modelling engine that removes the need to define custom model structs.\n\n# Define a model type. Stores the log density function.\nstruct DensityModel{F&lt;:Function} &lt;: AbstractMCMC.AbstractModel\n    ℓπ::F\nend\n\n\n\n\nThe next step is to define some transition which we will return from each step! call. We’ll keep it simple by just defining a wrapper struct that contains the parameter draws and the log density of that draw:\n\n# Create a very basic Transition type, only stores the \n# parameter draws and the log probability of the draw.\nstruct Transition{T,L}\n    θ::T\n    lp::L\nend\n\n# Store the new draw and its log density.\nTransition(model::DensityModel, θ) = Transition(θ, ℓπ(model, θ))\n\nTransition\n\n\nTransition can now store any type of parameter, whether it’s a vector of draws from multiple parameters or a single univariate draw.\n\n\n\nNow it’s time to get into the actual inference. We’ve defined all of the core pieces we need, but we need to implement the step! function which actually performs inference.\nAs a refresher, Metropolis-Hastings implements a very basic algorithm:\n\nPick some initial state, \\theta_0.\nFor t in [1,N], do\n\nGenerate a proposal parameterisation \\theta^\\prime_t \\sim q(\\theta^\\prime_t \\mid \\theta_{t-1}).\nCalculate the acceptance probability, \\alpha = \\text{min}\\left[1,\\frac{\\pi(\\theta'_t)}{\\pi(\\theta_{t-1})} \\frac{q(\\theta_{t-1} \\mid \\theta'_t)}{q(\\theta'_t \\mid \\theta_{t-1})}) \\right].\nIf U \\le \\alpha where U \\sim [0,1], then \\theta_t = \\theta'_t. Otherwise, \\theta_t = \\theta_{t-1}.\n\n\nOf course, it’s much easier to do this in the log space, so the acceptance probability is more commonly written as\n\\log \\alpha = \\min\\left[0, \\log \\pi(\\theta'_t) - \\log \\pi(\\theta_{t-1}) + \\log q(\\theta_{t-1} \\mid \\theta^\\prime_t) - \\log q(\\theta\\prime_t \\mid \\theta_{t-1}) \\right].\nIn interface terms, we should do the following:\n\nMake a new transition containing a proposed sample.\nCalculate the acceptance probability.\nIf we accept, return the new transition, otherwise, return the old one.\n\n\n\n\nThe step! function is the function that performs the bulk of your inference. In our case, we will implement two step! functions – one for the very first iteration, and one for every subsequent iteration.\n\n# Define the first step! function, which is called at the \n# beginning of sampling. Return the initial parameter used\n# to define the sampler.\nfunction AbstractMCMC.step!(\n    rng::AbstractRNG,\n    model::DensityModel,\n    spl::MetropolisHastings,\n    N::Integer,\n    ::Nothing;\n    kwargs...,\n)\n    return Transition(model, spl.init_θ)\nend\n\nThe first step! function just packages up the initial parameterisation inside the sampler, and returns it. We implicitly accept the very first parameterisation.\nThe other step! function performs the usual steps from Metropolis-Hastings. Included are several helper functions, proposal and q, which are designed to replicate the functions in the pseudocode above.\n\nproposal generates a new proposal in the form of a Transition, which can be univariate if the value passed in is univariate, or it can be multivariate if the Transition given is multivariate. Proposals use a basic Normal or MvNormal proposal distribution.\nq returns the log density of one parameterisation conditional on another, according to the proposal distribution.\nstep! generates a new proposal, checks the acceptance probability, and then returns either the previous transition or the proposed transition.\n\n\n# Define a function that makes a basic proposal depending on a univariate\n# parameterisation or a multivariate parameterisation.\nfunction propose(spl::MetropolisHastings, model::DensityModel, θ::Real)\n    return Transition(model, θ + rand(spl.proposal))\nend\nfunction propose(spl::MetropolisHastings, model::DensityModel, θ::Vector{&lt;:Real})\n    return Transition(model, θ + rand(spl.proposal))\nend\nfunction propose(spl::MetropolisHastings, model::DensityModel, t::Transition)\n    return propose(spl, model, t.θ)\nend\n\n# Calculates the probability `q(θ|θcond)`, using the proposal distribution `spl.proposal`.\nq(spl::MetropolisHastings, θ::Real, θcond::Real) = logpdf(spl.proposal, θ - θcond)\nfunction q(spl::MetropolisHastings, θ::Vector{&lt;:Real}, θcond::Vector{&lt;:Real})\n    return logpdf(spl.proposal, θ - θcond)\nend\nq(spl::MetropolisHastings, t1::Transition, t2::Transition) = q(spl, t1.θ, t2.θ)\n\n# Calculate the density of the model given some parameterisation.\nℓπ(model::DensityModel, θ) = model.ℓπ(θ)\nℓπ(model::DensityModel, t::Transition) = t.lp\n\n# Define the other step function. Returns a Transition containing\n# either a new proposal (if accepted) or the previous proposal \n# (if not accepted).\nfunction AbstractMCMC.step!(\n    rng::AbstractRNG,\n    model::DensityModel,\n    spl::MetropolisHastings,\n    ::Integer,\n    θ_prev::Transition;\n    kwargs...,\n)\n    # Generate a new proposal.\n    θ = propose(spl, model, θ_prev)\n\n    # Calculate the log acceptance probability.\n    α = ℓπ(model, θ) - ℓπ(model, θ_prev) + q(spl, θ_prev, θ) - q(spl, θ, θ_prev)\n\n    # Decide whether to return the previous θ or the new one.\n    if log(rand(rng)) &lt; min(α, 0.0)\n        return θ\n    else\n        return θ_prev\n    end\nend\n\n\n\n\nIn the default implementation, sample just returns a vector of all transitions. If instead you would like to obtain a Chains object (e.g., to simplify downstream analysis), you have to implement the bundle_samples function as well. It accepts the vector of transitions and returns a collection of samples. Fortunately, our Transition is incredibly simple, and we only need to build a little bit of functionality to accept custom parameter names passed in by the user.\n\n# A basic chains constructor that works with the Transition struct we defined.\nfunction AbstractMCMC.bundle_samples(\n    rng::AbstractRNG,\n    ℓ::DensityModel,\n    s::MetropolisHastings,\n    N::Integer,\n    ts::Vector{&lt;:Transition},\n    chain_type::Type{Any};\n    param_names=missing,\n    kwargs...,\n)\n    # Turn all the transitions into a vector-of-vectors.\n    vals = copy(reduce(hcat, [vcat(t.θ, t.lp) for t in ts])')\n\n    # Check if we received any parameter names.\n    if ismissing(param_names)\n        param_names = [\"Parameter $i\" for i in 1:(length(first(vals)) - 1)]\n    end\n\n    # Add the log density field to the parameter names.\n    push!(param_names, \"lp\")\n\n    # Bundle everything up and return a Chains struct.\n    return Chains(vals, param_names, (internals=[\"lp\"],))\nend\n\nAll done!\nYou can even implement different output formats by implementing bundle_samples for different chain_types, which can be provided as keyword argument to sample. As default sample uses chain_type = Any.\n\n\n\nNow that we have all the pieces, we should test the implementation by defining a model to calculate the mean and variance parameters of a Normal distribution. We can do this by constructing a target density function, providing a sample of data, and then running the sampler with sample.\n\n# Generate a set of data from the posterior we want to estimate.\ndata = rand(Normal(5, 3), 30)\n\n# Define the components of a basic model.\ninsupport(θ) = θ[2] &gt;= 0\ndist(θ) = Normal(θ[1], θ[2])\ndensity(θ) = insupport(θ) ? sum(logpdf.(dist(θ), data)) : -Inf\n\n# Construct a DensityModel.\nmodel = DensityModel(density)\n\n# Set up our sampler with initial parameters.\nspl = MetropolisHastings([0.0, 0.0])\n\n# Sample from the posterior.\nchain = sample(model, spl, 100000; param_names=[\"μ\", \"σ\"])\n\nIf all the interface functions have been extended properly, you should get an output from display(chain) that looks something like this:\nObject of type Chains, with data of type 100000×3×1 Array{Float64,3}\n\nIterations        = 1:100000\nThinning interval = 1\nChains            = 1\nSamples per chain = 100000\ninternals         = lp\nparameters        = μ, σ\n\n2-element Array{ChainDataFrame,1}\n\nSummary Statistics\n\n│ Row │ parameters │ mean    │ std      │ naive_se   │ mcse       │ ess     │ r_hat   │\n│     │ Symbol     │ Float64 │ Float64  │ Float64    │ Float64    │ Any     │ Any     │\n├─────┼────────────┼─────────┼──────────┼────────────┼────────────┼─────────┼─────────┤\n│ 1   │ μ          │ 5.33157 │ 0.854193 │ 0.0027012  │ 0.00893069 │ 8344.75 │ 1.00009 │\n│ 2   │ σ          │ 4.54992 │ 0.632916 │ 0.00200146 │ 0.00534942 │ 14260.8 │ 1.00005 │\n\nQuantiles\n\n│ Row │ parameters │ 2.5%    │ 25.0%   │ 50.0%   │ 75.0%   │ 97.5%   │\n│     │ Symbol     │ Float64 │ Float64 │ Float64 │ Float64 │ Float64 │\n├─────┼────────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n│ 1   │ μ          │ 3.6595  │ 4.77754 │ 5.33182 │ 5.89509 │ 6.99651 │\n│ 2   │ σ          │ 3.5097  │ 4.09732 │ 4.47805 │ 4.93094 │ 5.96821 │\nIt looks like we’re extremely close to our true parameters of Normal(5,3), though with a fairly high variance due to the low sample size.\n\n\n\n\nWe’ve seen how to implement the sampling interface for general projects. Turing’s interface methods are ever-evolving, so please open an issue at AbstractMCMC with feature requests or problems.",
    "crumbs": null
  },
  {
    "objectID": "developers/inference/abstractmcmc-interface/index.html#interface-overview",
    "href": "developers/inference/abstractmcmc-interface/index.html#interface-overview",
    "title": "Interface Guide",
    "section": "",
    "text": "Any implementation of an inference method that uses the AbstractMCMC interface should implement a subset of the following types and functions:\n\nA subtype of AbstractSampler, defined as a mutable struct containing state information or sampler parameters.\nA function sample_init! which performs any necessary set-up (default: do not perform any set-up).\nA function step! which returns a transition that represents a single draw from the sampler.\nA function transitions_init which returns a container for the transitions obtained from the sampler (default: return a Vector{T} of length N where T is the type of the transition obtained in the first step and N is the number of requested samples).\nA function transitions_save! which saves transitions to the container (default: save the transition of iteration i at position i in the vector of transitions).\nA function sample_end! which handles any sampler wrap-up (default: do not perform any wrap-up).\nA function bundle_samples which accepts the container of transitions and returns a collection of samples (default: return the vector of transitions).\n\nThe interface methods with exclamation points are those that are intended to allow for state mutation. Any mutating function is meant to allow mutation where needed – you might use:\n\nsample_init! to run some kind of sampler preparation, before sampling begins. This could mutate a sampler’s state.\nstep! might mutate a sampler flag after each sample.\nsample_end! contains any wrap-up you might need to do. If you were sampling in a transformed space, this might be where you convert everything back to a constrained space.",
    "crumbs": null
  },
  {
    "objectID": "developers/inference/abstractmcmc-interface/index.html#why-do-you-have-an-interface",
    "href": "developers/inference/abstractmcmc-interface/index.html#why-do-you-have-an-interface",
    "title": "Interface Guide",
    "section": "",
    "text": "The motivation for the interface is to allow Julia’s fantastic probabilistic programming language community to have a set of standards and common implementations so we can all thrive together. Markov chain Monte Carlo methods tend to have a very similar framework to one another, and so a common interface should help more great inference methods built in single-purpose packages to experience more use among the community.",
    "crumbs": null
  },
  {
    "objectID": "developers/inference/abstractmcmc-interface/index.html#implementing-metropolis-hastings-without-turing",
    "href": "developers/inference/abstractmcmc-interface/index.html#implementing-metropolis-hastings-without-turing",
    "title": "Interface Guide",
    "section": "",
    "text": "Metropolis-Hastings is often the first sampling method that people are exposed to. It is a very straightforward algorithm and is accordingly the easiest to implement, so it makes for a good example. In this section, you will learn how to use the types and functions listed above to implement the Metropolis-Hastings sampler using the MCMC interface.\nThe full code for this implementation is housed in AdvancedMH.jl.\n\n\nLet’s begin by importing the relevant libraries. We’ll import AbstractMCMC, which contains the interface framework we’ll fill out. We also need Distributions and Random.\n\n# Import the relevant libraries.\nusing AbstractMCMC: AbstractMCMC\nusing Distributions\nusing Random\n\nAn interface extension (like the one we’re writing right now) typically requires that you overload or implement several functions. Specifically, you should import the functions you intend to overload. This next code block accomplishes that.\nFrom Distributions, we need Sampleable, VariateForm, and ValueSupport, three abstract types that define a distribution. Models in the interface are assumed to be subtypes of Sampleable{VariateForm, ValueSupport}. In this section our model is going to be extremely simple, so we will not end up using these except to make sure that the inference functions are dispatching correctly.\n\n\n\nLet’s begin our sampler definition by defining a sampler called MetropolisHastings which is a subtype of AbstractSampler. Correct typing is very important for proper interface implementation – if you are missing a subtype, your method may not be dispatched to when you call sample.\n\n# Define a sampler type.\nstruct MetropolisHastings{T,D} &lt;: AbstractMCMC.AbstractSampler\n    init_θ::T\n    proposal::D\nend\n\n# Default constructors.\nMetropolisHastings(init_θ::Real) = MetropolisHastings(init_θ, Normal(0, 1))\nfunction MetropolisHastings(init_θ::Vector{&lt;:Real})\n    return MetropolisHastings(init_θ, MvNormal(zero(init_θ), I))\nend\n\nMetropolisHastings\n\n\nAbove, we have defined a sampler that stores the initial parameterisation of the prior, and a distribution object from which proposals are drawn. You can have a struct that has no fields, and simply use it for dispatching onto the relevant functions, or you can store a large amount of state information in your sampler.\nThe general intuition for what to store in your sampler struct is that anything you may need to perform inference between samples but you don’t want to store in a transition should go into the sampler struct. It’s the only way you can carry non-sample related state information between step! calls.\n\n\n\nNext, we need to have a model of some kind. A model is a struct that’s a subtype of AbstractModel that contains whatever information is necessary to perform inference on your problem. In our case we want to know the mean and variance parameters for a standard Normal distribution, so we can keep our model to the log density of a Normal.\nNote that we only have to do this because we are not yet integrating the sampler with Turing – Turing has a very sophisticated modelling engine that removes the need to define custom model structs.\n\n# Define a model type. Stores the log density function.\nstruct DensityModel{F&lt;:Function} &lt;: AbstractMCMC.AbstractModel\n    ℓπ::F\nend\n\n\n\n\nThe next step is to define some transition which we will return from each step! call. We’ll keep it simple by just defining a wrapper struct that contains the parameter draws and the log density of that draw:\n\n# Create a very basic Transition type, only stores the \n# parameter draws and the log probability of the draw.\nstruct Transition{T,L}\n    θ::T\n    lp::L\nend\n\n# Store the new draw and its log density.\nTransition(model::DensityModel, θ) = Transition(θ, ℓπ(model, θ))\n\nTransition\n\n\nTransition can now store any type of parameter, whether it’s a vector of draws from multiple parameters or a single univariate draw.\n\n\n\nNow it’s time to get into the actual inference. We’ve defined all of the core pieces we need, but we need to implement the step! function which actually performs inference.\nAs a refresher, Metropolis-Hastings implements a very basic algorithm:\n\nPick some initial state, \\theta_0.\nFor t in [1,N], do\n\nGenerate a proposal parameterisation \\theta^\\prime_t \\sim q(\\theta^\\prime_t \\mid \\theta_{t-1}).\nCalculate the acceptance probability, \\alpha = \\text{min}\\left[1,\\frac{\\pi(\\theta'_t)}{\\pi(\\theta_{t-1})} \\frac{q(\\theta_{t-1} \\mid \\theta'_t)}{q(\\theta'_t \\mid \\theta_{t-1})}) \\right].\nIf U \\le \\alpha where U \\sim [0,1], then \\theta_t = \\theta'_t. Otherwise, \\theta_t = \\theta_{t-1}.\n\n\nOf course, it’s much easier to do this in the log space, so the acceptance probability is more commonly written as\n\\log \\alpha = \\min\\left[0, \\log \\pi(\\theta'_t) - \\log \\pi(\\theta_{t-1}) + \\log q(\\theta_{t-1} \\mid \\theta^\\prime_t) - \\log q(\\theta\\prime_t \\mid \\theta_{t-1}) \\right].\nIn interface terms, we should do the following:\n\nMake a new transition containing a proposed sample.\nCalculate the acceptance probability.\nIf we accept, return the new transition, otherwise, return the old one.\n\n\n\n\nThe step! function is the function that performs the bulk of your inference. In our case, we will implement two step! functions – one for the very first iteration, and one for every subsequent iteration.\n\n# Define the first step! function, which is called at the \n# beginning of sampling. Return the initial parameter used\n# to define the sampler.\nfunction AbstractMCMC.step!(\n    rng::AbstractRNG,\n    model::DensityModel,\n    spl::MetropolisHastings,\n    N::Integer,\n    ::Nothing;\n    kwargs...,\n)\n    return Transition(model, spl.init_θ)\nend\n\nThe first step! function just packages up the initial parameterisation inside the sampler, and returns it. We implicitly accept the very first parameterisation.\nThe other step! function performs the usual steps from Metropolis-Hastings. Included are several helper functions, proposal and q, which are designed to replicate the functions in the pseudocode above.\n\nproposal generates a new proposal in the form of a Transition, which can be univariate if the value passed in is univariate, or it can be multivariate if the Transition given is multivariate. Proposals use a basic Normal or MvNormal proposal distribution.\nq returns the log density of one parameterisation conditional on another, according to the proposal distribution.\nstep! generates a new proposal, checks the acceptance probability, and then returns either the previous transition or the proposed transition.\n\n\n# Define a function that makes a basic proposal depending on a univariate\n# parameterisation or a multivariate parameterisation.\nfunction propose(spl::MetropolisHastings, model::DensityModel, θ::Real)\n    return Transition(model, θ + rand(spl.proposal))\nend\nfunction propose(spl::MetropolisHastings, model::DensityModel, θ::Vector{&lt;:Real})\n    return Transition(model, θ + rand(spl.proposal))\nend\nfunction propose(spl::MetropolisHastings, model::DensityModel, t::Transition)\n    return propose(spl, model, t.θ)\nend\n\n# Calculates the probability `q(θ|θcond)`, using the proposal distribution `spl.proposal`.\nq(spl::MetropolisHastings, θ::Real, θcond::Real) = logpdf(spl.proposal, θ - θcond)\nfunction q(spl::MetropolisHastings, θ::Vector{&lt;:Real}, θcond::Vector{&lt;:Real})\n    return logpdf(spl.proposal, θ - θcond)\nend\nq(spl::MetropolisHastings, t1::Transition, t2::Transition) = q(spl, t1.θ, t2.θ)\n\n# Calculate the density of the model given some parameterisation.\nℓπ(model::DensityModel, θ) = model.ℓπ(θ)\nℓπ(model::DensityModel, t::Transition) = t.lp\n\n# Define the other step function. Returns a Transition containing\n# either a new proposal (if accepted) or the previous proposal \n# (if not accepted).\nfunction AbstractMCMC.step!(\n    rng::AbstractRNG,\n    model::DensityModel,\n    spl::MetropolisHastings,\n    ::Integer,\n    θ_prev::Transition;\n    kwargs...,\n)\n    # Generate a new proposal.\n    θ = propose(spl, model, θ_prev)\n\n    # Calculate the log acceptance probability.\n    α = ℓπ(model, θ) - ℓπ(model, θ_prev) + q(spl, θ_prev, θ) - q(spl, θ, θ_prev)\n\n    # Decide whether to return the previous θ or the new one.\n    if log(rand(rng)) &lt; min(α, 0.0)\n        return θ\n    else\n        return θ_prev\n    end\nend\n\n\n\n\nIn the default implementation, sample just returns a vector of all transitions. If instead you would like to obtain a Chains object (e.g., to simplify downstream analysis), you have to implement the bundle_samples function as well. It accepts the vector of transitions and returns a collection of samples. Fortunately, our Transition is incredibly simple, and we only need to build a little bit of functionality to accept custom parameter names passed in by the user.\n\n# A basic chains constructor that works with the Transition struct we defined.\nfunction AbstractMCMC.bundle_samples(\n    rng::AbstractRNG,\n    ℓ::DensityModel,\n    s::MetropolisHastings,\n    N::Integer,\n    ts::Vector{&lt;:Transition},\n    chain_type::Type{Any};\n    param_names=missing,\n    kwargs...,\n)\n    # Turn all the transitions into a vector-of-vectors.\n    vals = copy(reduce(hcat, [vcat(t.θ, t.lp) for t in ts])')\n\n    # Check if we received any parameter names.\n    if ismissing(param_names)\n        param_names = [\"Parameter $i\" for i in 1:(length(first(vals)) - 1)]\n    end\n\n    # Add the log density field to the parameter names.\n    push!(param_names, \"lp\")\n\n    # Bundle everything up and return a Chains struct.\n    return Chains(vals, param_names, (internals=[\"lp\"],))\nend\n\nAll done!\nYou can even implement different output formats by implementing bundle_samples for different chain_types, which can be provided as keyword argument to sample. As default sample uses chain_type = Any.\n\n\n\nNow that we have all the pieces, we should test the implementation by defining a model to calculate the mean and variance parameters of a Normal distribution. We can do this by constructing a target density function, providing a sample of data, and then running the sampler with sample.\n\n# Generate a set of data from the posterior we want to estimate.\ndata = rand(Normal(5, 3), 30)\n\n# Define the components of a basic model.\ninsupport(θ) = θ[2] &gt;= 0\ndist(θ) = Normal(θ[1], θ[2])\ndensity(θ) = insupport(θ) ? sum(logpdf.(dist(θ), data)) : -Inf\n\n# Construct a DensityModel.\nmodel = DensityModel(density)\n\n# Set up our sampler with initial parameters.\nspl = MetropolisHastings([0.0, 0.0])\n\n# Sample from the posterior.\nchain = sample(model, spl, 100000; param_names=[\"μ\", \"σ\"])\n\nIf all the interface functions have been extended properly, you should get an output from display(chain) that looks something like this:\nObject of type Chains, with data of type 100000×3×1 Array{Float64,3}\n\nIterations        = 1:100000\nThinning interval = 1\nChains            = 1\nSamples per chain = 100000\ninternals         = lp\nparameters        = μ, σ\n\n2-element Array{ChainDataFrame,1}\n\nSummary Statistics\n\n│ Row │ parameters │ mean    │ std      │ naive_se   │ mcse       │ ess     │ r_hat   │\n│     │ Symbol     │ Float64 │ Float64  │ Float64    │ Float64    │ Any     │ Any     │\n├─────┼────────────┼─────────┼──────────┼────────────┼────────────┼─────────┼─────────┤\n│ 1   │ μ          │ 5.33157 │ 0.854193 │ 0.0027012  │ 0.00893069 │ 8344.75 │ 1.00009 │\n│ 2   │ σ          │ 4.54992 │ 0.632916 │ 0.00200146 │ 0.00534942 │ 14260.8 │ 1.00005 │\n\nQuantiles\n\n│ Row │ parameters │ 2.5%    │ 25.0%   │ 50.0%   │ 75.0%   │ 97.5%   │\n│     │ Symbol     │ Float64 │ Float64 │ Float64 │ Float64 │ Float64 │\n├─────┼────────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n│ 1   │ μ          │ 3.6595  │ 4.77754 │ 5.33182 │ 5.89509 │ 6.99651 │\n│ 2   │ σ          │ 3.5097  │ 4.09732 │ 4.47805 │ 4.93094 │ 5.96821 │\nIt looks like we’re extremely close to our true parameters of Normal(5,3), though with a fairly high variance due to the low sample size.",
    "crumbs": null
  },
  {
    "objectID": "developers/inference/abstractmcmc-interface/index.html#conclusion",
    "href": "developers/inference/abstractmcmc-interface/index.html#conclusion",
    "title": "Interface Guide",
    "section": "",
    "text": "We’ve seen how to implement the sampling interface for general projects. Turing’s interface methods are ever-evolving, so please open an issue at AbstractMCMC with feature requests or problems.",
    "crumbs": null
  },
  {
    "objectID": "developers/transforms/distributions/index.html",
    "href": "developers/transforms/distributions/index.html",
    "title": "Distributions and the Jacobian",
    "section": "",
    "text": "This series of articles will seek to motivate the Bijectors.jl package, which provides the tools for transforming distributions in the Turing.jl probabilistic programming language.\nIt assumes:\nimport Random\nRandom.seed!(468);\n\nusing Distributions: Normal, LogNormal, logpdf, Distributions\nusing Plots: histogram",
    "crumbs": [
      "Get Started",
      "Developers",
      "Variable Transformations",
      "Distributions and the Jacobian"
    ]
  },
  {
    "objectID": "developers/transforms/distributions/index.html#sampling-from-a-distribution",
    "href": "developers/transforms/distributions/index.html#sampling-from-a-distribution",
    "title": "Distributions and the Jacobian",
    "section": "Sampling from a distribution",
    "text": "Sampling from a distribution\nTo sample from a distribution (as defined in Distributions.jl), we can use the rand function. Let’s sample from a normal distribution and then plot a histogram of the samples.\n\nsamples = rand(Normal(), 5000)\nhistogram(samples, bins=50)\n\n\n\n\n(Calling Normal() without any arguments, as we do here, gives us a normal distribution with mean 0 and standard deviation 1.) If you want to know the log probability density of observing any of the samples, you can use logpdf:\n\nprintln(\"sample: $(samples[1])\")\nprintln(\"logpdf: $(logpdf(Normal(), samples[1]))\")\n\nsample: 0.04374853981619864\nlogpdf: -0.9198955005726975\n\n\nThe probability density function for the normal distribution with mean 0 and standard deviation 1 is\n\\[p(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp{\\left(-\\frac{x^2}{2}\\right)},\\]\nso we could also have calculated this manually using:\n\nlog(1 / sqrt(2π) * exp(-samples[1]^2 / 2))\n\n-0.9198955005726974\n\n\n(or more efficiently, -(samples[1]^2 + log2π) / 2, where log2π is from the IrrationalConstants.jl package).",
    "crumbs": [
      "Get Started",
      "Developers",
      "Variable Transformations",
      "Distributions and the Jacobian"
    ]
  },
  {
    "objectID": "developers/transforms/distributions/index.html#sampling-from-a-transformed-distribution",
    "href": "developers/transforms/distributions/index.html#sampling-from-a-transformed-distribution",
    "title": "Distributions and the Jacobian",
    "section": "Sampling from a transformed distribution",
    "text": "Sampling from a transformed distribution\nSay that \\(x\\) is distributed according to Normal(), and we want to draw samples of \\(y = \\exp(x)\\). Now, \\(y\\) is itself a random variable, and like any other random variable, will have a probability distribution, which we’ll call \\(q(y)\\).\nIn this specific case, the distribution of \\(y\\) is known as a log-normal distribution. For the purposes of this tutorial, let’s implement our own MyLogNormal distribution that we can sample from. (Distributions.jl already defines its own LogNormal, so we have to use a different name.) To do this, we need to overload Base.rand for our new distribution.\n\nstruct MyLogNormal &lt;: Distributions.ContinuousUnivariateDistribution\n    μ::Float64\n    σ::Float64\nend\nMyLogNormal() = MyLogNormal(0.0, 1.0)\n\nfunction Base.rand(rng::Random.AbstractRNG, d::MyLogNormal)\n  exp(rand(rng, Normal(d.μ, d.σ)))\nend\n\nNow we can do the same as above:\n\nsamples_lognormal = rand(MyLogNormal(), 5000)\n# Cut off the tail for clearer visualisation\nhistogram(samples_lognormal, bins=0:0.1:5; xlims=(0, 5))\n\n\n\n\nHow do we implement logpdf for our new distribution, though? Or in other words, if we observe a sample \\(y\\), how do we know what the probability of drawing that sample was?\nNaively, we might think to just un-transform the variable y by reversing the exponential, i.e. taking the logarithm. We could then use the logpdf of the original distribution of x.\n\nnaive_logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y))\n\nnaive_logpdf (generic function with 1 method)\n\n\nWe can compare this function against the logpdf implemented in Distributions.jl:\n\nprintln(\"Sample   : $(samples_lognormal[1])\")\nprintln(\"Expected : $(logpdf(LogNormal(), samples_lognormal[1]))\")\nprintln(\"Actual   : $(naive_logpdf(MyLogNormal(), samples_lognormal[1]))\")\n\nSample   : 2.2331001636281114\nExpected : -2.0450477723405234\nActual   : -1.2416569444078478\n\n\nClearly this approach is not quite correct!",
    "crumbs": [
      "Get Started",
      "Developers",
      "Variable Transformations",
      "Distributions and the Jacobian"
    ]
  },
  {
    "objectID": "developers/transforms/distributions/index.html#the-derivative",
    "href": "developers/transforms/distributions/index.html#the-derivative",
    "title": "Distributions and the Jacobian",
    "section": "The derivative",
    "text": "The derivative\nThe reason why this doesn’t work is because transforming a (continuous) distribution causes probability density to be stretched and otherwise moved around. For example, in the normal distribution, half of the probability density is between \\(-\\infty\\) and \\(0\\), and half is between \\(0\\) and \\(\\infty\\). When exponentiated (i.e. in the log-normal distribution), the first half of the density is mapped to the interval \\((0, 1)\\), and the second half to \\((1, \\infty)\\).\nThis ‘explanation’ on its own does not really mean much, though. A perhaps more useful approach is to not talk about probability densities, but instead to make it more concrete by relating them to actual probabilities. If we think about the normal distribution as a continuous curve, what the probability density function \\(p(x)\\) really tells us is that: for any two points \\(a\\) and \\(b\\) (where \\(a \\leq b\\)), the probability of drawing a sample between \\(a\\) and \\(b\\) is the corresponding area under the curve, i.e.\n\\[\\int_a^b p(x) \\, \\mathrm{d}x.\\]\nFor example, if \\((a, b) = (-\\infty, \\infty)\\), then the probability of drawing a sample between \\(a\\) and \\(b\\) is 1.\nLet’s say that the probability density function of the log-normal distribution is \\(q(y)\\). Then, the area under the curve between the two points \\(\\exp(a)\\) and \\(\\exp(b)\\) is:\n\\[\\int_{\\exp(a)}^{\\exp(b)} q(y) \\, \\mathrm{d}y.\\]\nThis integral should be equal to the one above, because the probability of drawing from \\([a, b]\\) in the original distribution should be the same as the probability of drawing from \\([\\exp(a), \\exp(b)]\\) in the transformed distribution. The question we have to solve here is: how do we find a function \\(q(y)\\) such that this equality holds?\nWe can approach this by making the substitution \\(y = \\exp(x)\\) in the first integral (see Wikipedia for a refresher on substitutions in integrals, if needed). We have that:\n\\[\\frac{\\mathrm{d}y}{\\mathrm{d}x} = \\exp(x) = y \\implies \\mathrm{d}x = \\frac{1}{y}\\,\\mathrm{d}y\\]\nand so\n\\[\\int_{x=a}^{x=b} p(x) \\, \\mathrm{d}x\n  = \\int_{y=\\exp(a)}^{y=\\exp(b)} p(\\log(y)) \\frac{1}{y} \\,\\mathrm{d}y\n  = \\int_{\\exp(a)}^{\\exp(b)} q(y) \\, \\mathrm{d}y,\n\\]\nfrom which we can read off \\(q(y) = p(\\log(y)) / y\\).\nIn contrast, when we implemented naive_logpdf\n\nnaive_logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y))\n\nnaive_logpdf (generic function with 1 method)\n\n\nthat was the equivalent of saying that \\(q(y) = p(\\log(y))\\). We left out a factor of \\(1/y\\)!\nIndeed, now we can define the correct logpdf function. Since everything is a logarithm here, instead of multiplying by \\(1/y\\) we subtract \\(\\log(y)\\):\n\nDistributions.logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y)) - log(y)\n\nand check that it works:\n\nprintln(\"Sample   : $(samples_lognormal[1])\")\nprintln(\"Expected : $(logpdf(LogNormal(), samples_lognormal[1]))\")\nprintln(\"Actual   : $(logpdf(MyLogNormal(), samples_lognormal[1]))\")\n\nSample   : 2.2331001636281114\nExpected : -2.0450477723405234\nActual   : -2.0450477723405234\n\n\nThe same process can be applied to any kind of (invertible) transformation. If we have some transformation from \\(x\\) to \\(y\\), and the probability density functions of \\(x\\) and \\(y\\) are \\(p(x)\\) and \\(q(y)\\) respectively, then we have a general formula that:\n\\[q(y) = p(x) \\left| \\frac{\\mathrm{d}x}{\\mathrm{d}y} \\right|.\\]\nIn this case, we had \\(y = \\exp(x)\\), so \\(\\mathrm{d}x/\\mathrm{d}y = 1/y\\). (This equation is (11.5) in Bishop’s textbook.)\n\n\n\n\n\n\nNote\n\n\n\nThe absolute value here takes care of the case where \\(f\\) is a decreasing function, i.e., \\(f(x) &gt; f(y)\\) when \\(x &lt; y\\). You can try this out with the transformation \\(y = -\\exp(x)\\). If \\(a &lt; b\\), then \\(-\\exp(a) &gt; -\\exp(b)\\), and so you will have to swap the integration limits to ensure that the integral comes out positive.\n\n\nNote that \\(\\mathrm{d}y/\\mathrm{d}x\\) is equal to \\((\\mathrm{d}x/\\mathrm{d}y)^{-1}\\), so the formula above can also be written as:\n\\[q(y) \\left| \\frac{\\mathrm{d}y}{\\mathrm{d}x} \\right| = p(x).\\]",
    "crumbs": [
      "Get Started",
      "Developers",
      "Variable Transformations",
      "Distributions and the Jacobian"
    ]
  },
  {
    "objectID": "developers/transforms/distributions/index.html#the-jacobian",
    "href": "developers/transforms/distributions/index.html#the-jacobian",
    "title": "Distributions and the Jacobian",
    "section": "The Jacobian",
    "text": "The Jacobian\nIn general, we may have transforms that act on multivariate distributions: for example, something mapping \\(p(x_1, x_2)\\) to \\(q(y_1, y_2)\\). In this case, we need to extend the rule above by introducing what is known as the Jacobian matrix:\nIn this case, the rule above has to be extended by replacing the derivative \\(\\mathrm{d}x/\\mathrm{d}y\\) with the determinant of the inverse Jacobian matrix:\n\\[\\mathbf{J} = \\begin{pmatrix}\n\\partial y_1/\\partial x_1 & \\partial y_1/\\partial x_2 \\\\\n\\partial y_2/\\partial x_1 & \\partial y_2/\\partial x_2\n\\end{pmatrix}.\\]\nThis allows us to write the direct generalisation as:\n\\[q(y_1, y_2) \\left| \\det(\\mathbf{J}) \\right| = p(x_1, x_2),\\]\nor equivalently,\n\\[q(y_1, y_2) = p(x_1, x_2) \\left| \\det(\\mathbf{J}^{-1}) \\right|.\\]\nwhere \\(\\mathbf{J}^{-1}\\) is the inverse of the Jacobian matrix. This is the same as equation (11.9) in Bishop.\n\n\n\n\n\n\nNote\n\n\n\nInstead of inverting the original Jacobian matrix to get \\(\\mathbf{J}^{-1}\\), we could also use the Jacobian of the inverse function:\n\\[\\mathbf{J}_\\text{inv} = \\begin{pmatrix}\n\\partial x_1/\\partial y_1 & \\partial x_1/\\partial y_2 \\\\\n\\partial x_2/\\partial y_1 & \\partial x_2/\\partial y_2\n\\end{pmatrix}.\\]\nAs it turns out, these are entirely equivalent: the Jacobian of the inverse function is the inverse of the original Jacobian matrix.\n\n\nThe rest of this section will be devoted to an example to show that this works, and contains some slightly less pretty mathematics. If you are already suitably convinced by this stage, then you can skip the rest of this section. (Or if you prefer something more formal, the Wikipedia article on integration by substitution discusses the multivariate case as well.)\n\nAn example: the Box–Muller transform\nA motivating example where one might like to use a Jacobian is the Box–Muller transform, which is a technique for sampling from a normal distribution.\nThe Box–Muller transform works by first sampling two random variables from the uniform distribution between 0 and 1:\n\\[\\begin{align}\nx_1 &\\sim U(0, 1) \\\\\nx_2 &\\sim U(0, 1).\n\\end{align}\\]\nBoth of these have a probability density function of \\(p(x) = 1\\) for \\(0 &lt; x \\leq 1\\), and 0 otherwise. Because they are independent, we can write that\n\\[p(x_1, x_2) = p(x_1) p(x_2) = \\begin{cases}\n1 & \\text{if } 0 &lt; x_1 \\leq 1 \\text{ and } 0 &lt; x_2 \\leq 1, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\\]\nThe next step is to perform the transforms\n\\[\\begin{align}\ny_1 &= \\sqrt{-2 \\log(x_1)} \\cos(2\\pi x_2); \\\\\ny_2 &= \\sqrt{-2 \\log(x_1)} \\sin(2\\pi x_2),\n\\end{align}\\]\nand it turns out that with these transforms, both \\(y_1\\) and \\(y_2\\) are independent and normally distributed with mean 0 and standard deviation 1, i.e.\n\\[q(y_1, y_2) = \\frac{1}{2\\pi} \\exp{\\left(-\\frac{y_1^2}{2}\\right)} \\exp{\\left(-\\frac{y_2^2}{2}\\right)}.\\]\nHow can we show that this is the case?\nThere are many ways to work out the required calculus. Some are more elegant and some rather less so! One of the less headache-inducing ways is to define the intermediate variables:\n\\[r = \\sqrt{-2 \\log(x_1)}; \\quad \\theta = 2\\pi x_2,\\]\nfrom which we can see that \\(y_1 = r\\cos\\theta\\) and \\(y_2 = r\\sin\\theta\\), and hence\n\\[\\begin{align}\nx_1 &= \\exp{\\left(-\\frac{r^2}{2}\\right)} = \\exp{\\left(-\\frac{y_1^2}{2}\\right)}\\exp{\\left(-\\frac{y_2^2}{2}\\right)}; \\\\\nx_2 &= \\frac{\\theta}{2\\pi} = \\frac{1}{2\\pi} \\, \\arctan\\left(\\frac{y_2}{y_1}\\right).\n\\end{align}\\]\nThis lets us obtain the requisite partial derivatives in a way that doesn’t involve too much algebra. As an example, we have\n\\[\\frac{\\partial x_1}{\\partial y_1} = -y_1 \\exp{\\left(-\\frac{y_1^2}{2}\\right)}\\exp{\\left(-\\frac{y_2^2}{2}\\right)} = -y_1 x_1,\\]\n(where we used the product rule), and\n\\[\\frac{\\partial x_2}{\\partial y_1} = \\frac{1}{2\\pi} \\left(\\frac{1}{1 + (y_2/y_1)^2}\\right) \\left(-\\frac{y_2}{y_1^2}\\right),\\]\n(where we used the chain rule, and the derivative \\(\\mathrm{d}(\\arctan(a))/\\mathrm{d}a = 1/(1 + a^2)\\)).\nPutting together the Jacobian matrix, we have:\n\\[\\mathbf{J} = \\begin{pmatrix}\n-y_1 x_1 & -y_2 x_1 \\\\\n-cy_2/y_1^2 & c/y_1 \\\\\n\\end{pmatrix},\\]\nwhere \\(c = [2\\pi(1 + (y_2/y_1)^2)]^{-1}\\). The determinant of this matrix is\n\\[\\begin{align}\n\\det(\\mathbf{J}) &= -cx_1 - cx_1(y_2/y_1)^2 \\\\\n&= -cx_1\\left[1 + \\left(\\frac{y_2}{y_1}\\right)^2\\right] \\\\\n&= -\\frac{1}{2\\pi} x_1 \\\\\n&= -\\frac{1}{2\\pi}\\exp{\\left(-\\frac{y_1^2}{2}\\right)}\\exp{\\left(-\\frac{y_2^2}{2}\\right)},\n\\end{align}\\]\nComing right back to our probability density, we have that\n\\[\\begin{align}\nq(y_1, y_2) &= p(x_1, x_2) \\cdot |\\det(\\mathbf{J})| \\\\\n&= \\frac{1}{2\\pi}\\exp{\\left(-\\frac{y_1^2}{2}\\right)}\\exp{\\left(-\\frac{y_2^2}{2}\\right)},\n\\end{align}\\]\nas desired.\n\n\n\n\n\n\nNote\n\n\n\nWe haven’t yet explicitly accounted for the fact that \\(p(x_1, x_2)\\) is 0 if either \\(x_1\\) or \\(x_2\\) are outside the range \\((0, 1]\\). For example, if this constraint on \\(x_1\\) and \\(x_2\\) were to result in inaccessible values of \\(y_1\\) or \\(y_2\\), then \\(q(y_1, y_2)\\) should be 0 for those values. Formally, for the transformation \\(f: X \\to Y\\) where \\(X\\) is the unit square (i.e. \\(0 &lt; x_1, x_2 \\leq 1\\)), \\(q(y_1, y_2)\\) should only take the above value for the image of \\(f\\), and anywhere outside of the image it should be 0.\nIn our case, the \\(\\log(x_1)\\) term in the transform varies between 0 and \\(\\infty\\), and the \\(\\cos(2\\pi x_2)\\) term ranges from \\(-1\\) to \\(1\\). Hence \\(y_1\\), which is the product of these two terms, ranges from \\(-\\infty\\) to \\(\\infty\\), and likewise for \\(y_2\\). So the image of \\(f\\) is the entire real plane, and we don’t have to worry about this.\n\n\nHaving seen the theory that underpins how distributions can be transformed, let’s now turn to how this is implemented in the Turing ecosystem.",
    "crumbs": [
      "Get Started",
      "Developers",
      "Variable Transformations",
      "Distributions and the Jacobian"
    ]
  },
  {
    "objectID": "developers/transforms/bijectors/index.html",
    "href": "developers/transforms/bijectors/index.html",
    "title": "Bijectors in MCMC",
    "section": "",
    "text": "All the above has been purely a mathematical discussion of how distributions can be transformed. Now, we turn to their implementation in Julia, specifically using the Bijectors.jl package.",
    "crumbs": [
      "Get Started",
      "Developers",
      "Variable Transformations",
      "Bijectors in MCMC"
    ]
  },
  {
    "objectID": "developers/transforms/bijectors/index.html#bijectors.jl",
    "href": "developers/transforms/bijectors/index.html#bijectors.jl",
    "title": "Bijectors in MCMC",
    "section": "Bijectors.jl",
    "text": "Bijectors.jl\n\nimport Random\nRandom.seed!(468);\n\nusing Distributions: Normal, LogNormal, logpdf\nusing Statistics: mean, var\nusing Plots: histogram\n\nA bijection between two sets (Wikipedia) is, essentially, a one-to-one mapping between the elements of these sets. That is to say, if we have two sets \\(X\\) and \\(Y\\), then a bijection maps each element of \\(X\\) to a unique element of \\(Y\\). To return to our univariate example, where we transformed \\(x\\) to \\(y\\) using \\(y = \\exp(x)\\), the exponentiation function is a bijection because every value of \\(x\\) maps to one unique value of \\(y\\). The input set (the domain) is \\((-\\infty, \\infty)\\), and the output set (the codomain) is \\((0, \\infty)\\). (Here, \\((a, b)\\) denotes the open interval from \\(a\\) to \\(b\\) but excluding \\(a\\) and \\(b\\) themselves.)\nSince bijections are a one-to-one mapping between elements, we can also reverse the direction of this mapping to create an inverse function. In the case of \\(y = \\exp(x)\\), the inverse function is \\(x = \\log(y)\\).\n\n\n\n\n\n\nNote\n\n\n\nTechnically, the bijections in Bijectors.jl are functions \\(f: X \\to Y\\) for which:\n\n\\(f\\) is continuously differentiable, i.e. the derivative \\(\\mathrm{d}f(x)/\\mathrm{d}x\\) exists and is continuous (over the domain of interest \\(X\\));\nIf \\(f^{-1}: Y \\to X\\) is the inverse of \\(f\\), then that is also continuously differentiable (over its own domain, i.e. \\(Y\\)).\n\nThe technical mathematical term for this is a diffeomorphism (Wikipedia), but we call them ‘bijectors’.\nWhen thinking about continuous differentiability, it’s important to be conscious of the domains or codomains that we care about. For example, taking the inverse function \\(\\log(y)\\) from above, its derivative is \\(1/y\\), which is not continuous at \\(y = 0\\). However, we specified that the bijection \\(y = \\exp(x)\\) maps values of \\(x \\in (-\\infty, \\infty)\\) to \\(y \\in (0, \\infty)\\), so the point \\(y = 0\\) is not within the domain of the inverse function.\n\n\nSpecifically, one of the primary purposes of Bijectors.jl is to construct bijections which map constrained distributions to unconstrained ones. For example, the log-normal distribution which we saw in the previous page is constrained: its support, i.e. the range over which \\(p(x) &gt; 0\\), is \\((0, \\infty)\\). However, we can transform that to an unconstrained distribution (the normal distribution) using the transformation \\(y = \\log(x)\\).\n\n\n\n\n\n\nNote\n\n\n\nBijectors.jl, as well as DynamicPPL (which we’ll come to later), can work with a much broader class of bijective transformations of variables, not just ones that go to the entire real line. But for the purposes of MCMC, unconstraining is the most common transformation, so we’ll stick with that terminology.\n\n\nThe bijector function, when applied to a distribution, returns a bijection \\(f\\) that can be used to map the constrained distribution to an unconstrained one. Unsurprisingly, for the log-normal distribution, the bijection is (a broadcasted version of) the \\(\\log\\) function.\n\nimport Bijectors as B\n\nf = B.bijector(LogNormal())\n\n(::Base.Fix1{typeof(broadcast), typeof(log)}) (generic function with 1 method)\n\n\nWe can apply this transformation to samples from the original distribution, for example:\n\nsamples_lognormal = rand(LogNormal(), 5)\n\nsamples_normal = f(samples_lognormal)\n\n5-element Vector{Float64}:\n  0.07200886749732066\n -0.07404375655951738\n  0.6327762377562545\n -0.9799776018729268\n  1.6115229499167665\n\n\nWe can also obtain the inverse of a bijection, \\(f^{-1}\\):\n\nf_inv = B.inverse(f)\n\nf_inv(samples_normal) == samples_lognormal\n\ntrue\n\n\nWe know that the transformation \\(y = \\log(x)\\) changes the log-normal distribution to the normal distribution. Bijectors.jl also gives us a way to access that transformed distribution:\n\ntransformed_dist = B.transformed(LogNormal(), f)\n\nBijectors.UnivariateTransformed{Distributions.LogNormal{Float64}, Base.Fix1{typeof(broadcast), typeof(log)}}(\ndist: Distributions.LogNormal{Float64}(μ=0.0, σ=1.0)\ntransform: Base.Fix1{typeof(broadcast), typeof(log)}(broadcast, log)\n)\n\n\nThis type doesn’t immediately look like a Normal(), but it behaves in exactly the same way. For example, we can sample from it and plot a histogram:\n\nsamples_plot = rand(transformed_dist, 5000)\nhistogram(samples_plot, bins=50)\n\n\n\n\nWe can also obtain the logpdf of the transformed distribution and check that it is the same as that of a normal distribution:\n\nprintln(\"Sample:   $(samples_plot[1])\")\nprintln(\"Expected: $(logpdf(Normal(), samples_plot[1]))\")\nprintln(\"Actual:   $(logpdf(transformed_dist, samples_plot[1]))\")\n\nSample:   -0.2031149013821452\nExpected: -0.9395663647864121\nActual:   -0.9395663647864121\n\n\nGiven the discussion in the previous sections, you might not be surprised to find that the logpdf of the transformed distribution is implemented using the Jacobian of the transformation. In particular, it directly uses the formula\n\\[\\log(q(\\mathbf{y})) = \\log(p(\\mathbf{x})) - \\log(|\\det(\\mathbf{J})|).\\]\nYou can access \\(\\log(|\\det(\\mathbf{J})|)\\) (evaluated at the point \\(\\mathbf{x}\\)) using the logabsdetjac function:\n\n# Reiterating the setup, just to be clear\noriginal_dist = LogNormal()\nx = rand(original_dist)\nf = B.bijector(original_dist)\ny = f(x)\ntransformed_dist = B.transformed(LogNormal(), f)\n\nprintln(\"log(q(y))     : $(logpdf(transformed_dist, y))\")\nprintln(\"log(p(x))     : $(logpdf(original_dist, x))\")\nprintln(\"log(|det(J)|) : $(B.logabsdetjac(f, x))\")\n\nlog(q(y))     : -0.9258400203646245\nlog(p(x))     : -0.8083539602557612\nlog(|det(J)|) : 0.11748606010886327\n\n\nfrom which you can see that the equation above holds. There are more functions available in the Bijectors.jl API; for full details do check out the documentation. For example, logpdf_with_trans can directly give us \\(\\log(q(\\mathbf{y}))\\) without going through the effort of constructing the bijector:\n\nB.logpdf_with_trans(original_dist, x, true)\n\n-0.9258400203646245",
    "crumbs": [
      "Get Started",
      "Developers",
      "Variable Transformations",
      "Bijectors in MCMC"
    ]
  },
  {
    "objectID": "developers/transforms/bijectors/index.html#the-case-for-bijectors-in-mcmc",
    "href": "developers/transforms/bijectors/index.html#the-case-for-bijectors-in-mcmc",
    "title": "Bijectors in MCMC",
    "section": "The case for bijectors in MCMC",
    "text": "The case for bijectors in MCMC\nConstraints pose a challenge for many numerical methods such as optimisation, and sampling is no exception to this. The problem is that for any value \\(x\\) outside of the support of a constrained distribution, \\(p(x)\\) will be zero, and the logpdf will be \\(-\\infty\\). Thus, any term that involves some ratio of probabilities (or equivalently, the logpdf) will be infinite.\n\nMetropolis with rejection\nTo see the practical impact of this on sampling, let’s attempt to sample from a log-normal distribution using a random walk Metropolis algorithm.\nOne way of handling constraints is to simply reject any steps that would take us out of bounds. This is a barebones implementation which does precisely that:\n\n# Take a step where the proposal is a normal distribution centred around\n# the current value. Return the new value, plus a flag to indicate whether\n# the new value was in bounds.\nfunction mh_step(logp, x, in_bounds)\n    x_proposed = rand(Normal(x, 1))\n    in_bounds(x_proposed) || return (x, false)  # bounds check\n    acceptance_logp = logp(x_proposed) - logp(x)\n    return if log(rand()) &lt; acceptance_logp\n        (x_proposed, true)  # successful step\n    else\n        (x, true)  # failed step\n    end\nend\n\n# Run a random walk Metropolis sampler.\n# `logp`      : a function that takes `x` and returns the log pdf of the\n#               distribution we're trying to sample from (up to a constant\n#               additive factor)\n# `n_samples` : the number of samples to draw\n# `in_bounds` : a function that takes `x` and returns whether `x` is within\n#               the support of the distribution\n# `x0`        : the initial value\n# Returns a vector of samples, plus the number of times we went out of bounds.\nfunction mh(logp, n_samples, in_bounds; x0=1.0)\n    samples = [x0]\n    x = x0\n    n_out_of_bounds = 0\n    for _ in 2:n_samples\n        x, inb = mh_step(logp, x, in_bounds)\n        if !inb\n            n_out_of_bounds += 1\n        end\n        push!(samples, x)\n    end\n    return (samples, n_out_of_bounds)\nend\n\nmh (generic function with 1 method)\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the MH algorithm, we technically do not need to explicitly check the proposal, because for any \\(x \\leq 0\\), we have that \\(p(x) = 0\\); thus, the acceptance probability will be zero. However, doing so here allows us to track how often this happens, and also illustrates the general principle of handling constraints by rejection.\n\n\nNow to actually perform the sampling:\n\nlogp(x) = logpdf(LogNormal(), x)\nsamples, n_out_of_bounds = mh(logp, 10000, x -&gt; x &gt; 0)\nhistogram(samples, bins=0:0.1:5; xlims=(0, 5))\n\n\n\n\nHow do we know that this has sampled correctly? For one, we can check that the mean of the samples are what we expect them to be. From Wikipedia, the mean of a log-normal distribution is given by \\(\\exp[\\mu + (\\sigma^2/2)]\\). For our log-normal distribution, we set \\(\\mu = 0\\) and \\(\\sigma = 1\\), so:\n\nprintln(\"expected mean: $(exp(0 + (1^2/2)))\")\nprintln(\"  actual mean: $(mean(samples))\")\n\nexpected mean: 1.6487212707001282\n  actual mean: 1.3347941996487\n\n\n\n\nMetropolis with transformation\nThe issue with this is that many of the sampling steps are unproductive, in that they bring us to the region of \\(x \\leq 0\\) and get rejected:\n\nprintln(\"went out of bounds $n_out_of_bounds/10000 times\")\n\nwent out of bounds 1870/10000 times\n\n\nAnd this could have been even worse if we had chosen a wider proposal distribution in the Metropolis step, or if the support of the distribution was narrower! In general, we probably don’t want to have to re-parameterise our proposal distribution each time we sample from a distribution with different constraints.\nThis is where the transformation functions from Bijectors.jl come in: we can use them to map the distribution to an unconstrained one and sample from that instead. Since the sampler only ever sees an unconstrained distribution, it doesn’t have to worry about checking for bounds.\nTo make this happen, instead of passing \\(\\log(p(x))\\) to the sampler, we pass \\(\\log(q(y))\\). This can be obtained using the Bijectors.logpdf_with_trans function that was introduced above.\n\nd = LogNormal()\nf = B.bijector(d)     # Transformation function\nf_inv = B.inverse(f)  # Inverse transformation function\nfunction logq(y)\n    x = f_inv(y)\n    return B.logpdf_with_trans(d, x, true)\nend\nsamples_transformed, n_oob_transformed = mh(logq, 10000, x -&gt; true);\n\nNow, this process gives us samples that have been transformed, so we need to un-transform them to get the samples from the original distribution:\n\nsamples_untransformed = f_inv(samples_transformed)\nhistogram(samples_untransformed, bins=0:0.1:5; xlims=(0, 5))\n\n\n\n\nWe can check the mean of the samples too, to see that it is what we expect:\n\nprintln(\"expected mean: $(exp(0 + (1^2/2)))\")\nprintln(\"  actual mean: $(mean(samples_untransformed))\")\n\nexpected mean: 1.6487212707001282\n  actual mean: 1.7184757306010636\n\n\nOn top of that, we can also verify that we don’t ever go out of bounds:\n\nprintln(\"went out of bounds $n_oob_transformed/10000 times\")\n\nwent out of bounds 0/10000 times\n\n\n\n\nWhich one is better?\nIn the subsections above, we’ve seen two different methods of sampling from a constrained distribution:\n\nSample directly from the distribution and reject any samples outside of its support.\nTransform the distribution to an unconstrained one and sample from that instead.\n\n(Note that both of these methods are applicable to other samplers as well, such as Hamiltonian Monte Carlo.)\nOf course, a natural question to then ask is which one of these is better!\nOne option might be look at the sample means above to see which one is ‘closer’ to the expected mean. However, that’s not a very robust method because the sample mean is itself random, and if we were to use a different random seed we might well reach a different conclusion.\nAnother possibility we could look at the number of times the sample was rejected. Does a lower rejection rate (as in the transformed case) imply that the method is better? As it happens, this might seem like an intuitive conclusion, but it’s not necessarily the case: for example, the sampling in unconstrained space could be much less efficient, such that even though we’re not rejecting samples, the ones that we do get are overly correlated and thus not representative of the distribution.\nA robust comparison would involve performing both methods many times and seeing how reliable the sample mean is.\n\nfunction get_sample_mean(; transform)\n    if transform\n       # Sample from transformed distribution\n       samples = f_inv(first(mh(logq, 10000, x -&gt; true)))\n    else\n       # Sample from original distribution and reject if out of bounds\n       samples = first(mh(logp, 10000, x -&gt; x &gt; 0))\n    end\n    return mean(samples)\nend\n\nget_sample_mean (generic function with 1 method)\n\n\n\nmeans_with_rejection = [get_sample_mean(; transform=false) for _ in 1:1000]\nmean(means_with_rejection), var(means_with_rejection)\n\n(1.652032684314151, 0.30454613712270745)\n\n\n\nmeans_with_transformation = [get_sample_mean(; transform=true) for _ in 1:1000]\nmean(means_with_transformation), var(means_with_transformation)\n\n(1.6489347143276902, 0.003945513418875533)\n\n\nWe can see from this small study that although both methods give us the correct mean (on average), the method with the transformation is more reliable, in that the variance is much lower!\n\n\n\n\n\n\nNote\n\n\n\nAlternatively, we could also try to directly measure how correlated the samples are. One way to do this is to calculate the effective sample size (ESS), which is described in the Stan documentation, and implemented in MCMCChains.jl. A larger ESS implies that the samples are less correlated, and thus more representative of the underlying distribution:\n\nusing MCMCChains: Chains, ess\n\nrejection = first(mh(logp, 10000, x -&gt; x &gt; 0))\ntransformation = f_inv(first(mh(logq, 10000, x -&gt; true)))\nchn = Chains(hcat(rejection, transformation), [:rejection, :transformation])\ness(chn)\n\n\nESS\n\n      parameters         ess   ess_per_sec\n          Symbol     Float64       Missing\n\n       rejection    503.4349       missing\n  transformation   1106.6909       missing\n\n\n\n\n\n\n\n\n\nWhat happens without the Jacobian?\nIn the transformation method above, we used Bijectors.logpdf_with_trans to calculate the log probability density of the transformed distribution. This function makes sure to include the Jacobian term when performing the transformation, and this is what makes sure that when we un-transform the samples, we get the correct distribution.\nThe next code block shows what happens if we don’t include the Jacobian term. In this logq_wrong, we’ve un-transformed y to x and calculated the logpdf with respect to its original distribution. This is exactly the same mistake that we made at the start of this article with naive_logpdf.\n\nfunction logq_wrong(y)\n    x = f_inv(y)\n    return logpdf(d, x)  # no Jacobian term!\nend\nsamples_questionable, _ = mh(logq_wrong, 100000, x -&gt; true)\nsamples_questionable_untransformed = f_inv(samples_questionable)\n\nprintln(\"mean: $(mean(samples_questionable_untransformed))\")\n\nmean: 0.5919166187308191\n\n\nYou can see that even though we used ten times more samples, the mean is quite wrong, which implies that our samples are not being drawn from the correct distribution.\nIn the next page, we’ll see how to use these transformations in the context of a probabilistic programming language, paying particular attention to their handling in DynamicPPL.",
    "crumbs": [
      "Get Started",
      "Developers",
      "Variable Transformations",
      "Bijectors in MCMC"
    ]
  },
  {
    "objectID": "developers/compiler/minituring-compiler/index.html",
    "href": "developers/compiler/minituring-compiler/index.html",
    "title": "A Mini Turing Implementation I: Compiler",
    "section": "",
    "text": "In this tutorial we develop a very simple probabilistic programming language. The implementation is similar to DynamicPPL. This is intentional as we want to demonstrate some key ideas from Turing’s internal implementation.\nTo make things easy to understand and to implement we restrict our language to a very simple subset of the language that Turing actually supports. Defining an accurate syntax description is not our goal here, instead, we give a simple example and all similar programs should work.\n\nConsider a probabilistic model defined by\n\\[\n\\begin{aligned}\na &\\sim \\operatorname{Normal}(0.5, 1^2) \\\\\nb &\\sim \\operatorname{Normal}(a, 2^2) \\\\\nx &\\sim \\operatorname{Normal}(b, 0.5^2)\n\\end{aligned}\n\\]\nWe assume that x is data, i.e., an observed variable. In our small language this model will be defined as\n\n@mini_model function m(x)\n    a ~ Normal(0.5, 1)\n    b ~ Normal(a, 2)\n    x ~ Normal(b, 0.5)\n    return nothing\nend\n\nSpecifically, we demand that\n\nall observed variables are arguments of the program,\nthe model definition does not contain any control flow,\nall variables are scalars, and\nthe function returns nothing.\n\nFirst, we import some required packages:\n\nusing MacroTools, Distributions, Random, AbstractMCMC, MCMCChains\n\nBefore getting to the actual “compiler”, we first build the data structure for the program trace. A program trace for a probabilistic programming language needs to at least record the values of stochastic variables and their log-probabilities.\n\nstruct VarInfo{V,L}\n    values::V\n    logps::L\nend\n\nVarInfo() = VarInfo(Dict{Symbol,Float64}(), Dict{Symbol,Float64}())\n\nfunction Base.setindex!(varinfo::VarInfo, (value, logp), var_id)\n    varinfo.values[var_id] = value\n    varinfo.logps[var_id] = logp\n    return varinfo\nend\n\nInternally, our probabilistic programming language works with two main functions:\n\nassume for sampling unobserved variables and computing their log-probabilities, and\nobserve for computing log-probabilities of observed variables (but not sampling them).\n\nFor different inference algorithms we may have to use different sampling procedures and different log-probability computations. For instance, in some cases we might want to sample all variables from their prior distributions and in other cases we might only want to compute the log-likelihood of the observations based on a given set of values for the unobserved variables. Thus depending on the inference algorithm we want to use different assume and observe implementations. We can achieve this by providing this context information as a function argument to assume and observe.\nNote: Although the context system in this tutorial is inspired by DynamicPPL, it is very simplistic. We expand this mini Turing example in the contexts tutorial with some more complexity, to illustrate how and why contexts are central to Turing’s design. For the full details one still needs to go to the actual source of DynamicPPL though.\nHere we can see the implementation of a sampler that draws values of unobserved variables from the prior and computes the log-probability for every variable.\n\nstruct SamplingContext{S&lt;:AbstractMCMC.AbstractSampler,R&lt;:Random.AbstractRNG}\n    rng::R\n    sampler::S\nend\n\nstruct PriorSampler &lt;: AbstractMCMC.AbstractSampler end\n\nfunction observe(context::SamplingContext, varinfo, dist, var_id, var_value)\n    logp = logpdf(dist, var_value)\n    varinfo[var_id] = (var_value, logp)\n    return nothing\nend\n\nfunction assume(context::SamplingContext{PriorSampler}, varinfo, dist, var_id)\n    sample = Random.rand(context.rng, dist)\n    logp = logpdf(dist, sample)\n    varinfo[var_id] = (sample, logp)\n    return sample\nend;\n\nNext we define the “compiler” for our simple programming language. The term compiler is actually a bit misleading here since its only purpose is to transform the function definition in the @mini_model macro by\n\nadding the context information (context) and the tracing data structure (varinfo) as additional arguments, and\nreplacing tildes with calls to assume and observe.\n\nAfterwards, as usual the Julia compiler will just-in-time compile the model function when it is called.\nThe manipulation of Julia expressions is an advanced part of the Julia language. The Julia documentation provides an introduction to and more details about this so-called metaprogramming.\n\nmacro mini_model(expr)\n    return esc(mini_model(expr))\nend\n\nfunction mini_model(expr)\n    # Split the function definition into a dictionary with its name, arguments, body etc.\n    def = MacroTools.splitdef(expr)\n\n    # Replace tildes in the function body with calls to `assume` or `observe`\n    def[:body] = MacroTools.postwalk(def[:body]) do sub_expr\n        if MacroTools.@capture(sub_expr, var_ ~ dist_)\n            if var in def[:args]\n                # If the variable is an argument of the model function, it is observed\n                return :($(observe)(context, varinfo, $dist, $(Meta.quot(var)), $var))\n            else\n                # Otherwise it is unobserved\n                return :($var = $(assume)(context, varinfo, $dist, $(Meta.quot(var))))\n            end\n        else\n            return sub_expr\n        end\n    end\n\n    # Add `context` and `varinfo` arguments to the model function\n    def[:args] = vcat(:varinfo, :context, def[:args])\n\n    # Reassemble the function definition from its name, arguments, body etc.\n    return MacroTools.combinedef(def)\nend;\n\nFor inference, we make use of the AbstractMCMC interface. It provides a default implementation of a sample function for sampling a Markov chain. The default implementation already supports e.g. sampling of multiple chains in parallel, thinning of samples, or discarding initial samples.\nThe AbstractMCMC interface requires us to at least\n\ndefine a model that is a subtype of AbstractMCMC.AbstractModel,\ndefine a sampler that is a subtype of AbstractMCMC.AbstractSampler,\nimplement AbstractMCMC.step for our model and sampler.\n\nThus here we define a MiniModel model. In this model we store the model function and the observed data.\n\nstruct MiniModel{F,D} &lt;: AbstractMCMC.AbstractModel\n    f::F\n    data::D # a NamedTuple of all the data\nend\n\nIn the Turing compiler, the model-specific DynamicPPL.Model is constructed automatically when calling the model function. But for the sake of simplicity here we construct the model manually.\nTo illustrate probabilistic inference with our mini language we implement an extremely simplistic Random-Walk Metropolis-Hastings sampler. We hard-code the proposal step as part of the sampler and only allow normal distributions with zero mean and fixed standard deviation. The Metropolis-Hastings sampler in Turing is more flexible.\n\nstruct MHSampler{T&lt;:Real} &lt;: AbstractMCMC.AbstractSampler\n    sigma::T\nend\n\nMHSampler() = MHSampler(1)\n\nfunction assume(context::SamplingContext{&lt;:MHSampler}, varinfo, dist, var_id)\n    sampler = context.sampler\n    old_value = varinfo.values[var_id]\n\n    # propose a random-walk step, i.e, add the current value to a random\n    # value sampled from a Normal distribution centred at 0\n    value = rand(context.rng, Normal(old_value, sampler.sigma))\n    logp = Distributions.logpdf(dist, value)\n    varinfo[var_id] = (value, logp)\n\n    return value\nend;\n\nWe need to define two step functions, one for the first step and the other for the following steps. In the first step we sample values from the prior distributions and in the following steps we sample with the random-walk proposal. The two functions are identified by the different arguments they take.\n\n# The first step: Sampling from the prior distributions\nfunction AbstractMCMC.step(\n    rng::Random.AbstractRNG, model::MiniModel, sampler::MHSampler; kwargs...\n)\n    vi = VarInfo()\n    ctx = SamplingContext(rng, PriorSampler())\n    model.f(vi, ctx, values(model.data)...)\n    return vi, vi\nend\n\n# The following steps: Sampling with random-walk proposal\nfunction AbstractMCMC.step(\n    rng::Random.AbstractRNG,\n    model::MiniModel,\n    sampler::MHSampler,\n    prev_state::VarInfo; # is just the old trace\n    kwargs...\n)\n    vi = prev_state\n    new_vi = deepcopy(vi)\n    ctx = SamplingContext(rng, sampler)\n    model.f(new_vi, ctx, values(model.data)...)\n\n    # Compute log acceptance probability\n    # Since the proposal is symmetric the computation can be simplified\n    logα = sum(values(new_vi.logps)) - sum(values(vi.logps))\n\n    # Accept proposal with computed acceptance probability\n    if -randexp(rng) &lt; logα\n        return new_vi, new_vi\n    else\n        return prev_state, prev_state\n    end\nend;\n\nTo make it easier to analyse the samples and compare them with results from Turing, additionally we define a version of AbstractMCMC.bundle_samples for our model and sampler that returns a MCMCChains.Chains object of samples.\n\nfunction AbstractMCMC.bundle_samples(\n    samples, model::MiniModel, ::MHSampler, ::Any, ::Type{Chains}; kwargs...\n)\n    # We get a vector of traces\n    values = [sample.values for sample in samples]\n    params = [key for key in keys(values[1]) if key ∉ keys(model.data)]\n    vals = reduce(hcat, [value[p] for value in values] for p in params)\n    # Composing the `Chains` data-structure, of which analysing infrastructure is provided\n    chains = Chains(vals, params)\n    return chains\nend;\n\nLet us check how our mini probabilistic programming language works. We define the probabilistic model:\n\n@mini_model function m(x)\n    a ~ Normal(0.5, 1)\n    b ~ Normal(a, 2)\n    x ~ Normal(b, 0.5)\n    return nothing\nend;\n\nThe @mini_model macro expands this into another function, m, which effectively calls either assume or observe on each variable as needed:\n\n@macroexpand @mini_model function m(x)\n    a ~ Normal(0.5, 1)\n    b ~ Normal(a, 2)\n    x ~ Normal(b, 0.5)\n    return nothing\nend\n\n\n:(function m(varinfo, context, x; )\n      #= /home/runner/work/docs/docs/developers/compiler/minituring-compiler/index.qmd:276 =#\n      #= /home/runner/work/docs/docs/developers/compiler/minituring-compiler/index.qmd:277 =#\n      a = (assume)(context, varinfo, Normal(0.5, 1), :a)\n      #= /home/runner/work/docs/docs/developers/compiler/minituring-compiler/index.qmd:278 =#\n      b = (assume)(context, varinfo, Normal(a, 2), :b)\n      #= /home/runner/work/docs/docs/developers/compiler/minituring-compiler/index.qmd:279 =#\n      (observe)(context, varinfo, Normal(b, 0.5), :x, x)\n      #= /home/runner/work/docs/docs/developers/compiler/minituring-compiler/index.qmd:280 =#\n      return nothing\n  end)\n\n\n\nWe can use this function to construct the MiniModel, and then perform inference with data x = 3.0:\n\nsample(MiniModel(m, (x=3.0,)), MHSampler(), 1_000_000; chain_type=Chains, progress=false)\n\nChains MCMC chain (1000000×2×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000000\nNumber of chains  = 1\nSamples per chain = 1000000\nparameters        = a, b\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nWe compare these results with Turing.\n\nusing Turing\nusing PDMats\n\n@model function turing_m(x)\n    a ~ Normal(0.5, 1)\n    b ~ Normal(a, 2)\n    x ~ Normal(b, 0.5)\n    return nothing\nend\n\nsample(turing_m(3.0), MH(ScalMat(2, 1.0)), 1_000_000, progress=false)\n\nChains MCMC chain (1000000×5×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000000\nNumber of chains  = 1\nSamples per chain = 1000000\nWall duration     = 8.75 seconds\nCompute duration  = 8.75 seconds\nparameters        = a, b\ninternals         = logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nAs you can see, with our simple probabilistic programming language and custom samplers we get similar results as Turing.\n\n\n\n\n Back to top",
    "crumbs": [
      "Get Started",
      "Developers",
      "DynamicPPL's Compiler",
      "A Mini Turing Implementation I: Compiler"
    ]
  },
  {
    "objectID": "developers/compiler/design-overview/index.html",
    "href": "developers/compiler/design-overview/index.html",
    "title": "Turing Compiler Design (Outdated)",
    "section": "",
    "text": "In this section, the current design of Turing’s model “compiler” is described, which enables Turing to perform various types of Bayesian inference without changing the model definition. The “compiler” is essentially just a macro that rewrites the user’s model definition to a function that generates a Model struct that Julia’s dispatch can operate on and that Julia’s compiler can successfully do type inference on for efficient machine code generation.",
    "crumbs": null
  },
  {
    "objectID": "developers/compiler/design-overview/index.html#the-model",
    "href": "developers/compiler/design-overview/index.html#the-model",
    "title": "Turing Compiler Design (Outdated)",
    "section": "The model",
    "text": "The model\n&lt;!– Very outdated A model::Model is a callable struct that one can sample from by calling\n#| eval: false\n(model::Model)([rng, varinfo, sampler, context])\nwhere rng is a random number generator (default: Random.default_rng()), varinfo is a data structure that stores information about the random variables (default: DynamicPPL.VarInfo()), sampler is a sampling algorithm (default: DynamicPPL.SampleFromPrior()), and context is a sampling context that can, for example, modify how the log probability is accumulated (default: DynamicPPL.DefaultContext()).\nSampling resets the log joint probability of varinfo and increases the evaluation counter of sampler. If context is a LikelihoodContext, only the log likelihood of D will be accumulated, whereas with PriorContext only the log prior probability of P is. With the DefaultContext the log joint probability of both P and D is accumulated.\nThe Model struct contains the four internal fields f, args, defaults, and context. When model::Model is called, then the internal function model.f is called as model.f(rng, varinfo, sampler, context, model.args...) (for multithreaded sampling, instead of varinfo a threadsafe wrapper is passed to model.f). The positional and keyword arguments that were passed to the user-defined model function when the model was created are saved as a NamedTuple in model.args. The default values of the positional and keyword arguments of the user-defined model functions, if any, are saved as a NamedTuple in model.defaults. They are used for constructing model instances with different arguments by the logprob and prob string macros. The context variable sets an evaluation context that can be used to control, for instance, whether log probabilities should be evaluated for the prior, likelihood, or joint probability. By default it is set to evaluate the log joint.",
    "crumbs": null
  },
  {
    "objectID": "developers/compiler/design-overview/index.html#step-1-break-up-the-model-definition",
    "href": "developers/compiler/design-overview/index.html#step-1-break-up-the-model-definition",
    "title": "Turing Compiler Design (Outdated)",
    "section": "Step 1: Break up the model definition",
    "text": "Step 1: Break up the model definition\nFirst, the @model macro breaks up the user-provided function definition using DynamicPPL.build_model_info. This function returns a dictionary consisting of:\n\nallargs_exprs: The expressions of the positional and keyword arguments, without default values.\nallargs_syms: The names of the positional and keyword arguments, e.g., [:x, :y, :TV] above.\nallargs_namedtuple: An expression that constructs a NamedTuple of the positional and keyword arguments, e.g., :((x = x, y = y, TV = TV)) above.\ndefaults_namedtuple: An expression that constructs a NamedTuple of the default positional and keyword arguments, if any, e.g., :((x = missing, y = 1, TV = Vector{Float64})) above.\nmodeldef: A dictionary with the name, arguments, and function body of the model definition, as returned by MacroTools.splitdef.",
    "crumbs": null
  },
  {
    "objectID": "developers/compiler/design-overview/index.html#step-2-generate-the-body-of-the-internal-model-function",
    "href": "developers/compiler/design-overview/index.html#step-2-generate-the-body-of-the-internal-model-function",
    "title": "Turing Compiler Design (Outdated)",
    "section": "Step 2: Generate the body of the internal model function",
    "text": "Step 2: Generate the body of the internal model function\nIn a second step, DynamicPPL.generate_mainbody generates the main part of the transformed function body using the user-provided function body and the provided function arguments, without default values, for figuring out if a variable denotes an observation or a random variable. Hereby the function DynamicPPL.generate_tilde replaces the L ~ R lines in the model and the function DynamicPPL.generate_dot_tilde replaces the @. L ~ R and L .~ R lines in the model.\nIn the above example, p[1] ~ InverseGamma(2, 3) is replaced with something similar to\n#| eval: false\n#= REPL[25]:6 =#\nbegin\n    var\"##tmpright#323\" = InverseGamma(2, 3)\n    var\"##tmpright#323\" isa Union{Distribution,AbstractVector{&lt;:Distribution}} || throw(\n        ArgumentError(\n            \"Right-hand side of a ~ must be subtype of Distribution or a vector of Distributions.\",\n        ),\n    )\n    var\"##vn#325\" = (DynamicPPL.VarName)(:p, ((1,),))\n    var\"##inds#326\" = ((1,),)\n    p[1] = (DynamicPPL.tilde_assume)(\n        _rng,\n        _context,\n        _sampler,\n        var\"##tmpright#323\",\n        var\"##vn#325\",\n        var\"##inds#326\",\n        _varinfo,\n    )\nend\nHere the first line is a so-called line number node that enables more helpful error messages by providing users with the exact location of the error in their model definition. Then the right hand side (RHS) of the ~ is assigned to a variable (with an automatically generated name). We check that the RHS is a distribution or an array of distributions, otherwise an error is thrown. Next we extract a compact representation of the variable with its name and index (or indices). Finally, the ~ expression is replaced with a call to DynamicPPL.tilde_assume since the compiler figured out that p[1] is a random variable using the following heuristic:\n\nIf the symbol on the LHS of ~, :p in this case, is not among the arguments to the model, (:x, :y, :T) in this case, it is a random variable.\nIf the symbol on the LHS of ~, :p in this case, is among the arguments to the model but has a value of missing, it is a random variable.\nIf the value of the LHS of ~, p[1] in this case, is missing, then it is a random variable.\nOtherwise, it is treated as an observation.\n\nThe DynamicPPL.tilde_assume function takes care of sampling the random variable, if needed, and updating its value and the accumulated log joint probability in the _varinfo object. If L ~ R is an observation, DynamicPPL.tilde_observe is called with the same arguments except the random number generator _rng (since observations are never sampled).\nA similar transformation is performed for expressions of the form @. L ~ R and L .~ R. For instance, @. x[1:2] ~ Normal(p[2], sqrt(p[1])) is replaced with\n#| eval: false\n#= REPL[25]:8 =#\nbegin\n    var\"##tmpright#331\" = Normal.(p[2], sqrt.(p[1]))\n    var\"##tmpright#331\" isa Union{Distribution,AbstractVector{&lt;:Distribution}} || throw(\n        ArgumentError(\n            \"Right-hand side of a ~ must be subtype of Distribution or a vector of Distributions.\",\n        ),\n    )\n    var\"##vn#333\" = (DynamicPPL.VarName)(:x, ((1:2,),))\n    var\"##inds#334\" = ((1:2,),)\n    var\"##isassumption#335\" = begin\n        let var\"##vn#336\" = (DynamicPPL.VarName)(:x, ((1:2,),))\n            if !((DynamicPPL.inargnames)(var\"##vn#336\", _model)) ||\n                (DynamicPPL.inmissings)(var\"##vn#336\", _model)\n                true\n            else\n                x[1:2] === missing\n            end\n        end\n    end\n    if var\"##isassumption#335\"\n        x[1:2] .= (DynamicPPL.dot_tilde_assume)(\n            _rng,\n            _context,\n            _sampler,\n            var\"##tmpright#331\",\n            x[1:2],\n            var\"##vn#333\",\n            var\"##inds#334\",\n            _varinfo,\n        )\n    else\n        (DynamicPPL.dot_tilde_observe)(\n            _context,\n            _sampler,\n            var\"##tmpright#331\",\n            x[1:2],\n            var\"##vn#333\",\n            var\"##inds#334\",\n            _varinfo,\n        )\n    end\nend\nThe main difference in the expanded code between L ~ R and @. L ~ R is that the former doesn’t assume L to be defined, it can be a new Julia variable in the scope, while the latter assumes L already exists. Moreover, DynamicPPL.dot_tilde_assume and DynamicPPL.dot_tilde_observe are called instead of DynamicPPL.tilde_assume and DynamicPPL.tilde_observe.",
    "crumbs": null
  },
  {
    "objectID": "developers/compiler/design-overview/index.html#step-3-replace-the-user-provided-function-body",
    "href": "developers/compiler/design-overview/index.html#step-3-replace-the-user-provided-function-body",
    "title": "Turing Compiler Design (Outdated)",
    "section": "Step 3: Replace the user-provided function body",
    "text": "Step 3: Replace the user-provided function body\nFinally, we replace the user-provided function body using DynamicPPL.build_output. This function uses MacroTools.combinedef to reassemble the user-provided function with a new function body. In the modified function body an anonymous function is created whose function body was generated in step 2 above and whose arguments are\n\na random number generator _rng,\na model _model,\na datastructure _varinfo,\na sampler _sampler,\na sampling context _context,\nand all positional and keyword arguments of the user-provided model function as positional arguments without any default values. Finally, in the new function body a model::Model with this anonymous function as internal function is returned.",
    "crumbs": null
  },
  {
    "objectID": "developers/compiler/design-overview/index.html#overview-1",
    "href": "developers/compiler/design-overview/index.html#overview-1",
    "title": "Turing Compiler Design (Outdated)",
    "section": "Overview",
    "text": "Overview\nVarInfo is the data structure in Turing that facilitates tracking random variables and certain metadata about them that are required for sampling. For instance, the distribution of every random variable is stored in VarInfo because we need to know the support of every random variable when sampling using HMC for example. Random variables whose distributions have a constrained support are transformed using a bijector from Bijectors.jl so that the sampling happens in the unconstrained space. Different samplers require different metadata about the random variables.\nThe definition of VarInfo in Turing is:\n#| eval: false\nstruct VarInfo{Tmeta, Tlogp} &lt;: AbstractVarInfo\n    metadata::Tmeta\n    logp::Base.RefValue{Tlogp}\n    num_produce::Base.RefValue{Int}\nend\nBased on the type of metadata, the VarInfo is either aliased UntypedVarInfo or TypedVarInfo. metadata can be either a subtype of the union type Metadata or a NamedTuple of multiple such subtypes. Let vi be an instance of VarInfo. If vi isa VarInfo{&lt;:Metadata}, then it is called an UntypedVarInfo. If vi isa VarInfo{&lt;:NamedTuple}, then vi.metadata would be a NamedTuple mapping each symbol in P to an instance of Metadata. vi would then be called a TypedVarInfo. The other fields of VarInfo include logp which is used to accumulate the log probability or log probability density of the variables in P and D. num_produce keeps track of how many observations have been made in the model so far. This is incremented when running a ~ statement when the symbol on the LHS is in D.",
    "crumbs": null
  },
  {
    "objectID": "developers/compiler/design-overview/index.html#metadata",
    "href": "developers/compiler/design-overview/index.html#metadata",
    "title": "Turing Compiler Design (Outdated)",
    "section": "Metadata",
    "text": "Metadata\nThe Metadata struct stores some metadata about the random variables sampled. This helps query certain information about a variable such as: its distribution, which samplers sample this variable, its value and whether this value is transformed to real space or not. Let md be an instance of Metadata:\n\nmd.vns is the vector of all VarName instances. Let vn be an arbitrary element of md.vns\nmd.idcs is the dictionary that maps each VarName instance to its index in md.vns, md.ranges, md.dists, md.orders and md.flags.\nmd.vns[md.idcs[vn]] == vn.\nmd.dists[md.idcs[vn]] is the distribution of vn.\nmd.gids[md.idcs[vn]] is the set of algorithms used to sample vn. This was used by the Gibbs sampler. Since Turing v0.36 it is unused and will eventually be deleted.\nmd.orders[md.idcs[vn]] is the number of observe statements before vn is sampled.\nmd.ranges[md.idcs[vn]] is the index range of vn in md.vals.\nmd.vals[md.ranges[md.idcs[vn]]] is the linearized vector of values of corresponding to vn.\nmd.flags is a dictionary of true/false flags. md.flags[flag][md.idcs[vn]] is the value of flag corresponding to vn.\n\nNote that in order to make md::Metadata type stable, all the md.vns must have the same symbol and distribution type. However, one can have a single Julia variable, e.g. x, that is a matrix or a hierarchical array sampled in partitions, e.g. x[1][:] ~ MvNormal(zeros(2), I); x[2][:] ~ MvNormal(ones(2), I). The symbol x can still be managed by a single md::Metadata without hurting the type stability since all the distributions on the RHS of ~ are of the same type.\nHowever, in Turing models one cannot have this restriction, so we must use a type unstable Metadata if we want to use one Metadata instance for the whole model. This is what UntypedVarInfo does. A type unstable Metadata will still work but will have inferior performance.\nTo strike a balance between flexibility and performance when constructing the spl::Sampler instance, the model is first run by sampling the parameters in P from their priors using an UntypedVarInfo, i.e. a type unstable Metadata is used for all the variables. Then once all the symbols and distribution types have been identified, a vi::TypedVarInfo is constructed where vi.metadata is a NamedTuple mapping each symbol in P to a specialized instance of Metadata. So as long as each symbol in P is sampled from only one type of distributions, vi::TypedVarInfo will have fully concretely typed fields which brings out the peak performance of Julia.",
    "crumbs": null
  },
  {
    "objectID": "uri/initial-parameters.html",
    "href": "uri/initial-parameters.html",
    "title": "Troubleshooting - Initial parameters",
    "section": "",
    "text": "If you are not redirected, please click here.\n\nNo matching items\n Back to top",
    "crumbs": null
  },
  {
    "objectID": "getting-started/index.html",
    "href": "getting-started/index.html",
    "title": "Getting Started",
    "section": "",
    "text": "Installation\nTo use Turing, you need to install Julia first and then install Turing.\nYou will need to install Julia 1.10.8 or greater, which you can get from the official Julia website.\nTuring is officially registered in the Julia General package registry, which means that you can install a stable version of Turing by running the following in the Julia REPL:\n\nusing Pkg\nPkg.add(\"Turing\")\n\n\n\nSupported versions and platforms\nFormally, we only run continuous integration tests on: (1) the minimum supported minor version (typically an LTS release), and (2) the latest minor version of Julia. We test on Linux (x64), macOS (Apple Silicon), and Windows (x64). The Turing developer team will prioritise fixing issues on these platforms and versions.\nIf you run into a problem on a different version (e.g. older patch releases) or platforms (e.g. 32-bit), please do feel free to post an issue! If we are able to help, we will try to fix it, but we cannot guarantee support for untested versions.\n\n\nExample usage\nFirst, we load the Turing and StatsPlots modules. The latter is required for visualising the results.\n\nusing Turing\nusing StatsPlots\n\nWe then specify our model, which is a simple Gaussian model with unknown mean and variance. Models are defined as ordinary Julia functions, prefixed with the @model macro. Each statement inside closely resembles how the model would be defined with mathematical notation. Here, both x and y are observed values, and are therefore passed as function parameters. m and s² are the parameters to be inferred.\n\n@model function gdemo(x, y)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    x ~ Normal(m, sqrt(s²))\n    y ~ Normal(m, sqrt(s²))\nend\n\ngdemo (generic function with 2 methods)\n\n\nSuppose we observe x = 1.5 and y = 2, and want to infer the mean and variance. We can pass these data as arguments to the gdemo function, and run a sampler to collect the results. Here, we collect 1000 samples using the No U-Turn Sampler (NUTS) algorithm.\n\nchain = sample(gdemo(1.5, 2), NUTS(), 1000, progress=false)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.8\n\n\n\n\nChains MCMC chain (1000×16×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 6.45 seconds\nCompute duration  = 6.45 seconds\nparameters        = s², m\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nWe can plot the results:\n\nplot(chain)\n\n\n\n\nand obtain summary statistics by indexing the chain:\n\nmean(chain[:m]), mean(chain[:s²])\n\n(1.19770351161832, 2.067633856603383)\n\n\n\n\nWhere to go next\n\n\n\n\n\n\nNoteNote on prerequisites\n\n\n\nFamiliarity with Julia is assumed throughout the Turing documentation. If you are new to Julia, Learning Julia is a good starting point.\nThe underlying theory of Bayesian machine learning is not explained in detail in this documentation. A thorough introduction to the field is Pattern Recognition and Machine Learning (Bishop, 2006); an online version is available here (PDF, 18.1 MB).\n\n\nThe next page on Turing’s core functionality explains the basic features of the Turing language. From there, you can either look at worked examples of how different models are implemented in Turing, or specific tips and tricks that can help you get the most out of Turing.\n\n\n\n\n Back to top",
    "crumbs": [
      "Get Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "tutorials/hidden-markov-models/index.html",
    "href": "tutorials/hidden-markov-models/index.html",
    "title": "Hidden Markov Models",
    "section": "",
    "text": "This tutorial illustrates training Bayesian hidden Markov models (HMMs) using Turing. The main goals are learning the transition matrix, emission parameter, and hidden states. For a more rigorous academic overview of hidden Markov models, see An Introduction to Hidden Markov Models and Bayesian Networks (Ghahramani, 2001).\nIn this tutorial, we assume there are \\(k\\) discrete hidden states; the observations are continuous and normally distributed - centred around the hidden states. This assumption reduces the number of parameters to be estimated in the emission matrix.\nLet’s load the libraries we’ll need, and set a random seed for reproducibility.\n# Load libraries.\nusing Turing, StatsPlots, Random, Bijectors\n\n# Set a random seed\nRandom.seed!(12345678);",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Hidden Markov Models"
    ]
  },
  {
    "objectID": "tutorials/hidden-markov-models/index.html#simple-state-detection",
    "href": "tutorials/hidden-markov-models/index.html#simple-state-detection",
    "title": "Hidden Markov Models",
    "section": "Simple State Detection",
    "text": "Simple State Detection\nIn this example, we’ll use something where the states and emission parameters are straightforward.\n\n# Define the emission parameter.\ny = [fill(1.0, 6)..., fill(2.0, 6)..., fill(3.0, 7)...,\n  fill(2.0, 4)..., fill(1.0, 7)...]\nN = length(y);\nK = 3;\n\n# Plot the data we just made.\nplot(y; xlim=(0, 30), ylim=(-1, 5), size=(500, 250), legend = false)\nscatter!(y, color = :blue; xlim=(0, 30), ylim=(-1, 5), size=(500, 250), legend = false)\n\n\n\n\nWe can see that we have three states, one for each height of the plot (1, 2, 3). This height is also our emission parameter, so state one produces a value of one, state two produces a value of two, and so on.\nUltimately, we would like to understand three major parameters:\n\nThe transition matrix. This is a matrix that assigns a probability of switching from one state to any other state, including the state that we are already in.\nThe emission parameters, which describes a typical value emitted by some state. In the plot above, the emission parameter for state one is simply one.\nThe state sequence is our understanding of what state we were actually in when we observed some data. This is very important in more sophisticated HMMs, where the emission value does not equal our state.\n\nWith this in mind, let’s set up our model. We are going to use some of our knowledge as modelers to provide additional information about our system. This takes the form of the prior on our emission parameter.\n\\[\nm_i \\sim \\mathrm{Normal}(i, 0.5) \\quad \\text{where} \\quad m = \\{1,2,3\\}\n\\]\nSimply put, this says that we expect state one to emit values in a Normally distributed manner, where the mean of each state’s emissions is that state’s value. The variance of 0.5 helps the model converge more quickly — consider the case where we have a variance of 1 or 2. In this case, the likelihood of observing a 2 when we are in state 1 is actually quite high, as it is within a standard deviation of the true emission value. Applying the prior that we are likely to be tightly centred around the mean prevents our model from being too confused about the state that is generating our observations.\nThe priors on our transition matrix are noninformative, using T[i] ~ Dirichlet(ones(K)/K). The Dirichlet prior used in this way assumes that the state is likely to change to any other state with equal probability. As we’ll see, this transition matrix prior will be overwritten as we observe data.\n\n# Turing model definition.\n@model function BayesHmm(y, K)\n    # Get observation length.\n    N = length(y)\n\n    # State sequence.\n    s = zeros(Int, N)\n\n    # Emission matrix.\n    m = Vector(undef, K)\n\n    # Transition matrix.\n    T = Vector{Vector}(undef, K)\n\n    # Assign distributions to each element\n    # of the transition matrix and the\n    # emission matrix.\n    for i in 1:K\n        T[i] ~ Dirichlet(ones(K) / K)\n        m[i] ~ Normal(i, 0.5)\n    end\n\n    # Observe each point of the input.\n    s[1] ~ Categorical(K)\n    y[1] ~ Normal(m[s[1]], 0.1)\n\n    for i in 2:N\n        s[i] ~ Categorical(vec(T[s[i - 1]]))\n        y[i] ~ Normal(m[s[i]], 0.1)\n    end\nend;\n\nWe will use a combination of two samplers (HMC and Particle Gibbs) by passing them to the Gibbs sampler. The Gibbs sampler allows for compositional inference, where different samplers can be applied to different parameters based on their properties. (For API details of these samplers, please see Turing.jl’s API documentation.)\nIn this case, we use HMC for m and T, representing the emission and transition matrices respectively. We use the Particle Gibbs sampler for s, the state sequence. You may wonder why it is that we are not assigning s to the HMC sampler, and why it is that we need compositional Gibbs sampling at all.\nThe parameter s is not a continuous variable. It is a vector of integers, and thus Hamiltonian methods like HMC and NUTS won’t work correctly. Gibbs allows us to apply the right tools to the best effect. If you are a particularly advanced user interested in higher performance, you may benefit from setting up your Gibbs sampler to use different automatic differentiation backends for each parameter space.\nTime to run our sampler.\n\ng = Gibbs((:m, :T) =&gt; HMC(0.01, 50), :s =&gt; PG(120))\nchn = sample(BayesHmm(y, 3), g, 1000)\n\nChains MCMC chain (1000×45×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 1403.44 seconds\nCompute duration  = 1403.44 seconds\nparameters        = T[1][1], T[1][2], T[1][3], m[1], T[2][1], T[2][2], T[2][3], m[2], T[3][1], T[3][2], T[3][3], m[3], s[1], s[2], s[3], s[4], s[5], s[6], s[7], s[8], s[9], s[10], s[11], s[12], s[13], s[14], s[15], s[16], s[17], s[18], s[19], s[20], s[21], s[22], s[23], s[24], s[25], s[26], s[27], s[28], s[29], s[30]\ninternals         = logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nLet’s see how well our chain performed. Ordinarily, using display(chn) would be a good first step, but we have generated a lot of parameters here (s[1], s[2], m[1], and so on). It’s a bit easier to show how our model performed graphically.\nThe code below generates an animation showing the graph of the data above, and the data our model generates in each sample.\n\n# Extract our m and s parameters from the chain.\nm_set = MCMCChains.group(chn, :m).value\ns_set = MCMCChains.group(chn, :s).value\n\n# Iterate through the MCMC samples.\nNs = 1:length(chn)\n\n# Make an animation.\nanimation = @gif for i in Ns\n    m = m_set[i, :]\n    s = Int.(s_set[i, :])\n    emissions = m[s]\n\n    p = plot(\n        y;\n        chn=:red,\n        size=(500, 250),\n        xlabel=\"Time\",\n        ylabel=\"State\",\n        legend=:topright,\n        label=\"True data\",\n        xlim=(0, 30),\n        ylim=(-1, 5),\n    )\n    plot!(emissions; color=:blue, label=\"Sample $i\")\nend every 3\n\n\n[ Info: Saved animation to /tmp/jl_KzkPAlIXaB.gif\n\n\n\n\n\n\n\nLooks like our model did a pretty good job, but we should also check to make sure our chain converges. A quick check is to examine whether the diagonal (representing the probability of remaining in the current state) of the transition matrix appears to be stationary. The code below extracts the diagonal and shows a traceplot of each persistence probability.\n\n# Index the chain with the persistence probabilities.\nsubchain = chn[[\"T[1][1]\", \"T[2][2]\", \"T[3][3]\"]]\n\nplot(subchain; seriestype=:traceplot, title=\"Persistence Probability\", legend=false)\n\n\n\n\nA cursory examination of the traceplot above indicates that all three chains converged to something resembling stationary. We can use the diagnostic functions provided by MCMCChains to engage in some more formal tests, like the Heidelberg and Welch diagnostic:\n\nheideldiag(MCMCChains.group(chn, :T))[1]\n\n\nHeidelberger and Welch diagnostic - Chain 1\n\n  parameters     burnin   stationarity    pvalue      mean   halfwidth     tes ⋯\n      Symbol      Int64           Bool   Float64   Float64     Float64     Boo ⋯\n\n     T[1][1]     0.0000         1.0000    0.0528    0.8866      0.0182   1.000 ⋯\n     T[1][2]   200.0000         1.0000    0.1035    0.0538      0.0203   0.000 ⋯\n     T[1][3]   500.0000         0.0000    0.0000    0.0486      0.0294   0.000 ⋯\n     T[2][1]   400.0000         1.0000    0.0839    0.0949      0.0512   0.000 ⋯\n     T[2][2]   500.0000         0.0000    0.0001    0.7064      0.1004   0.000 ⋯\n     T[2][3]   500.0000         0.0000    0.0000    0.1987      0.1553   0.000 ⋯\n     T[3][1]   500.0000         0.0000    0.0032    0.0777      0.1131   0.000 ⋯\n     T[3][2]   500.0000         0.0000    0.0000    0.3093      0.1709   0.000 ⋯\n     T[3][3]   500.0000         0.0000    0.0000    0.6130      0.2720   0.000 ⋯\n\n                                                                1 column omitted\n\n\n\n\nThe p-values on the test suggest that we cannot reject the hypothesis that the observed sequence comes from a stationary distribution, so we can be reasonably confident that our transition matrix has converged to something reasonable.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Hidden Markov Models"
    ]
  },
  {
    "objectID": "tutorials/hidden-markov-models/index.html#efficient-inference-with-the-forward-algorithm",
    "href": "tutorials/hidden-markov-models/index.html#efficient-inference-with-the-forward-algorithm",
    "title": "Hidden Markov Models",
    "section": "Efficient Inference With The Forward Algorithm",
    "text": "Efficient Inference With The Forward Algorithm\nWhile the above method works well for the simple example in this tutorial, some users may desire a more efficient method, especially when their model is more complicated. One simple way to improve inference is to marginalize out the hidden states of the model with an appropriate algorithm, calculating only the posterior over the continuous random variables. Not only does this allow more efficient inference via Rao-Blackwellization, but now we can sample our model with NUTS() alone, which is usually a much more performant MCMC kernel.\nThankfully, HiddenMarkovModels.jl provides an extremely efficient implementation of many algorithms related to hidden Markov models. This allows us to rewrite our model as:\n\nusing HiddenMarkovModels\nusing FillArrays\nusing LinearAlgebra\nusing LogExpFunctions\n\n\n@model function BayesHmm2(y, K)\n    m ~ Bijectors.ordered(MvNormal([1.0, 2.0, 3.0], 0.5I))\n    T ~ filldist(Dirichlet(fill(1/K, K)), K)\n\n    hmm = HMM(softmax(ones(K)), copy(T'), [Normal(m[i], 0.1) for i in 1:K])\n    @addlogprob! logdensityof(hmm, y)\nend\n\nchn2 = sample(BayesHmm2(y, 3), NUTS(), 1000)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.025\n\n\n\n\nChains MCMC chain (1000×26×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 7.4 seconds\nCompute duration  = 7.4 seconds\nparameters        = m[1], m[2], m[3], T[1, 1], T[2, 1], T[3, 1], T[1, 2], T[2, 2], T[3, 2], T[1, 3], T[2, 3], T[3, 3]\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nWe can compare the chains of these two models, confirming the posterior estimate is similar (modulo label switching concerns with the Gibbs model):\n\n\nPlotting Chains\nplot(chn[\"m[1]\"], label = \"m[1], Model 1, Gibbs\", color = :lightblue)\nplot!(chn2[\"m[1]\"], label = \"m[1], Model 2, NUTS\", color = :blue)\nplot!(chn[\"m[2]\"], label = \"m[2], Model 1, Gibbs\", color = :pink)\nplot!(chn2[\"m[2]\"], label = \"m[2], Model 2, NUTS\", color = :red)\nplot!(chn[\"m[3]\"], label = \"m[3], Model 1, Gibbs\", color = :yellow)\nplot!(chn2[\"m[3]\"], label = \"m[3], Model 2, NUTS\", color = :orange)\n\n\n\n\n\n\nRecovering Marginalized Trajectories\nWe can use the viterbi() algorithm, also from the HiddenMarkovModels package, to recover the most probable state for each parameter set in our posterior sample:\n\n@model function BayesHmmRecover(y, K, IncludeGenerated = false)\n    m ~ Bijectors.ordered(MvNormal([1.0, 2.0, 3.0], 0.5I))\n    T ~ filldist(Dirichlet(fill(1/K, K)), K)\n\n    hmm = HMM(softmax(ones(K)), copy(T'), [Normal(m[i], 0.1) for i in 1:K])\n    @addlogprob! logdensityof(hmm, y)\n\n    # Conditional generation of the hidden states.\n    if IncludeGenerated\n        seq, _ = viterbi(hmm, y)\n        s := [m[s] for s in seq]\n    end\nend\n\nchn_recover = sample(BayesHmmRecover(y, 3, true), NUTS(), 1000)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.025\n\n\n\n\nChains MCMC chain (1000×56×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 3.59 seconds\nCompute duration  = 3.59 seconds\nparameters        = m[1], m[2], m[3], T[1, 1], T[2, 1], T[3, 1], T[1, 2], T[2, 2], T[3, 2], T[1, 3], T[2, 3], T[3, 3], s[1], s[2], s[3], s[4], s[5], s[6], s[7], s[8], s[9], s[10], s[11], s[12], s[13], s[14], s[15], s[16], s[17], s[18], s[19], s[20], s[21], s[22], s[23], s[24], s[25], s[26], s[27], s[28], s[29], s[30]\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nPlotting the estimated states, we can see that the results align well with our expectations:\n\np = plot(xlim=(0, 30), ylim=(-1, 5), size=(500, 250))\nfor i in 1:100\n    ind = rand(DiscreteUniform(1, 1000))\n    plot!(MCMCChains.group(chn_recover, :s).value[ind,:], color = :grey, opacity = 0.1, legend = :false)\nend\nscatter!(y, color = :blue)\n\np",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Hidden Markov Models"
    ]
  },
  {
    "objectID": "tutorials/gaussian-processes-introduction/index.html",
    "href": "tutorials/gaussian-processes-introduction/index.html",
    "title": "Gaussian Processes: Introduction",
    "section": "",
    "text": "JuliaGPs packages integrate well with Turing.jl because they implement the Distributions.jl interface. This tutorial assumes basic knowledge of Gaussian processes (i.e., a general understanding of what they are); for a comprehensive introduction, see Rasmussen and Williams (2006). For a more in-depth understanding of the JuliaGPs functionality used here, please consult the JuliaGPs docs.\nIn this tutorial, we will model the putting dataset discussed in Chapter 21 of Bayesian Data Analysis. The dataset comprises the result of measuring how often a golfer successfully gets the ball in the hole, depending on how far away from it they are. The goal of inference is to estimate the probability of any given shot being successful at a given distance.\n\nLet’s download the data and take a look at it:\n\nusing CSV, DataFrames\n\ndf = CSV.read(\"golf.dat\", DataFrame; delim=' ', ignorerepeated=true)\ndf[1:5, :]\n\n5×3 DataFrame\n\n\n\nRow\ndistance\nn\ny\n\n\n\nInt64\nInt64\nInt64\n\n\n\n\n1\n2\n1443\n1346\n\n\n2\n3\n694\n577\n\n\n3\n4\n455\n337\n\n\n4\n5\n353\n208\n\n\n5\n6\n272\n149\n\n\n\n\n\n\nThese are the first 5 rows of the dataset (which comprises only 19 rows in total). Observe it has three columns:\n\ndistance – how far away from the hole. We will refer to distance as d throughout the rest of this tutorial\nn – how many shots were taken from a given distance\ny – how many shots were successful from a given distance\n\nWe will use a Binomial model for the data, whose success probability is parametrised by a transformation of a GP. Something along the lines of:\n\\[\n\\begin{aligned}\nf & \\sim \\operatorname{GP}(0, k) \\\\\ny_j \\mid f(d_j) & \\sim \\operatorname{Binomial}(n_j, g(f(d_j))) \\\\\ng(x) & := \\frac{1}{1 + e^{-x}}\n\\end{aligned}\n\\]\nTo do this, let’s define our Turing.jl model:\n\nusing AbstractGPs, LogExpFunctions, Turing\n\n@model function putting_model(d, n; jitter=1e-4)\n    v ~ Gamma(2, 1)\n    l ~ Gamma(4, 1)\n    f = GP(v * with_lengthscale(SEKernel(), l))\n    f_latent ~ f(d, jitter)\n    y ~ product_distribution(Binomial.(n, logistic.(f_latent)))\n    return (fx=f(d, jitter), f_latent=f_latent, y=y)\nend\n\nputting_model (generic function with 2 methods)\n\n\nWe first define an AbstractGPs.GP, which represents a distribution over functions, and is entirely separate from Turing.jl. We place a prior over its variance v and length-scale l. f(d, jitter) constructs the multivariate Gaussian comprising the random variables in f whose indices are in d (plus a bit of independent Gaussian noise with variance jitter – see the docs for more details). f(d, jitter) has the type AbstractMvNormal, and is the bit of AbstractGPs.jl that implements the Distributions.jl interface, so it’s legal to put it on the right-hand side of a ~. From this you should deduce that f_latent is distributed according to a multivariate Gaussian. The remaining lines comprise standard Turing.jl code that is encountered in other tutorials and Turing documentation.\nBefore performing inference, we might want to inspect the prior that our model places over the data, to see whether there is anything obviously wrong. These kinds of prior predictive checks are straightforward to perform using Turing.jl, since it is possible to sample from the prior easily by just calling the model:\n\nm = putting_model(Float64.(df.distance), df.n)\nm().y\n\n19-element Vector{Int64}:\n 950\n 468\n 303\n 233\n 185\n 160\n 146\n 146\n 125\n 164\n 143\n 154\n 149\n 149\n 177\n 177\n 162\n 117\n 109\n\n\nWe make use of this to see what kinds of datasets we simulate from the prior:\n\nusing Plots\n\nfunction plot_data(d, n, y, xticks, yticks)\n    ylims = (0, round(maximum(n), RoundUp; sigdigits=2))\n    margin = -0.5 * Plots.mm\n    plt = plot(; xticks=xticks, yticks=yticks, ylims=ylims, margin=margin, grid=false)\n    bar!(plt, d, n; color=:red, label=\"\", alpha=0.5)\n    bar!(plt, d, y; label=\"\", color=:blue, alpha=0.7)\n    return plt\nend\n\n# Construct model and run some prior predictive checks.\nm = putting_model(Float64.(df.distance), df.n)\nhists = map(1:20) do j\n    xticks = j &gt; 15 ? :auto : nothing\n    yticks = rem(j, 5) == 1 ? :auto : nothing\n    return plot_data(df.distance, df.n, m().y, xticks, yticks)\nend\nplot(hists...; layout=(4, 5))\n\n\n\n\nIn this case, the only prior knowledge we is that the proportion of successful shots ought to decrease monotonically as the distance from the hole increases, which should show up in the data as the blue lines generally go down as we move from left to right on each graph. Unfortunately, there is not a simple way to enforce monotonicity in the samples from a GP, and we can see this in some of the plots above, so we must hope that we have enough data to ensure that this relationship holds approximately under the posterior. In any case, you can judge for yourself whether you think this is the most useful visualisation that we can perform; if you think there is something better to look at, please let us know!\nMoving on, we generate samples from the posterior using the default NUTS sampler. We’ll make use of ReverseDiff.jl, as it has better performance than ForwardDiff.jl on this example. See the automatic differentiation docs for more info.\n\nusing Random, ReverseDiff\n\nm_post = m | (y=df.y,)\nchn = sample(Xoshiro(123456), m_post, NUTS(; adtype=AutoReverseDiff()), 1_000, progress=false)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.0125\n\n\n\n\nChains MCMC chain (1000×35×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 149.94 seconds\nCompute duration  = 149.94 seconds\nparameters        = v, l, f_latent[1], f_latent[2], f_latent[3], f_latent[4], f_latent[5], f_latent[6], f_latent[7], f_latent[8], f_latent[9], f_latent[10], f_latent[11], f_latent[12], f_latent[13], f_latent[14], f_latent[15], f_latent[16], f_latent[17], f_latent[18], f_latent[19]\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nWe can use these samples and the posterior function from AbstractGPs to sample from the posterior probability of success at any distance we choose:\n\nd_pred = 1:0.2:21\nsamples = map(returned(m_post, chn)[1:10:end]) do x\n    return logistic.(rand(posterior(x.fx, x.f_latent)(d_pred, 1e-4)))\nend\np = plot()\nplot!(d_pred, reduce(hcat, samples); label=\"\", color=:blue, alpha=0.2)\nscatter!(df.distance, df.y ./ df.n; label=\"\", color=:red)\n\n\n\n\nWe can see that the general trend is indeed down as the distance from the hole increases, and that if we move away from the data, the posterior uncertainty quickly inflates. This suggests that the model is probably going to do a reasonable job of interpolating between observed data, but less good a job at extrapolating to larger distances.\n\n\n\n\n Back to top",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Gaussian Processes: Introduction"
    ]
  },
  {
    "objectID": "tutorials/probabilistic-pca/index.html",
    "href": "tutorials/probabilistic-pca/index.html",
    "title": "Probabilistic PCA",
    "section": "",
    "text": "Principal component analysis (PCA) is a fundamental technique to analyse and visualise data. It is an unsupervised learning method mainly used for dimensionality reduction.\nFor example, we have a data matrix \\(\\mathbf{X} \\in \\mathbb{R}^{N \\times D}\\), and we would like to extract \\(k \\ll D\\) principal components which captures most of the information from the original matrix. The goal is to understand \\(\\mathbf{X}\\) through a lower dimensional subspace (e.g. two-dimensional subspace for visualisation convenience) spanned by the principal components.\nIn order to project the original data matrix into low dimensions, we need to find the principal directions where most of the variations of \\(\\mathbf{X}\\) lie in. Traditionally, this is implemented via singular value decomposition (SVD) which provides a robust and accurate computational framework for decomposing matrix into products of rotation-scaling-rotation matrices, particularly for large datasets (see an illustration here):\n\\[\n\\mathbf{X}_{N \\times D} =  \\mathbf{U}_{N \\times r} \\times \\boldsymbol{\\Sigma}_{r \\times r}  \\times  \\mathbf{V}^T_{r \\times D}\n\\]\nwhere \\(\\Sigma_{r \\times r}\\) contains only \\(r := \\operatorname{rank} \\mathbf{X} \\leq \\min\\{N,D\\}\\) non-zero singular values of \\(\\mathbf{X}\\). If we pad \\(\\Sigma\\) with zeros and add arbitrary orthonormal columns to \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\), we obtain the more compact form:1\n\\[\n\\mathbf{X}_{N \\times D} = \\mathbf{U}_{N \\times N} \\mathbf{\\Sigma}_{N \\times D} \\mathbf{V}_{D \\times D}^T\n\\]\nwhere \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are unitary matrices (i.e. with orthonormal columns). Such a decomposition always exists for any matrix. Columns of \\(\\mathbf{V}\\) are the principal directions/axes. The percentage of variations explained can be calculated using the ratios of singular values.2\nHere we take a probabilistic perspective. For more details and a mathematical derivation, we recommend Bishop’s textbook (Christopher M. Bishop, Pattern Recognition and Machine Learning, 2006). The idea of probabilistic PCA is to find a latent variable \\(z\\) that can be used to describe the hidden structure in a dataset.3 Consider a data set \\(\\mathbf{X}_{D \\times N}=\\{x_i\\}\\) with \\(i=1,2,...,N\\) data points, where each data point \\(x_i\\) is \\(D\\)-dimensional (i.e. \\(x_i \\in \\mathcal{R}^D\\)). Note that, here we use the flipped version of the data matrix. We aim to represent the original \\(n\\) dimensional vector using a lower dimensional latent variable \\(z_i \\in \\mathcal{R}^k\\).\nWe first assume that each latent variable \\(z_i\\) is normally distributed:\n\\[\nz_i \\sim \\mathcal{N}(0, I)\n\\]\nand the corresponding data point is generated via projection:\n\\[\nx_i | z_i \\sim \\mathcal{N}(\\mathbf{W} z_i + \\boldsymbol{μ}, \\sigma^2 \\mathbf{I})\n\\]\nwhere the projection matrix \\(\\mathbf{W}_{D \\times k}\\) accommodates the principal axes. The above formula expresses \\(x_i\\) as a linear combination of the basis columns in the projection matrix W, where the combination coefficients sit in z_i (they are the coordinates of x_i in the new \\(k\\)-dimensional space.). We can also express the above formula in matrix form: \\(\\mathbf{X}_{D \\times N} \\approx \\mathbf{W}_{D \\times k} \\mathbf{Z}_{k \\times N}\\). We are interested in inferring \\(\\mathbf{W}\\), \\(μ\\) and \\(\\sigma\\).\nClassical PCA is the limiting case of probabilistic PCA when the noise variance approaches zero, i.e. \\(\\sigma^2 \\to 0\\). Probabilistic PCA generalises classical PCA; this connection can be seen by marginalising out the latent variable.4",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Probabilistic PCA"
    ]
  },
  {
    "objectID": "tutorials/probabilistic-pca/index.html#overview-of-pca",
    "href": "tutorials/probabilistic-pca/index.html#overview-of-pca",
    "title": "Probabilistic PCA",
    "section": "",
    "text": "Principal component analysis (PCA) is a fundamental technique to analyse and visualise data. It is an unsupervised learning method mainly used for dimensionality reduction.\nFor example, we have a data matrix \\(\\mathbf{X} \\in \\mathbb{R}^{N \\times D}\\), and we would like to extract \\(k \\ll D\\) principal components which captures most of the information from the original matrix. The goal is to understand \\(\\mathbf{X}\\) through a lower dimensional subspace (e.g. two-dimensional subspace for visualisation convenience) spanned by the principal components.\nIn order to project the original data matrix into low dimensions, we need to find the principal directions where most of the variations of \\(\\mathbf{X}\\) lie in. Traditionally, this is implemented via singular value decomposition (SVD) which provides a robust and accurate computational framework for decomposing matrix into products of rotation-scaling-rotation matrices, particularly for large datasets (see an illustration here):\n\\[\n\\mathbf{X}_{N \\times D} =  \\mathbf{U}_{N \\times r} \\times \\boldsymbol{\\Sigma}_{r \\times r}  \\times  \\mathbf{V}^T_{r \\times D}\n\\]\nwhere \\(\\Sigma_{r \\times r}\\) contains only \\(r := \\operatorname{rank} \\mathbf{X} \\leq \\min\\{N,D\\}\\) non-zero singular values of \\(\\mathbf{X}\\). If we pad \\(\\Sigma\\) with zeros and add arbitrary orthonormal columns to \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\), we obtain the more compact form:1\n\\[\n\\mathbf{X}_{N \\times D} = \\mathbf{U}_{N \\times N} \\mathbf{\\Sigma}_{N \\times D} \\mathbf{V}_{D \\times D}^T\n\\]\nwhere \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are unitary matrices (i.e. with orthonormal columns). Such a decomposition always exists for any matrix. Columns of \\(\\mathbf{V}\\) are the principal directions/axes. The percentage of variations explained can be calculated using the ratios of singular values.2\nHere we take a probabilistic perspective. For more details and a mathematical derivation, we recommend Bishop’s textbook (Christopher M. Bishop, Pattern Recognition and Machine Learning, 2006). The idea of probabilistic PCA is to find a latent variable \\(z\\) that can be used to describe the hidden structure in a dataset.3 Consider a data set \\(\\mathbf{X}_{D \\times N}=\\{x_i\\}\\) with \\(i=1,2,...,N\\) data points, where each data point \\(x_i\\) is \\(D\\)-dimensional (i.e. \\(x_i \\in \\mathcal{R}^D\\)). Note that, here we use the flipped version of the data matrix. We aim to represent the original \\(n\\) dimensional vector using a lower dimensional latent variable \\(z_i \\in \\mathcal{R}^k\\).\nWe first assume that each latent variable \\(z_i\\) is normally distributed:\n\\[\nz_i \\sim \\mathcal{N}(0, I)\n\\]\nand the corresponding data point is generated via projection:\n\\[\nx_i | z_i \\sim \\mathcal{N}(\\mathbf{W} z_i + \\boldsymbol{μ}, \\sigma^2 \\mathbf{I})\n\\]\nwhere the projection matrix \\(\\mathbf{W}_{D \\times k}\\) accommodates the principal axes. The above formula expresses \\(x_i\\) as a linear combination of the basis columns in the projection matrix W, where the combination coefficients sit in z_i (they are the coordinates of x_i in the new \\(k\\)-dimensional space.). We can also express the above formula in matrix form: \\(\\mathbf{X}_{D \\times N} \\approx \\mathbf{W}_{D \\times k} \\mathbf{Z}_{k \\times N}\\). We are interested in inferring \\(\\mathbf{W}\\), \\(μ\\) and \\(\\sigma\\).\nClassical PCA is the limiting case of probabilistic PCA when the noise variance approaches zero, i.e. \\(\\sigma^2 \\to 0\\). Probabilistic PCA generalises classical PCA; this connection can be seen by marginalising out the latent variable.4",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Probabilistic PCA"
    ]
  },
  {
    "objectID": "tutorials/probabilistic-pca/index.html#the-gene-expression-example",
    "href": "tutorials/probabilistic-pca/index.html#the-gene-expression-example",
    "title": "Probabilistic PCA",
    "section": "The gene expression example",
    "text": "The gene expression example\nIn the first example, we illustrate:\n\nhow to specify the probabilistic model and\nhow to perform inference on \\(\\mathbf{W}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\sigma\\) using MCMC.\n\nWe use simulated genome data to demonstrate these. The simulation is inspired by biological measurement of expression of genes in cells, and each cell is characterised by different gene features. While the human genome is (mostly) identical between all the cells in the body, there exist interesting differences in gene expression in different human tissues and disease conditions. One way to investigate certain diseases is to look at differences in gene expression in cells from patients and healthy controls (usually from the same tissue).\nUsually, we can assume that the changes in gene expression only affect a subset of all genes (and these can be linked to diseases in some way). One of the challenges for this kind of data is to explore the underlying structure, e.g. to make the connection between a certain state (healthy/disease) and gene expression. This becomes difficult when the dimension is very large (up to 20000 genes across 1000s of cells). So in order to find structure in this data, it is useful to project the data into a lower dimensional space.\nRegardless of the biological background, the more abstract problem formulation is to project the data living in high-dimensional space onto a representation in lower-dimensional space where most of the variation is concentrated in the first few dimensions. We use PCA to explore underlying structure or pattern which may not necessarily be obvious from looking at the raw data itself.\n\nStep 1: configuration of dependencies\nFirst, we load the dependencies used.\n\nusing Turing\nusing Mooncake\nusing LinearAlgebra, FillArrays\n\n# Packages for visualisation\nusing DataFrames, StatsPlots, Measures\n\n# Set a seed for reproducibility.\nusing Random\nRandom.seed!(1789);\n\nAll packages used in this tutorial are listed here. You can install them via using Pkg; Pkg.add(\"package_name\").\n\n\n\n\n\n\nCautionPackage usages:\n\n\n\nWe use DataFrames for instantiating matrices, LinearAlgebra and FillArrays to perform matrix operations; Turing for model specification and MCMC sampling, Mooncake for automatic differentiation when sampling. StatsPlots for visualising the results. , Measures for setting plot margin units. As all examples involve sampling, for reproducibility we set a fixed seed using the Random standard library.\n\n\n\n\nStep 2: Data generation\nHere, we simulate the biological gene expression problem described earlier. We simulate 60 cells, each cell has 9 gene features. This is a simplified problem with only a few cells and genes for demonstration purpose, which is not comparable to the complexity in real-life (e.g. thousands of features for each individual). Even so, spotting the structures or patterns in a 9-feature space would be a challenging task; it would be nice to reduce the dimensionality using p-PCA.\nBy design, we manually divide the 60 cells into two groups. the first 3 gene features of the first 30 cells have mean 10, while those of the last 30 cells have mean 10. These two groups of cells differ in the expression of genes.\n\nn_genes = 9 # D\nn_cells = 60 # N\n\n# create a diagonal block like expression matrix, with some non-informative genes;\n# not all features/genes are informative, some might just not differ very much between cells)\nmat_exp = randn(n_genes, n_cells)\nmat_exp[1:(n_genes ÷ 3), 1:(n_cells ÷ 2)] .+= 10\nmat_exp[(2 * (n_genes ÷ 3) + 1):end, (n_cells ÷ 2 + 1):end] .+= 10\n\n3×30 view(::Matrix{Float64}, 7:9, 31:60) with eltype Float64:\n 11.7413   10.5735   11.3817    8.50923  …   7.38716   8.7734   11.4717\n  9.28533  11.1225    9.43421  10.8904      11.6846   10.7264    9.64063\n  9.92113   8.42122   9.59885   9.90799      9.40715   8.40956  10.2522\n\n\nTo visualise the \\((D=9) \\times (N=60)\\) data matrix mat_exp, we use the heatmap plot.\n\nheatmap(\n    mat_exp;\n    c=:summer,\n    colors=:value,\n    xlabel=\"cell number\",\n    yflip=true,\n    ylabel=\"gene feature\",\n    yticks=1:9,\n    colorbar_title=\"expression\",\n)\n\n\n\n\nNote that:\n\nWe have made distinct feature differences between these two groups of cells (it is fairly obvious from looking at the raw data), in practice and with large enough data sets, it is often impossible to spot the differences from the raw data alone.\nIf you have some patience and compute resources you can increase the size of the dataset, or play around with the noise levels to make the problem increasingly harder.\n\n\n\nStep 3: Create the pPCA model\nHere we construct the probabilistic model pPCA(). As per the p-PCA formula, we think of each row (i.e. each gene feature) following an \\(N=60\\) dimensional multivariate normal distribution centred around the corresponding row of \\(\\mathbf{W}_{D \\times k} \\times \\mathbf{Z}_{k \\times N} + \\boldsymbol{\\mu}_{D \\times N}\\).\n\n@model function pPCA(X::AbstractMatrix{&lt;:Real}, k::Int)\n    # retrieve the dimension of input matrix X.\n    N, D = size(X)\n\n    # weights/loadings W\n    W ~ filldist(Normal(), D, k)\n\n    # latent variable z\n    Z ~ filldist(Normal(), k, N)\n\n    # mean offset\n    μ ~ MvNormal(Eye(D))\n    genes_mean = W * Z .+ reshape(μ, n_genes, 1)\n    return X ~ arraydist([MvNormal(m, Eye(N)) for m in eachcol(genes_mean')])\nend;\n\nThe function pPCA() accepts:\n\na data array \\(\\mathbf{X}\\) (with no. of instances x dimension no. of features, NB: it is a transpose of the original data matrix);\nan integer \\(k\\) which indicates the dimension of the latent space (the space the original feature matrix is projected onto).\n\nSpecifically:\n\nit first extracts the dimension \\(D\\) and number of instances \\(N\\) of the input matrix;\ndraw samples of each entries of the projection matrix \\(\\mathbf{W}\\) from a standard normal;\ndraw samples of the latent variable \\(\\mathbf{Z}_{k \\times N}\\) from an MND;\ndraw samples of the offset \\(\\boldsymbol{\\mu}\\) from an MND, assuming uniform offset for all instances;\nFinally, we iterate through each gene dimension in \\(\\mathbf{X}\\), and define an MND for the sampling distribution (i.e. likelihood).\n\n\n\nStep 4: Sampling-based inference of the pPCA model\nHere we aim to perform MCMC sampling to infer the projection matrix \\(\\mathbf{W}_{D \\times k}\\), the latent variable matrix \\(\\mathbf{Z}_{k \\times N}\\), and the offsets \\(\\boldsymbol{\\mu}_{N \\times 1}\\).\nWe run the inference using the NUTS sampler. By default, sample samples a single chain (in this case with 500 samples). You can also use different samplers if you wish.\n\nsetprogress!(false)\n\n\nk = 2 # k is the dimension of the projected space, i.e. the number of principal components/axes of choice\nppca = pPCA(mat_exp', k) # instantiate the probabilistic model\nchain_ppca = sample(ppca, NUTS(; adtype=AutoMooncake()), 500);\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.4\n\n\n\n\nThe samples are saved in chain_ppca, which is an MCMCChains.Chains object. We can check its shape:\n\nsize(chain_ppca) # (no. of iterations, no. of vars, no. of chains) = (500, 159, 1)\n\n(500, 161, 1)\n\n\nSampling statistics such as R-hat, ESS, mean estimates, and so on can also be obtained from this:\n\ndescribe(chain_ppca)\n\n\nChains MCMC chain (500×161×1 Array{Float64, 3}):\n\nIterations        = 251:1:750\nNumber of chains  = 1\nSamples per chain = 500\nWall duration     = 293.56 seconds\nCompute duration  = 293.56 seconds\nparameters        = W[1, 1], W[2, 1], W[3, 1], W[4, 1], W[5, 1], W[6, 1], W[7, 1], W[8, 1], W[9, 1], W[1, 2], W[2, 2], W[3, 2], W[4, 2], W[5, 2], W[6, 2], W[7, 2], W[8, 2], W[9, 2], Z[1, 1], Z[2, 1], Z[1, 2], Z[2, 2], Z[1, 3], Z[2, 3], Z[1, 4], Z[2, 4], Z[1, 5], Z[2, 5], Z[1, 6], Z[2, 6], Z[1, 7], Z[2, 7], Z[1, 8], Z[2, 8], Z[1, 9], Z[2, 9], Z[1, 10], Z[2, 10], Z[1, 11], Z[2, 11], Z[1, 12], Z[2, 12], Z[1, 13], Z[2, 13], Z[1, 14], Z[2, 14], Z[1, 15], Z[2, 15], Z[1, 16], Z[2, 16], Z[1, 17], Z[2, 17], Z[1, 18], Z[2, 18], Z[1, 19], Z[2, 19], Z[1, 20], Z[2, 20], Z[1, 21], Z[2, 21], Z[1, 22], Z[2, 22], Z[1, 23], Z[2, 23], Z[1, 24], Z[2, 24], Z[1, 25], Z[2, 25], Z[1, 26], Z[2, 26], Z[1, 27], Z[2, 27], Z[1, 28], Z[2, 28], Z[1, 29], Z[2, 29], Z[1, 30], Z[2, 30], Z[1, 31], Z[2, 31], Z[1, 32], Z[2, 32], Z[1, 33], Z[2, 33], Z[1, 34], Z[2, 34], Z[1, 35], Z[2, 35], Z[1, 36], Z[2, 36], Z[1, 37], Z[2, 37], Z[1, 38], Z[2, 38], Z[1, 39], Z[2, 39], Z[1, 40], Z[2, 40], Z[1, 41], Z[2, 41], Z[1, 42], Z[2, 42], Z[1, 43], Z[2, 43], Z[1, 44], Z[2, 44], Z[1, 45], Z[2, 45], Z[1, 46], Z[2, 46], Z[1, 47], Z[2, 47], Z[1, 48], Z[2, 48], Z[1, 49], Z[2, 49], Z[1, 50], Z[2, 50], Z[1, 51], Z[2, 51], Z[1, 52], Z[2, 52], Z[1, 53], Z[2, 53], Z[1, 54], Z[2, 54], Z[1, 55], Z[2, 55], Z[1, 56], Z[2, 56], Z[1, 57], Z[2, 57], Z[1, 58], Z[2, 58], Z[1, 59], Z[2, 59], Z[1, 60], Z[2, 60], μ[1], μ[2], μ[3], μ[4], μ[5], μ[6], μ[7], μ[8], μ[9]\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nSummary Statistics\n\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n\n     W[1, 1]    1.3284    1.7279    0.9374     2.9703    24.3028    1.2530     ⋯\n     W[2, 1]    1.5002    1.7222    0.9351     3.0139    24.2569    1.2516     ⋯\n     W[3, 1]    1.1340    1.8007    0.9934     3.0381    24.8849    1.2452     ⋯\n     W[4, 1]    0.1882    0.2581    0.0797    10.2921    64.4100    1.0671     ⋯\n     W[5, 1]    0.3682    0.3336    0.0927    14.3598    56.5900    1.0548     ⋯\n     W[6, 1]    0.0155    0.1886    0.0154   149.2448   239.5260    1.0060     ⋯\n     W[7, 1]   -1.6148    1.5684    0.8366     3.2328    24.3028    1.2434     ⋯\n     W[8, 1]   -1.2184    1.8600    1.0247     2.9526    22.3444    1.2544     ⋯\n     W[9, 1]   -1.4505    1.7355    0.9552     2.9305    24.3028    1.2598     ⋯\n     W[1, 2]   -2.0711    1.7468    0.7249     8.8112    41.1560    1.1781     ⋯\n     W[2, 2]   -2.0142    1.8710    0.8112     7.7251    42.7040    1.1640     ⋯\n     W[3, 2]   -2.1445    1.6318    0.6192     8.8950    39.2365    1.1430     ⋯\n     W[4, 2]    0.1780    0.1960    0.0916     4.6181    19.6975    1.1564     ⋯\n     W[5, 2]    0.2065    0.3480    0.1816     3.6786    23.8599    1.1951     ⋯\n     W[6, 2]   -0.0530    0.1597    0.0098   293.5427   190.4159    1.0149     ⋯\n           ⋮         ⋮         ⋮         ⋮          ⋮          ⋮         ⋮     ⋱\n\n                                                   1 column and 132 rows omitted\n\nQuantiles\n\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n     W[1, 1]   -2.3996    0.0050    1.5787    3.0173    3.5904\n     W[2, 1]   -2.3393    0.2861    1.7104    3.1118    3.6721\n     W[3, 1]   -2.6584   -0.3000    1.3341    2.8551    3.4988\n     W[4, 1]   -0.3292    0.0298    0.2063    0.3537    0.6589\n     W[5, 1]   -0.4058    0.1808    0.4253    0.5976    0.9142\n     W[6, 1]   -0.3843   -0.0878    0.0334    0.1344    0.3389\n     W[7, 1]   -3.5421   -3.0392   -1.8516   -0.5158    2.0227\n     W[8, 1]   -3.7061   -3.0084   -1.5031    0.2091    2.6876\n     W[9, 1]   -3.6758   -3.0970   -1.7335   -0.1512    2.4604\n     W[1, 2]   -3.7083   -3.3240   -2.8928   -1.1979    2.6044\n     W[2, 2]   -3.8018   -3.3873   -2.8195   -1.0707    2.8529\n     W[3, 2]   -3.6660   -3.2979   -2.9321   -1.5214    2.2661\n     W[4, 2]   -0.2152    0.0493    0.1726    0.3197    0.5602\n     W[5, 2]   -0.4794   -0.0542    0.2104    0.4961    0.8195\n     W[6, 2]   -0.3703   -0.1515   -0.0607    0.0354    0.2796\n           ⋮         ⋮         ⋮         ⋮         ⋮         ⋮\n\n                                                132 rows omitted\n\n\n\n\n\n\nStep 5: posterior predictive checks\nWe try to reconstruct the input data using the posterior mean as parameter estimates. We first retrieve the samples for the projection matrix W from chain_ppca. This can be done using the Julia group(chain, parameter_name) function. Then we calculate the mean value for each element in \\(W\\), averaging over the whole chain of samples.\n\n# Extract parameter estimates for predicting x - mean of posterior\nW = reshape(mean(group(chain_ppca, :W))[:, 2], (n_genes, k))\nZ = reshape(mean(group(chain_ppca, :Z))[:, 2], (k, n_cells))\nμ = mean(group(chain_ppca, :μ))[:, 2]\n\nmat_rec = W * Z .+ repeat(μ; inner=(1, n_cells))\n\n9×60 Matrix{Float64}:\n  7.35504     7.43658     7.75482    …   2.50049    2.68854     2.30275\n  7.67766     7.71899     8.13066        2.91618    2.90557     2.3779\n  7.30597     7.43439     7.6455         2.32617    2.74508     2.52231\n -0.179518   -0.24162    -0.120676       0.179117  -0.11784    -0.305972\n  0.122041    0.0227109   0.236468       0.508999   0.0254333  -0.302422\n  0.0088857   0.0143518   0.0134078  …  -0.11183   -0.0897972  -0.0861072\n  2.55867     2.56661     2.0695         6.93879    7.18056     7.84353\n  2.438       2.31582     2.07272        7.56422    7.18352     7.45152\n  2.25855     2.1994      1.82112        7.13474    7.06074     7.53617\n\n\n\nheatmap(\n    mat_rec;\n    c=:summer,\n    colors=:value,\n    xlabel=\"cell number\",\n    yflip=true,\n    ylabel=\"gene feature\",\n    yticks=1:9,\n    colorbar_title=\"expression\",\n)\n\n\n\n\nWe can quantitatively check the absolute magnitudes of the column average of the gap between mat_exp and mat_rec:\n\ndiff_matrix = mat_exp .- mat_rec\nfor col in 4:6\n    @assert abs(mean(diff_matrix[:, col])) &lt;= 0.5\nend\n\nWe observe that, using posterior mean, the recovered data matrix mat_rec has values align with the original data matrix - particularly the same pattern in the first and last 3 gene features are captured, which implies the inference and p-PCA decomposition are successful. This is satisfying as we have just projected the original 9-dimensional space onto a 2-dimensional space - some info has been cut off in the projection process, but we haven’t lost any important info, e.g. the key differences between the two groups. The is the desirable property of PCA: it picks up the principal axes along which most of the (original) data variations cluster, and remove those less relevant. If we choose the reduced space dimension \\(k\\) to be exactly \\(D\\) (the original data dimension), we would recover exactly the same original data matrix mat_exp, i.e. all information will be preserved.\nNow we have represented the original high-dimensional data in two dimensions, without losing the key information about the two groups of cells in the input data. Finally, the benefits of performing PCA is to analyse and visualise the dimension-reduced data in the projected, low-dimensional space. we save the dimension-reduced matrix \\(\\mathbf{Z}\\) as a DataFrame, rename the columns and visualise the first two dimensions.\n\ndf_pca = DataFrame(Z', :auto)\nrename!(df_pca, Symbol.([\"z\" * string(i) for i in collect(1:k)]))\ndf_pca[!, :type] = repeat([1, 2]; inner=n_cells ÷ 2)\n\nscatter(df_pca[:, :z1], df_pca[:, :z2]; xlabel=\"z1\", ylabel=\"z2\", group=df_pca[:, :type])\n\n\n\n\nWe see the two groups are well separated in this 2-D space. As an unsupervised learning method, performing PCA on this dataset gives membership for each cell instance. Another way to put it: 2 dimensions is enough to capture the main structure of the data.\n\n\nFurther extension: automatic choice of the number of principal components with ARD\nA direct question arises from above practice is: how many principal components do we want to keep, in order to sufficiently represent the latent structure in the data? This is a very central question for all latent factor models, i.e. how many dimensions are needed to represent that data in the latent space. In the case of PCA, there exist a lot of heuristics to make that choice. For example, We can tune the number of principal components using empirical methods such as cross-validation based on some criteria such as MSE between the posterior predicted (e.g. mean predictions) data matrix and the original data matrix or the percentage of variation explained 5.\nFor p-PCA, this can be done in an elegant and principled way, using a technique called Automatic Relevance Determination (ARD). ARD can help pick the correct number of principal directions by regularising the solution space using a parameterised, data-dependent prior distribution that effectively prunes away redundant or superfluous features 6. Essentially, we are using a specific prior over the factor loadings \\(\\mathbf{W}\\) that allows us to prune away dimensions in the latent space. The prior is determined by a precision hyperparameter \\(\\alpha\\). Here, smaller values of \\(\\alpha\\) correspond to more important components. You can find more details about this in, for example, Bishop (2006) 7.\n\n@model function pPCA_ARD(X)\n    # Dimensionality of the problem.\n    N, D = size(X)\n\n    # latent variable Z\n    Z ~ filldist(Normal(), D, N)\n\n    # weights/loadings w with Automatic Relevance Determination part\n    α ~ filldist(Gamma(1.0, 1.0), D)\n    W ~ filldist(MvNormal(zeros(D), 1.0 ./ sqrt.(α)), D)\n\n    mu = (W' * Z)'\n\n    tau ~ Gamma(1.0, 1.0)\n    return X ~ arraydist([MvNormal(m, 1.0 / sqrt(tau)) for m in eachcol(mu)])\nend;\n\nInstead of drawing samples of each entry in \\(\\mathbf{W}\\) from a standard normal, this time we repeatedly draw \\(D\\) samples from the \\(D\\)-dimensional MND, forming a \\(D \\times D\\) matrix \\(\\mathbf{W}\\). This matrix is a function of \\(\\alpha\\) as the samples are drawn from the MND parameterised by \\(\\alpha\\). We also introduce a hyper-parameter \\(\\tau\\) which is the precision in the sampling distribution. We also re-parametrise the sampling distribution, i.e. each dimension across all instances is a 60-dimensional multivariate normal distribution. Re-parametrisation can sometimes accelerate the sampling process.\nWe instantiate the model and ask Turing to sample from it using NUTS sampler. The sample trajectories of \\(\\alpha\\) is plotted using the plot function from the package StatsPlots.\n\nppca_ARD = pPCA_ARD(mat_exp') # instantiate the probabilistic model\nchain_ppcaARD = sample(ppca_ARD, NUTS(; adtype=AutoMooncake()), 500) # sampling\nplot(group(chain_ppcaARD, :α); margin=6.0mm)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.0125\n\n\n\n\n\n\n\nAgain, we do some inference diagnostics. Here we look at the convergence of the chains for the \\(α\\) parameter. This parameter determines the relevance of individual components. We see that the chains have converged and the posterior of the \\(\\alpha\\) parameters is centred around much smaller values in two instances. In the following, we will use the mean of the small values to select the relevant dimensions (remember that, smaller values of \\(\\alpha\\) correspond to more important components.). We can clearly see from the values of \\(\\alpha\\) that there should be two dimensions (corresponding to \\(\\bar{\\alpha}_3=\\bar{\\alpha}_5≈0.05\\)) for this dataset.\n\n# Extract parameter mean estimates of the posterior\nW = permutedims(reshape(mean(group(chain_ppcaARD, :W))[:, 2], (n_genes, n_genes)))\nZ = permutedims(reshape(mean(group(chain_ppcaARD, :Z))[:, 2], (n_genes, n_cells)))'\nα = mean(group(chain_ppcaARD, :α))[:, 2]\nplot(α; label=\"α\")\n\n\n\n\nWe can inspect α to see which elements are small (i.e. high relevance). To do this, we first sort α using sortperm() (in ascending order by default), and record the indices of the first two smallest values (among the \\(D=9\\) \\(\\alpha\\) values). After picking the desired principal directions, we extract the corresponding subset loading vectors from \\(\\mathbf{W}\\), and the corresponding dimensions of \\(\\mathbf{Z}\\). We obtain a posterior predicted matrix \\(\\mathbf{X} \\in \\mathbb{R}^{2 \\times 60}\\) as the product of the two sub-matrices, and compare the recovered info with the original matrix.\n\nα_indices = sortperm(α)[1:2]\nk = size(α_indices)[1]\nX_rec = W[:, α_indices] * Z[α_indices, :]\n\ndf_rec = DataFrame(X_rec', :auto)\nheatmap(\n    X_rec;\n    c=:summer,\n    colors=:value,\n    xlabel=\"cell number\",\n    yflip=true,\n    ylabel=\"gene feature\",\n    yticks=1:9,\n    colorbar_title=\"expression\",\n)\n\n\n\n\nWe observe that, the data in the original space is recovered with key information, the distinct feature values in the first and last three genes for the two cell groups, are preserved. We can also examine the data in the dimension-reduced space, i.e. the selected components (rows) in \\(\\mathbf{Z}\\).\n\ndf_pro = DataFrame(Z[α_indices, :]', :auto)\nrename!(df_pro, Symbol.([\"z\" * string(i) for i in collect(1:k)]))\ndf_pro[!, :type] = repeat([1, 2]; inner=n_cells ÷ 2)\nscatter(\n    df_pro[:, 1], df_pro[:, 2]; xlabel=\"z1\", ylabel=\"z2\", color=df_pro[:, \"type\"], label=\"\"\n)\n\n\n\n\nThis plot is very similar to the low-dimensional plot above, with the relevant dimensions chosen based on the values of \\(α\\) via ARD. When you are in doubt about the number of dimensions to project onto, ARD might provide an answer to that question.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Probabilistic PCA"
    ]
  },
  {
    "objectID": "tutorials/probabilistic-pca/index.html#final-comments.",
    "href": "tutorials/probabilistic-pca/index.html#final-comments.",
    "title": "Probabilistic PCA",
    "section": "Final comments.",
    "text": "Final comments.\np-PCA is a linear map which linearly transforms the data between the original and projected spaces. It can also be thought of as a matrix factorisation method, in which \\(\\mathbf{X}=(\\mathbf{W} \\times \\mathbf{Z})^T\\). The projection matrix can be understood as a new basis in the projected space, and \\(\\mathbf{Z}\\) are the new coordinates.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Probabilistic PCA"
    ]
  },
  {
    "objectID": "tutorials/probabilistic-pca/index.html#footnotes",
    "href": "tutorials/probabilistic-pca/index.html#footnotes",
    "title": "Probabilistic PCA",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGilbert Strang, Introduction to Linear Algebra, 5th Ed., Wellesley-Cambridge Press, 2016.↩︎\nGareth M. James, Daniela Witten, Trevor Hastie, Robert Tibshirani, An Introduction to Statistical Learning, Springer, 2013.↩︎\nProbabilistic PCA by TensorFlow, “https://www.tensorflow.org/probability/examples/Probabilistic_PCA”.↩︎\nProbabilistic PCA by TensorFlow, “https://www.tensorflow.org/probability/examples/Probabilistic_PCA”.↩︎\nGareth M. James, Daniela Witten, Trevor Hastie, Robert Tibshirani, An Introduction to Statistical Learning, Springer, 2013.↩︎\nDavid Wipf, Srikantan Nagarajan, A New View of Automatic Relevance Determination, NIPS 2007.↩︎\nChristopher Bishop, Pattern Recognition and Machine Learning, Springer, 2006.↩︎",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Probabilistic PCA"
    ]
  },
  {
    "objectID": "tutorials/gaussian-process-latent-variable-models/index.html",
    "href": "tutorials/gaussian-process-latent-variable-models/index.html",
    "title": "Gaussian Process Latent Variable Models",
    "section": "",
    "text": "In a previous tutorial, we have discussed latent variable models, in particular probabilistic principal component analysis (pPCA). Here, we show how we can extend the mapping provided by pPCA to non-linear mappings between input and output. For more details about the Gaussian Process Latent Variable Model (GPLVM), we refer the reader to the original publication and a further extension.\nIn short, the GPVLM is a dimensionality reduction technique that allows us to embed a high-dimensional dataset in a lower-dimensional embedding. Importantly, it provides the advantage that the linear mappings from the embedded space can be non-linearised through the use of Gaussian Processes.\n\nLet’s start by loading some dependencies.\n\nusing Turing\nusing AbstractGPs\nusing FillArrays\nusing LaTeXStrings\nusing Plots\nusing RDatasets\nusing ReverseDiff\nusing StatsBase\n\nusing LinearAlgebra\nusing Random\n\nRandom.seed!(1789);\n\nWe demonstrate the GPLVM with a very small dataset: Fisher’s Iris data set. This is mostly for reasons of run time, so the tutorial can be run quickly. As you will see, one of the major drawbacks of using GPs is their speed, although this is an active area of research. We will briefly touch on some ways to speed things up at the end of this tutorial. We transform the original data with non-linear operations in order to demonstrate the power of GPs to work on non-linear relationships, while keeping the problem reasonably small.\n\ndata = dataset(\"datasets\", \"iris\")\nspecies = data[!, \"Species\"]\nindex = shuffle(1:150)\n# we extract the four measured quantities,\n# so the dimension of the data is only d=4 for this toy example\ndat = Matrix(data[index, 1:4])\nlabels = data[index, \"Species\"]\n\n# non-linearize data to demonstrate ability of GPs to deal with non-linearity\ndat[:, 1] = 0.5 * dat[:, 1] .^ 2 + 0.1 * dat[:, 1] .^ 3\ndat[:, 2] = dat[:, 2] .^ 3 + 0.2 * dat[:, 2] .^ 4\ndat[:, 3] = 0.1 * exp.(dat[:, 3]) - 0.2 * dat[:, 3] .^ 2\ndat[:, 4] = 0.5 * log.(dat[:, 4]) .^ 2 + 0.01 * dat[:, 3] .^ 5\n\n# normalise data\ndt = fit(ZScoreTransform, dat; dims=1);\nStatsBase.transform!(dt, dat);\n\nWe will start out by demonstrating the basic similarity between pPCA (see the tutorial on this topic) and the GPLVM model. Indeed, pPCA is basically equivalent to running the GPLVM model with an automatic relevance determination (ARD) linear kernel.\nFirst, we re-introduce the pPCA model (see the tutorial on pPCA for details)\n\n@model function pPCA(x)\n    # Dimensionality of the problem.\n    N, D = size(x)\n    # latent variable z\n    z ~ filldist(Normal(), D, N)\n    # weights/loadings W\n    w ~ filldist(Normal(), D, D)\n    mu = (w * z)'\n    for d in 1:D\n        x[:, d] ~ MvNormal(mu[:, d], I)\n    end\n    return nothing\nend;\n\nWe define two different kernels, a simple linear kernel with an Automatic Relevance Determination transform and a squared exponential kernel.\n\nlinear_kernel(α) = LinearKernel() ∘ ARDTransform(α)\nsekernel(α, σ) = σ * SqExponentialKernel() ∘ ARDTransform(α);\n\nAnd here is the GPLVM model. We create separate models for the two types of kernel.\n\n@model function GPLVM_linear(Y, K)\n    # Dimensionality of the problem.\n    N, D = size(Y)\n    # K is the dimension of the latent space\n    @assert K &lt;= D\n    noise = 1e-3\n\n    # Priors\n    α ~ MvLogNormal(MvNormal(Zeros(K), I))\n    Z ~ filldist(Normal(), K, N)\n    mu ~ filldist(Normal(), N)\n\n    gp = GP(linear_kernel(α))\n    gpz = gp(ColVecs(Z), noise)\n    Y ~ filldist(MvNormal(mu, cov(gpz)), D)\n\n    return nothing\nend;\n\n@model function GPLVM(Y, K)\n    # Dimensionality of the problem.\n    N, D = size(Y)\n    # K is the dimension of the latent space\n    @assert K &lt;= D\n    noise = 1e-3\n\n    # Priors\n    α ~ MvLogNormal(MvNormal(Zeros(K), I))\n    σ ~ LogNormal(0.0, 1.0)\n    Z ~ filldist(Normal(), K, N)\n    mu ~ filldist(Normal(), N)\n\n    gp = GP(sekernel(α, σ))\n    gpz = gp(ColVecs(Z), noise)\n    Y ~ filldist(MvNormal(mu, cov(gpz)), D)\n\n    return nothing\nend;\n\n\n# Standard GPs don't scale very well in n, so we use a small subsample for the purpose of this tutorial\nn_data = 40\n# number of features to use from dataset\nn_features = 4\n# latent dimension for GP case\nndim = 4;\n\n\nppca = pPCA(dat[1:n_data, 1:n_features])\nchain_ppca = sample(ppca, NUTS{Turing.ReverseDiffAD{true}}(), 1000);\n\n\n# we extract the posterior mean estimates of the parameters from the chain\nz_mean = reshape(mean(group(chain_ppca, :z))[:, 2], (n_features, n_data))\nscatter(z_mean[1, :], z_mean[2, :]; group=labels[1:n_data], xlabel=L\"z_1\", ylabel=L\"z_2\")\n\nWe can see that the pPCA fails to distinguish the groups. In particular, the setosa species is not clearly separated from versicolor and virginica. This is due to the non-linearities that we introduced, as without them the two groups can be clearly distinguished using pPCA (see the pPCA tutorial).\nLet’s try the same with our linear kernel GPLVM model.\n\ngplvm_linear = GPLVM_linear(dat[1:n_data, 1:n_features], ndim)\nchain_linear = sample(gplvm_linear, NUTS{Turing.ReverseDiffAD{true}}(), 500);\n\n\n# we extract the posterior mean estimates of the parameters from the chain\nz_mean = reshape(mean(group(chain_linear, :Z))[:, 2], (n_features, n_data))\nalpha_mean = mean(group(chain_linear, :α))[:, 2]\n\nalpha1, alpha2 = partialsortperm(alpha_mean, 1:2; rev=true)\nscatter(\n    z_mean[alpha1, :],\n    z_mean[alpha2, :];\n    group=labels[1:n_data],\n    xlabel=L\"z_{\\mathrm{ard}_1}\",\n    ylabel=L\"z_{\\mathrm{ard}_2}\",\n)\n\nWe can see that similar to the pPCA case, the linear kernel GPLVM fails to distinguish between the two groups (setosa on the one hand, and virginica and verticolor on the other).\nFinally, we demonstrate that by changing the kernel to a non-linear function, we are able to separate the data again.\n\ngplvm = GPLVM(dat[1:n_data, 1:n_features], ndim)\nchain_gplvm = sample(gplvm, NUTS{Turing.ReverseDiffAD{true}}(), 500);\n\n\n# we extract the posterior mean estimates of the parameters from the chain\nz_mean = reshape(mean(group(chain_gplvm, :Z))[:, 2], (ndim, n_data))\nalpha_mean = mean(group(chain_gplvm, :α))[:, 2]\n\nalpha1, alpha2 = partialsortperm(alpha_mean, 1:2; rev=true)\nscatter(\n    z_mean[alpha1, :],\n    z_mean[alpha2, :];\n    group=labels[1:n_data],\n    xlabel=L\"z_{\\mathrm{ard}_1}\",\n    ylabel=L\"z_{\\mathrm{ard}_2}\",\n)\n\n\nlet\n    @assert abs(\n        mean(z_mean[alpha1, labels[1:n_data] .== \"setosa\"]) -\n        mean(z_mean[alpha1, labels[1:n_data] .!= \"setosa\"]),\n    ) &gt; 1\nend\n\nNow, the split between the two groups is visible again.\n\n\n\n\n Back to top",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Gaussian Process Latent Variable Models"
    ]
  },
  {
    "objectID": "tutorials/bayesian-linear-regression/index.html",
    "href": "tutorials/bayesian-linear-regression/index.html",
    "title": "Bayesian Linear Regression",
    "section": "",
    "text": "Turing is powerful when applied to complex hierarchical models, but it can also be applied to common statistical procedures, like linear regression. This tutorial covers how to implement a linear regression model in Turing.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Linear Regression"
    ]
  },
  {
    "objectID": "tutorials/bayesian-linear-regression/index.html#set-up",
    "href": "tutorials/bayesian-linear-regression/index.html#set-up",
    "title": "Bayesian Linear Regression",
    "section": "Set Up",
    "text": "Set Up\nWe begin by importing all the necessary libraries.\n\n# Import Turing.\nusing Turing\n\n# Package for loading the data set.\nusing RDatasets\n\n# Package for visualisation.\nusing StatsPlots\n\n# Functionality for splitting the data.\nusing MLUtils: splitobs\n\n# Functionality for constructing arrays with identical elements efficiently.\nusing FillArrays\n\n# Functionality for normalising the data and evaluating the model predictions.\nusing StatsBase\n\n# Functionality for working with scaled identity matrices.\nusing LinearAlgebra\n\n# For ensuring reproducibility.\nusing StableRNGs: StableRNG\n\n\nsetprogress!(false)\n\nWe will use the mtcars dataset from the RDatasets package. mtcars contains a variety of statistics on different car models, including their miles per gallon, number of cylinders, and horsepower, among others.\nWe want to know if we can construct a Bayesian linear regression model to predict the miles per gallon of a car, given the other statistics it has. Let us take a look at the data we have.\n\n# Load the dataset.\ndata = RDatasets.dataset(\"datasets\", \"mtcars\")\n\n# Show the first six rows of the dataset.\nfirst(data, 6)\n\n6×12 DataFrame\n\n\n\nRow\nModel\nMPG\nCyl\nDisp\nHP\nDRat\nWT\nQSec\nVS\nAM\nGear\nCarb\n\n\n\nString31\nFloat64\nInt64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nInt64\nInt64\nInt64\nInt64\n\n\n\n\n1\nMazda RX4\n21.0\n6\n160.0\n110\n3.9\n2.62\n16.46\n0\n1\n4\n4\n\n\n2\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.9\n2.875\n17.02\n0\n1\n4\n4\n\n\n3\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.32\n18.61\n1\n1\n4\n1\n\n\n4\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n5\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\n6\nValiant\n18.1\n6\n225.0\n105\n2.76\n3.46\n20.22\n1\n0\n3\n1\n\n\n\n\n\n\n\nsize(data)\n\n(32, 12)\n\n\nThe next step is to get our data ready for testing. We’ll split the mtcars dataset into two subsets, one for training our model and one for evaluating our model. Then, we separate the targets we want to learn (MPG, in this case) and standardise the datasets by subtracting each column’s mean and dividing by the standard deviation of that column. This standardisation ensures all features have similar scales (mean 0, standard deviation 1), which helps the sampler explore the parameter space more efficiently.\n\n# Remove the model column.\nselect!(data, Not(:Model))\n\n# Split our dataset 70%/30% into training/test sets.\ntrainset, testset = map(DataFrame, splitobs(StableRNG(468), data; at=0.7, shuffle=true))\n\n# Turing requires data in matrix form.\ntarget = :MPG\ntrain = Matrix(select(trainset, Not(target)))\ntest = Matrix(select(testset, Not(target)))\ntrain_target = trainset[:, target]\ntest_target = testset[:, target]\n\n# Standardise the features.\ndt_features = fit(ZScoreTransform, train; dims=1)\nStatsBase.transform!(dt_features, train)\nStatsBase.transform!(dt_features, test)\n\n# Standardise the targets.\ndt_targets = fit(ZScoreTransform, train_target)\nStatsBase.transform!(dt_targets, train_target)\nStatsBase.transform!(dt_targets, test_target);",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Linear Regression"
    ]
  },
  {
    "objectID": "tutorials/bayesian-linear-regression/index.html#model-specification",
    "href": "tutorials/bayesian-linear-regression/index.html#model-specification",
    "title": "Bayesian Linear Regression",
    "section": "Model Specification",
    "text": "Model Specification\nIn a traditional frequentist model using OLS, our model might look like:\n\\[\n\\mathrm{MPG}_i = \\alpha + \\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{X_i}\n\\]\nwhere \\(\\boldsymbol{\\beta}\\) is a vector of coefficients and \\(\\boldsymbol{X}\\) is a vector of inputs for observation \\(i\\). The Bayesian model we are more concerned with is the following:\n\\[\n\\mathrm{MPG}_i \\sim \\mathcal{N}(\\alpha + \\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{X_i}, \\sigma^2)\n\\]\nwhere \\(\\alpha\\) is an intercept term common to all observations, \\(\\boldsymbol{\\beta}\\) is a coefficient vector, \\(\\boldsymbol{X_i}\\) is the observed data for car \\(i\\), and \\(\\sigma^2\\) is a common variance term.\nFor \\(\\sigma^2\\), we assign a prior of truncated(Normal(0, 100); lower=0). This is consistent with Andrew Gelman’s recommendations on noninformative priors for variance. The intercept term (\\(\\alpha\\)) is assumed to be normally distributed with a mean of zero and a variance of three. This represents our assumptions that miles per gallon can be explained mostly by our various variables, but a high variance term indicates our uncertainty about that. Each coefficient is assumed to be normally distributed with a mean of zero and a variance of 10. We do not know that our coefficients are different from zero, and we do not know which ones are likely to be the most important, so the variance term is quite high. Lastly, each observation \\(y_i\\) is distributed according to the calculated mu term given by \\(\\alpha + \\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{X_i}\\).\n\n# Bayesian linear regression.\n@model function linear_regression(x, y)\n    # Set variance prior.\n    σ² ~ truncated(Normal(0, 100); lower=0)\n\n    # Set intercept prior.\n    intercept ~ Normal(0, sqrt(3))\n\n    # Set the priors on our coefficients.\n    nfeatures = size(x, 2)\n    coefficients ~ MvNormal(Zeros(nfeatures), 10.0 * I)\n\n    # Calculate all the mu terms.\n    mu = intercept .+ x * coefficients\n    return y ~ MvNormal(mu, σ² * I)\nend\n\nlinear_regression (generic function with 2 methods)\n\n\nWith our model specified, we can call the sampler. We will use the No U-Turn Sampler (NUTS) here.\n\nmodel = linear_regression(train, train_target)\nchain = sample(StableRNG(468), model, NUTS(), 20_000)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.2\n\n\n\n\nChains MCMC chain (20000×26×1 Array{Float64, 3}):\n\nIterations        = 1001:1:21000\nNumber of chains  = 1\nSamples per chain = 20000\nWall duration     = 11.49 seconds\nCompute duration  = 11.49 seconds\nparameters        = σ², intercept, coefficients[1], coefficients[2], coefficients[3], coefficients[4], coefficients[5], coefficients[6], coefficients[7], coefficients[8], coefficients[9], coefficients[10]\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nWe can also check the densities and traces of the parameters visually using the plot functionality.\n\nplot(chain)\n\n\n\n\nIt looks like all parameters have converged.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Linear Regression"
    ]
  },
  {
    "objectID": "tutorials/bayesian-linear-regression/index.html#comparing-to-ols",
    "href": "tutorials/bayesian-linear-regression/index.html#comparing-to-ols",
    "title": "Bayesian Linear Regression",
    "section": "Comparing to OLS",
    "text": "Comparing to OLS\nA satisfactory test of our model is to evaluate how well it predicts. Importantly, we want to compare our model to existing tools like OLS. The code below uses the GLM.jl package to generate a traditional OLS multiple regression model on the same data as our probabilistic model.\n\n# Import the GLM package.\nusing GLM\n\n# Perform multiple regression OLS.\ntrain_with_intercept = hcat(ones(size(train, 1)), train)\nols = lm(train_with_intercept, train_target)\n\n# Compute predictions on the training data set and unstandardise them.\ntrain_prediction_ols = GLM.predict(ols)\nStatsBase.reconstruct!(dt_targets, train_prediction_ols)\n\n# Compute predictions on the test data set and unstandardise them.\ntest_with_intercept = hcat(ones(size(test, 1)), test)\ntest_prediction_ols = GLM.predict(ols, test_with_intercept)\nStatsBase.reconstruct!(dt_targets, test_prediction_ols);\n\nThe function below accepts a chain and an input matrix and calculates predictions. We use the samples starting from sample 200 onwards, discarding the initial samples as burn-in to allow the sampler to reach the typical set.\n\n# Make a prediction given an input vector.\nfunction prediction(chain, x)\n    p = get_params(chain[200:end, :, :])\n    targets = p.intercept' .+ x * reduce(hcat, p.coefficients)'\n    return vec(mean(targets; dims=2))\nend\n\nprediction (generic function with 1 method)\n\n\nWhen we make predictions, we unstandardise them so they are more understandable.\n\n# Calculate the predictions for the training and testing sets and unstandardise them.\ntrain_prediction_bayes = prediction(chain, train)\nStatsBase.reconstruct!(dt_targets, train_prediction_bayes)\ntest_prediction_bayes = prediction(chain, test)\nStatsBase.reconstruct!(dt_targets, test_prediction_bayes)\n\n# Show the predictions on the test data set.\nDataFrame(; MPG=testset[!, target], Bayes=test_prediction_bayes, OLS=test_prediction_ols)\n\n10×3 DataFrame\n\n\n\nRow\nMPG\nBayes\nOLS\n\n\n\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n30.4\n26.9694\n26.9093\n\n\n2\n19.2\n16.3308\n16.0834\n\n\n3\n14.3\n12.0415\n11.9393\n\n\n4\n22.8\n26.7172\n26.5984\n\n\n5\n22.8\n31.437\n32.153\n\n\n6\n13.3\n9.18556\n8.83012\n\n\n7\n18.7\n17.092\n17.1669\n\n\n8\n10.4\n13.6425\n13.821\n\n\n9\n19.7\n20.0464\n20.0243\n\n\n10\n15.2\n17.0071\n17.0774\n\n\n\n\n\n\nNow let’s evaluate the loss for each method, and each prediction set. We will use the mean squared error to evaluate loss, given by \\[\n\\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^n {(y_i - \\hat{y_i})^2}\n\\] where \\(y_i\\) is the actual value (true MPG) and \\(\\hat{y_i}\\) is the predicted value using either OLS or Bayesian linear regression. A lower MSE indicates a closer fit to the data.\n\nprintln(\n    \"Training set:\",\n    \"\\n\\tBayes loss: \",\n    msd(train_prediction_bayes, trainset[!, target]),\n    \"\\n\\tOLS loss: \",\n    msd(train_prediction_ols, trainset[!, target]),\n)\n\nprintln(\n    \"Test set:\",\n    \"\\n\\tBayes loss: \",\n    msd(test_prediction_bayes, testset[!, target]),\n    \"\\n\\tOLS loss: \",\n    msd(test_prediction_ols, testset[!, target]),\n)\n\nTraining set:\n    Bayes loss: 4.916576423299954\n    OLS loss: 4.909437479783827\nTest set:\n    Bayes loss: 14.845850769901645\n    OLS loss: 16.70400759383217\n\n\nWe can see from this that both linear regression techniques perform fairly similarly. The Bayesian linear regression approach performs worse on the training set, but better on the test set. This indicates that the Bayesian approach is more able to generalise to unseen data, i.e., it is not overfitting the training data as much.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Linear Regression"
    ]
  },
  {
    "objectID": "tutorials/coin-flipping/index.html",
    "href": "tutorials/coin-flipping/index.html",
    "title": "Introduction: Coin Flipping",
    "section": "",
    "text": "This is the first of a series of guided tutorials on the Turing language. In this tutorial, we will use Bayesian inference to estimate the probability that a coin flip will result in heads, given a series of observations.\n\nSetup\nFirst, let us load some packages that we need to simulate a coin flip:\n\nusing Distributions\n\nusing Random\nRandom.seed!(12); # Set seed for reproducibility\n\nand to visualise our results.\n\nusing StatsPlots\n\nNote that Turing is not loaded here — we do not use it in this example. Next, we configure the data generating model. Let us set the true probability that a coin flip turns up heads\n\np_true = 0.5;\n\nand set the number of coin flips we will show our model.\n\nN = 100;\n\nWe simulate N coin flips by drawing N random samples from the Bernoulli distribution with success probability p_true. The draws are collected in a variable called data:\n\ndata = rand(Bernoulli(p_true), N);\n\nHere are the first five coin flips:\n\ndata[1:5]\n\n5-element Vector{Bool}:\n 1\n 0\n 0\n 0\n 1\n\n\n\n\nCoin Flipping Without Turing\nThe following example illustrates the effect of updating our beliefs with every piece of new evidence we observe.\nAssume that we are unsure about the probability of heads in a coin flip. To get an intuitive understanding of what “updating our beliefs” is, we will visualise the probability of heads in a coin flip after each observed evidence.\nWe begin by specifying a prior belief about the distribution of heads and tails in a coin toss. Here we choose a Beta distribution as prior distribution for the probability of heads. Before any coin flip is observed, we assume a uniform distribution \\(\\operatorname{U}(0, 1) = \\operatorname{Beta}(1, 1)\\) of the probability of heads. I.e., every probability is equally likely initially.\n\nprior_belief = Beta(1, 1);\n\nWith our priors set and our data at hand, we can perform Bayesian inference.\nThis is a fairly simple process. We expose one additional coin flip to our model every iteration, such that the first run only sees the first coin flip, while the last iteration sees all the coin flips. In each iteration we update our belief to an updated version of the original Beta distribution that accounts for the new proportion of heads and tails. The update is particularly simple since our prior distribution is a conjugate prior, which allows for analytical posterior computation. Note that such closed-form expressions (as implemented in the updated_belief function below) are not available for most models, which is why we need sampling methods like MCMC.\n\nfunction updated_belief(prior_belief::Beta, data::AbstractArray{Bool})\n    # Count the number of heads and tails.\n    heads = sum(data)\n    tails = length(data) - heads\n\n    # Update our prior belief in closed form (this is possible because we use a conjugate prior).\n    return Beta(prior_belief.α + heads, prior_belief.β + tails)\nend\n\n# Show updated belief for increasing number of observations\n@gif for n in 0:N\n    plot(\n        updated_belief(prior_belief, data[1:n]);\n        size=(500, 250),\n        title=\"Updated belief after $n observations\",\n        xlabel=\"probability of heads\",\n        ylabel=\"\",\n        legend=nothing,\n        xlim=(0, 1),\n        fill=0,\n        α=0.3,\n        w=3,\n    )\n    vline!([p_true])\nend\n\n\nGKS: cannot open display - headless operation mode active\n[ Info: Saved animation to /tmp/jl_NsXuHuHaVk.gif\n\n\n\n\n\n\n\nThe animation above shows that with increasing evidence our belief about the probability of heads in a coin flip slowly adjusts towards the true value. The orange line in the animation represents the true probability of seeing heads on a single coin flip, while the mode of the distribution shows what the model believes the probability of a heads is given the evidence it has seen.\nFor the mathematically inclined, the \\(\\operatorname{Beta}\\) distribution is updated by adding each coin flip to the parameters \\(\\alpha\\) and \\(\\beta\\) of the distribution. Initially, the parameters are defined as \\(\\alpha = 1\\) and \\(\\beta = 1\\). Over time, with more and more coin flips, \\(\\alpha\\) and \\(\\beta\\) will be approximately equal to each other as we are equally likely to flip a heads or a tails.\nThe mean of the \\(\\operatorname{Beta}(\\alpha, \\beta)\\) distribution is\n\\[\\operatorname{E}[X] = \\dfrac{\\alpha}{\\alpha+\\beta}.\\]\nThis implies that the plot of the distribution will become centred around 0.5 for a large enough number of coin flips, as we expect \\(\\alpha \\approx \\beta\\).\nThe variance of the \\(\\operatorname{Beta}(\\alpha, \\beta)\\) distribution is\n\\[\\operatorname{var}[X] = \\dfrac{\\alpha\\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}.\\]\nThus the variance of the distribution will approach 0 with more and more samples, as the denominator will grow faster than will the numerator. More samples means less variance. This implies that the distribution will reflect less uncertainty about the probability of receiving a heads and the plot will become more tightly centred around 0.5 for a large enough number of coin flips.\n\n\nCoin Flipping With Turing\nWe now move away from the closed-form expression above. We use Turing to specify the same model and to approximate the posterior distribution with samples. To do so, we first need to load Turing.\n\nusing Turing\n\nAdditionally, we load MCMCChains, a library for analysing and visualising the samples with which we approximate the posterior distribution.\n\nusing MCMCChains\n\nFirst, we define the coin-flip model using Turing.\n\n# Unconditioned coinflip model with `N` observations.\n@model function coinflip(; N::Int)\n    # Our prior belief about the probability of heads in a coin toss.\n    p ~ Beta(1, 1)\n\n    # Heads or tails of a coin are drawn from `N` independent and identically\n    # distributed Bernoulli distributions with success rate `p`.\n    y ~ filldist(Bernoulli(p), N)\n\n    return y\nend;\n\nIn the Turing model the prior distribution of the variable p, the probability of heads in a coin toss, and the distribution of the observations y are specified on the right-hand side of the ~ expressions. The @model macro modifies the body of the Julia function coinflip and, e.g., replaces the ~ statements with internal function calls that are used for sampling.\nHere we defined a model that is not conditioned on any specific observations as this allows us to easily obtain samples of both p and y with\n\nrand(coinflip(; N))\n\n(p = 0.9520583115441003, y = Bool[1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\nThe model can be conditioned on observations using the | operator, which fixes certain variables to observed values. See the documentation of the condition syntax in DynamicPPL.jl for more details. In the conditioned model below, the observations y are fixed to data.\n\ncoinflip(y::AbstractVector{&lt;:Real}) = coinflip(; N=length(y)) | (; y)\n\nmodel = coinflip(data);\n\nAfter defining the model, we can approximate the posterior distribution by drawing samples from the distribution. In this example, we use a Hamiltonian Monte Carlo sampler to draw these samples. Other tutorials give more information on the samplers available in Turing and discuss their use for different models.\n\nsampler = NUTS();\n\nWe approximate the posterior distribution with 1000 samples:\n\nchain = sample(model, sampler, 2_000, progress=false);\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.8\n\n\n\n\nThe sample function and common keyword arguments are explained more extensively in the documentation of AbstractMCMC.jl.\nAfter finishing the sampling process, we can visually compare the closed-form posterior distribution with the approximation obtained with Turing.\n\nhistogram(chain)\n\n\n\n\nNow we can build our plot:\n\n# Visualise a blue density plot of the approximate posterior distribution using HMC (see Chain 1 in the legend).\ndensity(chain; xlim=(0, 1), legend=:best, w=2, c=:blue)\n\n# Visualise a green density plot of the posterior distribution in closed-form.\nplot!(\n    0:0.01:1,\n    pdf.(updated_belief(prior_belief, data), 0:0.01:1);\n    xlabel=\"probability of heads\",\n    ylabel=\"\",\n    title=\"\",\n    xlim=(0, 1),\n    label=\"Closed-form\",\n    fill=0,\n    α=0.3,\n    w=3,\n    c=:lightgreen,\n)\n\n# Visualise the true probability of heads in red.\nvline!([p_true]; label=\"True probability\", c=:red)\n\n\n\n\nAs we can see, the samples obtained with Turing closely approximate the true posterior distribution. Hopefully this tutorial has provided an easy-to-follow, yet informative introduction to Turing’s simpler applications. More advanced usage is demonstrated in other tutorials.\n\n\n\n\n Back to top",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Introduction: Coin Flipping"
    ]
  },
  {
    "objectID": "tutorials/bayesian-differential-equations/index.html",
    "href": "tutorials/bayesian-differential-equations/index.html",
    "title": "Bayesian Differential Equations",
    "section": "",
    "text": "A basic scientific problem is to mathematically model a system of interest, then compare this model to the observable reality around us. Such models often involve dynamical systems of differential equations. In practice, these equations often have unknown parameters we would like to estimate. The “forward problem” of simulation consists of solving the differential equations for a given set of parameters, while the “inverse problem” of parameter estimation uses observed data to infer the unknown model parameters. Bayesian inference provides a robust approach to parameter estimation with quantified uncertainty.\nusing Turing\nusing DifferentialEquations\n# Load StatsPlots for visualizations and diagnostics.\nusing StatsPlots\nusing LinearAlgebra\nusing Distributions\n# Set a seed for reproducibility.\nusing Random\nRandom.seed!(14);",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Differential Equations"
    ]
  },
  {
    "objectID": "tutorials/bayesian-differential-equations/index.html#the-lotkavolterra-model",
    "href": "tutorials/bayesian-differential-equations/index.html#the-lotkavolterra-model",
    "title": "Bayesian Differential Equations",
    "section": "The Lotka–Volterra Model",
    "text": "The Lotka–Volterra Model\nThe Lotka–Volterra equations, also known as the predator–prey equations, are a pair of first-order nonlinear differential equations. These differential equations are frequently used to describe the dynamics of biological systems in which two species interact, one as a predator and the other as prey. The populations change through time according to the pair of equations\n\\[\n\\begin{aligned}\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} &= (\\alpha - \\beta y(t))x(t), \\\\\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} &= (\\delta x(t) - \\gamma)y(t),\n\\end{aligned}\n\\]\nwhere \\(x(t)\\) and \\(y(t)\\) denote the populations of prey and predator at time \\(t\\), respectively, and \\(\\alpha, \\beta, \\gamma, \\delta\\) are positive parameters. In the absence of predators, the prey population \\(x\\) would increase exponentially at rate \\(\\alpha\\) (with dimensions of time-1). However, the predators kill some prey at a rate \\(\\beta\\) (prey predator-1 time-1), which enables the predator population to increase at rate \\(\\delta\\) (predators prey-1 time-1). Finally, predators are removed by natural mortality at rate \\(\\gamma\\) (time-1).\nWe implement the Lotka–Volterra model and simulate it with parameters \\(\\alpha = 1.5\\), \\(\\beta = 1\\), \\(\\gamma = 3\\), and \\(\\delta = 1\\) and initial conditions \\(x(0) = y(0) = 1\\).\n\n# Define Lotka–Volterra model.\nfunction lotka_volterra(du, u, p, t)\n    # Model parameters.\n    α, β, γ, δ = p\n    # Current state.\n    x, y = u\n\n    # Evaluate differential equations.\n    du[1] = (α - β * y) * x # prey\n    du[2] = (δ * x - γ) * y # predator\n\n    return nothing\nend\n\n# Define initial-value problem.\nu0 = [1.0, 1.0]\np = [1.5, 1.0, 3.0, 1.0]\ntspan = (0.0, 10.0)\nprob = ODEProblem(lotka_volterra, u0, tspan, p)\n\n# Plot simulation.\nplot(solve(prob, Tsit5()))\n\n\n\n\nWe generate noisy observations to use for the parameter estimation tasks in this tutorial. With the saveat argument to the differential equation solver, we specify that the solution is stored only at 0.1 time units.\nTo make the example more realistic, we generate data as random Poisson counts based on the “true” population densities of predator and prey from the simulation. Poisson-distributed data are common in ecology (for instance, counts of animals detected by a camera trap). We assume the Poisson rate parameter \\(\\lambda\\) is proportional to the underlying animal densities, with proportionality constant \\(q = 1.7\\) (representing observation efficiency).\n\nsol = solve(prob, Tsit5(); saveat=0.1)\nq = 1.7\nodedata = rand.(Poisson.(q * Array(sol)))\n\n# Plot simulation and noisy observations.\nplot(sol, label=[\"Prey\" \"Predator\"])\nscatter!(sol.t, odedata'; color=[1 2], label=\"\")\n\n\n\n\nAn even more realistic example could be fitted to the famous hare-and-lynx system using the long-term trapping records of the Hudson’s Bay Company. A Stan implementation of this problem with slightly different priors can be found here. For this tutorial, though, we will stick with simulated data.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Differential Equations"
    ]
  },
  {
    "objectID": "tutorials/bayesian-differential-equations/index.html#direct-handling-of-bayesian-estimation-with-turing",
    "href": "tutorials/bayesian-differential-equations/index.html#direct-handling-of-bayesian-estimation-with-turing",
    "title": "Bayesian Differential Equations",
    "section": "Direct Handling of Bayesian Estimation with Turing",
    "text": "Direct Handling of Bayesian Estimation with Turing\nDifferentialEquations.jl is the main Julia package for numerically solving differential equations. Its functionality is completely interoperable with Turing.jl, which means that we can directly simulate differential equations inside a Turing @model.\nFor the purposes of this tutorial, we choose priors for the parameters that are quite close to the ground truth. As justification, we can imagine we have preexisting estimates for the biological rates. Practically, this helps us to illustrate the results without needing to run overly long MCMC chains.\nNote we also have to take special care with the ODE solver. For certain parameter combinations, the numerical solver may predict animal densities that are just barely below zero. This causes errors with the Poisson distribution, which needs a non-negative mean \\(\\lambda\\). To avoid this happening, we tell the solver to aim for small absolute and relative errors (abstol=1e-6, reltol=1e-6). We also add a fudge factor ϵ = 1e-5 to the predicted data. Since ϵ is greater than the solver’s tolerance, it should overcome any remaining numerical error, making sure all predicted values are positive. At the same time, it is so small compared to the data that it should have a negligible effect on inference. If this approach doesn’t work, there are some more ideas to try here. In the case of continuous observations (e.g. data derived from modelling chemical reactions), it is sufficient to use a normal distribution with the mean as the data point and an appropriately chosen variance (which can itself also be a parameter with a prior distribution).\n\n@model function fitlv(data, prob)\n    # Prior distributions.\n    α ~ truncated(Normal(1.5, 0.2); lower=0.5, upper=2.5)\n    β ~ truncated(Normal(1.1, 0.2); lower=0, upper=2)\n    γ ~ truncated(Normal(3.0, 0.2); lower=1, upper=4)\n    δ ~ truncated(Normal(1.0, 0.2); lower=0, upper=2)\n    q ~ truncated(Normal(1.7, 0.2); lower=0, upper=3)\n\n    # Simulate Lotka–Volterra model. \n    p = [α, β, γ, δ]\n    predicted = solve(prob, Tsit5(); p=p, saveat=0.1, abstol=1e-6, reltol=1e-6)\n    ϵ = 1e-5\n    \n    # Observations.\n    for i in eachindex(predicted)\n        data[:, i] ~ arraydist(Poisson.(q .* predicted[i] .+ ϵ))\n    end\n\n    return nothing\nend\n\nmodel = fitlv(odedata, prob)\n\n# Sample 3 independent chains with forward-mode automatic differentiation (the default).\nchain = sample(model, NUTS(), MCMCSerial(), 1000, 3; progress=false)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.2\n\n\n\n\nChains MCMC chain (1000×19×3 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 3\nSamples per chain = 1000\nWall duration     = 51.99 seconds\nCompute duration  = 47.44 seconds\nparameters        = α, β, γ, δ, q\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nThe estimated parameters are close to the parameter values the observations were generated with. We can also check visually that the chains have converged.\n\nplot(chain)\n\n\n\n\n\nData retrodiction\nIn Bayesian analysis it is often useful to retrodict the data, i.e. generate simulated data using samples from the posterior distribution, and compare to the original data (see for instance section 3.3.2 - model checking of McElreath’s book “Statistical Rethinking”). Here, we solve the ODE for 300 randomly picked posterior samples in the chain. We plot the ensemble of solutions to check if the solution resembles the data. The 300 retrodicted time courses from the posterior are plotted in gray, the noisy observations are shown as blue and red dots, and the green and purple lines are the ODE solution that was used to generate the data.\n\nplot(; legend=false)\nposterior_samples = sample(chain[[:α, :β, :γ, :δ]], 300; replace=false)\nfor p in eachrow(Array(posterior_samples))\n    sol_p = solve(prob, Tsit5(); p=p, saveat=0.1)\n    plot!(sol_p; alpha=0.1, color=\"#BBBBBB\")\nend\n\n# Plot simulation and noisy observations.\nplot!(sol; color=[1 2], linewidth=1)\nscatter!(sol.t, odedata'; color=[1 2])\n\n\n\n\nWe can see that, even though we added quite a bit of noise to the data the posterior distribution reproduces quite accurately the “true” ODE solution.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Differential Equations"
    ]
  },
  {
    "objectID": "tutorials/bayesian-differential-equations/index.html#lotkavolterra-model-without-data-of-prey",
    "href": "tutorials/bayesian-differential-equations/index.html#lotkavolterra-model-without-data-of-prey",
    "title": "Bayesian Differential Equations",
    "section": "Lotka–Volterra model without data of prey",
    "text": "Lotka–Volterra model without data of prey\nOne can also perform parameter inference for a Lotka–Volterra model with incomplete data. For instance, let us suppose we have only observations of the predators but not of the prey. We can fit the model only to the \\(y\\) variable of the system without providing any data for \\(x\\):\n\n@model function fitlv2(data::AbstractVector, prob)\n    # Prior distributions.\n    α ~ truncated(Normal(1.5, 0.2); lower=0.5, upper=2.5)\n    β ~ truncated(Normal(1.1, 0.2); lower=0, upper=2)\n    γ ~ truncated(Normal(3.0, 0.2); lower=1, upper=4)\n    δ ~ truncated(Normal(1.0, 0.2); lower=0, upper=2)\n    q ~ truncated(Normal(1.7, 0.2); lower=0, upper=3)\n\n    # Simulate Lotka–Volterra model but save only the second state of the system (predators).\n    p = [α, β, γ, δ]\n    predicted = solve(prob, Tsit5(); p=p, saveat=0.1, save_idxs=2, abstol=1e-6, reltol=1e-6)\n    ϵ = 1e-5\n\n    # Observations of the predators.\n    data ~ arraydist(Poisson.(q .* predicted.u .+ ϵ))\n\n    return nothing\nend\n\nmodel2 = fitlv2(odedata[2, :], prob)\n\n# Sample 3 independent chains.\nchain2 = sample(model2, NUTS(0.45), MCMCSerial(), 5000, 3; progress=false)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.2\n\n\n\n\nChains MCMC chain (5000×19×3 Array{Float64, 3}):\n\nIterations        = 1001:1:6000\nNumber of chains  = 3\nSamples per chain = 5000\nWall duration     = 18.7 seconds\nCompute duration  = 18.3 seconds\nparameters        = α, β, γ, δ, q\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nAgain we inspect the trajectories of 300 randomly selected posterior samples.\n\nplot(; legend=false)\nposterior_samples = sample(chain2[[:α, :β, :γ, :δ]], 300; replace=false)\nfor p in eachrow(Array(posterior_samples))\n    sol_p = solve(prob, Tsit5(); p=p, saveat=0.1)\n    plot!(sol_p; alpha=0.1, color=\"#BBBBBB\")\nend\n\n# Plot simulation and noisy observations.\nplot!(sol; color=[1 2], linewidth=1)\nscatter!(sol.t, odedata'; color=[1 2])\n\n\n\n\nNote that here the observations of the prey (blue dots) were not used in the parameter estimation! Yet, the model can predict the values of \\(x\\) relatively accurately, albeit with a wider distribution of solutions, reflecting the greater uncertainty in the prediction of the \\(x\\) values.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Differential Equations"
    ]
  },
  {
    "objectID": "tutorials/bayesian-differential-equations/index.html#inference-of-delay-differential-equations",
    "href": "tutorials/bayesian-differential-equations/index.html#inference-of-delay-differential-equations",
    "title": "Bayesian Differential Equations",
    "section": "Inference of Delay Differential Equations",
    "text": "Inference of Delay Differential Equations\nHere we show an example of inference with another type of differential equation: a delay differential equation (DDE). DDEs are differential equations where derivatives are functions of values at an earlier point in time. This is useful to model a delayed effect, such as the incubation time of a virus.\nHere is a delayed version of the Lotka–Volterra system:\n\\[\n\\begin{aligned}\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} &= \\alpha x(t-\\tau) - \\beta y(t) x(t),\\\\\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} &= - \\gamma y(t) + \\delta x(t) y(t),\n\\end{aligned}\n\\]\nwhere \\(\\tau\\) is a (positive) delay and \\(x(t-\\tau)\\) is the variable \\(x\\) at an earlier time point \\(t - \\tau\\).\nThe initial-value problem of the delayed system can be implemented as a DDEProblem. As described in the DDE example, here the function h is the history function that can be used to obtain a state at an earlier time point. Again we use parameters \\(\\alpha = 1.5\\), \\(\\beta = 1\\), \\(\\gamma = 3\\), and \\(\\delta = 1\\) and initial conditions \\(x(0) = y(0) = 1\\). Moreover, we assume \\(x(t) = 1\\) for \\(t &lt; 0\\).\n\nfunction delay_lotka_volterra(du, u, h, p, t)\n    # Model parameters.\n    α, β, γ, δ = p\n\n    # Current state.\n    x, y = u\n    # Evaluate differential equations\n    du[1] = α * h(p, t - 1; idxs=1) - β * x * y\n    du[2] = -γ * y + δ * x * y\n\n    return nothing\nend\n\n# Define initial-value problem.\np = (1.5, 1.0, 3.0, 1.0)\nu0 = [1.0; 1.0]\ntspan = (0.0, 10.0)\nh(p, t; idxs::Int) = 1.0\nprob_dde = DDEProblem(delay_lotka_volterra, u0, h, tspan, p);\n\nWe generate observations by sampling from the corresponding Poisson distributions derived from the simulation results:\n\nsol_dde = solve(prob_dde; saveat=0.1)\nddedata = rand.(Poisson.(q .* Array(sol_dde)))\n\n# Plot simulation and noisy observations.\nplot(sol_dde)\nscatter!(sol_dde.t, ddedata'; color=[1 2], label=\"\")\n\n\n\n\nNow we define the Turing model for the Lotka–Volterra model with a delay, and sample 3 independent chains.\n\n@model function fitlv_dde(data, prob)\n    # Prior distributions.\n    α ~ truncated(Normal(1.5, 0.2); lower=0.5, upper=2.5)\n    β ~ truncated(Normal(1.1, 0.2); lower=0, upper=2)\n    γ ~ truncated(Normal(3.0, 0.2); lower=1, upper=4)\n    δ ~ truncated(Normal(1.0, 0.2); lower=0, upper=2)\n    q ~ truncated(Normal(1.7, 0.2); lower=0, upper=3)\n\n    # Simulate Lotka–Volterra model.\n    p = [α, β, γ, δ]\n    predicted = solve(prob, MethodOfSteps(Tsit5()); p=p, saveat=0.1, abstol=1e-6, reltol=1e-6)\n    ϵ = 1e-5\n\n    # Observations.\n    for i in eachindex(predicted)\n        data[:, i] ~ arraydist(Poisson.(q .* predicted[i] .+ ϵ))\n    end\nend\n\nmodel_dde = fitlv_dde(ddedata, prob_dde)\n\nchain_dde = sample(model_dde, NUTS(), MCMCSerial(), 300, 3; progress=false)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.2\n┌ Info: Found initial step size\n└   ϵ = 0.025\n┌ Info: Found initial step size\n└   ϵ = 0.2\n\n\n\n\nChains MCMC chain (300×19×3 Array{Float64, 3}):\n\nIterations        = 151:1:450\nNumber of chains  = 3\nSamples per chain = 300\nWall duration     = 13.19 seconds\nCompute duration  = 13.06 seconds\nparameters        = α, β, γ, δ, q\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n\nplot(chain_dde)\n\n\n\n\nFinally, we plot trajectories of 300 randomly selected samples from the posterior. Again, the dots indicate our observations, the coloured lines are the “true” simulations without noise, and the gray lines are trajectories from the posterior samples.\n\nplot(; legend=false)\nposterior_samples = sample(chain_dde[[:α, :β, :γ, :δ]], 300; replace=false)\nfor p in eachrow(Array(posterior_samples))\n    sol_p = solve(prob_dde, MethodOfSteps(Tsit5()); p=p, saveat=0.1)\n    plot!(sol_p; alpha=0.1, color=\"#BBBBBB\")\nend\n\n# Plot simulation and noisy observations.\nplot!(sol_dde; color=[1 2], linewidth=1)\nscatter!(sol_dde.t, ddedata'; color=[1 2])\n\n\n\n\nThe fit is pretty good even though the data was quite noisy to start.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Differential Equations"
    ]
  },
  {
    "objectID": "tutorials/bayesian-differential-equations/index.html#scaling-to-large-models-adjoint-sensitivities",
    "href": "tutorials/bayesian-differential-equations/index.html#scaling-to-large-models-adjoint-sensitivities",
    "title": "Bayesian Differential Equations",
    "section": "Scaling to Large Models: Adjoint Sensitivities",
    "text": "Scaling to Large Models: Adjoint Sensitivities\nTuring’s gradient-based MCMC algorithms, such as NUTS, use ForwardDiff by default. This works well for small models, but for larger models with many parameters, reverse-mode automatic differentiation is often more efficient (see the automatic differentiation page for more information).\nTo use reverse-mode AD with differential equations, you need to first load the SciMLSensitivity.jl package, which forms part of SciML’s differential equation suite. Here, ‘sensitivity’ refers to the derivative of the solution of a differential equation with respect to its parameters. More details on the mathematical theory that underpins these methods can be found in the SciMLSensitivity documentation.\nOnce SciMLSensitivity has been loaded, you can use one of the AD backends which are compatible with SciMLSensitivity.jl. For example, if we wanted to use Mooncake.jl, we could run:\n\nimport Mooncake\nimport SciMLSensitivity\n\n# Define the AD backend to use\nadtype = AutoMooncake()\n\n# Sample a single chain with 1000 samples using Mooncake\nsample(model, NUTS(; adtype=adtype), 1000; progress=false)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.025\n\n\n\n\nChains MCMC chain (1000×19×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 535.5 seconds\nCompute duration  = 535.5 seconds\nparameters        = α, β, γ, δ, q\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n(If SciMLSensitivity is not loaded, the call to sample will error.)\nSciMLSensitivity has a number of sensitivity analysis algorithms: in this case it will automatically choose a default for you. You can also manually specify an algorithm by providing the sensealg keyword argument to the solve function; the existing algorithms are covered in this page of the SciMLSensitivity docs.\nFor more examples of adjoint usage on large parameter models, consult the DiffEqFlux documentation.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Bayesian Differential Equations"
    ]
  },
  {
    "objectID": "core-functionality/index.html",
    "href": "core-functionality/index.html",
    "title": "Core Functionality",
    "section": "",
    "text": "This article provides an overview of the core functionality in Turing.jl, which are likely to be used across a wide range of models.",
    "crumbs": [
      "Get Started",
      "Core Functionality"
    ]
  },
  {
    "objectID": "core-functionality/index.html#basics",
    "href": "core-functionality/index.html#basics",
    "title": "Core Functionality",
    "section": "Basics",
    "text": "Basics\n\nIntroduction\nA probabilistic program is a Julia function wrapped in a @model macro. In this function, arbitrary Julia code can be used, but to ensure correctness of inference it should not have external effects or modify global state.\nTo specify distributions of random variables, Turing models use ~ notation: x ~ distr where x is an identifier. This resembles the notation used in statistical models. For example, the model:\n\\[\\begin{align}\na &\\sim \\text{Normal}(0, 1) \\\\\nx &\\sim \\text{Normal}(a, 1)\n\\end{align}\\]\nis written in Turing as:\nusing Turing\n\n@model function mymodel()\n    a ~ Normal(0, 1)\n    x ~ Normal(a, 1)\nend\n\n\nTilde-statements\nIndexing and field access is supported, so that x[i] ~ distr and x.field ~ distr are valid statements. However, in these cases, x must be defined in the scope of the model function. distr is typically either a distribution from Distributions.jl (see this page for implementing custom distributions), or another Turing model wrapped in to_submodel() (see this page for submodels).\nThere are two classes of tilde-statements: observe statements, where the left-hand side contains an observed value, and assume statements, where the left-hand side is not observed. These respectively correspond to likelihood and prior terms.\nIt is easier to start by explaining when a variable is treated as an observed value. This can happen in one of two ways:\n\nThe variable is passed as one of the arguments to the model function; or\nThe value of the variable in the model is explicitly conditioned or fixed.\n\n\n\n\n\n\n\nCaution\n\n\n\nNote that it is not enough for the variable to be defined in the current scope. For example, in\n@model function mymodel(x)\n    y = x + 1\n    y ~ Normal(0, 1)\nend\ny is not treated as an observed value.\n\n\nIn such a case, x is considered to be an observed value, assumed to have been drawn from the distribution distr. The likelihood (if needed) is computed using loglikelihood(distr, x).\nOn the other hand, if neither of the above are true, then this is treated as an assume-statement: inside the probabilistic program, this samples a new variable called x, distributed according to distr, and places it in the current scope.\n\n\nSimple Gaussian Demo\nBelow is a simple Gaussian demo illustrating the basic usage of Turing.jl.\n\n# Import packages.\nusing Turing\nusing StatsPlots\n\n# Define a simple Normal model with unknown mean and variance.\n@model function gdemo(x, y)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    x ~ Normal(m, sqrt(s²))\n    return y ~ Normal(m, sqrt(s²))\nend\n\ngdemo (generic function with 2 methods)\n\n\nIn Turing.jl, MCMC sampling is performed using the sample() function, which (at its most basic) takes a model, a sampler, and the number of samples to draw.\nFor this model, the prior expectation of s² is mean(InverseGamma(2, 3)) = 3/(2 - 1) = 3, and the prior expectation of m is 0. We can check this using the Prior sampler:\n\nsetprogress!(false)\n\n\np1 = sample(gdemo(missing, missing), Prior(), 100000)\n\nChains MCMC chain (100000×7×1 Array{Float64, 3}):\n\nIterations        = 1:1:100000\nNumber of chains  = 1\nSamples per chain = 100000\nWall duration     = 0.77 seconds\nCompute duration  = 0.77 seconds\nparameters        = s², m, x, y\ninternals         = logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nTo perform inference, we simply need to specify the sampling algorithm we want to use.\n\n#  Run sampler, collect results.\nc1 = sample(gdemo(1.5, 2), SMC(), 1000)\nc2 = sample(gdemo(1.5, 2), PG(10), 1000)\nc3 = sample(gdemo(1.5, 2), HMC(0.1, 5), 1000)\nc4 = sample(gdemo(1.5, 2), Gibbs(:m =&gt; PG(10), :s² =&gt; HMC(0.1, 5)), 1000)\nc5 = sample(gdemo(1.5, 2), HMCDA(0.15, 0.65), 1000)\nc6 = sample(gdemo(1.5, 2), NUTS(0.65), 1000)\n\n\n┌ Info: Found initial step size\n└   ϵ = 0.8\n┌ Info: Found initial step size\n└   ϵ = 0.8\n\n\n\n\nChains MCMC chain (1000×16×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 1.21 seconds\nCompute duration  = 1.21 seconds\nparameters        = s², m\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nThe arguments for each sampler are:\n\nSMC: number of particles.\nPG: number of particles, number of iterations.\nHMC: leapfrog step size, leapfrog step numbers.\nGibbs: component sampler 1, component sampler 2, …\nHMCDA: total leapfrog length, target accept ratio.\nNUTS: number of adaptation steps (optional), target accept ratio.\n\nMore information about each sampler can be found in Turing.jl’s API docs.\nThe MCMCChains module (which is re-exported by Turing) provides plotting tools for the Chain objects returned by a sample function. See the MCMCChains repository for more information on the suite of tools available for diagnosing MCMC chains.\n# Summarise results\ndescribe(c3)\n\n# Plot results\nplot(c3)\nsavefig(\"gdemo-plot.png\")\n\n\nConditioning on data\nUsing this syntax, a probabilistic model is defined in Turing. The model function generated by Turing can then be used to condition the model on data. Subsequently, the sample function can be used to generate samples from the posterior distribution.\nIn the following example, the defined model is conditioned to the data (arg_1 = 1, arg_2 = 2) by passing the arguments 1 and 2 to the model function.\n@model function model_name(arg_1, arg_2)\n    arg_1 ~ ...\n    arg_2 ~ ...\nend\nThe conditioned model can then be passed onto the sample function to run posterior inference.\nmodel = model_name(1, 2)\nchn = sample(model, HMC(0.5, 20), 1000) # Sample with HMC.\nAlternatively, one can also use the conditioning operator | to condition the model on data. In this case, the model does not need to be defined with arg_1 and arg_2 as parameters.\n@model function model_name()\n    arg_1 ~ ...\n    arg_2 ~ ...\nend\n\n# Condition the model on data.\nmodel = model_name() | (arg_1 = 1, arg_2 = 2) \n\n\nAnalysing MCMC chains\nThe returned chain contains samples of the variables in the model.\nvar_1 = mean(chn[:var_1]) # Taking the mean of a variable named var_1.\nThe key (:var_1) can either be a Symbol or a String. For example, to fetch x[1], one can use chn[Symbol(\"x[1]\")] or chn[\"x[1]\"]. If you want to retrieve all parameters associated with a specific symbol, you can use group. As an example, if you have the parameters \"x[1]\", \"x[2]\", and \"x[3]\", calling group(chn, :x) or group(chn, \"x\") will return a new chain with only \"x[1]\", \"x[2]\", and \"x[3]\".\n\n\nTilde-statement ordering\nTuring does not have a declarative form. Thus, the ordering of tilde-statements in a Turing model is important: random variables cannot be used until they have been first declared in a tilde-statement. For example, the following example works:\n# Define a simple Normal model with unknown mean and variance.\n@model function model_function(y)\n    s ~ Poisson(1)\n    y ~ Normal(s, 1)\n    return y\nend\n\nsample(model_function(10), SMC(), 100)\nBut if we switch the s ~ Poisson(1) and y ~ Normal(s, 1) lines, the model will no longer sample correctly:\n# Define a simple Normal model with unknown mean and variance.\n@model function model_function(y)\n    y ~ Normal(s, 1)\n    s ~ Poisson(1)\n    return y\nend\n\nsample(model_function(10), SMC(), 100)\n\n\nSampling Multiple Chains\nTuring supports distributed and threaded parallel sampling. To do so, call sample(model, sampler, parallel_type, n, n_chains), where parallel_type can be either MCMCThreads() or MCMCDistributed() for thread and parallel sampling, respectively.\nHaving multiple chains in the same object is valuable for evaluating convergence. Some diagnostic functions like gelmandiag require multiple chains.\nIf you want to sample multiple chains without using parallelism, you can use MCMCSerial():\n# Sample 3 chains in a serial fashion.\nchains = sample(model, sampler, MCMCSerial(), 1000, 3)\nThe chains variable now contains a Chains object which can be indexed by chain. To pull out the first chain from the chains object, use chains[:,:,1]. The method is the same if you use either of the below parallel sampling methods.\n\nMultithreaded sampling\nIf you wish to perform multithreaded sampling, you can call sample with the following signature:\n# Sample four chains using multiple threads, each with 1000 samples.\nsample(model, sampler, MCMCThreads(), 1000, 4)\nBe aware that Turing cannot add threads for you – you must have started your Julia instance with multiple threads to experience any kind of parallelism. See the Julia documentation for details on how to achieve this.\n\n\nDistributed sampling\nTo perform distributed sampling (using multiple processes), you must first import Distributed.\nProcess parallel sampling can be done like so:\n\n# Load Distributed to add processes and the @everywhere macro.\nusing Distributed\n\n# Load Turing.\nusing Turing\n\n# Add four processes to use for sampling.\naddprocs(4; exeflags=\"--project=$(Base.active_project())\")\n\n# Initialise everything on all the processes.\n# Note: Make sure to do this after you've already loaded Turing,\n#       so each process does not have to precompile.\n#       Parallel sampling may fail silently if you do not do this.\n@everywhere using Turing\n\n# Define a model on all processes.\n@everywhere @model function gdemo(x)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n\n    for i in eachindex(x)\n        x[i] ~ Normal(m, sqrt(s²))\n    end\nend\n\n# Declare the model instance everywhere.\n@everywhere model = gdemo([1.5, 2.0])\n\n# Sample four chains using multiple processes, each with 1000 samples.\nsample(model, NUTS(), MCMCDistributed(), 1000, 4)\n\n\n\n\nSampling from an Unconditional Distribution (The Prior)\nTuring allows you to sample from a declared model’s prior. If you wish to draw a chain from the prior to inspect your prior distributions, you can run\n\nchain = sample(model, Prior(), n_samples)\n\nYou can also run your model (as if it were a function) from the prior distribution, by calling the model without specifying inputs or a sampler. In the below example, we specify a gdemo model which returns two variables, x and y. Here, including the return statement is necessary to retrieve the sampled x and y values.\n\n@model function gdemo(x, y)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    x ~ Normal(m, sqrt(s²))\n    y ~ Normal(m, sqrt(s²))\n    return x, y\nend\n\ngdemo (generic function with 2 methods)\n\n\nTo produce a sample from the prior distribution, we instantiate the model with missing inputs:\n\n# Samples from p(x,y)\ng_prior_sample = gdemo(missing, missing)\ng_prior_sample()\n\n(-1.7345226151044124, -5.106238141135371)\n\n\n\n\nSampling from a Conditional Distribution (The Posterior)\n\nTreating observations as random variables\nInputs to the model that have a value missing are treated as parameters, aka random variables, to be estimated/sampled. This can be useful if you want to simulate draws for that parameter, or if you are sampling from a conditional distribution. Turing supports the following syntax:\n\n@model function gdemo(x, ::Type{T}=Float64) where {T}\n    if x === missing\n        # Initialise `x` if missing\n        x = Vector{T}(undef, 2)\n    end\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    for i in eachindex(x)\n        x[i] ~ Normal(m, sqrt(s²))\n    end\nend\n\n# Construct a model with x = missing\nmodel = gdemo(missing)\nc = sample(model, HMC(0.05, 20), 500)\n\nChains MCMC chain (500×16×1 Array{Union{Missing, Float64}, 3}):\n\nIterations        = 1:1:500\nNumber of chains  = 1\nSamples per chain = 500\nWall duration     = 2.37 seconds\nCompute duration  = 2.37 seconds\nparameters        = s², m, x[1], x[2]\ninternals         = logprior, loglikelihood, logjoint, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, numerical_error, step_size, nom_step_size\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nNote the need to initialise x when missing since we are iterating over its elements later in the model. The generated values for x can be extracted from the Chains object using c[:x].\nTuring also supports mixed missing and non-missing values in x, where the missing ones will be treated as random variables to be sampled while the others get treated as observations. For example:\n\n@model function gdemo(x)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    for i in eachindex(x)\n        x[i] ~ Normal(m, sqrt(s²))\n    end\nend\n\n# x[1] is a parameter, but x[2] is an observation\nmodel = gdemo([missing, 2.4])\nc = sample(model, HMC(0.01, 5), 500)\n\nChains MCMC chain (500×15×1 Array{Union{Missing, Float64}, 3}):\n\nIterations        = 1:1:500\nNumber of chains  = 1\nSamples per chain = 500\nWall duration     = 1.26 seconds\nCompute duration  = 1.26 seconds\nparameters        = s², m, x[1]\ninternals         = logprior, loglikelihood, logjoint, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, numerical_error, step_size, nom_step_size\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n\n\nDefault Values\nArguments to Turing models can have default values much like how default values work in normal Julia functions. For instance, the following will assign missing to x and treat it as a random variable. If the default value is not missing, x will be assigned that value and will be treated as an observation instead.\n\nusing Turing\n\n@model function generative(x=missing, ::Type{T}=Float64) where {T&lt;:Real}\n    if x === missing\n        # Initialise x when missing\n        x = Vector{T}(undef, 10)\n    end\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n    for i in 1:length(x)\n        x[i] ~ Normal(m, sqrt(s²))\n    end\n    return s², m\nend\n\nm = generative()\nchain = sample(m, HMC(0.01, 5), 1000)\n\nChains MCMC chain (1000×24×1 Array{Union{Missing, Float64}, 3}):\n\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 2.53 seconds\nCompute duration  = 2.53 seconds\nparameters        = s², m, x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9], x[10]\ninternals         = logprior, loglikelihood, logjoint, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, numerical_error, step_size, nom_step_size\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n\n\nAccess Values inside Chain\nYou can access the values inside a chain in several ways:\n\nTurn them into a DataFrame object\nUse their raw AxisArray form\nCreate a three-dimensional Array object\n\nFor example, let c be a Chain:\n\nDataFrame(c) converts c to a DataFrame,\nc.value retrieves the values inside c as an AxisArray, and\nc.value.data retrieves the values inside c as a 3D Array.\n\n\n\nVariable Types and Type Parameters\nThe element type of a vector (or matrix) of random variables should match the eltype of its prior distribution, i.e., &lt;: Integer for discrete distributions and &lt;: AbstractFloat for continuous distributions.\nSome automatic differentiation backends (used in conjunction with Hamiltonian samplers such as HMC or NUTS) further require that the vector’s element type needs to either be:\n\nReal to enable auto-differentiation through the model which uses special number types that are sub-types of Real, or\nSome type parameter T defined in the model header using the type parameter syntax, e.g. function gdemo(x, ::Type{T} = Float64) where {T}.\n\nSimilarly, when using a particle sampler, the Julia variable used should either be:\n\nAn Array, or\nAn instance of some type parameter T defined in the model header using the type parameter syntax, e.g. function gdemo(x, ::Type{T} = Vector{Float64}) where {T}.\n\n\n\n\nQuerying Probabilities from Model or Chain\nTuring offers three functions: loglikelihood, logprior, and logjoint to query the log-likelihood, log-prior, and log-joint probabilities of a model, respectively.\nLet’s look at a simple model called gdemo:\n\n@model function gdemo0()\n    s ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s))\n    return x ~ Normal(m, sqrt(s))\nend\n\ngdemo0 (generic function with 2 methods)\n\n\nIf we observe x to be 1.0, we can condition the model on this datum using the condition syntax:\n\nmodel = gdemo0() | (x=1.0,)\n\nDynamicPPL.Model{typeof(gdemo0), (), (), (), Tuple{}, Tuple{}, DynamicPPL.ConditionContext{@NamedTuple{x::Float64}, DynamicPPL.DefaultContext}, false}(gdemo0, NamedTuple(), NamedTuple(), ConditionContext((x = 1.0,), DynamicPPL.DefaultContext()))\n\n\nNow, let’s compute the log-likelihood of the observation given specific values of the model parameters, s and m:\n\nloglikelihood(model, (s=1.0, m=1.0))\n\n-0.9189385332046728\n\n\nWe can easily verify that value in this case:\n\nlogpdf(Normal(1.0, 1.0), 1.0)\n\n-0.9189385332046728\n\n\nWe can also compute the log-prior probability of the model for the same values of s and m:\n\nlogprior(model, (s=1.0, m=1.0))\n\n-2.221713955868453\n\n\n\nlogpdf(InverseGamma(2, 3), 1.0) + logpdf(Normal(0, sqrt(1.0)), 1.0)\n\n-2.221713955868453\n\n\nFinally, we can compute the log-joint probability of the model parameters and data:\n\nlogjoint(model, (s=1.0, m=1.0))\n\n-3.1406524890731258\n\n\n\nlogpdf(Normal(1.0, 1.0), 1.0) +\nlogpdf(InverseGamma(2, 3), 1.0) +\nlogpdf(Normal(0, sqrt(1.0)), 1.0)\n\n-3.1406524890731258\n\n\nQuerying with Chains object is easy as well:\n\nchn = sample(model, Prior(), 10)\n\nChains MCMC chain (10×5×1 Array{Float64, 3}):\n\nIterations        = 1:1:10\nNumber of chains  = 1\nSamples per chain = 10\nWall duration     = 0.12 seconds\nCompute duration  = 0.12 seconds\nparameters        = s, m\ninternals         = logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\n\nloglikelihood(model, chn)\n\n10×1 Matrix{Float64}:\n -3.275761083300502\n -1.8603133338205948\n -2.140859721393203\n -2.6573180218739347\n -1.6681818696699147\n -3.3744667973496156\n -1.9032466250049387\n -2.2875491282211824\n -0.9487449336808311\n -3.7631928572180278\n\n\n\n\nMaximum likelihood and maximum a posteriori estimates\nTuring also has functions for estimating the maximum a posteriori and maximum likelihood parameters of a model. This can be done with\n\nmle_estimate = maximum_likelihood(model)\nmap_estimate = maximum_a_posteriori(model)\n\nModeResult with maximized lp of -2.81\n[0.8124999999896609, 0.5000000000018168]\n\n\nFor more details see the mode estimation page.",
    "crumbs": [
      "Get Started",
      "Core Functionality"
    ]
  },
  {
    "objectID": "core-functionality/index.html#beyond-the-basics",
    "href": "core-functionality/index.html#beyond-the-basics",
    "title": "Core Functionality",
    "section": "Beyond the Basics",
    "text": "Beyond the Basics\n\nCompositional Sampling Using Gibbs\nTuring.jl provides a Gibbs interface to combine different samplers. For example, one can combine an HMC sampler with a PG sampler to run inference for different parameters in a single model as below.\n\n@model function simple_choice(xs)\n    p ~ Beta(2, 2)\n    z ~ Bernoulli(p)\n    for i in 1:length(xs)\n        if z == 1\n            xs[i] ~ Normal(0, 1)\n        else\n            xs[i] ~ Normal(2, 1)\n        end\n    end\nend\n\nsimple_choice_f = simple_choice([1.5, 2.0, 0.3])\n\nchn = sample(simple_choice_f, Gibbs(:p =&gt; HMC(0.2, 3), :z =&gt; PG(20)), 1000)\n\nChains MCMC chain (1000×5×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 26.82 seconds\nCompute duration  = 26.82 seconds\nparameters        = p, z\ninternals         = logprior, loglikelihood, logjoint\n\nUse `describe(chains)` for summary statistics and quantiles.\n\n\nThe Gibbs sampler can be used to specify unique automatic differentiation backends for different variable spaces. Please see the Automatic Differentiation article for more.\nFor more details of compositional sampling in Turing.jl, please check the corresponding paper.\n\n\nWorking with filldist and arraydist\nTuring provides filldist(dist::Distribution, n::Int) and arraydist(dists::AbstractVector{&lt;:Distribution}) as a simplified interface to construct product distributions, e.g., to model a set of variables that share the same structure but vary by group.\n\nConstructing product distributions with filldist\nThe function filldist provides a general interface to construct product distributions over distributions of the same type and parameterisation. Note that, in contrast to the product distribution interface provided by Distributions.jl (Product), filldist supports product distributions over univariate or multivariate distributions.\nExample usage:\n\n@model function demo(x, g)\n    k = length(unique(g))\n    a ~ filldist(Exponential(), k) # = Product(fill(Exponential(), k))\n    mu = a[g]\n    for i in eachindex(x)\n        x[i] ~ Normal(mu[i])\n    end\n    return mu\nend\n\ndemo (generic function with 2 methods)\n\n\n\n\nConstructing product distributions with arraydist\nThe function arraydist provides a general interface to construct product distributions over distributions of varying type and parameterisation. Note that in contrast to the product distribution interface provided by Distributions.jl (Product), arraydist supports product distributions over univariate or multivariate distributions.\nExample usage:\n\n@model function demo(x, g)\n    k = length(unique(g))\n    a ~ arraydist([Exponential(i) for i in 1:k])\n    mu = a[g]\n    for i in eachindex(x)\n        x[i] ~ Normal(mu[i])\n    end\n    return mu\nend\n\ndemo (generic function with 2 methods)\n\n\n\n\n\nWorking with MCMCChains.jl\nTuring.jl wraps its samples using MCMCChains.Chain so that all the functions working for MCMCChains.Chain can be re-used in Turing.jl. Two typical functions are MCMCChains.describe and MCMCChains.plot, which can be used as follows for an obtained chain chn. For more information on MCMCChains, please see the GitHub repository.\n\ndescribe(chn) # Lists statistics of the samples.\nplot(chn) # Plots statistics of the samples.\n\n\nChains MCMC chain (1000×5×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 26.82 seconds\nCompute duration  = 26.82 seconds\nparameters        = p, z\ninternals         = logprior, loglikelihood, logjoint\n\nSummary Statistics\n\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n\n           p    0.4233    0.2101    0.0245    67.1119   215.7160    1.0300     ⋯\n           z    0.1600    0.3668    0.0190   372.0924        NaN    1.0014     ⋯\n\n                                                                1 column omitted\n\nQuantiles\n\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n           p    0.1015    0.2539    0.3916    0.5753    0.8404\n           z    0.0000    0.0000    0.0000    0.0000    1.0000\n\n\n\n\n\n\n\n\nThere are numerous functions in addition to describe and plot in the MCMCChains package, such as those used in convergence diagnostics. For more information on the package, please see the GitHub repository.\n\n\nChanging Default Settings\nSome of Turing.jl’s default settings can be changed for better usage.\n\nAD Backend\nTuring is thoroughly tested with three automatic differentiation (AD) backend packages. The default AD backend is ForwardDiff, which uses forward-mode AD. Two reverse-mode AD backends are also supported, namely Mooncake and ReverseDiff. Mooncake and ReverseDiff also require the user to explicitly load them using import Mooncake or import ReverseDiff next to using Turing.\nFor more information on Turing’s automatic differentiation backend, please see the Automatic Differentiation article as well as the ADTests website, where a number of AD backends (not just those above) are tested against Turing.jl.\n\n\nProgress Logging\nTuring.jl uses ProgressLogging.jl to log the sampling progress. Progress logging is enabled as default but might slow down inference. It can be turned on or off by setting the keyword argument progress of sample to true or false. Moreover, you can enable or disable progress logging globally by calling setprogress!(true) or setprogress!(false), respectively.\nTuring uses heuristics to select an appropriate visualisation backend. If you use Jupyter notebooks, the default backend is ConsoleProgressMonitor.jl. In all other cases, progress logs are displayed with TerminalLoggers.jl. Alternatively, if you provide a custom visualisation backend, Turing uses it instead of the default backend.",
    "crumbs": [
      "Get Started",
      "Core Functionality"
    ]
  },
  {
    "objectID": "api/Optimisation/#API:-Turing.Optimisation",
    "href": "../Turing.jl/api/Optimisation/#API:-Turing.Optimisation",
    "title": "API: Turing.Optimisation",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#DynamicPPL.InitFromParams-Tuple{Turing.Optimisation.ModeResult}",
    "href": "../Turing.jl/api/Optimisation/#DynamicPPL.InitFromParams-Tuple{Turing.Optimisation.ModeResult}",
    "title": "DynamicPPL.InitFromParams",
    "section": "method",
    "text": "InitFromParams(m::ModeResult)\n\nInitialize a model from the parameters stored in a ModeResult.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#SciMLBase.OptimizationProblem-Tuple{Turing.Optimisation.OptimLogDensity, AbstractVector, Any, Any}",
    "href": "../Turing.jl/api/Optimisation/#SciMLBase.OptimizationProblem-Tuple{Turing.Optimisation.OptimLogDensity, AbstractVector, Any, Any}",
    "title": "SciMLBase.OptimizationProblem",
    "section": "method",
    "text": "OptimizationProblem(log_density::OptimLogDensity, initial_params::AbstractVector, adtype, constraints)\n\nCreate an OptimizationProblem for the objective function defined by log_density.\n\nNote that the adtype parameter here overrides any adtype parameter the OptimLogDensity was constructed with.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#Turing.Optimisation.MAP",
    "href": "../Turing.jl/api/Optimisation/#Turing.Optimisation.MAP",
    "title": "Turing.Optimisation.MAP",
    "section": "type",
    "text": "MAP <: ModeEstimator\n\nConcrete type for maximum a posteriori estimation.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#Turing.Optimisation.MLE",
    "href": "../Turing.jl/api/Optimisation/#Turing.Optimisation.MLE",
    "title": "Turing.Optimisation.MLE",
    "section": "type",
    "text": "MLE <: ModeEstimator\n\nConcrete type for maximum likelihood estimation.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#Turing.Optimisation.ModeEstimationConstraints",
    "href": "../Turing.jl/api/Optimisation/#Turing.Optimisation.ModeEstimationConstraints",
    "title": "Turing.Optimisation.ModeEstimationConstraints",
    "section": "type",
    "text": "ModeEstimationConstraints\n\nA struct that holds constraints for mode estimation problems.\n\nThe fields are the same as possible constraints supported by the Optimization.jl: ub and lb specify lower and upper bounds of box constraints. cons is a function that takes the parameters of the model and returns a list of derived quantities, which are then constrained by the lower and upper bounds set by lcons and ucons. We refer to these as generic constraints. Please see the documentation of Optimization.jl for more details.\n\nAny of the fields can be nothing, disabling the corresponding constraints.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#Turing.Optimisation.ModeEstimator",
    "href": "../Turing.jl/api/Optimisation/#Turing.Optimisation.ModeEstimator",
    "title": "Turing.Optimisation.ModeEstimator",
    "section": "type",
    "text": "ModeEstimator\n\nAn abstract type to mark whether mode estimation is to be done with maximum a posteriori (MAP) or maximum likelihood estimation (MLE).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#Turing.Optimisation.ModeResult",
    "href": "../Turing.jl/api/Optimisation/#Turing.Optimisation.ModeResult",
    "title": "Turing.Optimisation.ModeResult",
    "section": "type",
    "text": "ModeResult{\n    V<:NamedArrays.NamedArray,\n    O<:Any,\n    M<:OptimLogDensity,\n    P<:AbstractDict{<:VarName,<:Any}\n    E<:ModeEstimator,\n}\n\nA wrapper struct to store various results from a MAP or MLE estimation.\n\nFields\n\nvalues::NamedArrays.NamedArray: A vector with the resulting point estimates.\noptim_result::Any: The stored optimiser results.\nlp::Float64: The final log likelihood or log joint, depending on whether MAP or MLE was run.\nf::Turing.Optimisation.OptimLogDensity: The evaluation function used to calculate the output.\nparams::AbstractDict{<:AbstractPPL.VarName}: Dictionary of parameter values\nlinked::Bool: Whether the optimization was done in a transformed space.\nestimator::Turing.Optimisation.ModeEstimator: The type of mode estimation (MAP or MLE).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#Turing.Optimisation.ModeResult-Tuple{Turing.Optimisation.OptimLogDensity, SciMLBase.OptimizationSolution, Bool, Turing.Optimisation.ModeEstimator}",
    "href": "../Turing.jl/api/Optimisation/#Turing.Optimisation.ModeResult-Tuple{Turing.Optimisation.OptimLogDensity, SciMLBase.OptimizationSolution, Bool, Turing.Optimisation.ModeEstimator}",
    "title": "Turing.Optimisation.ModeResult",
    "section": "method",
    "text": "ModeResult(\n    log_density::OptimLogDensity,\n    solution::SciMLBase.OptimizationSolution,\n    linked::Bool,\n    estimator::ModeEstimator,\n)\n\nCreate a ModeResult for a given log_density objective and a solution given by solve. The linked argument indicates whether the optimization was done in a transformed space.\n\nOptimization.solve returns its own result type. This function converts that into the richer format of ModeResult. It also takes care of transforming them back to the original parameter space in case the optimization was done in a transformed space.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#Turing.Optimisation.OptimLogDensity",
    "href": "../Turing.jl/api/Optimisation/#Turing.Optimisation.OptimLogDensity",
    "title": "Turing.Optimisation.OptimLogDensity",
    "section": "type",
    "text": "OptimLogDensity{L<:DynamicPPL.LogDensityFunction}\n\nA struct that represents a log-density function, which can be used with Optimization.jl. This is a thin wrapper around DynamicPPL.LogDensityFunction: the main difference is that the log-density is negated (because Optimization.jl performs minimisation, and we usually want to maximise the log-density).\n\nAn OptimLogDensity does not, in itself, obey the LogDensityProblems.jl interface. Thus, if you want to calculate the log density of its contents at the point z, you should manually call LogDensityProblems.logdensity(f.ldf, z), instead of LogDensityProblems.logdensity(f, z).\n\nHowever, because Optimization.jl requires the objective function to be callable, you can also call f(z) directly to get the negative log density at z.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#Turing.Optimisation.OptimLogDensity-Tuple{AbstractVector}",
    "href": "../Turing.jl/api/Optimisation/#Turing.Optimisation.OptimLogDensity-Tuple{AbstractVector}",
    "title": "Turing.Optimisation.OptimLogDensity",
    "section": "method",
    "text": "(f::OptimLogDensity)(z)\n(f::OptimLogDensity)(z, _)\n\nEvaluate the negative log probability density at the array z. Which kind of probability density is evaluated depends on the getlogdensity function used to construct the underlying LogDensityFunction (e.g., DynamicPPL.getlogjoint for MAP estimation, or DynamicPPL.getloglikelihood for MLE).\n\nAny second argument is ignored. The two-argument method only exists to match the interface required by Optimization.jl.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#Base.get-Tuple{Turing.Optimisation.ModeResult, AbstractVector{Symbol}}",
    "href": "../Turing.jl/api/Optimisation/#Base.get-Tuple{Turing.Optimisation.ModeResult, AbstractVector{Symbol}}",
    "title": "Base.get",
    "section": "method",
    "text": "Base.get(m::ModeResult, var_symbol::Symbol)\nBase.get(m::ModeResult, var_symbols::AbstractVector{Symbol})\n\nReturn the values of all the variables with the symbol(s) var_symbol in the mode result m. The return value is a NamedTuple with var_symbols as the key(s). The second argument should be either a Symbol or a vector of Symbols.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#StatsAPI.coeftable-Tuple{Turing.Optimisation.ModeResult}",
    "href": "../Turing.jl/api/Optimisation/#StatsAPI.coeftable-Tuple{Turing.Optimisation.ModeResult}",
    "title": "StatsAPI.coeftable",
    "section": "method",
    "text": "StatsBase.coeftable(m::ModeResult; level::Real=0.95, numerrors_warnonly::Bool=true)\n\nReturn a table with coefficients and related statistics of the model. level determines the level for confidence intervals (by default, 95%).\n\nIn case the numerrors_warnonly argument is true (the default) numerical errors encountered during the computation of the standard errors will be caught and reported in an extra \"Error notes\" column.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#Turing.Optimisation.estimate_mode",
    "href": "../Turing.jl/api/Optimisation/#Turing.Optimisation.estimate_mode",
    "title": "Turing.Optimisation.estimate_mode",
    "section": "function",
    "text": "estimate_mode(\n    model::DynamicPPL.Model,\n    estimator::ModeEstimator,\n    [solver];\n    kwargs...\n)\n\nFind the mode of the probability distribution of a model.\n\nUnder the hood this function calls Optimization.solve.\n\nArguments\n\nmodel::DynamicPPL.Model: The model for which to estimate the mode.\nestimator::ModeEstimator: Can be either MLE() for maximum likelihood estimation or   MAP() for maximum a posteriori estimation.\nsolver=nothing. The optimization algorithm to use. Optional. Can be any solver   recognised by Optimization.jl. If omitted a default solver is used: LBFGS, or IPNewton   if non-box constraints are present.\n\nKeyword arguments\n\ncheck_model::Bool=true: If true, the model is checked for errors before   optimisation begins.\ninitial_params::Union{AbstractVector,Nothing}=nothing: Initial value for the   optimization. Optional, unless non-box constraints are specified. If omitted it is   generated by either sampling from the prior distribution or uniformly from the box   constraints, if any.\nadtype::AbstractADType=AutoForwardDiff(): The automatic differentiation type to use.\nKeyword arguments lb, ub, cons, lcons, and ucons define constraints for the   optimization problem. Please see ModeEstimationConstraints for more details.\nAny extra keyword arguments are passed to Optimization.solve.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#Turing.Optimisation.generate_initial_params-Tuple{DynamicPPL.Model, Any, Any}",
    "href": "../Turing.jl/api/Optimisation/#Turing.Optimisation.generate_initial_params-Tuple{DynamicPPL.Model, Any, Any}",
    "title": "Turing.Optimisation.generate_initial_params",
    "section": "method",
    "text": "generate_initial_params(model::DynamicPPL.Model, initial_params, constraints)\n\nGenerate an initial value for the optimization problem.\n\nIf initial_params is not nothing, a copy of it is returned. Otherwise initial parameter values are generated either by sampling from the prior (if no constraints are present) or uniformly from the box constraints. If generic constraints are set, an error is thrown.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#Turing.Optimisation.maximum_a_posteriori-Tuple{DynamicPPL.Model, Vararg{Any}}",
    "href": "../Turing.jl/api/Optimisation/#Turing.Optimisation.maximum_a_posteriori-Tuple{DynamicPPL.Model, Vararg{Any}}",
    "title": "Turing.Optimisation.maximum_a_posteriori",
    "section": "method",
    "text": "maximum_a_posteriori(\n    model::DynamicPPL.Model,\n    [solver];\n    kwargs...\n)\n\nFind the maximum a posteriori estimate of a model.\n\nThis is a convenience function that calls estimate_mode with MAP() as the estimator. Please see the documentation of Turing.Optimisation.estimate_mode for more details.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Optimisation/#Turing.Optimisation.maximum_likelihood-Tuple{DynamicPPL.Model, Vararg{Any}}",
    "href": "../Turing.jl/api/Optimisation/#Turing.Optimisation.maximum_likelihood-Tuple{DynamicPPL.Model, Vararg{Any}}",
    "title": "Turing.Optimisation.maximum_likelihood",
    "section": "method",
    "text": "maximum_likelihood(\n    model::DynamicPPL.Model,\n    [solver];\n    kwargs...\n)\n\nFind the maximum likelihood estimate of a model.\n\nThis is a convenience function that calls estimate_mode with MLE() as the estimator. Please see the documentation of Turing.Optimisation.estimate_mode for more details.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#API:-Turing.Inference",
    "href": "../Turing.jl/api/Inference/#API:-Turing.Inference",
    "title": "API: Turing.Inference",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.CSMC",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.CSMC",
    "title": "Turing.Inference.CSMC",
    "section": "type",
    "text": "CSMC(...)\n\nEquivalent to PG.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.ESS",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.ESS",
    "title": "Turing.Inference.ESS",
    "section": "type",
    "text": "ESS\n\nElliptical slice sampling algorithm.\n\nExamples\n\njulia> @model function gdemo(x)\n           m ~ Normal()\n           x ~ Normal(m, 0.5)\n       end\ngdemo (generic function with 2 methods)\n\njulia> sample(gdemo(1.0), ESS(), 1_000) |> mean\nMean\n\n│ Row │ parameters │ mean     │\n│     │ Symbol     │ Float64  │\n├─────┼────────────┼──────────┤\n│ 1   │ m          │ 0.824853 │\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.Emcee",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.Emcee",
    "title": "Turing.Inference.Emcee",
    "section": "type",
    "text": "Emcee(n_walkers::Int, stretch_length=2.0)\n\nAffine-invariant ensemble sampling algorithm.\n\nReference\n\nForeman-Mackey, D., Hogg, D. W., Lang, D., & Goodman, J. (2013). emcee: The MCMC Hammer. Publications of the Astronomical Society of the Pacific, 125 (925), 306. https://doi.org/10.1086/670067\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.ExternalSampler",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.ExternalSampler",
    "title": "Turing.Inference.ExternalSampler",
    "section": "type",
    "text": "ExternalSampler{Unconstrained,S<:AbstractSampler,AD<:ADTypes.AbstractADType}\n\nRepresents a sampler that does not have a custom implementation of AbstractMCMC.step(rng, ::DynamicPPL.Model, spl).\n\nThe Unconstrained type-parameter is to indicate whether the sampler requires unconstrained space.\n\nFields\n\nsampler::AbstractMCMC.AbstractSampler: the sampler to wrap\nadtype::ADTypes.AbstractADType: the automatic differentiation (AD) backend to use\n\nTuring.jl's interface for external samplers\n\nIf you implement a new MySampler <: AbstractSampler and want it to work with Turing.jl models, there are two options:\n\nDirectly implement the AbstractMCMC.step methods for DynamicPPL.Model. That is to say, implement AbstractMCMC.step(rng::Random.AbstractRNG, model::DynamicPPL.Model, sampler::MySampler; kwargs...) and related methods. This is the most powerful option and is what Turing.jl's in-house samplers do. Implementing this means that you can directly call sample(model, MySampler(), N).\nImplement a generic AbstractMCMC.step method for AbstractMCMC.LogDensityModel (the same signature as above except that model::AbstractMCMC.LogDensityModel). This struct wraps an object that obeys the LogDensityProblems.jl interface, so your step implementation does not need to know anything about Turing.jl or DynamicPPL.jl. To use this with Turing.jl, you will need to wrap your sampler: sample(model, externalsampler(MySampler()), N).\n\nThis section describes the latter.\n\nMySampler must implement the following methods:\n\nAbstractMCMC.step (the main function for taking a step in MCMC sampling; this is documented in AbstractMCMC.jl). This function must return a tuple of two elements, a 'transition' and a 'state'.\nAbstractMCMC.getparams(external_state): How to extract the parameters from the state returned by your sampler (i.e., the second return value of step). For your sampler to work with Turing.jl, this function should return a Vector of parameter values. Note that this function does not need to perform any linking or unlinking; Turing.jl will take care of this for you. You should return the parameters exactly as your sampler sees them.\nAbstractMCMC.getstats(external_state): Extract sampler statistics corresponding to this iteration from the state returned by your sampler (i.e., the second return value of step). For your sampler to work with Turing.jl, this function should return a NamedTuple. If there are no statistics to return, return NamedTuple().\nNote that getstats should not include log-probabilities as these will be recalculated by Turing automatically for you.\n\nNotice that both of these functions take the state as input, not the transition. In other words, the transition is completely useless for the external sampler interface. This is in line with long-term plans for removing transitions from AbstractMCMC.jl and only using states.\n\nThere are a few more optional functions which you can implement to improve the integration with Turing.jl:\n\nAbstractMCMC.requires_unconstrained_space(::MySampler): If your sampler requires unconstrained space, you should return true. This tells Turing to perform linking on the VarInfo before evaluation, and ensures that the parameter values passed to your sampler will always be in unconstrained (Euclidean) space.\nTuring.Inference.isgibbscomponent(::MySampler): If you want to disallow your sampler from a component in Turing's Gibbs sampler, you should make this evaluate to false. Note that the default is true, so you should only need to implement this in special cases.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.Gibbs",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.Gibbs",
    "title": "Turing.Inference.Gibbs",
    "section": "type",
    "text": "Gibbs\n\nA type representing a Gibbs sampler.\n\nConstructors\n\nGibbs needs to be given a set of pairs of variable names and samplers. Instead of a single variable name per sampler, one can also give an iterable of variables, all of which are sampled by the same component sampler.\n\nEach variable name can be given as either a Symbol or a VarName.\n\nSome examples of valid constructors are:\n\nGibbs(:x => NUTS(), :y => MH())\nGibbs(@varname(x) => NUTS(), @varname(y) => MH())\nGibbs((@varname(x), :y) => NUTS(), :z => MH())\n\nFields\n\nvarnames::NTuple{N, AbstractVector{<:AbstractPPL.VarName}} where N: varnames representing variables for each sampler\nsamplers::NTuple{N, Any} where N: samplers for each entry in varnames\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.GibbsConditional",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.GibbsConditional",
    "title": "Turing.Inference.GibbsConditional",
    "section": "type",
    "text": "GibbsConditional(get_cond_dists)\n\nA Gibbs component sampler that samples variables according to user-provided analytical conditional posterior distributions.\n\nWhen using Gibbs sampling, sometimes one may know the analytical form of the posterior for a given variable, given the conditioned values of the other variables. In such cases one can use GibbsConditional as a component sampler to to sample from these known conditionals directly, avoiding any MCMC methods. One does so with\n\nsampler = Gibbs(\n    (@varname(var1), @varname(var2)) => GibbsConditional(get_cond_dists),\n    other samplers go here...\n)\n\nHere get_cond_dists(c::Dict{<:VarName}) should be a function that takes a Dict mapping the conditioned variables (anything other than var1 and var2) to their values, and returns the conditional posterior distributions for var1 and var2. You may, of course, have any number of variables being sampled as a block in this manner, we only use two as an example. The return value of get_cond_dists should be one of the following:\n\nA single Distribution, if only one variable is being sampled.\nAn AbstractDict{<:VarName,<:Distribution} that maps the variables being sampled to their conditional posteriors E.g. Dict(@varname(var1) => dist1, @varname(var2) => dist2).\nA NamedTuple of Distributions, which is like the AbstractDict case but can be used if all the variable names are single Symbols, and may be more performant. E.g. (; var1=dist1, var2=dist2).\n\nExamples\n\n# Define a model\n@model function inverse_gdemo(x)\n    precision ~ Gamma(2, inv(3))\n    std = sqrt(1 / precision)\n    m ~ Normal(0, std)\n    for i in eachindex(x)\n        x[i] ~ Normal(m, std)\n    end\nend\n\n# Define analytical conditionals. See\n# https://en.wikipedia.org/wiki/Conjugate_prior#When_likelihood_function_is_a_continuous_distribution\nfunction cond_precision(c)\n    a = 2.0\n    b = 3.0\n    # We use AbstractPPL.getvalue instead of indexing into `c` directly to guard against\n    # issues where e.g. you try to get `c[@varname(x[1])]` but only `@varname(x)` is present\n    # in `c`. `getvalue` handles that gracefully, `getindex` doesn't. In this case\n    # `getindex` would suffice, but `getvalue` is good practice.\n    m = AbstractPPL.getvalue(c, @varname(m))\n    x = AbstractPPL.getvalue(c, @varname(x))\n    n = length(x)\n    a_new = a + (n + 1) / 2\n    b_new = b + sum(abs2, x .- m) / 2 + m^2 / 2\n    return Gamma(a_new, 1 / b_new)\nend\n\nfunction cond_m(c)\n    precision = AbstractPPL.getvalue(c, @varname(precision))\n    x = AbstractPPL.getvalue(c, @varname(x))\n    n = length(x)\n    m_mean = sum(x) / (n + 1)\n    m_var = 1 / (precision * (n + 1))\n    return Normal(m_mean, sqrt(m_var))\nend\n\n# Sample using GibbsConditional\nmodel = inverse_gdemo([1.0, 2.0, 3.0])\nchain = sample(model, Gibbs(\n    :precision => GibbsConditional(cond_precision),\n    :m => GibbsConditional(cond_m)\n), 1000)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.GibbsContext",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.GibbsContext",
    "title": "Turing.Inference.GibbsContext",
    "section": "type",
    "text": "GibbsContext(target_varnames, global_varinfo, context)\n\nA context used in the implementation of the Turing.jl Gibbs sampler.\n\nThere will be one GibbsContext for each iteration of a component sampler.\n\ntarget_varnames is a a tuple of VarNames that the current component sampler is sampling. For those VarNames, GibbsContext will just pass tilde_assume!! calls to its child context. For other variables, their values will be fixed to the values they have in global_varinfo.\n\nFields\n\ntarget_varnames: the VarNames being sampled\n\nglobal_varinfo: a Ref to the global AbstractVarInfo object that holds values for all variables, both those fixed and those being sampled. We use a Ref because this field may need to be updated if new variables are introduced.\n\ncontext: the child context that tilde calls will eventually be passed onto.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.HMC",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.HMC",
    "title": "Turing.Inference.HMC",
    "section": "type",
    "text": "HMC(ϵ::Float64, n_leapfrog::Int; adtype::ADTypes.AbstractADType = AutoForwardDiff())\n\nHamiltonian Monte Carlo sampler with static trajectory.\n\nArguments\n\nϵ: The leapfrog step size to use.\nn_leapfrog: The number of leapfrog steps to use.\nadtype: The automatic differentiation (AD) backend.   If not specified, ForwardDiff is used, with its chunksize automatically determined.\n\nUsage\n\nHMC(0.05, 10)\n\nTips\n\nIf you are receiving gradient errors when using HMC, try reducing the leapfrog step size ϵ, e.g.\n\n# Original step size\nsample(gdemo([1.5, 2]), HMC(0.1, 10), 1000)\n\n# Reduced step size\nsample(gdemo([1.5, 2]), HMC(0.01, 10), 1000)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.HMCDA",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.HMCDA",
    "title": "Turing.Inference.HMCDA",
    "section": "type",
    "text": "HMCDA(\n    n_adapts::Int, δ::Float64, λ::Float64; ϵ::Float64 = 0.0;\n    adtype::ADTypes.AbstractADType = AutoForwardDiff(),\n)\n\nHamiltonian Monte Carlo sampler with Dual Averaging algorithm.\n\nUsage\n\nHMCDA(200, 0.65, 0.3)\n\nArguments\n\nn_adapts: Numbers of samples to use for adaptation.\nδ: Target acceptance rate. 65% is often recommended.\nλ: Target leapfrog length.\nϵ: Initial step size; 0 means automatically search by Turing.\nadtype: The automatic differentiation (AD) backend.   If not specified, ForwardDiff is used, with its chunksize automatically determined.\n\nReference\n\nFor more information, please view the following paper (arXiv link):\n\nHoffman, Matthew D., and Andrew Gelman. \"The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.\" Journal of Machine Learning Research 15, no. 1 (2014): 1593-1623.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.IS",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.IS",
    "title": "Turing.Inference.IS",
    "section": "type",
    "text": "IS()\n\nImportance sampling algorithm.\n\nUsage:\n\nIS()\n\nExample:\n\n# Define a simple Normal model with unknown mean and variance.\n@model function gdemo(x)\n    s² ~ InverseGamma(2,3)\n    m ~ Normal(0,sqrt.(s))\n    x[1] ~ Normal(m, sqrt.(s))\n    x[2] ~ Normal(m, sqrt.(s))\n    return s², m\nend\n\nsample(gdemo([1.5, 2]), IS(), 1000)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.MH",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.MH",
    "title": "Turing.Inference.MH",
    "section": "type",
    "text": "MH(proposals...)\n\nConstruct a Metropolis-Hastings algorithm.\n\nThe arguments proposals can be\n\nBlank (i.e. MH()), in which case MH defaults to using the prior for each parameter as the proposal distribution.\nAn iterable of pairs or tuples mapping a Symbol to a AdvancedMH.Proposal, Distribution, or Function that returns a conditional proposal distribution.\nA covariance matrix to use as for mean-zero multivariate normal proposals.\n\nExamples\n\nThe default MH will draw proposal samples from the prior distribution using AdvancedMH.StaticProposal.\n\n@model function gdemo(x, y)\n    s² ~ InverseGamma(2,3)\n    m ~ Normal(0, sqrt(s²))\n    x ~ Normal(m, sqrt(s²))\n    y ~ Normal(m, sqrt(s²))\nend\n\nchain = sample(gdemo(1.5, 2.0), MH(), 1_000)\nmean(chain)\n\nSpecifying a single distribution implies the use of static MH:\n\n# Use a static proposal for s² (which happens to be the same\n# as the prior) and a static proposal for m (note that this\n# isn't a random walk proposal).\nchain = sample(\n    gdemo(1.5, 2.0),\n    MH(\n        :s² => InverseGamma(2, 3),\n        :m => Normal(0, 1)\n    ),\n    1_000\n)\nmean(chain)\n\nSpecifying explicit proposals using the AdvancedMH interface:\n\n# Use a static proposal for s² and random walk with proposal\n# standard deviation of 0.25 for m.\nchain = sample(\n    gdemo(1.5, 2.0),\n    MH(\n        :s² => AdvancedMH.StaticProposal(InverseGamma(2,3)),\n        :m => AdvancedMH.RandomWalkProposal(Normal(0, 0.25))\n    ),\n    1_000\n)\nmean(chain)\n\nUsing a custom function to specify a conditional distribution:\n\n# Use a static proposal for s and and a conditional proposal for m,\n# where the proposal is centered around the current sample.\nchain = sample(\n    gdemo(1.5, 2.0),\n    MH(\n        :s² => InverseGamma(2, 3),\n        :m => x -> Normal(x, 1)\n    ),\n    1_000\n)\nmean(chain)\n\nProviding a covariance matrix will cause MH to perform random-walk sampling in the transformed space with proposals drawn from a multivariate normal distribution. The provided matrix must be positive semi-definite and square:\n\n# Providing a custom variance-covariance matrix\nchain = sample(\n    gdemo(1.5, 2.0),\n    MH(\n        [0.25 0.05;\n         0.05 0.50]\n    ),\n    1_000\n)\nmean(chain)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.MHState",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.MHState",
    "title": "Turing.Inference.MHState",
    "section": "type",
    "text": "MHState(varinfo::AbstractVarInfo, logjoint_internal::Real)\n\nState for Metropolis-Hastings sampling.\n\nvarinfo must have the correct parameters set inside it, but its other fields (e.g. accumulators, which track logp) can in general be missing or incorrect.\n\nlogjoint_internal is the log joint probability of the model, evaluated using the parameters and linking status of varinfo. It should be equal to DynamicPPL.getlogjoint_internal(varinfo). This information is returned by the MH sampler so we store this here to avoid re-evaluating the model unnecessarily.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.NUTS",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.NUTS",
    "title": "Turing.Inference.NUTS",
    "section": "type",
    "text": "NUTS(n_adapts::Int, δ::Float64; max_depth::Int=10, Δ_max::Float64=1000.0, init_ϵ::Float64=0.0; adtype::ADTypes.AbstractADType=AutoForwardDiff()\n\nNo-U-Turn Sampler (NUTS) sampler.\n\nUsage:\n\nNUTS()            # Use default NUTS configuration.\nNUTS(1000, 0.65)  # Use 1000 adaption steps, and target accept ratio 0.65.\n\nArguments:\n\nn_adapts::Int : The number of samples to use with adaptation.\nδ::Float64 : Target acceptance rate for dual averaging.\nmax_depth::Int : Maximum doubling tree depth.\nΔ_max::Float64 : Maximum divergence during doubling tree.\ninit_ϵ::Float64 : Initial step size; 0 means automatically searching using a heuristic procedure.\nadtype::ADTypes.AbstractADType : The automatic differentiation (AD) backend.   If not specified, ForwardDiff is used, with its chunksize automatically determined.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.OldLogDensityFunction",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.OldLogDensityFunction",
    "title": "Turing.Inference.OldLogDensityFunction",
    "section": "type",
    "text": "OldLogDensityFunction\n\nThis is a clone of pre-0.39 DynamicPPL.LogDensityFunction. It is needed for MH because MH doesn't actually obey the LogDensityProblems.jl interface: it evaluates 'LogDensityFunctions' with a NamedTuple(!!)\n\nThis means that we can't really use DynamicPPL's LogDensityFunction, since that only promises to obey the interface of being called with a vector.\n\nIn particular, because set_namedtuple! acts on a VarInfo, we need to store the VarInfo inside this struct (which DynamicPPL's LogDensityFunction no longer does).\n\nThis SHOULD really be refactored to remove this requirement.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.PG",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.PG",
    "title": "Turing.Inference.PG",
    "section": "type",
    "text": "struct PG{R} <: Turing.Inference.ParticleInference\n\nParticle Gibbs sampler.\n\nFields\n\nnparticles::Int64: Number of particles.\nresampler::Any: Resampling algorithm.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.PG-Tuple{Int64}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.PG-Tuple{Int64}",
    "title": "Turing.Inference.PG",
    "section": "method",
    "text": "PG(n, [resampler = AdvancedPS.ResampleWithESSThreshold()]) PG(n, [resampler = AdvancedPS.resample_systematic, ]threshold)\n\nCreate a Particle Gibbs sampler of type PG with n particles.\n\nIf the algorithm for the resampling step is not specified explicitly, systematic resampling is performed if the estimated effective sample size per particle drops below 0.5.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.PolynomialStepsize-Union{Tuple{T}, Tuple{T, T, T}} where T<:Real",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.PolynomialStepsize-Union{Tuple{T}, Tuple{T, T, T}} where T<:Real",
    "title": "Turing.Inference.PolynomialStepsize",
    "section": "method",
    "text": "PolynomialStepsize(a[, b=0, γ=0.55])\n\nCreate a polynomially decaying stepsize function.\n\nAt iteration t, the step size is\n\na (b + t)^-γ\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.Prior",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.Prior",
    "title": "Turing.Inference.Prior",
    "section": "type",
    "text": "Prior()\n\nAlgorithm for sampling from the prior.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.ProduceLogLikelihoodAccumulator",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.ProduceLogLikelihoodAccumulator",
    "title": "Turing.Inference.ProduceLogLikelihoodAccumulator",
    "section": "type",
    "text": "ProduceLogLikelihoodAccumulator{T<:Real} <: AbstractAccumulator\n\nExactly like LogLikelihoodAccumulator, but calls Libtask.produce on change of value.\n\nFields\n\nlogp::Real: the scalar log likelihood value\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.RepeatSampler",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.RepeatSampler",
    "title": "Turing.Inference.RepeatSampler",
    "section": "type",
    "text": "RepeatSampler <: AbstractMCMC.AbstractSampler\n\nA RepeatSampler is a container for a sampler and a number of times to repeat it.\n\nFields\n\nsampler: The sampler to repeat\nnum_repeat: The number of times to repeat the sampler\n\nExamples\n\nrepeated_sampler = RepeatSampler(sampler, 10)\nAbstractMCMC.step(rng, model, repeated_sampler) # take 10 steps of `sampler`\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.SGHMC",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.SGHMC",
    "title": "Turing.Inference.SGHMC",
    "section": "type",
    "text": "SGHMC{AD}\n\nStochastic Gradient Hamiltonian Monte Carlo (SGHMC) sampler.\n\nFields\n\nlearning_rate::Real\nmomentum_decay::Real\nadtype::Any\n\nReference\n\nTianqi Chen, Emily Fox, & Carlos Guestrin (2014). Stochastic Gradient Hamiltonian Monte Carlo. In: Proceedings of the 31st International Conference on Machine Learning (pp. 1683–1691).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.SGHMC-Tuple{}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.SGHMC-Tuple{}",
    "title": "Turing.Inference.SGHMC",
    "section": "method",
    "text": "SGHMC(;\n    learning_rate::Real,\n    momentum_decay::Real,\n    adtype::ADTypes.AbstractADType = AutoForwardDiff(),\n)\n\nCreate a Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) sampler.\n\nIf the automatic differentiation (AD) backend adtype is not provided, ForwardDiff with automatically determined chunksize is used.\n\nReference\n\nTianqi Chen, Emily Fox, & Carlos Guestrin (2014). Stochastic Gradient Hamiltonian Monte Carlo. In: Proceedings of the 31st International Conference on Machine Learning (pp. 1683–1691).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.SGLD",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.SGLD",
    "title": "Turing.Inference.SGLD",
    "section": "type",
    "text": "SGLD\n\nStochastic gradient Langevin dynamics (SGLD) sampler.\n\nFields\n\nstepsize::Any: Step size function.\nadtype::Any\n\nReference\n\nMax Welling & Yee Whye Teh (2011). Bayesian Learning via Stochastic Gradient Langevin Dynamics. In: Proceedings of the 28th International Conference on Machine Learning (pp. 681–688).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.SGLD-Tuple{}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.SGLD-Tuple{}",
    "title": "Turing.Inference.SGLD",
    "section": "method",
    "text": "SGLD(;\n    stepsize = PolynomialStepsize(0.01),\n    adtype::ADTypes.AbstractADType = AutoForwardDiff(),\n)\n\nStochastic gradient Langevin dynamics (SGLD) sampler.\n\nBy default, a polynomially decaying stepsize is used.\n\nIf the automatic differentiation (AD) backend adtype is not provided, ForwardDiff with automatically determined chunksize is used.\n\nReference\n\nMax Welling & Yee Whye Teh (2011). Bayesian Learning via Stochastic Gradient Langevin Dynamics. In: Proceedings of the 28th International Conference on Machine Learning (pp. 681–688).\n\nSee also: PolynomialStepsize\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.SMC",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.SMC",
    "title": "Turing.Inference.SMC",
    "section": "type",
    "text": "struct SMC{R} <: Turing.Inference.ParticleInference\n\nSequential Monte Carlo sampler.\n\nFields\n\nresampler::Any\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.SMC-Tuple{}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.SMC-Tuple{}",
    "title": "Turing.Inference.SMC",
    "section": "method",
    "text": "SMC([resampler = AdvancedPS.ResampleWithESSThreshold()]) SMC([resampler = AdvancedPS.resample_systematic, ]threshold)\n\nCreate a sequential Monte Carlo sampler of type SMC.\n\nIf the algorithm for the resampling step is not specified explicitly, systematic resampling is performed if the estimated effective sample size per particle drops below 0.5.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference._convert_initial_params-Tuple{DynamicPPL.AbstractInitStrategy}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference._convert_initial_params-Tuple{DynamicPPL.AbstractInitStrategy}",
    "title": "Turing.Inference._convert_initial_params",
    "section": "method",
    "text": "_convert_initial_params(initial_params)\n\nConvert initial_params to a DynamicPPl.AbstractInitStrategy if it is not already one, or throw a useful error message.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.build_variable_dict-Tuple{DynamicPPL.Model}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.build_variable_dict-Tuple{DynamicPPL.Model}",
    "title": "Turing.Inference.build_variable_dict",
    "section": "method",
    "text": "build_variable_dict(model::DynamicPPL.Model)\n\nTraverse the context stack of model and build a Dict of all the variable values that are set in GibbsContext, ConditionContext, or FixedContext.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.default_varinfo-Tuple{Random.AbstractRNG, DynamicPPL.Model, AbstractMCMC.AbstractSampler}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.default_varinfo-Tuple{Random.AbstractRNG, DynamicPPL.Model, AbstractMCMC.AbstractSampler}",
    "title": "Turing.Inference.default_varinfo",
    "section": "method",
    "text": "default_varinfo(rng, model, sampler)\n\nReturn a default varinfo object for the given model and sampler. The default method for this returns a NTVarInfo (i.e. 'typed varinfo').\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.dist_val_tuple-Tuple{MH, Union{DynamicPPL.ThreadSafeVarInfo{<:DynamicPPL.VarInfo{Tmeta}}, DynamicPPL.VarInfo{Tmeta}} where Tmeta}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.dist_val_tuple-Tuple{MH, Union{DynamicPPL.ThreadSafeVarInfo{<:DynamicPPL.VarInfo{Tmeta}}, DynamicPPL.VarInfo{Tmeta}} where Tmeta}",
    "title": "Turing.Inference.dist_val_tuple",
    "section": "method",
    "text": "dist_val_tuple(spl::MH, vi::VarInfo)\n\nReturn two NamedTuples.\n\nThe first NamedTuple has symbols as keys and distributions as values. The second NamedTuple has model symbols as keys and their stored values as values.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.externalsampler-Tuple{AbstractMCMC.AbstractSampler}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.externalsampler-Tuple{AbstractMCMC.AbstractSampler}",
    "title": "Turing.Inference.externalsampler",
    "section": "method",
    "text": "externalsampler(\n    sampler::AbstractSampler;\n    adtype=AutoForwardDiff(),\n    unconstrained=AbstractMCMC.requires_unconstrained_space(sampler),\n)\n\nWrap a sampler so it can be used as an inference algorithm.\n\nArguments\n\nsampler::AbstractSampler: The sampler to wrap.\n\nKeyword Arguments\n\nadtype::ADTypes.AbstractADType=ADTypes.AutoForwardDiff(): The automatic differentiation (AD) backend to use.\nunconstrained::Bool=AbstractMCMC.requires_unconstrained_space(sampler): Whether the sampler requires unconstrained space.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.get_trace_local_resampled_maybe-Tuple{Bool}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.get_trace_local_resampled_maybe-Tuple{Bool}",
    "title": "Turing.Inference.get_trace_local_resampled_maybe",
    "section": "method",
    "text": "gettracelocalresampledmaybe(fallback_resampled::Bool)\n\nGet the Trace local resampled if one exists.\n\nIf executed within a TapedTask, return the resampled stored in the \"taped globals\" of the task, otherwise return fallback_resampled.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.get_trace_local_rng_maybe-Tuple{Random.AbstractRNG}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.get_trace_local_rng_maybe-Tuple{Random.AbstractRNG}",
    "title": "Turing.Inference.get_trace_local_rng_maybe",
    "section": "method",
    "text": "gettracelocalrngmaybe(rng::Random.AbstractRNG)\n\nGet the Trace local rng if one exists.\n\nIf executed within a TapedTask, return the rng stored in the \"taped globals\" of the task, otherwise return vi.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.get_trace_local_varinfo_maybe-Tuple{DynamicPPL.AbstractVarInfo}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.get_trace_local_varinfo_maybe-Tuple{DynamicPPL.AbstractVarInfo}",
    "title": "Turing.Inference.get_trace_local_varinfo_maybe",
    "section": "method",
    "text": "gettracelocalvarinfomaybe(vi::AbstractVarInfo)\n\nGet the Trace local varinfo if one exists.\n\nIf executed within a TapedTask, return the varinfo stored in the \"taped globals\" of the task, otherwise return vi.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.gibbs_initialstep_recursive",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.gibbs_initialstep_recursive",
    "title": "Turing.Inference.gibbs_initialstep_recursive",
    "section": "function",
    "text": "Take the first step of MCMC for the first component sampler, and call the same function recursively on the remaining samplers, until no samplers remain. Return the global VarInfo and a tuple of initial states for all component samplers.\n\nThe step_function argument should always be either AbstractMCMC.step or AbstractMCMC.step_warmup.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.gibbs_step_recursive",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.gibbs_step_recursive",
    "title": "Turing.Inference.gibbs_step_recursive",
    "section": "function",
    "text": "Run a Gibbs step for the first varname/sampler/state tuple, and recursively call the same function on the tail, until there are no more samplers left.\n\nThe step_function argument should always be either AbstractMCMC.step or AbstractMCMC.step_warmup.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.init_strategy-Tuple{AbstractMCMC.AbstractSampler}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.init_strategy-Tuple{AbstractMCMC.AbstractSampler}",
    "title": "Turing.Inference.init_strategy",
    "section": "method",
    "text": "Turing.Inference.init_strategy(spl::AbstractSampler)\n\nGet the default initialization strategy for a given sampler spl, i.e. how initial parameters for sampling are chosen if not specified by the user. By default, this is InitFromPrior(), which samples initial parameters from the prior distribution.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.initial_varinfo-Tuple{Any, Any, Any, DynamicPPL.AbstractInitStrategy}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.initial_varinfo-Tuple{Any, Any, Any, DynamicPPL.AbstractInitStrategy}",
    "title": "Turing.Inference.initial_varinfo",
    "section": "method",
    "text": "Initialise a VarInfo for the Gibbs sampler.\n\nThis is straight up copypasta from DynamicPPL's src/sampler.jl. It is repeated here to support calling both step and stepwarmup as the initial step. DynamicPPL initialstep is incompatible with stepwarmup.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.isgibbscomponent-Tuple{AbstractMCMC.AbstractSampler}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.isgibbscomponent-Tuple{AbstractMCMC.AbstractSampler}",
    "title": "Turing.Inference.isgibbscomponent",
    "section": "method",
    "text": "isgibbscomponent(spl::AbstractSampler)\n\nReturn a boolean indicating whether spl is a valid component for a Gibbs sampler.\n\nDefaults to true if no method has been defined for a particular sampler.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.loadstate-Tuple{Chains}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.loadstate-Tuple{Chains}",
    "title": "Turing.Inference.loadstate",
    "section": "method",
    "text": "loadstate(chain::MCMCChains.Chains)\n\nLoad the final state of the sampler from a MCMCChains.Chains object.\n\nTo save the final state of the sampler, you must use sample(...; save_state=true). If this argument was not used during sampling, calling loadstate will throw an error.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.make_conditional-Tuple{DynamicPPL.Model, AbstractVector{<:AbstractPPL.VarName}, Any}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.make_conditional-Tuple{DynamicPPL.Model, AbstractVector{<:AbstractPPL.VarName}, Any}",
    "title": "Turing.Inference.make_conditional",
    "section": "method",
    "text": "make_conditional(model, target_variables, varinfo)\n\nReturn a new, conditioned model for a component of a Gibbs sampler.\n\nArguments\n\nmodel::DynamicPPL.Model: The model to condition.\ntarget_variables::AbstractVector{<:VarName}: The target variables of the component\n\nsampler. These will not be conditioned.\n\nvarinfo::DynamicPPL.AbstractVarInfo: Values for all variables in the model. All the\n\nvalues in varinfo but not in target_variables will be conditioned to the values they have in varinfo.\n\nReturns\n\nA new model with the variables not in target_variables conditioned.\nThe GibbsContext object that will be used to condition the variables. This is necessary\n\nbecause evaluation can mutate its global_varinfo field, which we need to access later.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.match_linking!!-Tuple{Any, Any, Any}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.match_linking!!-Tuple{Any, Any, Any}",
    "title": "Turing.Inference.match_linking!!",
    "section": "method",
    "text": "match_linking!!(varinfo_local, prev_state_local, model)\n\nMake sure the linked/invlinked status of varinfo_local matches that of the previous state for this sampler. This is relevant when multiple samplers are sampling the same variables, and one might need it to be linked while the other doesn't.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.mh_accept-Tuple{Real, Real, Real}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.mh_accept-Tuple{Real, Real, Real}",
    "title": "Turing.Inference.mh_accept",
    "section": "method",
    "text": "mh_accept(logp_current::Real, logp_proposal::Real, log_proposal_ratio::Real)\n\nDecide if a proposal x with log probability log p(x) = logp_proposal and log proposal ratio log k(x x) - log k(x x) = log_proposal_ratio in a Metropolis-Hastings algorithm with Markov kernel k(x_t x_t+1) and current state x with log probability log p(x) = logp_current is accepted by evaluating the Metropolis-Hastings acceptance criterion\n\nlog U leq log p(x) - log p(x) + log k(x x) - log k(x x)\n\nfor a uniform random number U in 0 1).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.set_namedtuple!-Tuple{Union{DynamicPPL.ThreadSafeVarInfo{<:DynamicPPL.VarInfo{Tmeta}}, DynamicPPL.VarInfo{Tmeta}} where Tmeta, NamedTuple}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.set_namedtuple!-Tuple{Union{DynamicPPL.ThreadSafeVarInfo{<:DynamicPPL.VarInfo{Tmeta}}, DynamicPPL.VarInfo{Tmeta}} where Tmeta, NamedTuple}",
    "title": "Turing.Inference.set_namedtuple!",
    "section": "method",
    "text": "set_namedtuple!(vi::VarInfo, nt::NamedTuple)\n\nPlaces the values of a NamedTuple into the relevant places of a VarInfo.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.set_trace_local_varinfo_maybe-Tuple{DynamicPPL.AbstractVarInfo}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.set_trace_local_varinfo_maybe-Tuple{DynamicPPL.AbstractVarInfo}",
    "title": "Turing.Inference.set_trace_local_varinfo_maybe",
    "section": "method",
    "text": "settracelocalvarinfomaybe(vi::AbstractVarInfo)\n\nSet the Trace local varinfo if executing within a Trace. Return nothing.\n\nIf executed within a TapedTask, set the varinfo stored in the \"taped globals\" of the task. Otherwise do nothing.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Inference/#Turing.Inference.setparams_varinfo!!-Tuple{DynamicPPL.Model, AbstractMCMC.AbstractSampler, Any, DynamicPPL.AbstractVarInfo}",
    "href": "../Turing.jl/api/Inference/#Turing.Inference.setparams_varinfo!!-Tuple{DynamicPPL.Model, AbstractMCMC.AbstractSampler, Any, DynamicPPL.AbstractVarInfo}",
    "title": "Turing.Inference.setparams_varinfo!!",
    "section": "method",
    "text": "setparams_varinfo!!(model::DynamicPPL.Model, sampler::AbstractSampler, state, params::AbstractVarInfo)\n\nA lot like AbstractMCMC.setparams!!, but instead of taking a vector of parameters, takes an AbstractVarInfo object. Also takes the sampler as an argument. By default, falls back to AbstractMCMC.setparams!!(model, state, params[:]).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#API",
    "href": "../Turing.jl/api/#API",
    "title": "API",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "api/#Module-wide-re-exports",
    "href": "../Turing.jl/api/#Module-wide-re-exports",
    "title": "Module-wide re-exports",
    "section": "section",
    "text": "Turing.jl directly re-exports the entire public API of the following packages:\n\nDistributions.jl\nMCMCChains.jl\n\nPlease see the individual packages for their documentation.",
    "crumbs": null
  },
  {
    "objectID": "api/#Individual-exports-and-re-exports",
    "href": "../Turing.jl/api/#Individual-exports-and-re-exports",
    "title": "Individual exports and re-exports",
    "section": "section",
    "text": "In this API documentation, for the sake of clarity, we have listed the module that actually defines each of the exported symbols. Note, however, that all of the following symbols are exported unqualified by Turing. That means, for example, you can just write\n\nusing Turing\n\n@model function my_model() end\n\nsample(my_model(), Prior(), 100)\n\ninstead of\n\nDynamicPPL.@model function my_model() end\n\nsample(my_model(), Turing.Inference.Prior(), 100)\n\neven though Prior() is actually defined in the Turing.Inference module and @model in the DynamicPPL package.",
    "crumbs": null
  },
  {
    "objectID": "api/#Modelling",
    "href": "../Turing.jl/api/#Modelling",
    "title": "Modelling",
    "section": "section",
    "text": "Exported symbol Documentation Description\n@model DynamicPPL.@model Define a probabilistic model\n@varname AbstractPPL.@varname Generate a VarName from a Julia expression\nto_submodel DynamicPPL.to_submodel Define a submodel\nprefix DynamicPPL.prefix Prefix all variable names in a model with a given VarName\nLogDensityFunction DynamicPPL.LogDensityFunction A struct containing all information about how to evaluate a model. Mostly for advanced users\n@addlogprob! DynamicPPL.@addlogprob! Add arbitrary log-probability terms during model evaluation\nsetthreadsafe DynamicPPL.setthreadsafe Mark a model as requiring threadsafe evaluation",
    "crumbs": null
  },
  {
    "objectID": "api/#Inference",
    "href": "../Turing.jl/api/#Inference",
    "title": "Inference",
    "section": "section",
    "text": "Exported symbol Documentation Description\nsample StatsBase.sample Sample from a model\nMCMCThreads AbstractMCMC.MCMCThreads Run MCMC using multiple threads\nMCMCDistributed AbstractMCMC.MCMCDistributed Run MCMC using multiple processes\nMCMCSerial AbstractMCMC.MCMCSerial Run MCMC using without parallelism\nloadstate Turing.Inference.loadstate Load saved state from MCMCChains.Chains",
    "crumbs": null
  },
  {
    "objectID": "api/#Samplers",
    "href": "../Turing.jl/api/#Samplers",
    "title": "Samplers",
    "section": "section",
    "text": "Exported symbol Documentation Description\nPrior Turing.Inference.Prior Sample from the prior distribution\nMH Turing.Inference.MH Metropolis–Hastings\nEmcee Turing.Inference.Emcee Affine-invariant ensemble sampler\nESS Turing.Inference.ESS Elliptical slice sampling\nGibbs Turing.Inference.Gibbs Gibbs sampling\nGibbsConditional Turing.Inference.GibbsConditional Gibbs sampling with analytical conditional posterior distributions\nHMC Turing.Inference.HMC Hamiltonian Monte Carlo\nSGLD Turing.Inference.SGLD Stochastic gradient Langevin dynamics\nSGHMC Turing.Inference.SGHMC Stochastic gradient Hamiltonian Monte Carlo\nPolynomialStepsize Turing.Inference.PolynomialStepsize Returns a function which generates polynomially decaying step sizes\nHMCDA Turing.Inference.HMCDA Hamiltonian Monte Carlo with dual averaging\nNUTS Turing.Inference.NUTS No-U-Turn Sampler\nIS Turing.Inference.IS Importance sampling\nSMC Turing.Inference.SMC Sequential Monte Carlo\nPG Turing.Inference.PG Particle Gibbs\nCSMC Turing.Inference.CSMC The same as PG\nRepeatSampler Turing.Inference.RepeatSampler A sampler that runs multiple times on the same variable\nexternalsampler Turing.Inference.externalsampler Wrap an external sampler for use in Turing",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL-utilities",
    "href": "../Turing.jl/api/#DynamicPPL-utilities",
    "title": "DynamicPPL utilities",
    "section": "section",
    "text": "Please see the generated quantities and probability interface guides for more information.\n\nExported symbol Documentation Description\nreturned DynamicPPL.returned Calculate additional quantities defined in a model\npredict StatsAPI.predict Generate samples from posterior predictive distribution\npointwise_loglikelihoods DynamicPPL.pointwise_loglikelihoods Compute log likelihoods for each sample in a chain\nlogprior DynamicPPL.logprior Compute log prior probability\nlogjoint DynamicPPL.logjoint Compute log joint probability\ncondition AbstractPPL.condition Condition a model on data\ndecondition AbstractPPL.decondition Remove conditioning on data\nconditioned DynamicPPL.conditioned Return the conditioned values of a model\nfix DynamicPPL.fix Fix the value of a variable\nunfix DynamicPPL.unfix Unfix the value of a variable\nOrderedDict OrderedCollections.OrderedDict An ordered dictionary",
    "crumbs": null
  },
  {
    "objectID": "api/#Initialisation-strategies",
    "href": "../Turing.jl/api/#Initialisation-strategies",
    "title": "Initialisation strategies",
    "section": "section",
    "text": "Turing.jl provides several strategies to initialise parameters for models.\n\nExported symbol Documentation Description\nInitFromPrior DynamicPPL.InitFromPrior Obtain initial parameters from the prior distribution\nInitFromUniform DynamicPPL.InitFromUniform Obtain initial parameters by sampling uniformly in linked space\nInitFromParams DynamicPPL.InitFromParams Manually specify (possibly a subset of) initial parameters",
    "crumbs": null
  },
  {
    "objectID": "api/#Variational-inference",
    "href": "../Turing.jl/api/#Variational-inference",
    "title": "Variational inference",
    "section": "section",
    "text": "See the docs of AdvancedVI.jl for detailed usage and the variational inference tutorial for a basic walkthrough.\n\nExported symbol Documentation Description\nvi Turing.vi Perform variational inference\nq_locationscale Turing.Variational.q_locationscale Find a numerically non-degenerate initialization for a location-scale variational family\nq_meanfield_gaussian Turing.Variational.q_meanfield_gaussian Find a numerically non-degenerate initialization for a mean-field Gaussian family\nq_fullrank_gaussian Turing.Variational.q_fullrank_gaussian Find a numerically non-degenerate initialization for a full-rank Gaussian family\nKLMinRepGradDescent Turing.Variational.KLMinRepGradDescent KL divergence minimization via stochastic gradient descent with the reparameterization gradient\nKLMinRepGradProxDescent Turing.Variational.KLMinRepGradProxDescent KL divergence minimization via stochastic proximal gradient descent with the reparameterization gradient over location-scale variational families\nKLMinScoreGradDescent Turing.Variational.KLMinScoreGradDescent KL divergence minimization via stochastic gradient descent with the score gradient\nKLMinWassFwdBwd Turing.Variational.KLMinWassFwdBwd KL divergence minimization via Wasserstein proximal gradient descent\nKLMinNaturalGradDescent Turing.Variational.KLMinNaturalGradDescent KL divergence minimization via natural gradient descent\nKLMinSqrtNaturalGradDescent Turing.Variational.KLMinSqrtNaturalGradDescent KL divergence minimization via natural gradient descent in the square-root parameterization\nFisherMinBatchMatch Turing.Variational.FisherMinBatchMatch Covariance-weighted Fisher divergence minimization via the batch-and-match algorithm",
    "crumbs": null
  },
  {
    "objectID": "api/#Automatic-differentiation-types",
    "href": "../Turing.jl/api/#Automatic-differentiation-types",
    "title": "Automatic differentiation types",
    "section": "section",
    "text": "These are used to specify the automatic differentiation backend to use. See the AD guide for more information.\n\nExported symbol Documentation Description\nAutoForwardDiff ADTypes.AutoForwardDiff ForwardDiff.jl backend\nAutoReverseDiff ADTypes.AutoReverseDiff ReverseDiff.jl backend\nAutoMooncake ADTypes.AutoMooncake Mooncake.jl backend",
    "crumbs": null
  },
  {
    "objectID": "api/#Debugging",
    "href": "../Turing.jl/api/#Debugging",
    "title": "Debugging",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "api/#Distributions",
    "href": "../Turing.jl/api/#Distributions",
    "title": "Distributions",
    "section": "section",
    "text": "These distributions are defined in Turing.jl, but not in Distributions.jl.",
    "crumbs": null
  },
  {
    "objectID": "api/#Tools-to-work-with-distributions",
    "href": "../Turing.jl/api/#Tools-to-work-with-distributions",
    "title": "Tools to work with distributions",
    "section": "section",
    "text": "Exported symbol Documentation Description\nI LinearAlgebra.I Identity matrix\nfilldist DistributionsAD.filldist Create a product distribution from a distribution and integers\narraydist DistributionsAD.arraydist Create a product distribution from an array of distributions\nNamedDist DynamicPPL.NamedDist A distribution that carries the name of the variable",
    "crumbs": null
  },
  {
    "objectID": "api/#Point-estimates",
    "href": "../Turing.jl/api/#Point-estimates",
    "title": "Point estimates",
    "section": "section",
    "text": "See the mode estimation tutorial for more information.\n\nExported symbol Documentation Description\nmaximum_a_posteriori Turing.Optimisation.maximum_a_posteriori Find a MAP estimate for a model\nmaximum_likelihood Turing.Optimisation.maximum_likelihood Find a MLE estimate for a model\nMAP Turing.Optimisation.MAP Type to use with Optim.jl for MAP estimation\nMLE Turing.Optimisation.MLE Type to use with Optim.jl for MLE estimation",
    "crumbs": null
  },
  {
    "objectID": "api/#Turing.setprogress!",
    "href": "../Turing.jl/api/#Turing.setprogress!",
    "title": "Turing.setprogress!",
    "section": "function",
    "text": "setprogress!(progress::Bool)\n\nEnable progress logging in Turing if progress is true, and disable it otherwise.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Turing.Flat",
    "href": "../Turing.jl/api/#Turing.Flat",
    "title": "Turing.Flat",
    "section": "type",
    "text": "Flat()\n\nThe flat distribution is the improper distribution of real numbers that has the improper probability density function\n\nf(x) = 1\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Turing.FlatPos",
    "href": "../Turing.jl/api/#Turing.FlatPos",
    "title": "Turing.FlatPos",
    "section": "type",
    "text": "FlatPos(l::Real)\n\nThe positive flat distribution with real-valued parameter l is the improper distribution of real numbers that has the improper probability density function\n\nf(x) = begincases\n0  textif  x leq l \n1  textotherwise\nendcases\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Turing.BinomialLogit",
    "href": "../Turing.jl/api/#Turing.BinomialLogit",
    "title": "Turing.BinomialLogit",
    "section": "type",
    "text": "BinomialLogit(n, logitp)\n\nThe Binomial distribution with logit parameterization characterizes the number of successes in a sequence of independent trials.\n\nIt has two parameters: n, the number of trials, and logitp, the logit of the probability of success in an individual trial, with the distribution\n\nP(X = k) = n choose k(textlogistic(logitp))^k (1 - textlogistic(logitp))^n-k quad text for  k = 012 ldots n\n\nSee also: Distributions.Binomial\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Turing.OrderedLogistic",
    "href": "../Turing.jl/api/#Turing.OrderedLogistic",
    "title": "Turing.OrderedLogistic",
    "section": "type",
    "text": "OrderedLogistic(η, c::AbstractVector)\n\nThe ordered logistic distribution with real-valued parameter η and cutpoints c has the probability mass function\n\nP(X = k) = begincases\n    1 - textlogistic(eta - c_1)  textif  k = 1 \n    textlogistic(eta - c_k-1) - textlogistic(eta - c_k)  textif  1  k  K \n    textlogistic(eta - c_K-1)  textif  k = K\nendcases\n\nwhere K = length(c) + 1.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Turing.LogPoisson",
    "href": "../Turing.jl/api/#Turing.LogPoisson",
    "title": "Turing.LogPoisson",
    "section": "type",
    "text": "LogPoisson(logλ)\n\nThe Poisson distribution with logarithmic parameterization of the rate parameter describes the number of independent events occurring within a unit time interval, given the average rate of occurrence exp(loglambda).\n\nThe distribution has the probability mass function\n\nP(X = k) = frace^k cdot loglambdak e^-e^loglambda quad text for  k = 012ldots\n\nSee also: Distributions.Poisson\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/RandomMeasures/#API:-Turing.RandomMeasures",
    "href": "../Turing.jl/api/RandomMeasures/#API:-Turing.RandomMeasures",
    "title": "API: Turing.RandomMeasures",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "api/RandomMeasures/#Turing.RandomMeasures.ChineseRestaurantProcess",
    "href": "../Turing.jl/api/RandomMeasures/#Turing.RandomMeasures.ChineseRestaurantProcess",
    "title": "Turing.RandomMeasures.ChineseRestaurantProcess",
    "section": "type",
    "text": "ChineseRestaurantProcess(rpm, m)\n\nThe Chinese Restaurant Process for random probability measures rpm with counts m.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/RandomMeasures/#Turing.RandomMeasures.DirichletProcess",
    "href": "../Turing.jl/api/RandomMeasures/#Turing.RandomMeasures.DirichletProcess",
    "title": "Turing.RandomMeasures.DirichletProcess",
    "section": "type",
    "text": "DirichletProcess(α)\n\nThe Dirichlet Process with concentration parameter α. Samples from the Dirichlet process can be constructed via the following representations.\n\nSize-Biased Sampling Process\n\nj_k sim Beta(1 alpha) * surplus\n\nStick-Breaking Process\n\nv_k sim Beta(1 alpha)\n\nChinese Restaurant Process\n\np(z_n = k  z_1n-1) propto begincases\n        fracm_kn-1+alpha textif m_k  0\n        fracalphan-1+alpha\n    endcases\n\nFor more details see: https://www.stats.ox.ac.uk/~teh/research/npbayes/Teh2010a.pdf\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/RandomMeasures/#Turing.RandomMeasures.PitmanYorProcess",
    "href": "../Turing.jl/api/RandomMeasures/#Turing.RandomMeasures.PitmanYorProcess",
    "title": "Turing.RandomMeasures.PitmanYorProcess",
    "section": "type",
    "text": "PitmanYorProcess(d, θ, t)\n\nThe Pitman-Yor Process with discount d, concentration θ and t already drawn atoms. Samples from the Pitman-Yor Process can be constructed via the following representations.\n\nSize-Biased Sampling Process\n\nj_k sim Beta(1-d theta + t*d) * surplus\n\nStick-Breaking Process\n\nv_k sim Beta(1-d theta + t*d)\n\nChinese Restaurant Process\n\np(z_n = k  z_1n-1) propto begincases\n        fracm_k - dn+theta textif m_k  0\n        fractheta + d*tn+theta\n    endcases\n\nFor more details see: https://en.wikipedia.org/wiki/Pitman–Yor_process\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/RandomMeasures/#Turing.RandomMeasures.SizeBiasedSamplingProcess",
    "href": "../Turing.jl/api/RandomMeasures/#Turing.RandomMeasures.SizeBiasedSamplingProcess",
    "title": "Turing.RandomMeasures.SizeBiasedSamplingProcess",
    "section": "type",
    "text": "SizeBiasedSamplingProcess(rpm, surplus)\n\nThe Size-Biased Sampling Process for random probability measures rpm with a surplus mass of surplus.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/RandomMeasures/#Turing.RandomMeasures.StickBreakingProcess",
    "href": "../Turing.jl/api/RandomMeasures/#Turing.RandomMeasures.StickBreakingProcess",
    "title": "Turing.RandomMeasures.StickBreakingProcess",
    "section": "type",
    "text": "StickBreakingProcess(rpm)\n\nThe Stick-Breaking Process for random probability measures rpm.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/RandomMeasures/#Turing.RandomMeasures._logpdf_table",
    "href": "../Turing.jl/api/RandomMeasures/#Turing.RandomMeasures._logpdf_table",
    "title": "Turing.RandomMeasures._logpdf_table",
    "section": "function",
    "text": "_logpdf_table(d::AbstractRandomProbabilityMeasure, m::AbstractVector{Int})\n\nParameters:\n\nd: Random probability measure, e.g. DirichletProcess\nm: Cluster counts\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/RandomMeasures/#Turing.RandomMeasures.stickbreak-Tuple{Any}",
    "href": "../Turing.jl/api/RandomMeasures/#Turing.RandomMeasures.stickbreak-Tuple{Any}",
    "title": "Turing.RandomMeasures.stickbreak",
    "section": "method",
    "text": "Stick-breaking function.\n\nThis function accepts a vector (`v`) of length $K - 1$ where each element\nis assumed to be in the unit interval, and returns a simplex of length\n$K$.  If the supplied vector `v` is a vector of independent draws from\na Beta distribution (i.e., vⱼ | a ~ Beta(1, a), for j=1,...,K), then\nreturned simplex is generated via a stick-breaking process where\nthe first element of the stick is w₁ = v₁, the last element w_K =\n∏ⱼ (1 - vⱼ), and the other elements are wₖ = vₖ ∏ⱼ₌₁ᵏ⁻¹(1 - vⱼ).\nAs $K$ goes to infinity, w is a draw from the Chinese Restaurant process\nwith mass parameter a.\n\nArguments\n\nv: A vector of length K - 1, where K  1.\n\nReturn\n\nA simplex (w) of dimension K. Where ∑ₖ wₖ = 1, and each wₖ ≥ 0.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "#Turing.jl",
    "href": "../Turing.jl/#Turing.jl",
    "title": "Turing.jl",
    "section": "section",
    "text": "This site contains the API documentation for the identifiers exported by Turing.jl.\n\nIf you are looking for usage examples and guides, please visit https://turinglang.org/docs.",
    "crumbs": null
  },
  {
    "objectID": "api/Variational/#API:-Turing.Variational",
    "href": "../Turing.jl/api/Variational/#API:-Turing.Variational",
    "title": "API: Turing.Variational",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "api/Variational/#Turing.Variational.q_fullrank_gaussian-Tuple{Random.AbstractRNG, DynamicPPL.Model}",
    "href": "../Turing.jl/api/Variational/#Turing.Variational.q_fullrank_gaussian-Tuple{Random.AbstractRNG, DynamicPPL.Model}",
    "title": "Turing.Variational.q_fullrank_gaussian",
    "section": "method",
    "text": "q_fullrank_gaussian(\n    [rng::Random.AbstractRNG,]\n    model::DynamicPPL.Model;\n    location::Union{Nothing,<:AbstractVector} = nothing,\n    scale::Union{Nothing,<:LowerTriangular} = nothing,\n    kwargs...\n)\n\nFind a numerically non-degenerate Gaussian q with a scale with full-rank factors (traditionally referred to as a \"full-rank family\") for approximating the target model.\n\nIf the scale set as nothing, the default value will be a zero-mean Gaussian with a LowerTriangular scale matrix (resulting in a covariance with \"full-rank\" factors) no larger than 0.6*I (covariance of 0.6^2*I). This guarantees that the samples from the initial variational approximation will fall in the range of (-2, 2) with 99.9% probability, which mimics the behavior of the Turing.InitFromUniform() strategy. Whether the default choice is used or not, the scale may be adjusted via q_initialize_scale so that the log-densities of model are finite over the samples from q.\n\nArguments\n\nmodel: The target DynamicPPL.Model.\n\nKeyword Arguments\n\nlocation: The location parameter of the initialization. If nothing, a vector of zeros is used.\nscale: The scale parameter of the initialization. If nothing, an identity matrix is used.\n\nThe remaining keyword arguments are passed to q_locationscale.\n\nReturns\n\nq::Bijectors.TransformedDistribution: A AdvancedVI.LocationScale distribution matching the support of model.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Variational/#Turing.Variational.q_initialize_scale-Tuple{Random.AbstractRNG, DynamicPPL.Model, AbstractVector, AbstractMatrix, UnivariateDistribution}",
    "href": "../Turing.jl/api/Variational/#Turing.Variational.q_initialize_scale-Tuple{Random.AbstractRNG, DynamicPPL.Model, AbstractVector, AbstractMatrix, UnivariateDistribution}",
    "title": "Turing.Variational.q_initialize_scale",
    "section": "method",
    "text": "q_initialize_scale(\n    [rng::Random.AbstractRNG,]\n    model::DynamicPPL.Model,\n    location::AbstractVector,\n    scale::AbstractMatrix,\n    basedist::Distributions.UnivariateDistribution;\n    num_samples::Int = 10,\n    num_max_trials::Int = 10,\n    reduce_factor::Real = one(eltype(scale)) / 2\n)\n\nGiven an initial location-scale distribution q formed by location, scale, and basedist, shrink scale until the expectation of log-densities of model taken over q are finite. If the log-densities are not finite even after num_max_trials, throw an error.\n\nFor reference, a location-scale distribution q formed by location, scale, and basedist is a distribution where its sampling process z sim q can be represented as\n\nu = rand(basedist, d)\nz = scale * u + location\n\nArguments\n\nmodel: The target DynamicPPL.Model.\nlocation: The location parameter of the initialization.\nscale: The scale parameter of the initialization.\nbasedist: The base distribution of the location-scale family.\n\nKeyword Arguments\n\nnum_samples: Number of samples used to compute the average log-density at each trial.\nnum_max_trials: Number of trials until throwing an error.\nreduce_factor: Factor for shrinking the scale. After n trials, the scale is then scale*reduce_factor^n.\n\nReturns\n\nscale_adj: The adjusted scale matrix matching the type of scale.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Variational/#Turing.Variational.q_locationscale-Tuple{Random.AbstractRNG, DynamicPPL.Model}",
    "href": "../Turing.jl/api/Variational/#Turing.Variational.q_locationscale-Tuple{Random.AbstractRNG, DynamicPPL.Model}",
    "title": "Turing.Variational.q_locationscale",
    "section": "method",
    "text": "q_locationscale(\n    [rng::Random.AbstractRNG,]\n    model::DynamicPPL.Model;\n    location::Union{Nothing,<:AbstractVector} = nothing,\n    scale::Union{Nothing,<:Diagonal,<:LowerTriangular} = nothing,\n    meanfield::Bool = true,\n    basedist::Distributions.UnivariateDistribution = Normal()\n)\n\nFind a numerically non-degenerate variational distribution q for approximating the  target model within the location-scale variational family formed by the type of scale and basedist.\n\nThe distribution can be manually specified by setting location, scale, and basedist. Otherwise, it chooses a Gaussian with zero-mean and scale 0.6*I (covariance of 0.6^2*I) by default. This guarantees that the samples from the initial variational approximation will fall in the range of (-2, 2) with 99.9% probability, which mimics the behavior of the Turing.InitFromUniform() strategy.\n\nWhether the default choice is used or not, the scale may be adjusted via q_initialize_scale so that the log-densities of model are finite over the samples from q. If meanfield is set as true, the scale of q is restricted to be a diagonal matrix and only the diagonal of scale is used.\n\nFor reference, a location-scale distribution q formed by location, scale, and basedist is a distribution where its sampling process z sim q can be represented as\n\nu = rand(basedist, d)\nz = scale * u + location\n\nArguments\n\nmodel: The target DynamicPPL.Model.\n\nKeyword Arguments\n\nlocation: The location parameter of the initialization. If nothing, a vector of zeros is used.\nscale: The scale parameter of the initialization. If nothing, an identity matrix is used.\nmeanfield: Whether to use the mean-field approximation. If true, scale is converted into a Diagonal matrix. Otherwise, it is converted into a LowerTriangular matrix.\nbasedist: The base distribution of the location-scale family.\n\nThe remaining keywords are passed to q_initialize_scale.\n\nReturns\n\nq::Bijectors.TransformedDistribution: A AdvancedVI.LocationScale distribution matching the support of model.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Variational/#Turing.Variational.q_meanfield_gaussian-Tuple{Random.AbstractRNG, DynamicPPL.Model}",
    "href": "../Turing.jl/api/Variational/#Turing.Variational.q_meanfield_gaussian-Tuple{Random.AbstractRNG, DynamicPPL.Model}",
    "title": "Turing.Variational.q_meanfield_gaussian",
    "section": "method",
    "text": "q_meanfield_gaussian(\n    [rng::Random.AbstractRNG,]\n    model::DynamicPPL.Model;\n    location::Union{Nothing,<:AbstractVector} = nothing,\n    scale::Union{Nothing,<:Diagonal} = nothing,\n    kwargs...\n)\n\nFind a numerically non-degenerate mean-field Gaussian q for approximating the  target model.\n\nIf the scale set as nothing, the default value will be a zero-mean Gaussian with a Diagonal scale matrix (the \"mean-field\" approximation) no larger than 0.6*I (covariance of 0.6^2*I). This guarantees that the samples from the initial variational approximation will fall in the range of (-2, 2) with 99.9% probability, which mimics the behavior of the Turing.InitFromUniform() strategy. Whether the default choice is used or not, the scale may be adjusted via q_initialize_scale so that the log-densities of model are finite over the samples from q.\n\nArguments\n\nmodel: The target DynamicPPL.Model.\n\nKeyword Arguments\n\nlocation: The location parameter of the initialization. If nothing, a vector of zeros is used.\nscale: The scale parameter of the initialization. If nothing, an identity matrix is used.\n\nThe remaining keyword arguments are passed to q_locationscale.\n\nReturns\n\nq::Bijectors.TransformedDistribution: A AdvancedVI.LocationScale distribution matching the support of model.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/Variational/#Turing.Variational.vi-Tuple{Random.AbstractRNG, DynamicPPL.Model, Any, Int64, Vararg{Any}}",
    "href": "../Turing.jl/api/Variational/#Turing.Variational.vi-Tuple{Random.AbstractRNG, DynamicPPL.Model, Any, Int64, Vararg{Any}}",
    "title": "Turing.Variational.vi",
    "section": "method",
    "text": "vi(\n    [rng::Random.AbstractRNG,]\n    model::DynamicPPL.Model,\n    q,\n    max_iter::Int;\n    adtype::ADTypes.AbstractADType=DEFAULT_ADTYPE,\n    algorithm::AdvancedVI.AbstractVariationalAlgorithm = KLMinRepGradProxDescent(\n        adtype; n_samples=10\n    ),\n    show_progress::Bool = Turing.PROGRESS[],\n    kwargs...\n)\n\nApproximate the target model via the variational inference algorithm algorithm by starting from the initial variational approximation q. This is a thin wrapper around AdvancedVI.optimize.\n\nIf the chosen variational inference algorithm operates in an unconstrained space, then the provided initial variational approximation q must be a Bijectors.TransformedDistribution of an unconstrained distribution. For example, the initialization supplied by  q_meanfield_gaussian,q_fullrank_gaussian, q_locationscale.\n\nThe default algorithm, KLMinRepGradProxDescent (relevant docs), assumes q uses AdvancedVI.MvLocationScale, which can be constructed by invoking q_fullrank_gaussian or q_meanfield_gaussian. For other variational families, refer to the documentation of AdvancedVI to determine the best algorithm and other options.\n\nArguments\n\nmodel: The target DynamicPPL.Model.\nq: The initial variational approximation.\nmax_iter: Maximum number of steps.\nAny additional arguments are passed on to AdvancedVI.optimize.\n\nKeyword Arguments\n\nadtype: Automatic differentiation backend to be applied to the log-density. The default value for algorithm also uses this backend for differentiating the variational objective.\nalgorithm: Variational inference algorithm. The default is KLMinRepGradProxDescent, please refer to AdvancedVI docs for all the options.\nshow_progress: Whether to show the progress bar.\nunconstrained: Whether to transform the posterior to be unconstrained for running the variational inference algorithm. If true, then the output q will be wrapped into a Bijectors.TransformedDistribution with the transformation matching the support of the posterior. The default value depends on the chosen algorithm.\nAny additional keyword arguments are passed on to AdvancedVI.optimize.\n\nSee the docs of AdvancedVI.optimize for additional keyword arguments.\n\nReturns\n\nq: Output variational distribution of algorithm.\nstate: Collection of states used by algorithm. This can be used to resume from a past call to vi.\ninfo: Information generated while executing algorithm.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "internals/varinfo/#Design-of-VarInfo",
    "href": "../DynamicPPL.jl/internals/varinfo/#Design-of-VarInfo",
    "title": "Design of VarInfo",
    "section": "section",
    "text": "VarInfo is a fairly simple structure.\n\nIt contains\n\na logp field for accumulation of the log-density evaluation, and\na metadata field for storing information about the realizations of the different variables.\n\nRepresenting logp is fairly straight-forward: we'll just use a Real or an array of Real, depending on the context.\n\nRepresenting metadata is a bit trickier. This is supposed to contain all the necessary information for each VarName to enable the different executions of the model + extraction of different properties of interest after execution, e.g. the realization / value corresponding to a variable @varname(x).\n\nnote: Note\nWe want to work with VarName rather than something like Symbol or String as VarName contains additional structural information, e.g. a Symbol(\"x[1]\") can be a result of either var\"x[1]\" ~ Normal() or x[1] ~ Normal(); these scenarios are disambiguated by VarName.\n\nTo ensure that VarInfo is simple and intuitive to work with, we want VarInfo, and hence the underlying metadata, to replicate the following functionality of Dict:\n\nkeys(::Dict): return all the VarNames present in metadata.\nhaskey(::Dict): check if a particular VarName is present in metadata.\ngetindex(::Dict, ::VarName): return the realization corresponding to a particular VarName.\nsetindex!(::Dict, val, ::VarName): set the realization corresponding to a particular VarName.\npush!(::Dict, ::Pair): add a new key-value pair to the container.\ndelete!(::Dict, ::VarName): delete the realization corresponding to a particular VarName.\nempty!(::Dict): delete all realizations in metadata.\nmerge(::Dict, ::Dict): merge two metadata structures according to similar rules as Dict.\n\nBut for general-purpose samplers, we often want to work with a simple flattened structure, typically a Vector{<:Real}. One can access a vectorised version of a variable's value with the following vector-like functions:\n\ngetindex_internal(::VarInfo, ::VarName): get the flattened value of a single variable.\ngetindex_internal(::VarInfo, ::Colon): get the flattened values of all variables.\ngetindex_internal(::VarInfo, i::Int): get ith value of the flattened vector of all values\nsetindex_internal!(::VarInfo, ::AbstractVector, ::VarName): set the flattened value of a variable.\nsetindex_internal!(::VarInfo, val, i::Int): set the ith value of the flattened vector of all values\nlength_internal(::VarInfo): return the length of the flat representation of metadata.\n\nThe functions have _internal in their name because internally VarInfo always stores values as vectorised.\n\nMoreover, a link transformation can be applied to a VarInfo with link!! (and reversed with invlink!!), which applies a reversible transformation to the internal storage format of a variable that makes the range of the random variable cover all of Euclidean space. getindex_internal and setindex_internal! give direct access to the vectorised value after such a transformation, which is what samplers often need to be able sample in unconstrained space. One can also manually set a transformation by giving setindex_internal! a fourth, optional argument, that is a function that maps internally stored value to the actual value of the variable.\n\nFinally, we want want the underlying representation used in metadata to have a few performance-related properties:\n\nType-stable when possible, but functional when not.\nEfficient storage and iteration when possible, but functional when not.\n\nThe \"but functional when not\" is important as we want to support arbitrary models, which means that we can't always have these performance properties.\n\nIn the following sections, we'll outline how we achieve this in VarInfo.",
    "crumbs": null
  },
  {
    "objectID": "internals/varinfo/#Type-stability",
    "href": "../DynamicPPL.jl/internals/varinfo/#Type-stability",
    "title": "Type-stability",
    "section": "section",
    "text": "Ensuring type-stability is somewhat non-trivial to address since we want this to be the case even when models mix continuous (typically Float64) and discrete (typically Int) variables.\n\nSuppose we have an implementation of metadata which implements the functionality outlined in the previous section. The way we approach this in VarInfo is to use a NamedTuple with a separate metadata for each distinct Symbol used. For example, if we have a model of the form\n\nusing DynamicPPL, Distributions, FillArrays\n\n@model function demo()\n    x ~ product_distribution(Fill(Bernoulli(0.5), 2))\n    y ~ Normal(0, 1)\n    return nothing\nend\n\nthen we construct a type-stable representation by using a NamedTuple{(:x, :y), Tuple{Vx, Vy}} where\n\nVx is a container with eltype Bool, and\nVy is a container with eltype Float64.\n\nSince VarName contains the Symbol used in its type, something like getindex(varinfo, @varname(x)) can be resolved to getindex(varinfo.metadata.x, @varname(x)) at compile-time.\n\nFor example, with the model above we have\n\n# Type-unstable `VarInfo`\nvarinfo_untyped = DynamicPPL.untyped_varinfo(demo())\ntypeof(varinfo_untyped.metadata)\n\n# Type-stable `VarInfo`\nvarinfo_typed = DynamicPPL.typed_varinfo(demo())\ntypeof(varinfo_typed.metadata)\n\nThey both work as expected but one results in concrete typing and the other does not:\n\nvarinfo_untyped[@varname(x)], varinfo_untyped[@varname(y)]\n\nvarinfo_typed[@varname(x)], varinfo_typed[@varname(y)]\n\nNotice that the untyped VarInfo uses Vector{Real} to store the boolean entries while the typed uses Vector{Bool}. This is because the untyped version needs the underlying container to be able to handle both the Bool for x and the Float64 for y, while the typed version can use a Vector{Bool} for x and a Vector{Float64} for y due to its usage of NamedTuple.\n\nwarning: Warning\nOf course, this NamedTuple approach is not necessarily going to help us in scenarios where the Symbol does not correspond to a unique type, e.g.x[1] ~ Bernoulli(0.5)\nx[2] ~ Normal(0, 1)In this case we'll end up with a NamedTuple((:x,), Tuple{Vx}) where Vx is a container with eltype Union{Bool, Float64} or something worse. This is not type-stable but will still be functional.In practice, we rarely observe such mixing of types, therefore in DynamicPPL, and more widely in Turing.jl, we use a NamedTuple approach for type-stability with great success.\n\nwarning: Warning\nAnother downside with such a NamedTuple approach is that if we have a model with lots of tilde-statements, e.g. a ~ Normal(), b ~ Normal(), ..., z ~ Normal() will result in a NamedTuple with 27 entries, potentially leading to long compilation times.For these scenarios it can be useful to fall back to \"untyped\" representations.\n\nHence we obtain a \"type-stable when possible\"-representation by wrapping it in a NamedTuple and partially resolving the getindex, setindex!, etc. methods at compile-time. When type-stability is not desired, we can simply use a single metadata for all VarNames instead of a NamedTuple wrapping a collection of metadatas.",
    "crumbs": null
  },
  {
    "objectID": "internals/varinfo/#Efficient-storage-and-iteration",
    "href": "../DynamicPPL.jl/internals/varinfo/#Efficient-storage-and-iteration",
    "title": "Efficient storage and iteration",
    "section": "section",
    "text": "Efficient storage and iteration we achieve through implementation of the metadata. In particular, we do so with DynamicPPL.VarNamedVector:\n\nIn a DynamicPPL.VarNamedVector{<:VarName,T}, we achieve the desiderata by storing the values for different VarNames contiguously in a Vector{T} and keeping track of which ranges correspond to which VarNames.\n\nThis does require a bit of book-keeping, in particular when it comes to insertions and deletions. Internally, this is handled by assigning each VarName a unique Int index in the varname_to_index field, which is then used to index into the following fields:\n\nvarnames::Vector{<:VarName}: the VarNames in the order they appear in the Vector{T}.\nranges::Vector{UnitRange{Int}}: the ranges of indices in the Vector{T} that correspond to each VarName.\ntransforms::Vector: the transforms associated with each VarName.\n\nMutating functions, e.g. setindex_internal!(vnv::VarNamedVector, val, vn::VarName), are then treated according to the following rules:\n\nIf vn is not already present: add it to the end of vnv.varnames, add the val to the underlying vnv.vals, etc.\nIf vn is already present in vnv:\nIf val has the same length as the existing value for vn: replace existing value.\nIf val has a smaller length than the existing value for vn: replace existing value and mark the remaining indices as \"inactive\" by increasing the entry in vnv.num_inactive field.\nIf val has a larger length than the existing value for vn: expand the underlying vnv.vals to accommodate the new value, update all VarNames occuring after vn, and update the vnv.ranges to point to the new range for vn.\n\nThis means that VarNamedVector is allowed to grow as needed, while \"shrinking\" (i.e. insertion of smaller elements) is handled by simply marking the redundant indices as \"inactive\". This turns out to be efficient for use-cases that we are generally interested in.\n\nFor example, we want to optimize code-paths which effectively boil down to inner-loop in the following example:\n\n# Construct a `VarInfo` with types inferred from `model`.\nvarinfo = VarInfo(model)\n\n# Repeatedly sample from `model`.\nfor _ in 1:num_samples\n    rand!(rng, model, varinfo)\n\n    # Do something with `varinfo`.\n    # ...\nend\n\nThere are typically a few scenarios where we encounter changing representation sizes of a random variable x:\n\nWe're working with a transformed version x which is represented in a lower-dimensional space, e.g. transforming a x ~ LKJ(2, 1) to unconstrained y = f(x) takes us from 2-by-2 Matrix{Float64} to a 1-length Vector{Float64}.\nx has a random size, e.g. in a mixture model with a prior on the number of components. Here the size of x can vary widly between every realization of the Model.\n\nIn scenario (1), we're usually shrinking the representation of x, and so we end up not making any allocations for the underlying Vector{T} but instead just marking the redundant part as \"inactive\".\n\nIn scenario (2), we  end up increasing the allocated memory for the randomly sized x, eventually leading to a vector that is large enough to hold realizations without needing to reallocate. But this can still lead to unnecessary memory usage, which might be undesirable. Hence one has to make a decision regarding the trade-off between memory usage and performance for the use-case at hand.\n\nTo help with this, we have the following functions:\n\nFor example, one might encounter the following scenario:\n\nvnv = DynamicPPL.VarNamedVector(@varname(x) => [true])\nprintln(\"Before insertion: number of allocated entries  $(DynamicPPL.num_allocated(vnv))\")\n\nfor i in 1:5\n    x = fill(true, rand(1:100))\n    DynamicPPL.update!(vnv, x, @varname(x))\n    println(\n        \"After insertion #$(i) of length $(length(x)): number of allocated entries  $(DynamicPPL.num_allocated(vnv))\",\n    )\nend\n\nWe can then insert a call to DynamicPPL.contiguify! after every insertion whenever the allocation grows too large to reduce overall memory usage:\n\nvnv = DynamicPPL.VarNamedVector(@varname(x) => [true])\nprintln(\"Before insertion: number of allocated entries  $(DynamicPPL.num_allocated(vnv))\")\n\nfor i in 1:5\n    x = fill(true, rand(1:100))\n    DynamicPPL.update!(vnv, x, @varname(x))\n    if DynamicPPL.num_allocated(vnv) > 10\n        DynamicPPL.contiguify!(vnv)\n    end\n    println(\n        \"After insertion #$(i) of length $(length(x)): number of allocated entries  $(DynamicPPL.num_allocated(vnv))\",\n    )\nend\n\nThis does incur a runtime cost as it requires re-allocation of the ranges in addition to a resize! of the underlying Vector{T}. However, this also ensures that the the underlying Vector{T} is contiguous, which is important for performance. Hence, if we're about to do a lot of work with the VarNamedVector without insertions, etc., it can be worth it to do a sweep to ensure that the underlying Vector{T} is contiguous.\n\nnote: Note\nHigher-dimensional arrays, e.g. Matrix, are handled by simply vectorizing them before storing them in the Vector{T}, and composing the VarName's transformation with a DynamicPPL.ReshapeTransform.\n\nContinuing from the example from the previous section, we can use a VarInfo with a VarNamedVector as the metadata field:\n\n# Type-unstable\nvarinfo_untyped_vnv = DynamicPPL.untyped_vector_varinfo(varinfo_untyped)\nvarinfo_untyped_vnv[@varname(x)], varinfo_untyped_vnv[@varname(y)]\n\n# Type-stable\nvarinfo_typed_vnv = DynamicPPL.typed_vector_varinfo(varinfo_typed)\nvarinfo_typed_vnv[@varname(x)], varinfo_typed_vnv[@varname(y)]\n\nIf we now try to delete! @varname(x)\n\nhaskey(varinfo_untyped_vnv, @varname(x))\n\nDynamicPPL.has_inactive(varinfo_untyped_vnv.metadata)\n\n# `delete!`\nDynamicPPL.delete!(varinfo_untyped_vnv.metadata, @varname(x))\nDynamicPPL.has_inactive(varinfo_untyped_vnv.metadata)\n\nhaskey(varinfo_untyped_vnv, @varname(x))\n\nOr insert a differently-sized value for @varname(x)\n\nDynamicPPL.insert!(varinfo_untyped_vnv.metadata, fill(true, 1), @varname(x))\nvarinfo_untyped_vnv[@varname(x)]\n\nDynamicPPL.num_allocated(varinfo_untyped_vnv.metadata, @varname(x))\n\nDynamicPPL.update!(varinfo_untyped_vnv.metadata, fill(true, 4), @varname(x))\nvarinfo_untyped_vnv[@varname(x)]\n\nDynamicPPL.num_allocated(varinfo_untyped_vnv.metadata, @varname(x))",
    "crumbs": null
  },
  {
    "objectID": "internals/varinfo/#Performance-summary",
    "href": "../DynamicPPL.jl/internals/varinfo/#Performance-summary",
    "title": "Performance summary",
    "section": "section",
    "text": "In the end, we have the following \"rough\" performance characteristics for VarNamedVector:\n\nMethod Is blazingly fast?\ngetindex colorgreen checkmark\nsetindex! on a new VarName colorgreen checkmark\ndelete! colorred times\nupdate! on existing VarName colorgreen checkmark if smaller or same size / colorred times if larger size\nvalues_as(::VarNamedVector, Vector{T}) colorgreen checkmark if contiguous / colororange div otherwise",
    "crumbs": null
  },
  {
    "objectID": "internals/varinfo/#Other-methods",
    "href": "../DynamicPPL.jl/internals/varinfo/#Other-methods",
    "title": "Other methods",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "internals/varinfo/#DynamicPPL.VarInfo-internals-varinfo",
    "href": "../DynamicPPL.jl/internals/varinfo/#DynamicPPL.VarInfo-internals-varinfo",
    "title": "DynamicPPL.VarInfo",
    "section": "type",
    "text": "struct VarInfo{Tmeta,Accs<:AccumulatorTuple} <: AbstractVarInfo\n    metadata::Tmeta\n    accs::Accs\nend\n\nA light wrapper over some kind of metadata.\n\nThe type of the metadata can be one of a number of options. It may either be a Metadata or a VarNamedVector, or, it may be a NamedTuple which maps symbols to Metadata or VarNamedVector instances. Here, a symbol refers to a Julia variable and may consist of one or more VarNames which appear on the left-hand side of tilde statements. For example, x[1] and x[2] both have the same symbol x.\n\nSeveral type aliases are provided for these forms of VarInfos:\n\nVarInfo{<:Metadata} is UntypedVarInfo\nVarInfo{<:VarNamedVector} is UntypedVectorVarInfo\nVarInfo{<:NamedTuple} is NTVarInfo\n\nThe NamedTuple form, i.e. NTVarInfo, is useful for maintaining type stability of model evaluation. However, the element type of NamedTuples are not contained in its type itself: thus, there is no way to use the type system to determine whether the elements of the NamedTuple are Metadata or VarNamedVector.\n\nNote that for NTVarInfo, it is the user's responsibility to ensure that each symbol is visited at least once during model evaluation, regardless of any stochastic branching.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "internals/varinfo/#DynamicPPL.VarNamedVector",
    "href": "../DynamicPPL.jl/internals/varinfo/#DynamicPPL.VarNamedVector",
    "title": "DynamicPPL.VarNamedVector",
    "section": "type",
    "text": "VarNamedVector\n\nA container that stores values in a vectorised form, but indexable by variable names.\n\nA VarNamedVector can be thought of as an ordered mapping from VarNames to pairs of (internal_value, transform). Here internal_value is a vectorised value for the variable and transform is a function such that transform(internal_value) is the \"original\" value of the variable, the one that the user sees. For instance, if the variable has a matrix value, internal_value could bea flattened Vector of its elements, and transform would be a reshape call.\n\ntransform may implement simply vectorisation, but it may do more. Most importantly, it may implement linking, where the internal storage of a random variable is in a form where all values in Euclidean space are valid. This is useful for sampling, because the sampler can make changes to internal_value without worrying about constraints on the space of the random variable.\n\nThe way to access this storage format directly is through the functions getindex_internal and setindex_internal. The transform argument for setindex_internal is optional, by default it is either the identity, or the existing transform if a value already exists for this VarName.\n\nVarNamedVector also provides a Dict-like interface that hides away the internal vectorisation. This can be accessed with getindex and setindex!. setindex! only takes the value, the transform is automatically set to be a simple vectorisation. The only notable deviation from the behavior of a Dict is that setindex! will throw an error if one tries to set a new value for a variable that lives in a different \"space\" than the old one (e.g. is of a different type or size). This is because setindex! does not change the transform of a variable, e.g. preserve linking, and thus the new value must be compatible with the old transform.\n\nFor now, a third value is in fact stored for each VarName: a boolean indicating whether the variable has been transformed to unconstrained Euclidean space or not. This is only in place temporarily due to the needs of our old Gibbs sampler.\n\nInternally, VarNamedVector stores the values of all variables in a single contiguous vector. This makes some operations more efficient, and means that one can access the entire contents of the internal storage quickly with getindex_internal(vnv, :). The other fields of VarNamedVector are mostly used to keep track of which part of the internal storage belongs to which VarName.\n\nAll constructors accept a keyword argument check_consistency::Bool=true that controls whether to run checks like the number of values matching the number of variables. Some of these checks can be expensive, so if you are confident in the input, you may want to turn check_consistency off for performance.\n\nFields\n\nvarname_to_index: mapping from a VarName to its integer index in varnames, ranges and transforms\n\nvarnames: vector of VarNames for the variables, where varnames[varname_to_index[vn]] == vn\n\nranges: vector of index ranges in vals corresponding to varnames; each VarName vn has a single index or a set of contiguous indices, such that the values of vn can be found at vals[ranges[varname_to_index[vn]]]\n\nvals: vector of values of all variables; the value(s) of vn is/are vals[ranges[varname_to_index[vn]]]\n\ntransforms: vector of transformations, so that transforms[varname_to_index[vn]] is a callable that transforms the value of vn back to its original space, undoing any linking and vectorisation\n\nis_unconstrained: vector of booleans indicating whether a variable has been explicitly transformed to unconstrained Euclidean space, i.e. whether its domain is all of ℝ^ⁿ. If is_unconstrained[varname_to_index[vn]] is true, it guarantees that the variable vn is not constrained. However, the converse does not hold: if is_unconstrained is false, the variable vn may still happen to be unconstrained, e.g. if its original distribution is itself unconstrained (like a normal distribution).\n\nnum_inactive: mapping from a variable index to the number of inactive entries for that variable. Inactive entries are elements in vals that are not part of the value of any variable. They arise when a variable is set to a new value with a different dimension, in-place. Inactive entries always come after the last active entry for the given variable. See the extended help with ??VarNamedVector for more details.\n\nExtended help\n\nThe values for different variables are internally all stored in a single vector. For instance,\n\njulia> using DynamicPPL: ReshapeTransform, VarNamedVector, @varname, setindex!!, update!!, getindex_internal\n\njulia> vnv = VarNamedVector();\n\njulia> vnv = setindex!!(vnv, [0.0, 0.0, 0.0, 0.0], @varname(x));\n\njulia> vnv = setindex!!(vnv, reshape(1:6, (2,3)), @varname(y));\n\njulia> vnv.vals\n10-element Vector{Real}:\n 0.0\n 0.0\n 0.0\n 0.0\n 1\n 2\n 3\n 4\n 5\n 6\n\nThe varnames, ranges, and varname_to_index fields keep track of which value belongs to which variable. The transforms field stores the transformations that needed to transform the vectorised internal storage back to its original form:\n\njulia> vnv.transforms[vnv.varname_to_index[@varname(y)]] == DynamicPPL.ReshapeTransform((6,), (2,3))\ntrue\n\nIf a variable is updated with a new value that is of a smaller dimension than the old value, rather than resizing vnv.vals, some elements in vnv.vals are marked as inactive.\n\njulia> vnv = update!!(vnv, [46.0, 48.0], @varname(x));\n\njulia> vnv.vals\n10-element Vector{Real}:\n 46.0\n 48.0\n  0.0\n  0.0\n  1\n  2\n  3\n  4\n  5\n  6\n\njulia> println(vnv.num_inactive);\nDict(1 => 2)\n\nThis helps avoid unnecessary memory allocations for values that repeatedly change dimension. The user does not have to worry about the inactive entries as long as they use functions like setindex! and getindex! rather than directly accessing vnv.vals.\n\njulia> vnv[@varname(x)]\n2-element Vector{Real}:\n 46.0\n 48.0\n\njulia> getindex_internal(vnv, :)\n8-element Vector{Real}:\n 46.0\n 48.0\n  1\n  2\n  3\n  4\n  5\n  6\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "internals/varinfo/#DynamicPPL.has_inactive",
    "href": "../DynamicPPL.jl/internals/varinfo/#DynamicPPL.has_inactive",
    "title": "DynamicPPL.has_inactive",
    "section": "function",
    "text": "has_inactive(vnv::VarNamedVector)\n\nReturns true if vnv has inactive entries.\n\nSee also: num_inactive\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "internals/varinfo/#DynamicPPL.num_inactive",
    "href": "../DynamicPPL.jl/internals/varinfo/#DynamicPPL.num_inactive",
    "title": "DynamicPPL.num_inactive",
    "section": "function",
    "text": "num_inactive(vnv::VarNamedVector)\n\nReturn the number of inactive entries in vnv.\n\nSee also: has_inactive, num_allocated\n\n\n\n\n\nnum_inactive(vnv::VarNamedVector, vn::VarName)\n\nReturns the number of inactive entries for vn in vnv.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "internals/varinfo/#DynamicPPL.num_allocated",
    "href": "../DynamicPPL.jl/internals/varinfo/#DynamicPPL.num_allocated",
    "title": "DynamicPPL.num_allocated",
    "section": "function",
    "text": "num_allocated(vnv::VarNamedVector)\nnum_allocated(vnv::VarNamedVector[, vn::VarName])\nnum_allocated(vnv::VarNamedVector[, idx::Int])\n\nReturn the number of allocated entries in vnv, both active and inactive.\n\nIf either a VarName or an Int index is specified, only count entries allocated for that variable.\n\nAllocated entries take up memory in vnv.vals, but, if inactive, may not currently hold any meaningful data. One can remove them with contiguify!, but doing so may cause more memory allocations in the future if variables change dimension.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "internals/varinfo/#DynamicPPL.is_contiguous",
    "href": "../DynamicPPL.jl/internals/varinfo/#DynamicPPL.is_contiguous",
    "title": "DynamicPPL.is_contiguous",
    "section": "function",
    "text": "is_contiguous(vnv::VarNamedVector)\n\nReturns true if the underlying data of vnv is stored in a contiguous array.\n\nThis is equivalent to negating has_inactive(vnv).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "internals/varinfo/#DynamicPPL.contiguify!",
    "href": "../DynamicPPL.jl/internals/varinfo/#DynamicPPL.contiguify!",
    "title": "DynamicPPL.contiguify!",
    "section": "function",
    "text": "contiguify!(vnv::VarNamedVector)\n\nRe-contiguify the underlying vector and shrink if possible.\n\nExamples\n\njulia> using DynamicPPL: VarNamedVector, @varname, contiguify!, update!, has_inactive\n\njulia> vnv = VarNamedVector(@varname(x) => [1.0, 2.0, 3.0], @varname(y) => [3.0]);\n\njulia> update!(vnv, [23.0, 24.0], @varname(x));\n\njulia> has_inactive(vnv)\ntrue\n\njulia> length(vnv.vals)\n4\n\njulia> contiguify!(vnv);\n\njulia> has_inactive(vnv)\nfalse\n\njulia> length(vnv.vals)\n3\n\njulia> vnv[@varname(x)]  # All the values are still there.\n2-element Vector{Float64}:\n 23.0\n 24.0\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "internals/varinfo/#DynamicPPL.replace_raw_storage-Tuple{DynamicPPL.VarNamedVector, AbstractVector}",
    "href": "../DynamicPPL.jl/internals/varinfo/#DynamicPPL.replace_raw_storage-Tuple{DynamicPPL.VarNamedVector, AbstractVector}",
    "title": "DynamicPPL.replace_raw_storage",
    "section": "method",
    "text": "replace_raw_storage(vnv::VarNamedVector, vals::AbstractVector)\n\nReplace the values in vnv with vals, as they are stored internally.\n\nThis is useful when we want to update the entire underlying vector of values in one go or if we want to change the how the values are stored, e.g. alter the eltype.\n\nwarning: Warning\nThis replaces the raw underlying values, and so care should be taken when using this function. For example, if vnv has any inactive entries, then the provided vals should also contain the inactive entries to avoid unexpected behavior.\n\nExamples\n\njulia> using DynamicPPL: VarNamedVector, replace_raw_storage\n\njulia> vnv = VarNamedVector(@varname(x) => [1.0]);\n\njulia> replace_raw_storage(vnv, [2.0])[@varname(x)] == [2.0]\ntrue\n\nThis is also useful when we want to differentiate wrt. the values using automatic differentiation, e.g. ForwardDiff.jl.\n\njulia> using ForwardDiff: ForwardDiff\n\njulia> f(x) = sum(abs2, replace_raw_storage(vnv, x)[@varname(x)])\nf (generic function with 1 method)\n\njulia> ForwardDiff.gradient(f, [1.0])\n1-element Vector{Float64}:\n 2.0\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "internals/varinfo/#DynamicPPL.values_as-Tuple{DynamicPPL.VarNamedVector}-internals-varinfo",
    "href": "../DynamicPPL.jl/internals/varinfo/#DynamicPPL.values_as-Tuple{DynamicPPL.VarNamedVector}-internals-varinfo",
    "title": "DynamicPPL.values_as",
    "section": "method",
    "text": "values_as(vnv::VarNamedVector[, T])\n\nReturn the values/realizations in vnv as type T, if implemented.\n\nIf no type T is provided, return values as stored in vnv.\n\nExamples\n\njulia> using DynamicPPL: VarNamedVector\n\njulia> vnv = VarNamedVector(@varname(x) => 1, @varname(y) => [2.0]);\n\njulia> values_as(vnv) == [1.0, 2.0]\ntrue\n\njulia> values_as(vnv, Vector{Float32}) == Vector{Float32}([1.0, 2.0])\ntrue\n\njulia> values_as(vnv, OrderedDict) == OrderedDict(@varname(x) => 1.0, @varname(y) => [2.0])\ntrue\n\njulia> values_as(vnv, NamedTuple) == (x = 1.0, y = [2.0])\ntrue\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#API",
    "href": "../DynamicPPL.jl/api/#API",
    "title": "API",
    "section": "section",
    "text": "Part of the API of DynamicPPL is defined in the more lightweight interface package AbstractPPL.jl and reexported here.",
    "crumbs": null
  },
  {
    "objectID": "api/#Model",
    "href": "../DynamicPPL.jl/api/#Model",
    "title": "Model",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "api/#Macros",
    "href": "../DynamicPPL.jl/api/#Macros",
    "title": "Macros",
    "section": "section",
    "text": "A core component of DynamicPPL is the @model macro. It can be used to define probabilistic models in an intuitive way by specifying random variables and their distributions with ~ statements. These statements are rewritten by @model as calls of internal functions for sampling the variables and computing their log densities.",
    "crumbs": null
  },
  {
    "objectID": "api/#Type",
    "href": "../DynamicPPL.jl/api/#Type",
    "title": "Type",
    "section": "section",
    "text": "A Model can be created by calling the model function, as defined by @model.\n\nModels are callable structs.\n\nBasic properties of a model can be accessed with getargnames, getmissings, and nameof.\n\nThe context of a model can be set using contextualize:\n\nSome models require threadsafe evaluation (see the Turing docs for more information on when this is necessary). If this is the case, one must enable threadsafe evaluation for a model:",
    "crumbs": null
  },
  {
    "objectID": "api/#Evaluation",
    "href": "../DynamicPPL.jl/api/#Evaluation",
    "title": "Evaluation",
    "section": "section",
    "text": "With rand one can draw samples from the prior distribution of a Model.\n\nOne can also evaluate the log prior, log likelihood, and log joint probability.",
    "crumbs": null
  },
  {
    "objectID": "api/#LogDensityProblems.jl-interface",
    "href": "../DynamicPPL.jl/api/#LogDensityProblems.jl-interface",
    "title": "LogDensityProblems.jl interface",
    "section": "section",
    "text": "The LogDensityProblems.jl interface is also supported by wrapping a Model in a DynamicPPL.LogDensityFunction.\n\nInternally, this is accomplished using init!! on:",
    "crumbs": null
  },
  {
    "objectID": "api/#Condition-and-decondition",
    "href": "../DynamicPPL.jl/api/#Condition-and-decondition",
    "title": "Condition and decondition",
    "section": "section",
    "text": "A Model can be conditioned on a set of observations with AbstractPPL.condition or its alias |.\n\nSimilarly, one can specify with AbstractPPL.decondition that certain, or all, random variables are not observed.",
    "crumbs": null
  },
  {
    "objectID": "api/#Fixing-and-unfixing",
    "href": "../DynamicPPL.jl/api/#Fixing-and-unfixing",
    "title": "Fixing and unfixing",
    "section": "section",
    "text": "We can also fix a collection of variables in a Model to certain values using DynamicPPL.fix.\n\nThis is quite similar to the aforementioned condition and its siblings, but they are indeed different operations:\n\nconditioned variables are considered to be observations, and are thus included in the computation logjoint and loglikelihood, but not in logprior.\nfixed variables are considered to be constant, and are thus not included in any log-probability computations.\n\nThe differences are more clearly spelled out in the docstring of DynamicPPL.fix below.\n\nThe difference between DynamicPPL.fix and DynamicPPL.condition is described in the docstring of DynamicPPL.fix above.\n\nSimilarly, we can revert this with DynamicPPL.unfix, i.e. return the variables to their original meaning:",
    "crumbs": null
  },
  {
    "objectID": "api/#Predicting",
    "href": "../DynamicPPL.jl/api/#Predicting",
    "title": "Predicting",
    "section": "section",
    "text": "DynamicPPL provides functionality for generating samples from the posterior predictive distribution through the predict function. This allows you to use posterior parameter samples to generate predictions for unobserved data points.\n\nThe predict function has two main methods:\n\nFor AbstractVector{<:AbstractVarInfo} - useful when you have a collection of VarInfo objects representing posterior samples.\nFor MCMCChains.Chains (only available when MCMCChains.jl is loaded) - useful when you have posterior samples in the form of an MCMCChains.Chains object.",
    "crumbs": null
  },
  {
    "objectID": "api/#Basic-Usage",
    "href": "../DynamicPPL.jl/api/#Basic-Usage",
    "title": "Basic Usage",
    "section": "section",
    "text": "The typical workflow for posterior prediction involves:\n\nFitting a model to observed data to obtain posterior samples\nCreating a new model instance with some variables marked as missing (unobserved)\nUsing predict to generate samples for these missing variables based on the posterior parameter samples\n\nWhen using predict with MCMCChains.Chains, you can control which variables are included in the output with the include_all parameter:\n\ninclude_all=false (default): Include only newly predicted variables\ninclude_all=true: Include both parameters from the original chain and predicted variables",
    "crumbs": null
  },
  {
    "objectID": "api/#Marginalisation",
    "href": "../DynamicPPL.jl/api/#Marginalisation",
    "title": "Marginalisation",
    "section": "section",
    "text": "DynamicPPL provides the marginalize function to marginalise out variables from a model. This requires MarginalLogDensities.jl to be loaded in your environment.\n\nA MarginalLogDensity object acts as a function which maps non-marginalised parameter values to a marginal log-probability. To retrieve a VarInfo object from it, you can use:",
    "crumbs": null
  },
  {
    "objectID": "api/#Models-within-models",
    "href": "../DynamicPPL.jl/api/#Models-within-models",
    "title": "Models within models",
    "section": "section",
    "text": "One can include models and call another model inside the model function with left ~ to_submodel(model).\n\nNote that a [to_submodel](@ref) is only sampleable; one cannot compute logpdf for its realizations.\n\nIn the context of including models within models, it's also useful to prefix the variables in sub-models to avoid variable names clashing:",
    "crumbs": null
  },
  {
    "objectID": "api/#Utilities",
    "href": "../DynamicPPL.jl/api/#Utilities",
    "title": "Utilities",
    "section": "section",
    "text": "typed_identity is the same as identity, but with an overload for with_logabsdet_jacobian that ensures that it never errors.\n\nIt is possible to manually increase (or decrease) the accumulated log likelihood or prior from within a model function.\n\nReturn values of the model function can be obtained with returned(model, sample), where sample is either a MCMCChains.Chains object (which represents a collection of samples), or a single sample represented as a NamedTuple or a dictionary of VarNames.\n\nFor a chain of samples, one can compute the pointwise log-likelihoods of each observed random variable with pointwise_loglikelihoods. Similarly, the log-densities of the priors using pointwise_prior_logdensities or both, i.e. all variables, using pointwise_logdensities.\n\nFor converting a chain into a format that can more easily be fed into a Model again, for example using condition, you can use value_iterator_from_chain.\n\nSometimes it can be useful to extract the priors of a model. This is the possible using extract_priors.\n\nSafe extraction of values from a given AbstractVarInfo as they are seen in the model can be done using values_as_in_model.",
    "crumbs": null
  },
  {
    "objectID": "api/#AD-testing-and-benchmarking-utilities",
    "href": "../DynamicPPL.jl/api/#AD-testing-and-benchmarking-utilities",
    "title": "AD testing and benchmarking utilities",
    "section": "section",
    "text": "To test and/or benchmark the performance of an AD backend on a model, DynamicPPL provides the following utilities:\n\nThe default test setting is to compare against ForwardDiff. You can have more fine-grained control over how to test the AD backend using the following types:\n\nThese are returned / thrown by the run_ad function:",
    "crumbs": null
  },
  {
    "objectID": "api/#Demo-models",
    "href": "../DynamicPPL.jl/api/#Demo-models",
    "title": "Demo models",
    "section": "section",
    "text": "DynamicPPL provides several demo models in the DynamicPPL.TestUtils submodule.\n\nFor every demo model, one can define the true log prior, log likelihood, and log joint probabilities.\n\nAnd in the case where the model includes constrained variables, it can also be useful to define\n\nFinally, the following methods can also be of use:",
    "crumbs": null
  },
  {
    "objectID": "api/#Debugging-Utilities",
    "href": "../DynamicPPL.jl/api/#Debugging-Utilities",
    "title": "Debugging Utilities",
    "section": "section",
    "text": "DynamicPPL provides a few methods for checking validity of a model-definition.\n\nAnd some which might be useful to determine certain properties of the model based on the debug trace.\n\nFor determining whether one might have type instabilities in the model, the following can be useful\n\nInterally, the type-checking methods make use of the following method for construction of the call with the argument types:",
    "crumbs": null
  },
  {
    "objectID": "api/#Advanced",
    "href": "../DynamicPPL.jl/api/#Advanced",
    "title": "Advanced",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "api/#Variable-names",
    "href": "../DynamicPPL.jl/api/#Variable-names",
    "title": "Variable names",
    "section": "section",
    "text": "Names and possibly nested indices of variables are described with AbstractPPL.VarName. They can be defined with AbstractPPL.@varname. Please see the documentation of AbstractPPL.jl for further information.",
    "crumbs": null
  },
  {
    "objectID": "api/#Data-Structures-of-Variables",
    "href": "../DynamicPPL.jl/api/#Data-Structures-of-Variables",
    "title": "Data Structures of Variables",
    "section": "section",
    "text": "DynamicPPL provides different data structures used in for storing samples and accumulation of the log-probabilities, all of which are subtypes of AbstractVarInfo.\n\nBut exactly how a AbstractVarInfo stores this information can vary.",
    "crumbs": null
  },
  {
    "objectID": "api/#VarInfo",
    "href": "../DynamicPPL.jl/api/#VarInfo",
    "title": "VarInfo",
    "section": "section",
    "text": "One main characteristic of VarInfo is that samples are transformed to unconstrained Euclidean space and stored in a linearized form, as described in the main Turing documentation. The Transformations section below describes the methods used for this. In the specific case of VarInfo, it keeps track of whether samples have been transformed by setting flags on them, using the following functions.",
    "crumbs": null
  },
  {
    "objectID": "api/#SimpleVarInfo",
    "href": "../DynamicPPL.jl/api/#SimpleVarInfo",
    "title": "SimpleVarInfo",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "api/#Accumulators",
    "href": "../DynamicPPL.jl/api/#Accumulators",
    "title": "Accumulators",
    "section": "section",
    "text": "The subtypes of AbstractVarInfo store the cumulative log prior and log likelihood, and sometimes other variables that change during executing, in what are called accumulators.\n\nDynamicPPL provides the following default accumulators.",
    "crumbs": null
  },
  {
    "objectID": "api/#Common-API",
    "href": "../DynamicPPL.jl/api/#Common-API",
    "title": "Common API",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "api/#Accumulation-of-log-probabilities",
    "href": "../DynamicPPL.jl/api/#Accumulation-of-log-probabilities",
    "title": "Accumulation of log-probabilities",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "api/#Variables-and-their-realizations",
    "href": "../DynamicPPL.jl/api/#Variables-and-their-realizations",
    "title": "Variables and their realizations",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "api/#Transformations",
    "href": "../DynamicPPL.jl/api/#Transformations",
    "title": "Transformations",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "api/#Utils",
    "href": "../DynamicPPL.jl/api/#Utils",
    "title": "Utils",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "api/#Evaluation-Contexts",
    "href": "../DynamicPPL.jl/api/#Evaluation-Contexts",
    "title": "Evaluation Contexts",
    "section": "section",
    "text": "Internally, model evaluation is performed with AbstractPPL.evaluate!!.\n\nThis method mutates the varinfo used for execution. By default, it does not perform any actual sampling: it only evaluates the model using the values of the variables that are already in the varinfo. If you wish to sample new values, see the section on VarInfo initialisation just below this.\n\nThe behaviour of a model execution can be changed with evaluation contexts, which are a field of the model.\n\nAll contexts are subtypes of AbstractPPL.AbstractContext.\n\nContexts are split into two kinds:\n\nLeaf contexts: These are the most important contexts as they ultimately decide how model evaluation proceeds. For example, DefaultContext evaluates the model using values stored inside a VarInfo's metadata, whereas InitContext obtains new values either by sampling or from a known set of parameters. DynamicPPL has more leaf contexts which are used for internal purposes, but these are the two that are exported.\n\nTo implement a leaf context, you need to subtype AbstractPPL.AbstractContext and implement the tilde_assume!! and tilde_observe!! methods for your context.\n\nParent contexts: These essentially act as 'modifiers' for leaf contexts. For example, PrefixContext adds a prefix to all variable names during evaluation, while ConditionContext marks certain variables as observed.\n\nTo implement a parent context, you have to subtype DynamicPPL.AbstractParentContext, and implement the childcontext and setchildcontext methods. If needed, you can also implement tilde_assume!! and tilde_observe!! for your context. This is optional; the default implementation is to simply delegate to the child context.\n\nSince contexts form a tree structure, these functions are automatically defined for manipulating context stacks. They are mainly useful for modifying the fundamental behaviour (i.e. the leaf context), without affecting any of the modifiers (i.e. parent contexts).",
    "crumbs": null
  },
  {
    "objectID": "api/#VarInfo-initialisation",
    "href": "../DynamicPPL.jl/api/#VarInfo-initialisation",
    "title": "VarInfo initialisation",
    "section": "section",
    "text": "The function init!! is used to initialise, or overwrite, values in a VarInfo. It is really a thin wrapper around using evaluate!! with an InitContext.\n\nTo accomplish this, an initialisation strategy is required, which defines how new values are to be obtained. There are three concrete strategies provided in DynamicPPL:\n\nIf you wish to write your own, you have to subtype DynamicPPL.AbstractInitStrategy and implement the init method. In very rare situations, you may also need to implement get_param_eltype, which defines the element type of the parameters generated by the strategy.",
    "crumbs": null
  },
  {
    "objectID": "api/#Choosing-a-suitable-VarInfo",
    "href": "../DynamicPPL.jl/api/#Choosing-a-suitable-VarInfo",
    "title": "Choosing a suitable VarInfo",
    "section": "section",
    "text": "There is also the experimental DynamicPPL.Experimental.determine_suitable_varinfo, which uses static checking via JET.jl to determine whether one should use DynamicPPL.typed_varinfo or DynamicPPL.untyped_varinfo, depending on which supports the model:",
    "crumbs": null
  },
  {
    "objectID": "api/#Converting-VarInfos-to/from-chains",
    "href": "../DynamicPPL.jl/api/#Converting-VarInfos-to/from-chains",
    "title": "Converting VarInfos to/from chains",
    "section": "section",
    "text": "It is a fairly common operation to want to convert a collection of VarInfo objects into a chains object for downstream analysis.\n\nThis can be accomplished by first converting each VarInfo into a ParamsWithStats object:\n\nOnce you have a matrix of these, you can convert them into a chains object using:\n\nIf you only have a vector you can use hcat to convert it into an N×1 matrix first.\n\nFurthermore, one can convert chains back into a collection of parameter dictionaries and/or stats with:\n\nWith these, you can (for example) extract the parameter dictionaries and use InitFromParams to re-evaluate a model at each point in the chain.\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.@model",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.@model",
    "title": "DynamicPPL.@model",
    "section": "macro",
    "text": "@model(expr[, warn = false])\n\nMacro to specify a probabilistic model.\n\nIf warn is true, a warning is displayed if internal variable names are used in the model definition.\n\nExamples\n\nModel definition:\n\n@model function model(x, y = 42)\n    ...\nend\n\nTo generate a Model, call model(xvalue) or model(xvalue, yvalue).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.Model",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.Model",
    "title": "DynamicPPL.Model",
    "section": "type",
    "text": "struct Model{F,argnames,defaultnames,missings,Targs,Tdefaults,Ctx<:AbstractContext,Threaded}\n    f::F\n    args::NamedTuple{argnames,Targs}\n    defaults::NamedTuple{defaultnames,Tdefaults}\n    context::Ctx=DefaultContext()\nend\n\nA Model struct with model evaluation function of type F, arguments of names argnames types Targs, default arguments of names defaultnames with types Tdefaults, missing arguments missings, and evaluation context of type Ctx.\n\nHere argnames, defaultargnames, and missings are tuples of symbols, e.g. (:a, :b). context is by default DefaultContext().\n\nAn argument with a type of Missing will be in missings by default. However, in non-traditional use-cases missings can be defined differently. All variables in missings are treated as random variables rather than observations.\n\nThe Threaded type parameter indicates whether the model requires threadsafe evaluation (i.e., whether the model contains statements which modify the internal VarInfo that are executed in parallel). By default, this is set to false.\n\nThe default arguments are used internally when constructing instances of the same model with different arguments.\n\nExamples\n\njulia> Model(f, (x = 1.0, y = 2.0))\nModel{typeof(f),(:x, :y),(),(),Tuple{Float64,Float64},Tuple{}}(f, (x = 1.0, y = 2.0), NamedTuple())\n\njulia> Model(f, (x = 1.0, y = 2.0), (x = 42,))\nModel{typeof(f),(:x, :y),(:x,),(),Tuple{Float64,Float64},Tuple{Int64}}(f, (x = 1.0, y = 2.0), (x = 42,))\n\njulia> Model{(:y,)}(f, (x = 1.0, y = 2.0), (x = 42,)) # with special definition of missings\nModel{typeof(f),(:x, :y),(:x,),(:y,),Tuple{Float64,Float64},Tuple{Int64}}(f, (x = 1.0, y = 2.0), (x = 42,))\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.Model-Tuple{}",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.Model-Tuple{}",
    "title": "DynamicPPL.Model",
    "section": "method",
    "text": "(model::Model)([rng, varinfo])\n\nSample from the prior of the model with random number generator rng.\n\nReturns the model's return value.\n\nNote that calling this with an existing varinfo object will mutate it.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Base.nameof-Tuple{Model}",
    "href": "../DynamicPPL.jl/api/#Base.nameof-Tuple{Model}",
    "title": "Base.nameof",
    "section": "method",
    "text": "nameof(model::Model)\n\nGet the name of the model as Symbol.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.getargnames",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.getargnames",
    "title": "DynamicPPL.getargnames",
    "section": "function",
    "text": "getargnames(model::Model)\n\nGet a tuple of the argument names of the model.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.getmissings",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.getmissings",
    "title": "DynamicPPL.getmissings",
    "section": "function",
    "text": "getmissings(model::Model)\n\nGet a tuple of the names of the missing arguments of the model.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.contextualize",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.contextualize",
    "title": "DynamicPPL.contextualize",
    "section": "function",
    "text": "contextualize(model::Model, context::AbstractContext)\n\nReturn a new Model with the same evaluation function and other arguments, but with its underlying context set to context.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.setthreadsafe",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.setthreadsafe",
    "title": "DynamicPPL.setthreadsafe",
    "section": "function",
    "text": "setthreadsafe(model::Model, threadsafe::Bool)\n\nReturns a new Model with its threadsafe flag set to threadsafe.\n\nThreadsafe evaluation ensures correctness when executing model statements that mutate the internal VarInfo object in parallel. For example, this is needed if tilde-statements are nested inside Threads.@threads or similar constructs.\n\nIt is not needed for generic multithreaded operations that don't involve VarInfo. For example, calculating a log-likelihood term in parallel and then calling @addlogprob! outside of the parallel region is safe without needing to set threadsafe=true.\n\nIt is also not needed for multithreaded sampling with AbstractMCMC's MCMCThreads().\n\nSetting threadsafe to true increases the overhead in evaluating the model. Please see the Turing.jl docs for more details.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.requires_threadsafe",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.requires_threadsafe",
    "title": "DynamicPPL.requires_threadsafe",
    "section": "function",
    "text": "requires_threadsafe(model::Model)\n\nReturn whether model has been marked as needing threadsafe evaluation (using setthreadsafe).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Base.rand",
    "href": "../DynamicPPL.jl/api/#Base.rand",
    "title": "Base.rand",
    "section": "function",
    "text": "rand([rng=Random.default_rng()], [T=NamedTuple], model::Model)\n\nGenerate a sample of type T from the prior distribution of the model.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.logprior",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.logprior",
    "title": "DynamicPPL.logprior",
    "section": "function",
    "text": "logprior(model::Model, varinfo::AbstractVarInfo)\n\nReturn the log prior probability of variables varinfo for the probabilistic model.\n\nNote that this probability always refers to the parameters in unlinked space, i.e., the return value of logprior does not depend on whether VarInfo has been linked or not.\n\nSee also logjoint and loglikelihood.\n\n\n\n\n\nlogprior(model::Model, θ::Union{NamedTuple,AbstractDict})\n\nReturn the log prior probability of variables θ for the probabilistic model.\n\nSee also logjoint and loglikelihood.\n\nExamples\n\njulia> @model function demo(x)\n           m ~ Normal()\n           for i in eachindex(x)\n               x[i] ~ Normal(m, 1.0)\n           end\n       end\ndemo (generic function with 2 methods)\n\njulia> # Using a `NamedTuple`.\n       logprior(demo([1.0]), (m = 100.0, ))\n-5000.918938533205\n\njulia> # Using a `OrderedDict`.\n       logprior(demo([1.0]), OrderedDict(@varname(m) => 100.0))\n-5000.918938533205\n\njulia> # Truth.\n       logpdf(Normal(), 100.0)\n-5000.918938533205\n\n\n\n\n\nlogprior(model::DynamicPPL.Model, chain::MCMCChains.Chains)\n\nReturn an array of log prior probabilities evaluated at each sample in an MCMC chain.\n\nExamples\n\njulia> using MCMCChains, Distributions\n\njulia> @model function demo_model(x)\n           s ~ InverseGamma(2, 3)\n           m ~ Normal(0, sqrt(s))\n           for i in eachindex(x)\n               x[i] ~ Normal(m, sqrt(s))\n           end\n       end;\n\njulia> # Construct a chain of samples using MCMCChains.\n       # This sets s = 0.5 and m = 1.0 for all three samples.\n       chain = Chains(repeat([0.5 1.0;;;], 3, 1, 1), [:s, :m]);\n\njulia> logprior(demo_model([1., 2.]), chain)\n3×1 Matrix{Float64}:\n -3.2956988239086447\n -3.2956988239086447\n -3.2956988239086447\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#StatsAPI.loglikelihood",
    "href": "../DynamicPPL.jl/api/#StatsAPI.loglikelihood",
    "title": "StatsAPI.loglikelihood",
    "section": "function",
    "text": "loglikelihood(model::Model, varinfo::AbstractVarInfo)\n\nReturn the log likelihood of variables varinfo for the probabilistic model.\n\nSee also logjoint and logprior.\n\n\n\n\n\nloglikelihood(model::Model, θ::Union{NamedTuple,AbstractDict})\n\nReturn the log likelihood of variables θ for the probabilistic model.\n\nSee also logjoint and logprior.\n\nExamples\n\njulia> @model function demo(x)\n           m ~ Normal()\n           for i in eachindex(x)\n               x[i] ~ Normal(m, 1.0)\n           end\n       end\ndemo (generic function with 2 methods)\n\njulia> # Using a `NamedTuple`.\n       loglikelihood(demo([1.0]), (m = 100.0, ))\n-4901.418938533205\n\njulia> # Using a `OrderedDict`.\n       loglikelihood(demo([1.0]), OrderedDict(@varname(m) => 100.0))\n-4901.418938533205\n\njulia> # Truth.\n       logpdf(Normal(100.0, 1.0), 1.0)\n-4901.418938533205\n\n\n\n\n\nloglikelihood(model::DynamicPPL.Model, chain::MCMCChains.Chains)\n\nReturn an array of log likelihoods evaluated at each sample in an MCMC chain.\n\nExamples\n\njulia> using MCMCChains, Distributions\n\njulia> @model function demo_model(x)\n           s ~ InverseGamma(2, 3)\n           m ~ Normal(0, sqrt(s))\n           for i in eachindex(x)\n               x[i] ~ Normal(m, sqrt(s))\n           end\n       end;\n\njulia> # Construct a chain of samples using MCMCChains.\n       # This sets s = 0.5 and m = 1.0 for all three samples.\n       chain = Chains(repeat([0.5 1.0;;;], 3, 1, 1), [:s, :m]);\n\njulia> loglikelihood(demo_model([1., 2.]), chain)\n3×1 Matrix{Float64}:\n -2.1447298858494\n -2.1447298858494\n -2.1447298858494\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.logjoint",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.logjoint",
    "title": "DynamicPPL.logjoint",
    "section": "function",
    "text": "logjoint(model::Model, varinfo::AbstractVarInfo)\n\nReturn the log joint probability of variables varinfo for the probabilistic model.\n\nNote that this probability always refers to the parameters in unlinked space, i.e., the return value of logjoint does not depend on whether VarInfo has been linked or not.\n\nSee logprior and loglikelihood.\n\n\n\n\n\nlogjoint(model::Model, θ::Union{NamedTuple,AbstractDict})\n\nReturn the log joint probability of variables θ for the probabilistic model.\n\nSee logprior and loglikelihood.\n\nExamples\n\njulia> @model function demo(x)\n           m ~ Normal()\n           for i in eachindex(x)\n               x[i] ~ Normal(m, 1.0)\n           end\n       end\ndemo (generic function with 2 methods)\n\njulia> # Using a `NamedTuple`.\n       logjoint(demo([1.0]), (m = 100.0, ))\n-9902.33787706641\n\njulia> # Using a `OrderedDict`.\n       logjoint(demo([1.0]), OrderedDict(@varname(m) => 100.0))\n-9902.33787706641\n\njulia> # Truth.\n       logpdf(Normal(100.0, 1.0), 1.0) + logpdf(Normal(), 100.0)\n-9902.33787706641\n\n\n\n\n\nlogjoint(model::Model, chain::MCMCChains.Chains)\n\nReturn an array of log joint probabilities evaluated at each sample in an MCMC chain.\n\nExamples\n\njulia> using MCMCChains, Distributions\n\njulia> @model function demo_model(x)\n           s ~ InverseGamma(2, 3)\n           m ~ Normal(0, sqrt(s))\n           for i in eachindex(x)\n               x[i] ~ Normal(m, sqrt(s))\n           end\n       end;\n\njulia> # Construct a chain of samples using MCMCChains.\n       # This sets s = 0.5 and m = 1.0 for all three samples.\n       chain = Chains(repeat([0.5 1.0;;;], 3, 1, 1), [:s, :m]);\n\njulia> logjoint(demo_model([1., 2.]), chain)\n3×1 Matrix{Float64}:\n -5.440428709758045\n -5.440428709758045\n -5.440428709758045\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.LogDensityFunction",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.LogDensityFunction",
    "title": "DynamicPPL.LogDensityFunction",
    "section": "type",
    "text": "DynamicPPL.LogDensityFunction(\n    model::Model,\n    getlogdensity::Function=getlogjoint_internal,\n    varinfo::AbstractVarInfo=VarInfo(model);\n    adtype::Union{ADTypes.AbstractADType,Nothing}=nothing,\n)\n\nA struct which contains a model, along with all the information necessary to:\n\ncalculate its log density at a given point;\nand if adtype is provided, calculate the gradient of the log density at that point.\n\nThis information can be extracted using the LogDensityProblems.jl interface, specifically, using LogDensityProblems.logdensity and LogDensityProblems.logdensity_and_gradient. If adtype is nothing, then only logdensity is implemented. If adtype is a concrete AD backend type, then logdensity_and_gradient is also implemented.\n\nThere are several options for getlogdensity that are 'supported' out of the box:\n\ngetlogjoint_internal: calculate the log joint, including the log-Jacobian term for any variables that have been linked in the provided VarInfo.\ngetlogprior_internal: calculate the log prior, including the log-Jacobian term for any variables that have been linked in the provided VarInfo.\ngetlogjoint: calculate the log joint in the model space, ignoring any effects of linking\ngetlogprior: calculate the log prior in the model space, ignoring any effects of linking\ngetloglikelihood: calculate the log likelihood (this is unaffected by linking, since transforms are only applied to random variables)\n\nnote: Note\nBy default, LogDensityFunction uses getlogjoint_internal, i.e., the result of LogDensityProblems.logdensity(f, x) will depend on whether the LogDensityFunction was created with a linked or unlinked VarInfo. This is done primarily to ease interoperability with MCMC samplers.\n\nIf you provide one of these functions, a VarInfo will be automatically created for you. If you provide a different function, you have to manually create a VarInfo and pass it as the third argument.\n\nIf the adtype keyword argument is provided, then this struct will also store the adtype along with other information for efficient calculation of the gradient of the log density. Note that preparing a LogDensityFunction with an AD type AutoBackend() requires the AD backend itself to have been loaded (e.g. with import Backend).\n\nFields\n\nNote that it is undefined behaviour to access any of a LogDensityFunction's fields, apart from:\n\nldf.model: The original model from which this LogDensityFunction was constructed.\nldf.adtype: The AD type used for gradient calculations, or nothing if no AD type was provided.\n\nExtended help\n\nUp until DynamicPPL v0.38, there have been two ways of evaluating a DynamicPPL model at a given set of parameters:\n\nWith unflatten + evaluate!! with DefaultContext: this stores a vector of parameters inside a VarInfo's metadata, then reads parameter values from the VarInfo during evaluation.\nWith InitFromParams: this reads parameter values from a NamedTuple or a Dict, and stores them inside a VarInfo's metadata.\n\nIn general, both of these approaches work fine, but the fact that they modify the VarInfo's metadata can often be quite wasteful. In particular, it is very common that the only outputs we care about from model evaluation are those which are stored in accumulators, such as log probability densities, or ValuesAsInModel.\n\nTo avoid this issue, we use OnlyAccsVarInfo, which is a VarInfo that only contains accumulators. It implements enough of the AbstractVarInfo interface to not error during model evaluation.\n\nBecause OnlyAccsVarInfo does not store any parameter values, when evaluating a model with it, it is mandatory that parameters are provided from outside the VarInfo, namely via InitContext.\n\nThe main problem that we face is that it is not possible to directly implement DynamicPPL.init(rng, vn, dist, strategy) for strategy::InitFromParams{<:AbstractVector}. In particular, it is not clear:\n\nwhich parts of the vector correspond to which random variables, and\nwhether the variables are linked or unlinked.\n\nTraditionally, this problem has been solved by unflatten, because that function would place values into the VarInfo's metadata alongside the information about ranges and linking. That way, when we evaluate with DefaultContext, we can read this information out again. However, we want to avoid using a metadata. Thus, here, we extract this information from the VarInfo a single time when constructing a LogDensityFunction object. Inside the LogDensityFunction, we store a mapping from VarNames to ranges in that vector, along with link status.\n\nFor VarNames with identity optics, this is stored in a NamedTuple for efficiency. For all other VarNames, this is stored in a Dict. The internal data structure used to represent this could almost certainly be optimised further. See e.g. the discussion in https://github.com/TuringLang/DynamicPPL.jl/issues/1116.\n\nWhen evaluating the model, this allows us to combine the parameter vector together with those ranges to create an InitFromParams{VectorWithRanges}, which lets us very quickly read parameter values from the vector.\n\nNote that this assumes that the ranges and link status are static throughout the lifetime of the LogDensityFunction object. Therefore, a LogDensityFunction object cannot handle models which have variable numbers of parameters, or models which may visit random variables in different orders depending on stochastic control flow. Indeed, silent errors may occur with such models. This is a general limitation of vectorised parameters: the original unflatten + evaluate!! approach also fails with such models.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.OnlyAccsVarInfo",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.OnlyAccsVarInfo",
    "title": "DynamicPPL.OnlyAccsVarInfo",
    "section": "type",
    "text": "OnlyAccsVarInfo\n\nThis is a wrapper around an AccumulatorTuple that implements the minimal AbstractVarInfo interface to work with the tilde_assume!! and tilde_observe!! functions for InitContext.\n\nNote that this does not implement almost every other AbstractVarInfo interface function, and so using this with a different leaf context such as DefaultContext will result in errors.\n\nConceptually, one can also think of this as a VarInfo that doesn't contain a metadata field. This is also why it only works with InitContext: in this case, the parameters used for evaluation are supplied by the context instead of the metadata.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Base.:|-Tuple{Model, Union{Tuple, AbstractDict{<:VarName}, NamedTuple}}",
    "href": "../DynamicPPL.jl/api/#Base.:|-Tuple{Model, Union{Tuple, AbstractDict{<:VarName}, NamedTuple}}",
    "title": "Base.:|",
    "section": "method",
    "text": "model | (x = 1.0, ...)\n\nReturn a Model which now treats variables on the right-hand side as observations.\n\nSee condition for more information and examples.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#AbstractPPL.condition",
    "href": "../DynamicPPL.jl/api/#AbstractPPL.condition",
    "title": "AbstractPPL.condition",
    "section": "function",
    "text": "condition(model::Model; values...)\ncondition(model::Model, values::NamedTuple)\n\nReturn a Model which now treats the variables in values as observations.\n\nSee also: decondition, conditioned\n\nLimitations\n\nThis does currently not work with variables that are provided to the model as arguments, e.g. @model function demo(x) ... end means that condition will not affect the variable x.\n\nTherefore if one wants to make use of condition and decondition one should not be specifying any random variables as arguments.\n\nThis is done for the sake of backwards compatibility.\n\nExamples\n\nSimple univariate model\n\njulia> using Distributions\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n           return (; m=m, x=x)\n       end\ndemo (generic function with 2 methods)\n\njulia> model = demo();\n\njulia> m, x = model(); (m ≠ 1.0 && x ≠ 100.0)\ntrue\n\njulia> # Create a new instance which treats `x` as observed\n       # with value `100.0`, and similarly for `m=1.0`.\n       conditioned_model = condition(model, x=100.0, m=1.0);\n\njulia> m, x = conditioned_model(); (m == 1.0 && x == 100.0)\ntrue\n\njulia> # Let's only condition on `x = 100.0`.\n       conditioned_model = condition(model, x = 100.0);\n\njulia> m, x =conditioned_model(); (m ≠ 1.0 && x == 100.0)\ntrue\n\njulia> # We can also use the nicer `|` syntax.\n       conditioned_model = model | (x = 100.0, );\n\njulia> m, x = conditioned_model(); (m ≠ 1.0 && x == 100.0)\ntrue\n\nThe above uses a NamedTuple to hold the conditioning variables, which allows us to perform some additional optimizations; in many cases, the above has zero runtime-overhead.\n\nBut we can also use a Dict, which offers more flexibility in the conditioning (see examples further below) but generally has worse performance than the NamedTuple approach:\n\njulia> conditioned_model_dict = condition(model, Dict(@varname(x) => 100.0));\n\njulia> m, x = conditioned_model_dict(); (m ≠ 1.0 && x == 100.0)\ntrue\n\njulia> # There's also an option using `|` by letting the right-hand side be a tuple\n       # with elements of type `Pair{<:VarName}`, i.e. `vn => value` with `vn isa VarName`.\n       conditioned_model_dict = model | (@varname(x) => 100.0, );\n\njulia> m, x = conditioned_model_dict(); (m ≠ 1.0 && x == 100.0)\ntrue\n\nCondition only a part of a multivariate variable\n\nNot only can be condition on multivariate random variables, but we can also use the standard mechanism of setting something to missing in the call to condition to only condition on a part of the variable.\n\njulia> @model function demo_mv(::Type{TV}=Float64) where {TV}\n           m = Vector{TV}(undef, 2)\n           m[1] ~ Normal()\n           m[2] ~ Normal()\n           return m\n       end\ndemo_mv (generic function with 4 methods)\n\njulia> model = demo_mv();\n\njulia> conditioned_model = condition(model, m = [missing, 1.0]);\n\njulia> # (✓) `m[1]` sampled while `m[2]` is fixed\n       m = conditioned_model(); (m[1] ≠ 1.0 && m[2] == 1.0)\ntrue\n\nIntuitively one might also expect to be able to write model | (m[1] = 1.0, ). Unfortunately this is not supported as it has the potential of increasing compilation times but without offering any benefit with respect to runtime:\n\njulia> # (×) `m[2]` is not set to 1.0.\n       m = condition(model, var\"m[2]\" = 1.0)(); m[2] == 1.0\nfalse\n\nBut you can do this if you use a Dict as the underlying storage instead:\n\njulia> # Alternatives:\n       # - `model | (@varname(m[2]) => 1.0,)`\n       # - `condition(model, Dict(@varname(m[2] => 1.0)))`\n       # (✓) `m[2]` is set to 1.0.\n       m = condition(model, @varname(m[2]) => 1.0)(); (m[1] ≠ 1.0 && m[2] == 1.0)\ntrue\n\nNested models\n\ncondition of course also supports the use of nested models through the use of to_submodel.\n\njulia> @model demo_inner() = m ~ Normal()\ndemo_inner (generic function with 2 methods)\n\njulia> @model function demo_outer()\n           # By default, `to_submodel` prefixes the variables using the left-hand side of `~`.\n           inner ~ to_submodel(demo_inner())\n           return inner\n       end\ndemo_outer (generic function with 2 methods)\n\njulia> model = demo_outer();\n\njulia> model() ≠ 1.0\ntrue\n\njulia> # To condition the variable inside `demo_inner` we need to refer to it as `inner.m`.\n       conditioned_model = model | (@varname(inner.m) => 1.0, );\n\njulia> conditioned_model()\n1.0\n\njulia> # However, it's not possible to condition `inner` directly.\n       conditioned_model_fail = model | (inner = 1.0, );\n\njulia> conditioned_model_fail()\nERROR: ArgumentError: `x ~ to_submodel(...)` is not supported when `x` is observed\n[...]\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.conditioned",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.conditioned",
    "title": "DynamicPPL.conditioned",
    "section": "function",
    "text": "conditioned(context::AbstractContext)\n\nReturn NamedTuple of values that are conditioned on under context`.\n\nNote that this will recursively traverse the context stack and return a merged version of the condition values.\n\n\n\n\n\nconditioned(model::Model)\n\nReturn the conditioned values in model.\n\nExamples\n\njulia> using Distributions\n\njulia> using DynamicPPL: conditioned, contextualize, PrefixContext, ConditionContext\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n       end\ndemo (generic function with 2 methods)\n\njulia> m = demo();\n\njulia> # Returns all the variables we have conditioned on + their values.\n       conditioned(condition(m, x=100.0, m=1.0))\n(x = 100.0, m = 1.0)\n\njulia> # Nested ones also work.\n       # (Note that `PrefixContext` also prefixes the variables of any\n       # ConditionContext that is _inside_ it; because of this, the type of the\n       # container has to be broadened to a `Dict`.)\n       cm = condition(contextualize(m, PrefixContext(@varname(a), ConditionContext((m=1.0,)))), x=100.0);\n\njulia> Set(keys(conditioned(cm))) == Set([@varname(a.m), @varname(x)])\ntrue\n\njulia> # Since we conditioned on `a.m`, it is not treated as a random variable.\n       # However, `a.x` will still be a random variable.\n       keys(VarInfo(cm))\n1-element Vector{VarName{:a, Accessors.PropertyLens{:x}}}:\n a.x\n\njulia> # We can also condition on `a.m` _outside_ of the PrefixContext:\n       cm = condition(contextualize(m, PrefixContext(@varname(a))), (@varname(a.m) => 1.0));\n\njulia> conditioned(cm)\nDict{VarName{:a, Accessors.PropertyLens{:m}}, Float64} with 1 entry:\n  a.m => 1.0\n\njulia> # Now `a.x` will be sampled.\n       keys(VarInfo(cm))\n1-element Vector{VarName{:a, Accessors.PropertyLens{:x}}}:\n a.x\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#AbstractPPL.decondition",
    "href": "../DynamicPPL.jl/api/#AbstractPPL.decondition",
    "title": "AbstractPPL.decondition",
    "section": "function",
    "text": "decondition(model::Model)\ndecondition(model::Model, variables...)\n\nReturn a Model for which variables... are not considered observations. If no variables are provided, then all variables currently considered observations will no longer be.\n\nThis is essentially the inverse of condition. This also means that it suffers from the same limitiations.\n\nNote that currently we only support variables to take on explicit values provided to condition.\n\nExamples\n\njulia> using Distributions\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n           return (; m=m, x=x)\n       end\ndemo (generic function with 2 methods)\n\njulia> conditioned_model = condition(demo(), m = 1.0, x = 10.0);\n\njulia> conditioned_model()\n(m = 1.0, x = 10.0)\n\njulia> # By specifying the `VarName` to `decondition`.\n       model = decondition(conditioned_model, @varname(m));\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\njulia> # When `NamedTuple` is used as the underlying, you can also provide\n       # the symbol directly (though the `@varname` approach is preferable if\n       # if the variable is known at compile-time).\n       model = decondition(conditioned_model, :m);\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\njulia> # `decondition` multiple at once:\n       (m, x) = decondition(model, :m, :x)(); (m ≠ 1.0 && x ≠ 10.0)\ntrue\n\njulia> # `decondition` without any symbols will `decondition` all variables.\n       (m, x) = decondition(model)(); (m ≠ 1.0 && x ≠ 10.0)\ntrue\n\njulia> # Usage of `Val` to perform `decondition` at compile-time if possible\n       # is also supported.\n       model = decondition(conditioned_model, Val{:m}());\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\nSimilarly when using a Dict:\n\njulia> conditioned_model_dict = condition(demo(), @varname(m) => 1.0, @varname(x) => 10.0);\n\njulia> conditioned_model_dict()\n(m = 1.0, x = 10.0)\n\njulia> deconditioned_model_dict = decondition(conditioned_model_dict, @varname(m));\n\njulia> (m, x) = deconditioned_model_dict(); m ≠ 1.0 && x == 10.0\ntrue\n\nBut, as mentioned, decondition is only supported for variables explicitly provided to condition earlier;\n\njulia> @model function demo_mv(::Type{TV}=Float64) where {TV}\n           m = Vector{TV}(undef, 2)\n           m[1] ~ Normal()\n           m[2] ~ Normal()\n           return m\n       end\ndemo_mv (generic function with 4 methods)\n\njulia> model = demo_mv();\n\njulia> conditioned_model = condition(model, @varname(m) => [1.0, 2.0]);\n\njulia> conditioned_model()\n2-element Vector{Float64}:\n 1.0\n 2.0\n\njulia> deconditioned_model = decondition(conditioned_model, @varname(m[1]));\n\njulia> deconditioned_model()  # (×) `m[1]` is still conditioned\n2-element Vector{Float64}:\n 1.0\n 2.0\n\njulia> # (✓) this works though\n       deconditioned_model_2 = deconditioned_model | (@varname(m[1]) => missing);\n\njulia> m = deconditioned_model_2(); (m[1] ≠ 1.0 && m[2] == 2.0)\ntrue\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.fix",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.fix",
    "title": "DynamicPPL.fix",
    "section": "function",
    "text": "fix([context::AbstractContext,] values::NamedTuple)\nfix([context::AbstractContext]; values...)\n\nReturn FixedContext with values and context if values is non-empty, otherwise return context which is DefaultContext by default.\n\nSee also: unfix\n\n\n\n\n\nfix(model::Model; values...)\nfix(model::Model, values::NamedTuple)\n\nReturn a Model which now treats the variables in values as fixed.\n\nSee also: unfix, fixed\n\nExamples\n\nSimple univariate model\n\njulia> using Distributions\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n           return (; m=m, x=x)\n       end\ndemo (generic function with 2 methods)\n\njulia> model = demo();\n\njulia> m, x = model(); (m ≠ 1.0 && x ≠ 100.0)\ntrue\n\njulia> # Create a new instance which treats `x` as observed\n       # with value `100.0`, and similarly for `m=1.0`.\n       fixed_model = fix(model, x=100.0, m=1.0);\n\njulia> m, x = fixed_model(); (m == 1.0 && x == 100.0)\ntrue\n\njulia> # Let's only fix on `x = 100.0`.\n       fixed_model = fix(model, x = 100.0);\n\njulia> m, x = fixed_model(); (m ≠ 1.0 && x == 100.0)\ntrue\n\nThe above uses a NamedTuple to hold the fixed variables, which allows us to perform some additional optimizations; in many cases, the above has zero runtime-overhead.\n\nBut we can also use a Dict, which offers more flexibility in the fixing (see examples further below) but generally has worse performance than the NamedTuple approach:\n\njulia> fixed_model_dict = fix(model, Dict(@varname(x) => 100.0));\n\njulia> m, x = fixed_model_dict(); (m ≠ 1.0 && x == 100.0)\ntrue\n\njulia> # Alternative: pass `Pair{<:VarName}` as positional argument.\n       fixed_model_dict = fix(model, @varname(x) => 100.0, );\n\njulia> m, x = fixed_model_dict(); (m ≠ 1.0 && x == 100.0)\ntrue\n\nFix only a part of a multivariate variable\n\nWe can not only fix multivariate random variables, but we can also use the standard mechanism of setting something to missing in the call to fix to only fix a part of the variable.\n\njulia> @model function demo_mv(::Type{TV}=Float64) where {TV}\n           m = Vector{TV}(undef, 2)\n           m[1] ~ Normal()\n           m[2] ~ Normal()\n           return m\n       end\ndemo_mv (generic function with 4 methods)\n\njulia> model = demo_mv();\n\njulia> fixed_model = fix(model, m = [missing, 1.0]);\n\njulia> # (✓) `m[1]` sampled while `m[2]` is fixed\n       m = fixed_model(); (m[1] ≠ 1.0 && m[2] == 1.0)\ntrue\n\nIntuitively one might also expect to be able to write something like fix(model, var\"m[1]\" = 1.0, ). Unfortunately this is not supported as it has the potential of increasing compilation times but without offering any benefit with respect to runtime:\n\njulia> # (×) `m[2]` is not set to 1.0.\n       m = fix(model, var\"m[2]\" = 1.0)(); m[2] == 1.0\nfalse\n\nBut you can do this if you use a Dict as the underlying storage instead:\n\njulia> # Alternative: `fix(model, Dict(@varname(m[2] => 1.0)))`\n       # (✓) `m[2]` is set to 1.0.\n       m = fix(model, @varname(m[2]) => 1.0)(); (m[1] ≠ 1.0 && m[2] == 1.0)\ntrue\n\nNested models\n\nfix of course also supports the use of nested models through the use of to_submodel, similar to condition.\n\njulia> @model demo_inner() = m ~ Normal()\ndemo_inner (generic function with 2 methods)\n\njulia> @model function demo_outer()\n           inner ~ to_submodel(demo_inner())\n           return inner\n       end\ndemo_outer (generic function with 2 methods)\n\njulia> model = demo_outer();\n\njulia> model() ≠ 1.0\ntrue\n\njulia> fixed_model = fix(model, (@varname(inner.m) => 1.0, ));\n\njulia> fixed_model()\n1.0\n\nHowever, unlike condition, fix can also be used to fix the return-value of the submodel:\n\njulia> fixed_model = fix(model, inner = 2.0,);\n\njulia> fixed_model()\n2.0\n\nDifference from condition\n\nA very similar functionality is also provided by condition. The only difference between fixing and conditioning is as follows:\n\nconditioned variables are considered to be observations, and are thus included in the computation logjoint and loglikelihood, but not in logprior.\nfixed variables are considered to be constant, and are thus not included in any log-probability computations.\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n           return (; m=m, x=x)\n       end\ndemo (generic function with 2 methods)\n\njulia> model = demo();\n\njulia> model_fixed = fix(model, m = 1.0);\n\njulia> model_conditioned = condition(model, m = 1.0);\n\njulia> logjoint(model_fixed, (x=1.0,))\n-0.9189385332046728\n\njulia> # Different!\n       logjoint(model_conditioned, (x=1.0,))\n-2.3378770664093453\n\njulia> # And the difference is the missing log-probability of `m`:\n       logjoint(model_fixed, (x=1.0,)) + logpdf(Normal(), 1.0) == logjoint(model_conditioned, (x=1.0,))\ntrue\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.fixed",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.fixed",
    "title": "DynamicPPL.fixed",
    "section": "function",
    "text": "fixed(context::AbstractContext)\n\nReturn the values that are fixed under context.\n\nNote that this will recursively traverse the context stack and return a merged version of the fix values.\n\n\n\n\n\nfixed(model::Model)\n\nReturn the fixed values in model.\n\nExamples\n\njulia> using Distributions\n\njulia> using DynamicPPL: fixed, contextualize, PrefixContext\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n       end\ndemo (generic function with 2 methods)\n\njulia> m = demo();\n\njulia> # Returns all the variables we have fixed on + their values.\n       fixed(fix(m, x=100.0, m=1.0))\n(x = 100.0, m = 1.0)\n\njulia> # The rest of this is the same as the `condition` example above.\n       cm = fix(contextualize(m, PrefixContext(@varname(a), fix(m=1.0))), x=100.0);\n\njulia> Set(keys(fixed(cm))) == Set([@varname(a.m), @varname(x)])\ntrue\n\njulia> keys(VarInfo(cm))\n1-element Vector{VarName{:a, Accessors.PropertyLens{:x}}}:\n a.x\n\njulia> # We can also condition on `a.m` _outside_ of the PrefixContext:\n       cm = fix(contextualize(m, PrefixContext(@varname(a))), (@varname(a.m) => 1.0));\n\njulia> fixed(cm)\nDict{VarName{:a, Accessors.PropertyLens{:m}}, Float64} with 1 entry:\n  a.m => 1.0\n\njulia> # Now `a.x` will be sampled.\n       keys(VarInfo(cm))\n1-element Vector{VarName{:a, Accessors.PropertyLens{:x}}}:\n a.x\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.unfix",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.unfix",
    "title": "DynamicPPL.unfix",
    "section": "function",
    "text": "unfix(context::AbstractContext, syms...)\n\nReturn context but with syms no longer fixed.\n\nNote that this recursively traverses contexts, unfixing all along the way.\n\nSee also: fix\n\n\n\n\n\nunfix(model::Model)\nunfix(model::Model, variables...)\n\nReturn a Model for which variables... are not considered fixed. If no variables are provided, then all variables currently considered fixed will no longer be.\n\nThis is essentially the inverse of fix. This also means that it suffers from the same limitiations.\n\nNote that currently we only support variables to take on explicit values provided to fix.\n\nExamples\n\njulia> using Distributions\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n           return (; m=m, x=x)\n       end\ndemo (generic function with 2 methods)\n\njulia> fixed_model = fix(demo(), m = 1.0, x = 10.0);\n\njulia> fixed_model()\n(m = 1.0, x = 10.0)\n\njulia> # By specifying the `VarName` to `unfix`.\n       model = unfix(fixed_model, @varname(m));\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\njulia> # When `NamedTuple` is used as the underlying, you can also provide\n       # the symbol directly (though the `@varname` approach is preferable if\n       # if the variable is known at compile-time).\n       model = unfix(fixed_model, :m);\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\njulia> # `unfix` multiple at once:\n       (m, x) = unfix(model, :m, :x)(); (m ≠ 1.0 && x ≠ 10.0)\ntrue\n\njulia> # `unfix` without any symbols will `unfix` all variables.\n       (m, x) = unfix(model)(); (m ≠ 1.0 && x ≠ 10.0)\ntrue\n\njulia> # Usage of `Val` to perform `unfix` at compile-time if possible\n       # is also supported.\n       model = unfix(fixed_model, Val{:m}());\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\nSimilarly when using a Dict:\n\njulia> fixed_model_dict = fix(demo(), @varname(m) => 1.0, @varname(x) => 10.0);\n\njulia> fixed_model_dict()\n(m = 1.0, x = 10.0)\n\njulia> unfixed_model_dict = unfix(fixed_model_dict, @varname(m));\n\njulia> (m, x) = unfixed_model_dict(); m ≠ 1.0 && x == 10.0\ntrue\n\nBut, as mentioned, unfix is only supported for variables explicitly provided to fix earlier:\n\njulia> @model function demo_mv(::Type{TV}=Float64) where {TV}\n           m = Vector{TV}(undef, 2)\n           m[1] ~ Normal()\n           m[2] ~ Normal()\n           return m\n       end\ndemo_mv (generic function with 4 methods)\n\njulia> model = demo_mv();\n\njulia> fixed_model = fix(model, @varname(m) => [1.0, 2.0]);\n\njulia> fixed_model()\n2-element Vector{Float64}:\n 1.0\n 2.0\n\njulia> unfixed_model = unfix(fixed_model, @varname(m[1]));\n\njulia> unfixed_model()  # (×) `m[1]` is still fixed\n2-element Vector{Float64}:\n 1.0\n 2.0\n\njulia> # (✓) this works though\n       unfixed_model_2 = fix(unfixed_model, @varname(m[1]) => missing);\n\njulia> m = unfixed_model_2(); (m[1] ≠ 1.0 && m[2] == 2.0)\ntrue\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#StatsAPI.predict",
    "href": "../DynamicPPL.jl/api/#StatsAPI.predict",
    "title": "StatsAPI.predict",
    "section": "function",
    "text": "predict([rng::AbstractRNG,] model::Model, chain::MCMCChains.Chains; include_all=false)\n\nSample from the posterior predictive distribution by executing model with parameters fixed to each sample in chain, and return the resulting Chains.\n\nThe model passed to predict is often different from the one used to generate chain. Typically, the model from which chain originated treats certain variables as observed (i.e., data points), while the model you pass to predict may mark these same variables as missing or unobserved. Calling predict then leverages the previously inferred parameter values to simulate what new, unobserved data might look like, given your posterior beliefs.\n\nFor each parameter configuration in chain:\n\nAll random variables present in chain are fixed to their sampled values.\nAny variables not included in chain are sampled from their prior distributions.\n\nIf include_all is false, the returned Chains will contain only those variables that were not fixed by the samples in chain. This is useful when you want to sample only new variables from the posterior predictive distribution.\n\nExamples\n\nusing AbstractMCMC, Distributions, DynamicPPL, Random\n\n@model function linear_reg(x, y, σ = 0.1)\n    β ~ Normal(0, 1)\n    for i in eachindex(y)\n        y[i] ~ Normal(β * x[i], σ)\n    end\nend\n\n# Generate synthetic chain using known ground truth parameter\nground_truth_β = 2.0\n\n# Create chain of samples from a normal distribution centered on ground truth\nβ_chain = MCMCChains.Chains(\n    rand(Normal(ground_truth_β, 0.002), 1000), [:β,]\n)\n\n# Generate predictions for two test points\nxs_test = [10.1, 10.2]\n\nm_train = linear_reg(xs_test, fill(missing, length(xs_test)))\n\npredictions = DynamicPPL.AbstractPPL.predict(\n    Random.default_rng(), m_train, β_chain\n)\n\nys_pred = vec(mean(Array(predictions); dims=1))\n\n# Check if predictions match expected values within tolerance\n(\n    isapprox(ys_pred[1], ground_truth_β * xs_test[1], atol = 0.01),\n    isapprox(ys_pred[2], ground_truth_β * xs_test[2], atol = 0.01)\n)\n\n# output\n\n(true, true)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.marginalize",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.marginalize",
    "title": "DynamicPPL.marginalize",
    "section": "function",
    "text": "marginalize(\n    model::DynamicPPL.Model,\n    marginalized_varnames::AbstractVector{<:VarName};\n    varinfo::DynamicPPL.AbstractVarInfo=link(VarInfo(model), model),\n    getlogprob=DynamicPPL.getlogjoint,\n    method::MarginalLogDensities.AbstractMarginalizer=MarginalLogDensities.LaplaceApprox();\n    kwargs...,\n)\n\nConstruct a MarginalLogDensities.MarginalLogDensity object that represents the marginal log-density of the given model, after marginalizing out the variables specified in varnames.\n\nThe resulting object can be called with a vector of parameter values to compute the marginal log-density.\n\nKeyword arguments\n\nvarinfo: The varinfo to use for the model. By default we use a linked VarInfo,  meaning that the resulting log-density function accepts parameters that have been  transformed to unconstrained space.\ngetlogprob: A function which specifies which kind of marginal log-density to compute.  Its default value is DynamicPPL.getlogjoint which returns the marginal log-joint  probability.\nmethod: The marginalization method; defaults to a Laplace approximation. Please see the  MarginalLogDensities.jl package  for other options.\nOther keyword arguments are passed to the MarginalLogDensities.MarginalLogDensity constructor.\n\nExample\n\njulia> using DynamicPPL, Distributions, MarginalLogDensities\n\njulia> @model function demo()\n           x ~ Normal(1.0)\n           y ~ Normal(2.0)\n       end\ndemo (generic function with 2 methods)\n\njulia> marginalized = marginalize(demo(), [:x]);\n\njulia> # The resulting callable computes the marginal log-density of `y`.\n       marginalized([1.0])\n-1.4189385332046727\n\njulia> logpdf(Normal(2.0), 1.0)\n-1.4189385332046727\n\nwarning: Warning\nThe default usage of linked VarInfo means that, for example, optimization of the marginal log-density can be performed in unconstrained space. However, care must be taken if the model contains variables where the link transformation depends on a marginalized variable. For example:@model function f()\n    x ~ Normal()\n    y ~ truncated(Normal(); lower=x)\nendHere, the support of y, and hence the link transformation used, depends on the value of x. If we now marginalize over x, we obtain a function mapping linked values of y to log-probabilities. However, it will not be possible to use DynamicPPL to correctly retrieve unlinked values of y.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.VarInfo-Tuple{MarginalLogDensities.MarginalLogDensity{<:DynamicPPLMarginalLogDensitiesExt.LogDensityFunctionWrapper}, Union{Nothing, AbstractVector}}",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.VarInfo-Tuple{MarginalLogDensities.MarginalLogDensity{<:DynamicPPLMarginalLogDensitiesExt.LogDensityFunctionWrapper}, Union{Nothing, AbstractVector}}",
    "title": "DynamicPPL.VarInfo",
    "section": "method",
    "text": "VarInfo(\n    mld::MarginalLogDensities.MarginalLogDensity{<:LogDensityFunctionWrapper},\n    unmarginalized_params::Union{AbstractVector,Nothing}=nothing\n)\n\nRetrieve the VarInfo object used in the marginalisation process.\n\nIf a Laplace approximation was used for the marginalisation, the values of the marginalized parameters are also set to their mode (note that this only happens if the mld object has been used to compute the marginal log-density at least once, so that the mode has been computed).\n\nIf a vector of unmarginalized_params is specified, the values for the corresponding parameters will also be updated in the returned VarInfo. This vector may be obtained e.g. by performing an optimization of the marginal log-density.\n\nAll other aspects of the VarInfo, such as link status, are preserved from the original VarInfo used in the marginalisation.\n\nnote: Note\nThe other fields of the VarInfo, e.g. accumulated log-probabilities, will not be updated. If you wish to have a fully consistent VarInfo, you should re-evaluate the model with the returned VarInfo (e.g. using vi = last(DynamicPPL.evaluate!!(model, vi))).\n\nExample\n\njulia> using DynamicPPL, Distributions, MarginalLogDensities\n\njulia> @model function demo()\n           x ~ Normal()\n           y ~ Beta(2, 2)\n       end\ndemo (generic function with 2 methods)\n\njulia> # Note that by default `marginalize` uses a linked VarInfo.\n       mld = marginalize(demo(), [@varname(x)]);\n\njulia> using MarginalLogDensities: Optimization, OptimizationOptimJL\n\njulia> # Find the mode of the marginal log-density of `y`, with an initial point of `y0`.\n       y0 = 2.0; opt_problem = Optimization.OptimizationProblem(mld, [y0])\nOptimizationProblem. In-place: true\nu0: 1-element Vector{Float64}:\n 2.0\n\njulia> # This tells us the optimal (linked) value of `y` is around 0.\n       opt_solution = Optimization.solve(opt_problem, OptimizationOptimJL.NelderMead())\nretcode: Success\nu: 1-element Vector{Float64}:\n 4.88281250001733e-5\n\njulia> # Get the VarInfo corresponding to the mode of `y`.\n       vi = VarInfo(mld, opt_solution.u);\n\njulia> # `x` is set to its mode (which for `Normal()` is zero).\n       vi[@varname(x)]\n0.0\n\njulia> # `y` is set to the optimal value we found above.\n       DynamicPPL.getindex_internal(vi, @varname(y))\n1-element Vector{Float64}:\n 4.88281250001733e-5\n\njulia> # To obtain values in the original constrained space, we can either\n       # use `getindex`:\n       vi[@varname(y)]\n0.5000122070312476\n\njulia> # Or invlink the entire VarInfo object using the model:\n       vi_unlinked = DynamicPPL.invlink(vi, demo()); vi_unlinked[:]\n2-element Vector{Float64}:\n 0.0\n 0.5000122070312476\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.to_submodel",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.to_submodel",
    "title": "DynamicPPL.to_submodel",
    "section": "function",
    "text": "to_submodel(model::Model[, auto_prefix::Bool])\n\nReturn a model wrapper indicating that it is a sampleable model over the return-values.\n\nThis is mainly meant to be used on the right-hand side of a ~ operator to indicate that the model can be sampled from but not necessarily evaluated for its log density.\n\nwarning: Warning\nNote that some other operations that one typically associate with expressions of the form left ~ right such as condition, will also not work with to_submodel.\n\nwarning: Warning\nTo avoid variable names clashing between models, it is recommended to leave the argument auto_prefix equal to true. If one does not use automatic prefixing, then it's recommended to use prefix(::Model, input) explicitly, i.e. to_submodel(prefix(model, @varname(my_prefix)))\n\nArguments\n\nmodel::Model: the model to wrap.\nauto_prefix::Bool: whether to automatically prefix the variables in the model using the left-hand   side of the ~ statement. Default: true.\n\nExamples\n\nSimple example\n\njulia> @model function demo1(x)\n           x ~ Normal()\n           return 1 + abs(x)\n       end;\n\njulia> @model function demo2(x, y)\n            a ~ to_submodel(demo1(x))\n            return y ~ Uniform(0, a)\n       end;\n\nWhen we sample from the model demo2(missing, 0.4) random variable x will be sampled:\n\njulia> vi = VarInfo(demo2(missing, 0.4));\n\njulia> @varname(a.x) in keys(vi)\ntrue\n\nThe variable a is not tracked. However, it will be assigned the return value of demo1, and can be used in subsequent lines of the model, as shown above.\n\njulia> @varname(a) in keys(vi)\nfalse\n\nWe can check that the log joint probability of the model accumulated in vi is correct:\n\njulia> x = vi[@varname(a.x)];\n\njulia> getlogjoint(vi) ≈ logpdf(Normal(), x) + logpdf(Uniform(0, 1 + abs(x)), 0.4)\ntrue\n\nWithout automatic prefixing\n\nAs mentioned earlier, by default, the auto_prefix argument specifies whether to automatically prefix the variables in the submodel. If auto_prefix=false, then the variables in the submodel will not be prefixed.\n\njulia> @model function demo1(x)\n           x ~ Normal()\n           return 1 + abs(x)\n       end;\n\njulia> @model function demo2_no_prefix(x, z)\n            a ~ to_submodel(demo1(x), false)\n            return z ~ Uniform(-a, 1)\n       end;\n\njulia> vi = VarInfo(demo2_no_prefix(missing, 0.4));\n\njulia> @varname(x) in keys(vi)  # here we just use `x` instead of `a.x`\ntrue\n\nHowever, not using prefixing is generally not recommended as it can lead to variable name clashes unless one is careful. For example, if we're re-using the same model twice in a model, not using prefixing will lead to variable name clashes: However, one can manually prefix using the prefix(::Model, input):\n\njulia> @model function demo2(x, y, z)\n            a ~ to_submodel(prefix(demo1(x), :sub1), false)\n            b ~ to_submodel(prefix(demo1(y), :sub2), false)\n            return z ~ Uniform(-a, b)\n       end;\n\njulia> vi = VarInfo(demo2(missing, missing, 0.4));\n\njulia> @varname(sub1.x) in keys(vi)\ntrue\n\njulia> @varname(sub2.x) in keys(vi)\ntrue\n\nVariables a and b are not tracked, but are assigned the return values of the respective calls to demo1:\n\njulia> @varname(a) in keys(vi)\nfalse\n\njulia> @varname(b) in keys(vi)\nfalse\n\nWe can check that the log joint probability of the model accumulated in vi is correct:\n\njulia> sub1_x = vi[@varname(sub1.x)];\n\njulia> sub2_x = vi[@varname(sub2.x)];\n\njulia> logprior = logpdf(Normal(), sub1_x) + logpdf(Normal(), sub2_x);\n\njulia> loglikelihood = logpdf(Uniform(-1 - abs(sub1_x), 1 + abs(sub2_x)), 0.4);\n\njulia> getlogjoint(vi) ≈ logprior + loglikelihood\ntrue\n\nUsage as likelihood is illegal\n\nNote that it is illegal to use a to_submodel model as a likelihood in another model:\n\njulia> @model inner() = x ~ Normal()\ninner (generic function with 2 methods)\n\njulia> @model illegal_likelihood() = a ~ to_submodel(inner())\nillegal_likelihood (generic function with 2 methods)\n\njulia> model = illegal_likelihood() | (a = 1.0,);\n\njulia> model()\nERROR: ArgumentError: `x ~ to_submodel(...)` is not supported when `x` is observed\n[...]\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.prefix",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.prefix",
    "title": "DynamicPPL.prefix",
    "section": "function",
    "text": "prefix(ctx::AbstractContext, vn::VarName)\n\nApply the prefixes in the context ctx to the variable name vn.\n\n\n\n\n\nprefix(model::Model, x::VarName)\nprefix(model::Model, x::Val{sym})\nprefix(model::Model, x::Any)\n\nReturn model but with all random variables prefixed by x, where x is either:\n\na VarName (e.g. @varname(a)),\na Val{sym} (e.g. Val(:a)), or\nfor any other type, x is converted to a Symbol and then to a VarName. Note that this will introduce runtime overheads so is not recommended unless absolutely necessary.\n\nExamples\n\njulia> using DynamicPPL: prefix\n\njulia> @model demo() = x ~ Dirac(1)\ndemo (generic function with 2 methods)\n\njulia> rand(prefix(demo(), @varname(my_prefix)))\n(var\"my_prefix.x\" = 1,)\n\njulia> rand(prefix(demo(), Val(:my_prefix)))\n(var\"my_prefix.x\" = 1,)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.typed_identity",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.typed_identity",
    "title": "DynamicPPL.typed_identity",
    "section": "function",
    "text": "typed_identity(x)\n\nIdentity function, but with an overload for with_logabsdet_jacobian to ensure that it returns a sensible zero logjac.\n\nThe problem with plain old identity is that the default definition of with_logabsdet_jacobian for identity returns zero(eltype(x)): https://github.com/JuliaMath/ChangesOfVariables.jl/blob/d6a8115fc9b9419decbdb48e2c56ec9675b4c6a4/src/with_ladj.jl#L154\n\nThis is fine for most samples x, but if eltype(x) doesn't return a sensible type (e.g. if it's Any), then using identity will error with zero(Any). This can happen with, for example, ProductNamedTupleDistribution:\n\njulia> using Distributions; d = product_distribution((a = Normal(), b = LKJCholesky(3, 0.5)));\n\njulia> eltype(rand(d))\nAny\n\nThe same problem precludes us from eventually broadening the scope of DynamicPPL.jl to support distributions with non-numeric samples.\n\nFurthermore, in principle, the type of the log-probability should be separate from the type of the sample. Thus, instead of using zero(LogProbType), we should use the eltype of the LogJacobianAccumulator. There's no easy way to thread that through here, but if a way to do this is discovered, then typed_identity is what will allow us to obtain that custom behaviour.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.@addlogprob!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.@addlogprob!",
    "title": "DynamicPPL.@addlogprob!",
    "section": "macro",
    "text": "@addlogprob!(ex)\n\nAdd a term to the log joint.\n\nIf ex evaluates to a NamedTuple with keys :loglikelihood and/or :logprior, the values are added to the log likelihood and log prior respectively.\n\nIf ex evaluates to a number it is added to the log likelihood.\n\nExamples\n\njulia> mylogjoint(x, μ) = (; loglikelihood=loglikelihood(Normal(μ, 1), x), logprior=1.0);\n\njulia> @model function demo(x)\n           μ ~ Normal()\n           @addlogprob! mylogjoint(x, μ)\n       end;\n\njulia> x = [1.3, -2.1];\n\njulia> loglikelihood(demo(x), (μ=0.2,)) ≈ mylogjoint(x, 0.2).loglikelihood\ntrue\n\njulia> logprior(demo(x), (μ=0.2,)) ≈ logpdf(Normal(), 0.2) + mylogjoint(x, 0.2).logprior\ntrue\n\nand to reject samples:\n\njulia> @model function demo(x)\n           m ~ MvNormal(zero(x), I)\n           if dot(m, x) < 0\n               @addlogprob! (; loglikelihood=-Inf)\n               # Exit the model evaluation early\n               return\n           end\n           x ~ MvNormal(m, I)\n           return\n       end;\n\njulia> logjoint(demo([-2.1]), (m=[0.2],)) == -Inf\ntrue\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.returned-Tuple{Model, Chains}",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.returned-Tuple{Model, Chains}",
    "title": "DynamicPPL.returned",
    "section": "method",
    "text": "returned(model::Model, chain::MCMCChains.Chains)\n\nExecute model for each of the samples in chain and return an array of the values returned by the model for each sample.\n\nExamples\n\nGeneral\n\nOften you might have additional quantities computed inside the model that you want to inspect, e.g.\n\n@model function demo(x)\n    # sample and observe\n    θ ~ Prior()\n    x ~ Likelihood()\n    return interesting_quantity(θ, x)\nend\nm = demo(data)\nchain = sample(m, alg, n)\n# To inspect the `interesting_quantity(θ, x)` where `θ` is replaced by samples\n# from the posterior/`chain`:\nreturned(m, chain) # <= results in a `Vector` of returned values\n                               #    from `interesting_quantity(θ, x)`\n\nConcrete (and simple)\n\njulia> using Turing\n\njulia> @model function demo(xs)\n           s ~ InverseGamma(2, 3)\n           m_shifted ~ Normal(10, √s)\n           m = m_shifted - 10\n\n           for i in eachindex(xs)\n               xs[i] ~ Normal(m, √s)\n           end\n\n           return (m, )\n       end\ndemo (generic function with 1 method)\n\njulia> model = demo(randn(10));\n\njulia> chain = sample(model, MH(), 10);\n\njulia> returned(model, chain)\n10×1 Array{Tuple{Float64},2}:\n (2.1964758025119338,)\n (2.1964758025119338,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.043088571494005024,)\n (-0.16489786710222099,)\n (-0.16489786710222099,)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.returned-Tuple{Model, Union{AbstractDict{<:VarName}, NamedTuple}}",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.returned-Tuple{Model, Union{AbstractDict{<:VarName}, NamedTuple}}",
    "title": "DynamicPPL.returned",
    "section": "method",
    "text": "returned(model::Model, parameters::NamedTuple)\nreturned(model::Model, parameters::AbstractDict{<:VarName})\n\nExecute model with variables keys set to values and return the values returned by the model.\n\nExample\n\njulia> using DynamicPPL, Distributions\n\njulia> @model function demo()\n           m ~ Normal()\n           return (mp1 = m + 1,)\n       end\ndemo (generic function with 2 methods)\n\njulia> model = demo();\n\njulia> returned(model, (; m = 1.0))\n(mp1 = 2.0,)\n\njulia> returned(model, Dict{VarName,Float64}(@varname(m) => 2.0))\n(mp1 = 3.0,)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.pointwise_logdensities",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.pointwise_logdensities",
    "title": "DynamicPPL.pointwise_logdensities",
    "section": "function",
    "text": "DynamicPPL.pointwise_logdensities(\n    model::DynamicPPL.Model,\n    chain::MCMCChains.Chains,\n    ::Type{Tout}=MCMCChains.Chains\n    ::Val{whichlogprob}=Val(:both),\n)\n\nRuns model on each sample in chain, returning a new MCMCChains.Chains object where the log-density of each variable at each sample is stored (rather than its value).\n\nwhichlogprob specifies which log-probabilities to compute. It can be :both, :prior, or :likelihood.\n\nYou can pass Tout=OrderedDict to get the result as an OrderedDict{VarName, Matrix{Float64}} instead.\n\nSee also: DynamicPPL.pointwise_loglikelihoods, DynamicPPL.pointwise_prior_logdensities.\n\nExamples\n\njulia> using MCMCChains\n\njulia> @model function demo(xs, y)\n           s ~ InverseGamma(2, 3)\n           m ~ Normal(0, √s)\n           for i in eachindex(xs)\n               xs[i] ~ Normal(m, √s)\n           end\n           y ~ Normal(m, √s)\n       end\ndemo (generic function with 2 methods)\n\njulia> # Example observations.\n       model = demo([1.0, 2.0, 3.0], [4.0]);\n\njulia> # A chain with 3 iterations.\n       chain = Chains(\n           reshape(1.:6., 3, 2),\n           [:s, :m];\n           info=(varname_to_symbol=Dict(\n               @varname(s) => :s,\n               @varname(m) => :m,\n           ),),\n       );\n\njulia> plds = pointwise_logdensities(model, chain)\nChains MCMC chain (3×6×1 Array{Float64, 3}):\n\nIterations        = 1:1:3\nNumber of chains  = 1\nSamples per chain = 3\nparameters        = s, m, xs[1], xs[2], xs[3], y\n[...]\n\njulia> plds[:s]\n2-dimensional AxisArray{Float64,2,...} with axes:\n    :iter, 1:1:3\n    :chain, 1:1\nAnd data, a 3×1 Matrix{Float64}:\n -0.8027754226637804\n -1.3822169643436162\n -2.0986122886681096\n\njulia> # The above is the same as:\n       logpdf.(InverseGamma(2, 3), chain[:s])\n3×1 Matrix{Float64}:\n -0.8027754226637804\n -1.3822169643436162\n -2.0986122886681096\n\njulia> # Alternatively:        pldsdict = pointwiselogdensities(model, chain, OrderedDict) OrderedDict{VarName, Matrix{Float64}} with 6 entries:   s     => [-0.802775; -1.38222; -2.09861;;]   m     => [-8.91894; -7.51551; -7.46824;;]   xs[1] => [-5.41894; -5.26551; -5.63491;;]   xs[2] => [-2.91894; -3.51551; -4.13491;;]   xs[3] => [-1.41894; -2.26551; -2.96824;;]   y     => [-0.918939; -1.51551; -2.13491;;]\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.pointwise_loglikelihoods",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.pointwise_loglikelihoods",
    "title": "DynamicPPL.pointwise_loglikelihoods",
    "section": "function",
    "text": "DynamicPPL.pointwise_loglikelihoods(\n    model::DynamicPPL.Model,\n    chain::MCMCChains.Chains,\n    ::Type{Tout}=MCMCChains.Chains\n)\n\nCompute the pointwise log-likelihoods of the model given the chain. This is the same as pointwise_logdensities(model, chain), but only including the likelihood terms.\n\nSee also: DynamicPPL.pointwise_logdensities, DynamicPPL.pointwise_prior_logdensities.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.pointwise_prior_logdensities",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.pointwise_prior_logdensities",
    "title": "DynamicPPL.pointwise_prior_logdensities",
    "section": "function",
    "text": "DynamicPPL.pointwise_prior_logdensities(\n    model::DynamicPPL.Model,\n    chain::MCMCChains.Chains\n)\n\nCompute the pointwise log-prior-densities of the model given the chain. This is the same as pointwise_logdensities(model, chain), but only including the prior terms.\n\nSee also: DynamicPPL.pointwise_logdensities, DynamicPPL.pointwise_loglikelihoods.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.value_iterator_from_chain",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.value_iterator_from_chain",
    "title": "DynamicPPL.value_iterator_from_chain",
    "section": "function",
    "text": "value_iterator_from_chain(model::Model, chain)\nvalue_iterator_from_chain(varinfo::AbstractVarInfo, chain)\n\nReturn an iterator over the values in chain for each variable in model/varinfo.\n\nExample\n\njulia> using MCMCChains, DynamicPPL, Distributions, StableRNGs\n\njulia> rng = StableRNG(42);\n\njulia> @model function demo_model(x)\n           s ~ InverseGamma(2, 3)\n           m ~ Normal(0, sqrt(s))\n           for i in eachindex(x)\n               x[i] ~ Normal(m, sqrt(s))\n           end\n\n           return s, m\n       end\ndemo_model (generic function with 2 methods)\n\njulia> model = demo_model([1.0, 2.0]);\n\njulia> chain = Chains(rand(rng, 10, 2, 3), [:s, :m]);\n\njulia> iter = value_iterator_from_chain(model, chain);\n\njulia> first(iter)\nOrderedDict{VarName, Any} with 2 entries:\n  s => 0.580515\n  m => 0.739328\n\njulia> collect(iter)\n10×3 Matrix{OrderedDict{VarName, Any}}:\n OrderedDict(s=>0.580515, m=>0.739328)  …  OrderedDict(s=>0.186047, m=>0.402423)\n OrderedDict(s=>0.191241, m=>0.627342)     OrderedDict(s=>0.776277, m=>0.166342)\n OrderedDict(s=>0.971133, m=>0.637584)     OrderedDict(s=>0.651655, m=>0.712044)\n OrderedDict(s=>0.74345, m=>0.110359)      OrderedDict(s=>0.469214, m=>0.104502)\n OrderedDict(s=>0.170969, m=>0.598514)     OrderedDict(s=>0.853546, m=>0.185399)\n OrderedDict(s=>0.704776, m=>0.322111)  …  OrderedDict(s=>0.638301, m=>0.853802)\n OrderedDict(s=>0.441044, m=>0.162285)     OrderedDict(s=>0.852959, m=>0.0956922)\n OrderedDict(s=>0.803972, m=>0.643369)     OrderedDict(s=>0.245049, m=>0.871985)\n OrderedDict(s=>0.772384, m=>0.646323)     OrderedDict(s=>0.906603, m=>0.385502)\n OrderedDict(s=>0.70882, m=>0.253105)      OrderedDict(s=>0.413222, m=>0.953288)\n\njulia> # This can be used to `condition` a `Model`.\n       conditioned_model = model | first(iter);\n\njulia> conditioned_model()  # <= results in same values as the `first(iter)` above\n(0.5805148626851955, 0.7393275279160691)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.extract_priors",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.extract_priors",
    "title": "DynamicPPL.extract_priors",
    "section": "function",
    "text": "extract_priors([rng::Random.AbstractRNG, ]model::Model)\n\nExtract the priors from a model.\n\nThis is done by sampling from the model and recording the distributions that are used to generate the samples.\n\nwarning: Warning\nBecause the extraction is done by execution of the model, there are several caveats:If one variable, say, y ~ Normal(0, x), where x ~ Normal() is also a random variable, then the extracted prior will have different parameters in every extraction!\nIf the model does not have static support, say, n ~ Categorical(1:10); x ~ MvNormmal(zeros(n), I), then the extracted priors themselves will be different between extractions, not just their parameters.Both of these caveats are demonstrated below.\n\nExamples\n\nChanging parameters\n\njulia> using Distributions, StableRNGs\n\njulia> rng = StableRNG(42);\n\njulia> @model function model_dynamic_parameters()\n           x ~ Normal(0, 1)\n           y ~ Normal(x, 1)\n       end;\n\njulia> model = model_dynamic_parameters();\n\njulia> extract_priors(rng, model)[@varname(y)]\nNormal{Float64}(μ=-0.6702516921145671, σ=1.0)\n\njulia> extract_priors(rng, model)[@varname(y)]\nNormal{Float64}(μ=1.3736306979834252, σ=1.0)\n\nChanging support\n\njulia> using LinearAlgebra, Distributions, StableRNGs\n\njulia> rng = StableRNG(42);\n\njulia> @model function model_dynamic_support()\n           n ~ Categorical(ones(10) ./ 10)\n           x ~ MvNormal(zeros(n), I)\n       end;\n\njulia> model = model_dynamic_support();\n\njulia> length(extract_priors(rng, model)[@varname(x)])\n6\n\njulia> length(extract_priors(rng, model)[@varname(x)])\n9\n\n\n\n\n\nextract_priors(model::Model, varinfo::AbstractVarInfo)\n\nExtract the priors from a model.\n\nThis is done by evaluating the model at the values present in varinfo and recording the distributions that are present at each tilde statement.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.values_as_in_model",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.values_as_in_model",
    "title": "DynamicPPL.values_as_in_model",
    "section": "function",
    "text": "values_as_in_model(model::Model, include_colon_eq::Bool, varinfo::AbstractVarInfo)\n\nGet the values of varinfo as they would be seen in the model.\n\nMore specifically, this method attempts to extract the realization as seen in the model. For example, x[1] ~ truncated(Normal(); lower=0) will result in a realization that is compatible with truncated(Normal(); lower=0) – i.e. one where the value of x[1] is positive – regardless of whether varinfo is working in unconstrained space.\n\nHence this method is a \"safe\" way of obtaining realizations in constrained space at the cost of additional model evaluations.\n\nArguments\n\nmodel::Model: model to extract realizations from.\ninclude_colon_eq::Bool: whether to also include variables on the LHS of :=.\nvarinfo::AbstractVarInfo: variable information to use for the extraction.\n\nExamples\n\nWhen VarInfo fails\n\nThe following demonstrates a common pitfall when working with VarInfo and constrained variables.\n\njulia> using Distributions, StableRNGs\n\njulia> rng = StableRNG(42);\n\njulia> @model function model_changing_support()\n           x ~ Bernoulli(0.5)\n           y ~ x == 1 ? Uniform(0, 1) : Uniform(11, 12)\n       end;\n\njulia> model = model_changing_support();\n\njulia> # Construct initial type-stable `VarInfo`.\n       varinfo = VarInfo(rng, model);\n\njulia> # Link it so it works in unconstrained space.\n       varinfo_linked = DynamicPPL.link(varinfo, model);\n\njulia> # Perform computations in unconstrained space, e.g. changing the values of `θ`.\n       # Flip `x` so we hit the other support of `y`.\n       θ = [!varinfo[@varname(x)], rand(rng)];\n\njulia> # Update the `VarInfo` with the new values.\n       varinfo_linked = DynamicPPL.unflatten(varinfo_linked, θ);\n\njulia> # Determine the expected support of `y`.\n       lb, ub = θ[1] == 1 ? (0, 1) : (11, 12)\n(0, 1)\n\njulia> # Approach 1: Convert back to constrained space using `invlink` and extract.\n       varinfo_invlinked = DynamicPPL.invlink(varinfo_linked, model);\n\njulia> # (×) Fails! Because `VarInfo` _saves_ the original distributions\n       # used in the very first model evaluation, hence the support of `y`\n       # is not updated even though `x` has changed.\n       lb ≤ first(varinfo_invlinked[@varname(y)]) ≤ ub\nfalse\n\njulia> # Approach 2: Extract realizations using `values_as_in_model`.\n       # (✓) `values_as_in_model` will re-run the model and extract\n       # the correct realization of `y` given the new values of `x`.\n       lb ≤ values_as_in_model(model, true, varinfo_linked)[@varname(y)] ≤ ub\ntrue\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.NamedDist",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.NamedDist",
    "title": "DynamicPPL.NamedDist",
    "section": "type",
    "text": "A named distribution that carries the name of the random variable with it.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.AD.run_ad",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.AD.run_ad",
    "title": "DynamicPPL.TestUtils.AD.run_ad",
    "section": "function",
    "text": "run_ad(\n    model::Model,\n    adtype::ADTypes.AbstractADType;\n    test::Union{AbstractADCorrectnessTestSetting,Bool}=WithBackend(),\n    benchmark=false,\n    atol::AbstractFloat=1e-8,\n    rtol::AbstractFloat=sqrt(eps()),\n    getlogdensity::Function=getlogjoint_internal,\n    rng::Random.AbstractRNG=Random.default_rng(),\n    varinfo::AbstractVarInfo=link(VarInfo(model), model),\n    params::Union{Nothing,Vector{<:AbstractFloat}}=nothing,\n    verbose=true,\n)::ADResult\n\nDescription\n\nTest the correctness and/or benchmark the AD backend adtype for the model model.\n\nWhether to test and benchmark is controlled by the test and benchmark keyword arguments. By default, test is true and benchmark is false.\n\nNote that to run AD successfully you will need to import the AD backend itself. For example, to test with AutoReverseDiff() you will need to run import ReverseDiff.\n\nArguments\n\nThere are two positional arguments, which absolutely must be provided:\n\nmodel - The model being tested.\nadtype - The AD backend being tested.\n\nEverything else is optional, and can be categorised into several groups:\n\nHow to specify the VarInfo.\nDynamicPPL contains several different types of VarInfo objects which change the way model evaluation occurs. If you want to use a specific type of VarInfo, pass it as the varinfo argument. Otherwise, it will default to using a linked TypedVarInfo generated from the model. Here, linked means that the parameters in the VarInfo have been transformed to unconstrained Euclidean space if they aren't already in that space.\nHow to specify the parameters.\nFor maximum control over this, generate a vector of parameters yourself and pass this as the params argument. If you don't specify this, it will be taken from the contents of the VarInfo.\nNote that if the VarInfo is not specified (and thus automatically generated) the parameters in it will have been sampled from the prior of the model. If you want to seed the parameter generation for the VarInfo, you can pass the rng keyword argument, which will then be used to create the VarInfo.\nFinally, note that these only reflect the parameters used for evaluating the gradient. If you also want to control the parameters used for preparing the gradient, then you need to manually set these parameters in the VarInfo object, for example using vi = DynamicPPL.unflatten(vi, prep_params). You could then evaluate the gradient at a different set of parameters using the params keyword argument.\nWhich type of logp is being calculated.\nBy default, run_ad evaluates the 'internal log joint density' of the model, i.e., the log joint density in the unconstrained space. Thus, for example, in\n@model f() = x ~ LogNormal()\nthe internal log joint density is logpdf(Normal(), log(x)). This is the relevant log density for e.g. Hamiltonian Monte Carlo samplers and is therefore the most useful to test.\nIf you want the log joint density in the original model parameterisation, you can use getlogjoint. Likewise, if you want only the prior or likelihood, you can use getlogprior or getloglikelihood, respectively.\nHow to specify the results to compare against.\nOnce logp and its gradient has been calculated with the specified adtype, it can optionally be tested for correctness. The exact way this is tested is specified in the test parameter.\nThere are several options for this:\nYou can explicitly specify the correct value using WithExpectedResult().\nYou can compare against the result obtained with a different AD backend using WithBackend(adtype).\nYou can disable testing by passing NoTest().\nThe default is to compare against the result obtained with ForwardDiff, i.e. WithBackend(AutoForwardDiff()).\ntest=false and test=true are synonyms for NoTest() and WithBackend(AutoForwardDiff()), respectively.\nHow to specify the tolerances. (Only if testing is enabled.)\nBoth absolute and relative tolerances can be specified using the atol and rtol keyword arguments respectively. The behaviour of these is similar to isapprox(), i.e. the value and gradient are considered correct if either atol or rtol is satisfied. The default values are 100*eps() for atol and sqrt(eps()) for rtol.\nFor the most part, it is the rtol check that is more meaningful, because we cannot know the magnitude of logp and its gradient a priori. The atol value is supplied to handle the case where gradients are equal to zero.\nWhether to output extra logging information.\nBy default, this function prints messages when it runs. To silence it, set verbose=false.\n\nReturns / Throws\n\nReturns an ADResult object, which contains the results of the test and/or benchmark.\n\nIf test is true and the AD backend returns an incorrect value or gradient, an ADIncorrectException is thrown. If a different error occurs, it will be thrown as-is.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.AD.AbstractADCorrectnessTestSetting",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.AD.AbstractADCorrectnessTestSetting",
    "title": "DynamicPPL.TestUtils.AD.AbstractADCorrectnessTestSetting",
    "section": "type",
    "text": "AbstractADCorrectnessTestSetting\n\nDifferent ways of testing the correctness of an AD backend.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.AD.WithBackend",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.AD.WithBackend",
    "title": "DynamicPPL.TestUtils.AD.WithBackend",
    "section": "type",
    "text": "WithBackend(adtype::AbstractADType=AutoForwardDiff()) <: AbstractADCorrectnessTestSetting\n\nTest correctness by comparing it against the result obtained with adtype.\n\nadtype defaults to ForwardDiff.jl, since it's the default AD backend used in Turing.jl.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.AD.WithExpectedResult",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.AD.WithExpectedResult",
    "title": "DynamicPPL.TestUtils.AD.WithExpectedResult",
    "section": "type",
    "text": "WithExpectedResult(\n    value::T,\n    grad::AbstractVector{T}\n) where {T <: AbstractFloat}\n<: AbstractADCorrectnessTestSetting\n\nTest correctness by comparing it against a known result (e.g. one obtained analytically, or one obtained with a different backend previously). Both the value of the primal (i.e. the log-density) as well as its gradient must be supplied.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.AD.NoTest",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.AD.NoTest",
    "title": "DynamicPPL.TestUtils.AD.NoTest",
    "section": "type",
    "text": "NoTest() <: AbstractADCorrectnessTestSetting\n\nDisable correctness testing.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.AD.ADResult",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.AD.ADResult",
    "title": "DynamicPPL.TestUtils.AD.ADResult",
    "section": "type",
    "text": "ADResult{Tparams<:AbstractFloat,Tresult<:AbstractFloat,Ttol<:AbstractFloat}\n\nData structure to store the results of the AD correctness test.\n\nThe type parameter Tparams is the numeric type of the parameters passed in; Tresult is the type of the value and the gradient; and Ttol is the type of the absolute and relative tolerances used for correctness testing.\n\nFields\n\nmodel::Model: The DynamicPPL model that was tested\ngetlogdensity::Function: The function used to extract the log density from the model\nvarinfo::AbstractVarInfo: The VarInfo that was used\nparams::Vector{Tparams} where Tparams<:AbstractFloat: The values at which the model was evaluated\nadtype::ADTypes.AbstractADType: The AD backend that was tested\natol::AbstractFloat: Absolute tolerance used for correctness test\nrtol::AbstractFloat: Relative tolerance used for correctness test\nvalue_expected::Union{Nothing, Tresult} where Tresult<:AbstractFloat: The expected value of logp\ngrad_expected::Union{Nothing, Vector{Tresult}} where Tresult<:AbstractFloat: The expected gradient of logp\nvalue_actual::AbstractFloat: The value of logp (calculated using adtype)\ngrad_actual::Vector{Tresult} where Tresult<:AbstractFloat: The gradient of logp (calculated using adtype)\ngrad_time::Union{Nothing, Tresult} where Tresult<:AbstractFloat: If benchmarking was requested, the time taken by the AD backend to evaluate the gradient      of logp\nprimal_time::Union{Nothing, Tresult} where Tresult<:AbstractFloat: If benchmarking was requested, the time taken by the AD backend to evaluate logp\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.AD.ADIncorrectException",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.AD.ADIncorrectException",
    "title": "DynamicPPL.TestUtils.AD.ADIncorrectException",
    "section": "type",
    "text": "ADIncorrectException{T<:AbstractFloat}\n\nException thrown when an AD backend returns an incorrect value or gradient.\n\nThe type parameter T is the numeric type of the value and gradient.\n\nFields\n\nvalue_expected::AbstractFloat\nvalue_actual::AbstractFloat\ngrad_expected::Vector{T} where T<:AbstractFloat\ngrad_actual::Vector{T} where T<:AbstractFloat\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.DEMO_MODELS",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.DEMO_MODELS",
    "title": "DynamicPPL.TestUtils.DEMO_MODELS",
    "section": "constant",
    "text": "A collection of models corresponding to the posterior distribution defined by the generative process\n\ns ~ InverseGamma(2, 3)\nm ~ Normal(0, √s)\n1.5 ~ Normal(m, √s)\n2.0 ~ Normal(m, √s)\n\nor by\n\ns[1] ~ InverseGamma(2, 3)\ns[2] ~ InverseGamma(2, 3)\nm[1] ~ Normal(0, √s)\nm[2] ~ Normal(0, √s)\n1.5 ~ Normal(m[1], √s[1])\n2.0 ~ Normal(m[2], √s[2])\n\nThese are examples of a Normal-InverseGamma conjugate prior with Normal likelihood, for which the posterior is known in closed form.\n\nIn particular, for the univariate model (the former one):\n\nmean(s) == 49 / 24\nmean(m) == 7 / 6\n\nAnd for the multivariate one (the latter one):\n\nmean(s[1]) == 19 / 8\nmean(m[1]) == 3 / 4\nmean(s[2]) == 8 / 3\nmean(m[2]) == 1\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.logprior_true",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.logprior_true",
    "title": "DynamicPPL.TestUtils.logprior_true",
    "section": "function",
    "text": "logprior_true(model, args...)\n\nReturn the logprior of model for args.\n\nThis should generally be implemented by hand for every specific model.\n\nSee also: logjoint_true, loglikelihood_true.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.loglikelihood_true",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.loglikelihood_true",
    "title": "DynamicPPL.TestUtils.loglikelihood_true",
    "section": "function",
    "text": "loglikelihood_true(model, args...)\n\nReturn the loglikelihood of model for args.\n\nThis should generally be implemented by hand for every specific model.\n\nSee also: logjoint_true, logprior_true.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.logjoint_true",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.logjoint_true",
    "title": "DynamicPPL.TestUtils.logjoint_true",
    "section": "function",
    "text": "logjoint_true(model, args...)\n\nReturn the logjoint of model for args.\n\nDefaults to logprior_true(model, args...) + loglikelihood_true(model, args..).\n\nThis should generally be implemented by hand for every specific model so that the returned value can be used as a ground-truth for testing things like:\n\nValidity of evaluation of model using a particular implementation of AbstractVarInfo.\nValidity of a sampler when combined with DynamicPPL by running the sampler twice: once targeting ground-truth functions, e.g. logjoint_true, and once targeting model.\n\nAnd more.\n\nSee also: logprior_true, loglikelihood_true.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.logprior_true_with_logabsdet_jacobian",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.logprior_true_with_logabsdet_jacobian",
    "title": "DynamicPPL.TestUtils.logprior_true_with_logabsdet_jacobian",
    "section": "function",
    "text": "logprior_true_with_logabsdet_jacobian(model::Model, args...)\n\nReturn a tuple (args_unconstrained, logprior_unconstrained) of model for args....\n\nUnlike logprior_true, the returned logprior computation includes the log-absdet-jacobian adjustment, thus computing logprior for the unconstrained variables.\n\nNote that args are assumed be in the support of model, while args_unconstrained will be unconstrained.\n\nSee also: logprior_true.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.logjoint_true_with_logabsdet_jacobian",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.logjoint_true_with_logabsdet_jacobian",
    "title": "DynamicPPL.TestUtils.logjoint_true_with_logabsdet_jacobian",
    "section": "function",
    "text": "logjoint_true_with_logabsdet_jacobian(model::Model, args...)\n\nReturn a tuple (args_unconstrained, logjoint) of model for args.\n\nUnlike logjoint_true, the returned logjoint computation includes the log-absdet-jacobian adjustment, thus computing logjoint for the unconstrained variables.\n\nNote that args are assumed be in the support of model, while args_unconstrained will be unconstrained.\n\nThis should generally not be implemented directly, instead one should implement logprior_true_with_logabsdet_jacobian for a given model.\n\nSee also: logjoint_true, logprior_true_with_logabsdet_jacobian.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.varnames",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.varnames",
    "title": "DynamicPPL.TestUtils.varnames",
    "section": "function",
    "text": "varnames(model::Model)\n\nReturn a collection of VarName as they are expected to appear in the model.\n\nEven though it is recommended to implement this by hand for a particular Model, a default implementation using SimpleVarInfo{<:Dict} is provided.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.posterior_mean",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.posterior_mean",
    "title": "DynamicPPL.TestUtils.posterior_mean",
    "section": "function",
    "text": "posterior_mean(model::Model)\n\nReturn a NamedTuple compatible with varnames(model) where the values represent the posterior mean under model.\n\n\"Compatible\" means that a varname from varnames(model) can be used to extract the corresponding value using get, e.g. get(posterior_mean(model), varname).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.setup_varinfos",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.setup_varinfos",
    "title": "DynamicPPL.TestUtils.setup_varinfos",
    "section": "function",
    "text": "setup_varinfos(model::Model, example_values::NamedTuple, varnames; include_threadsafe::Bool=false)\n\nReturn a tuple of instances for different implementations of AbstractVarInfo with each vi, supposedly, satisfying vi[vn] == get(example_values, vn) for vn in varnames.\n\nIf include_threadsafe is true, then the returned tuple will also include thread-safe versions of the varinfo instances.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.update_values!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.update_values!!",
    "title": "DynamicPPL.update_values!!",
    "section": "function",
    "text": "update_values!!(vi::AbstractVarInfo, vals::NamedTuple, vns)\n\nReturn instance similar to vi but with vns set to values from vals.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.TestUtils.test_values",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.TestUtils.test_values",
    "title": "DynamicPPL.TestUtils.test_values",
    "section": "function",
    "text": "test_values(vi::AbstractVarInfo, vals::NamedTuple, vns)\n\nTest that vi[vn] corresponds to the correct value in vals for every vn in vns.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.DebugUtils.check_model",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.DebugUtils.check_model",
    "title": "DynamicPPL.DebugUtils.check_model",
    "section": "function",
    "text": "check_model(model::Model, varinfo::AbstractVarInfo; error_on_failure=false)\n\nCheck that model is valid, warning about any potential issues (or erroring if error_on_failure is true).\n\nReturns\n\nissuccess::Bool: Whether the model check succeeded.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.DebugUtils.check_model_and_trace",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.DebugUtils.check_model_and_trace",
    "title": "DynamicPPL.DebugUtils.check_model_and_trace",
    "section": "function",
    "text": "check_model_and_trace(model::Model, varinfo::AbstractVarInfo; error_on_failure=false)\n\nCheck that evaluating model with the given varinfo is valid, warning about any potential issues.\n\nThis will check the model for the following issues:\n\nRepeated usage of the same varname in a model.\nNaN on the left-hand side of observe statements.\n\nArguments\n\nmodel::Model: The model to check.\nvarinfo::AbstractVarInfo: The varinfo to use when evaluating the model.\n\nKeyword Argument\n\nerror_on_failure::Bool: Whether to throw an error if the model check fails. Default: false.\n\nReturns\n\nissuccess::Bool: Whether the model check succeeded.\ntrace::Vector{Stmt}: The trace of statements executed during the model check.\n\nExamples\n\nCorrect model\n\njulia> using StableRNGs\n\njulia> rng = StableRNG(42);\n\njulia> @model demo_correct() = x ~ Normal()\ndemo_correct (generic function with 2 methods)\n\njulia> model = demo_correct(); varinfo = VarInfo(rng, model);\n\njulia> issuccess, trace = check_model_and_trace(model, varinfo);\n\njulia> issuccess\ntrue\n\njulia> print(trace)\n assume: x ~ Normal{Float64}(μ=0.0, σ=1.0) ⟼ -0.670252\n\njulia> cond_model = model | (x = 1.0,);\n\njulia> issuccess, trace = check_model_and_trace(cond_model, VarInfo(cond_model));\n┌ Warning: The model does not contain any parameters.\n└ @ DynamicPPL.DebugUtils DynamicPPL.jl/src/debug_utils.jl:342\n\njulia> issuccess\ntrue\n\njulia> print(trace)\n observe: x (= 1.0) ~ Normal{Float64}(μ=0.0, σ=1.0)\n\nIncorrect model\n\njulia> @model function demo_incorrect()\n           # (×) Sampling `x` twice will lead to incorrect log-probabilities!\n           x ~ Normal()\n           x ~ Exponential()\n       end\ndemo_incorrect (generic function with 2 methods)\n\njulia> # Notice that VarInfo(model_incorrect) evaluates the model, but doesn't actually\n       # alert us to the issue of `x` being sampled twice.\n       model = demo_incorrect(); varinfo = VarInfo(model);\n\njulia> issuccess, trace = check_model_and_trace(model, varinfo; error_on_failure=true);\nERROR: varname x used multiple times in model\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.DebugUtils.has_static_constraints",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.DebugUtils.has_static_constraints",
    "title": "DynamicPPL.DebugUtils.has_static_constraints",
    "section": "function",
    "text": "has_static_constraints([rng, ]model::Model; num_evals=5, error_on_failure=false)\n\nReturn true if the model has static constraints, false otherwise.\n\nNote that this is a heuristic check based on sampling from the model multiple times and checking if the model is consistent across runs.\n\nArguments\n\nrng::Random.AbstractRNG: The random number generator to use when evaluating the model.\nmodel::Model: The model to check.\n\nKeyword Arguments\n\nnum_evals::Int: The number of evaluations to perform. Default: 5.\nerror_on_failure::Bool: Whether to throw an error if any of the num_evals model checks fail. Default: false.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.DebugUtils.model_warntype",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.DebugUtils.model_warntype",
    "title": "DynamicPPL.DebugUtils.model_warntype",
    "section": "function",
    "text": "model_warntype(model[, varinfo]; optimize=true)\n\nCheck the type stability of the model's evaluator, warning about any potential issues.\n\nThis simply calls @code_warntype on the model's evaluator, filling in internal arguments where needed.\n\nArguments\n\nmodel::Model: The model to check.\nvarinfo::AbstractVarInfo: The varinfo to use when evaluating the model. Default: VarInfo(model).\n\nKeyword Arguments\n\noptimize::Bool: Whether to generate optimized code. Default: false.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.DebugUtils.model_typed",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.DebugUtils.model_typed",
    "title": "DynamicPPL.DebugUtils.model_typed",
    "section": "function",
    "text": "model_typed(model[, varinfo]; optimize=true)\n\nReturn the type inference for the model's evaluator.\n\nThis simply calls @code_typed on the model's evaluator, filling in internal arguments where needed.\n\nArguments\n\nmodel::Model: The model to check.\nvarinfo::AbstractVarInfo: The varinfo to use when evaluating the model. Default: VarInfo(model).\n\nKeyword Arguments\n\noptimize::Bool: Whether to generate optimized code. Default: true.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.DebugUtils.gen_evaluator_call_with_types",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.DebugUtils.gen_evaluator_call_with_types",
    "title": "DynamicPPL.DebugUtils.gen_evaluator_call_with_types",
    "section": "function",
    "text": "gen_evaluator_call_with_types(model[, varinfo])\n\nGenerate the evaluator call and the types of the arguments.\n\nArguments\n\nmodel::Model: The model whose evaluator is of interest.\nvarinfo::AbstractVarInfo: The varinfo to use when evaluating the model. Default: VarInfo(model).\n\nReturns\n\nA 2-tuple with the following elements:\n\nf: This is either model.f or Core.kwcall, depending on whether   the model has keyword arguments.\nargtypes::Type{<:Tuple}: The types of the arguments for the evaluator.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.AbstractVarInfo",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.AbstractVarInfo",
    "title": "DynamicPPL.AbstractVarInfo",
    "section": "type",
    "text": "AbstractVarInfo\n\nAbstract supertype for data structures that capture random variables when executing a probabilistic model and accumulate log densities such as the log likelihood or the log joint probability of the model.\n\nSee also: VarInfo, SimpleVarInfo.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.VarInfo",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.VarInfo",
    "title": "DynamicPPL.VarInfo",
    "section": "type",
    "text": "struct VarInfo{Tmeta,Accs<:AccumulatorTuple} <: AbstractVarInfo\n    metadata::Tmeta\n    accs::Accs\nend\n\nA light wrapper over some kind of metadata.\n\nThe type of the metadata can be one of a number of options. It may either be a Metadata or a VarNamedVector, or, it may be a NamedTuple which maps symbols to Metadata or VarNamedVector instances. Here, a symbol refers to a Julia variable and may consist of one or more VarNames which appear on the left-hand side of tilde statements. For example, x[1] and x[2] both have the same symbol x.\n\nSeveral type aliases are provided for these forms of VarInfos:\n\nVarInfo{<:Metadata} is UntypedVarInfo\nVarInfo{<:VarNamedVector} is UntypedVectorVarInfo\nVarInfo{<:NamedTuple} is NTVarInfo\n\nThe NamedTuple form, i.e. NTVarInfo, is useful for maintaining type stability of model evaluation. However, the element type of NamedTuples are not contained in its type itself: thus, there is no way to use the type system to determine whether the elements of the NamedTuple are Metadata or VarNamedVector.\n\nNote that for NTVarInfo, it is the user's responsibility to ensure that each symbol is visited at least once during model evaluation, regardless of any stochastic branching.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.untyped_varinfo",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.untyped_varinfo",
    "title": "DynamicPPL.untyped_varinfo",
    "section": "function",
    "text": "untyped_varinfo([rng, ]model[, init_strategy])\n\nConstruct a VarInfo object for the given model, which has just a single Metadata as its metadata field.\n\nArguments\n\nrng::Random.AbstractRNG: The random number generator to use during model evaluation\nmodel::Model: The model for which to create the varinfo object\ninit_strategy::AbstractInitStrategy: How the values are to be initialised. Defaults to InitFromPrior().\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.typed_varinfo",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.typed_varinfo",
    "title": "DynamicPPL.typed_varinfo",
    "section": "function",
    "text": "typed_varinfo(vi::UntypedVarInfo)\n\nThis function finds all the unique syms from the instances of VarName{sym} found in vi.metadata.vns. It then extracts the metadata associated with each symbol from the global vi.metadata field. Finally, a new VarInfo is created with a new metadata as a NamedTuple mapping from symbols to type-stable Metadata instances, one for each symbol.\n\n\n\n\n\ntyped_varinfo([rng, ]model[, init_strategy])\n\nReturn a VarInfo object for the given model, which has a NamedTuple of Metadata structs as its metadata field.\n\nArguments\n\nrng::Random.AbstractRNG: The random number generator to use during model evaluation\nmodel::Model: The model for which to create the varinfo object\ninit_strategy::AbstractInitStrategy: How the values are to be initialised. Defaults to InitFromPrior().\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.untyped_vector_varinfo",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.untyped_vector_varinfo",
    "title": "DynamicPPL.untyped_vector_varinfo",
    "section": "function",
    "text": "untyped_vector_varinfo([rng, ]model[, init_strategy])\n\nReturn a VarInfo object for the given model, which has just a single VarNamedVector as its metadata field.\n\nArguments\n\nrng::Random.AbstractRNG: The random number generator to use during model evaluation\nmodel::Model: The model for which to create the varinfo object\ninit_strategy::AbstractInitStrategy: How the values are to be initialised. Defaults to InitFromPrior().\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.typed_vector_varinfo",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.typed_vector_varinfo",
    "title": "DynamicPPL.typed_vector_varinfo",
    "section": "function",
    "text": "typed_vector_varinfo([rng, ]model[, init_strategy])\n\nReturn a VarInfo object for the given model, which has a NamedTuple of VarNamedVectors as its metadata field.\n\nArguments\n\nrng::Random.AbstractRNG: The random number generator to use during model evaluation\nmodel::Model: The model for which to create the varinfo object\ninit_strategy::AbstractInitStrategy: How the values are to be initialised. Defaults to InitFromPrior().\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.is_transformed",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.is_transformed",
    "title": "DynamicPPL.is_transformed",
    "section": "function",
    "text": "is_transformed(vnv::VarNamedVector, vn::VarName)\n\nReturn a boolean for whether vn is guaranteed to have been transformed so that its domain is all of Euclidean space.\n\n\n\n\n\nis_transformed(vi::AbstractVarInfo[, vns::Union{VarName, AbstractVector{<:Varname}}])\n\nReturn true if vi is working in unconstrained space, and false if vi is assuming realizations to be in support of the corresponding distributions.\n\nIf vns is provided, then only check if this/these varname(s) are transformed.\n\nwarning: Warning\nNot all implementations of AbstractVarInfo support transforming only a subset of the variables.\n\n\n\n\n\nis_transformed(vi::VarInfo)\n\nCheck whether vi is in the transformed space.\n\nTuring's Hamiltonian samplers use the link and invlink functions from Bijectors.jl to map a constrained variable (for example, one bounded to the space [0, 1]) from its constrained space to the set of real numbers. is_transformed checks if the number is in the constrained space or the real space.\n\nIf some but only some of the variables in vi are transformed, this function will return true. This behavior will likely change in the future.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.set_transformed!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.set_transformed!!",
    "title": "DynamicPPL.set_transformed!!",
    "section": "function",
    "text": "set_transformed!!(vi::AbstractVarInfo, trans::Bool[, vn::VarName])\n\nReturn vi with is_transformed(vi, vn) evaluating to true.\n\nIf vn is not specified, then is_transformed(vi) evaluates to true for all variables.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Base.empty!",
    "href": "../DynamicPPL.jl/api/#Base.empty!",
    "title": "Base.empty!",
    "section": "function",
    "text": "empty!(meta::Metadata)\n\nEmpty the fields of meta.\n\nThis is useful when using a sampling algorithm that assumes an empty meta, e.g. SMC.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.SimpleVarInfo",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.SimpleVarInfo",
    "title": "DynamicPPL.SimpleVarInfo",
    "section": "type",
    "text": "struct SimpleVarInfo{NT, Accs<:DynamicPPL.AccumulatorTuple, C<:DynamicPPL.AbstractTransformation} <: AbstractVarInfo\n\nA simple wrapper of the parameters with a logp field for accumulation of the logdensity.\n\nCurrently only implemented for NT<:NamedTuple and NT<:AbstractDict.\n\nFields\n\nvalues: underlying representation of the realization represented\naccs: tuple of accumulators for things like log prior and log likelihood\ntransformation: represents whether it assumes variables to be transformed\n\nNotes\n\nThe major differences between this and NTVarInfo are:\n\nSimpleVarInfo does not require linearization.\nSimpleVarInfo can use more efficient bijectors.\nSimpleVarInfo is only type-stable if NT<:NamedTuple and either a) no indexing is used in tilde-statements, or b) the values have been specified with the correct shapes.\n\nExamples\n\nGeneral usage\n\njulia> using StableRNGs\n\njulia> @model function demo()\n           m ~ Normal()\n           x = Vector{Float64}(undef, 2)\n           for i in eachindex(x)\n               x[i] ~ Normal()\n           end\n           return x\n       end\ndemo (generic function with 2 methods)\n\njulia> m = demo();\n\njulia> rng = StableRNG(42);\n\njulia> # In the `NamedTuple` version we need to provide the place-holder values for\n       # the variables which are using \"containers\", e.g. `Array`.\n       # In this case, this means that we need to specify `x` but not `m`.\n       _, vi = DynamicPPL.init!!(rng, m, SimpleVarInfo((x = ones(2), )));\n\njulia> # (✓) Vroom, vroom! FAST!!!\n       vi[@varname(x[1])]\n0.4471218424633827\n\njulia> # We can also access arbitrary varnames pointing to `x`, e.g.\n       vi[@varname(x)]\n2-element Vector{Float64}:\n 0.4471218424633827\n 1.3736306979834252\n\njulia> vi[@varname(x[1:2])]\n2-element Vector{Float64}:\n 0.4471218424633827\n 1.3736306979834252\n\njulia> # (×) If we don't provide the container...\n       _, vi = DynamicPPL.init!!(rng, m, SimpleVarInfo());\nERROR: FieldError: type NamedTuple has no field `x`, available fields: `m`\n[...]\n\njulia> # If one does not know the varnames, we can use a `OrderedDict` instead.\n       _, vi = DynamicPPL.init!!(rng, m, SimpleVarInfo{Float64}(OrderedDict{VarName,Any}()));\n\njulia> # (✓) Sort of fast, but only possible at runtime.\n       vi[@varname(x[1])]\n-1.019202452456547\n\njulia> # In addtion, we can only access varnames as they appear in the model!\n       vi[@varname(x)]\nERROR: x was not found in the dictionary provided\n[...]\n\njulia> vi[@varname(x[1:2])]\nERROR: x[1:2] was not found in the dictionary provided\n[...]\n\nTechnically, it's possible to use any implementation of AbstractDict in place of OrderedDict, but OrderedDict ensures that certain operations, e.g. linearization/flattening of the values in the varinfo, are consistent between evaluations. Hence OrderedDict is the preferred implementation of AbstractDict to use here.\n\nYou can also sample in transformed space:\n\njulia> @model demo_constrained() = x ~ Exponential()\ndemo_constrained (generic function with 2 methods)\n\njulia> m = demo_constrained();\n\njulia> _, vi = DynamicPPL.init!!(rng, m, SimpleVarInfo());\n\njulia> vi[@varname(x)] # (✓) 0 ≤ x < ∞\n1.8632965762164932\n\njulia> _, vi = DynamicPPL.init!!(rng, m, DynamicPPL.set_transformed!!(SimpleVarInfo(), true));\n\njulia> vi[@varname(x)] # (✓) -∞ < x < ∞\n-0.21080155351918753\n\njulia> xs = [last(DynamicPPL.init!!(rng, m, DynamicPPL.set_transformed!!(SimpleVarInfo(), true)))[@varname(x)] for i = 1:10];\n\njulia> any(xs .< 0)  # (✓) Positive probability mass on negative numbers!\ntrue\n\njulia> # And with `OrderedDict` of course!\n       _, vi = DynamicPPL.init!!(rng, m, DynamicPPL.set_transformed!!(SimpleVarInfo(OrderedDict{VarName,Any}()), true));\n\njulia> vi[@varname(x)] # (✓) -∞ < x < ∞\n0.6225185067787314\n\njulia> xs = [last(DynamicPPL.init!!(rng, m, DynamicPPL.set_transformed!!(SimpleVarInfo(), true)))[@varname(x)] for i = 1:10];\n\njulia> any(xs .< 0) # (✓) Positive probability mass on negative numbers!\ntrue\n\nEvaluation in transformed space of course also works:\n\njulia> vi = DynamicPPL.set_transformed!!(SimpleVarInfo((x = -1.0,)), true)\nTransformed SimpleVarInfo((x = -1.0,), (LogPrior = LogPriorAccumulator(0.0), LogJacobian = LogJacobianAccumulator(0.0), LogLikelihood = LogLikelihoodAccumulator(0.0)))\n\njulia> # (✓) Positive probability mass on negative numbers!\n       getlogjoint_internal(last(DynamicPPL.evaluate!!(m, vi)))\n-1.3678794411714423\n\njulia> # While if we forget to indicate that it's transformed:\n       vi = DynamicPPL.set_transformed!!(SimpleVarInfo((x = -1.0,)), false)\nSimpleVarInfo((x = -1.0,), (LogPrior = LogPriorAccumulator(0.0), LogJacobian = LogJacobianAccumulator(0.0), LogLikelihood = LogLikelihoodAccumulator(0.0)))\n\njulia> # (✓) No probability mass on negative numbers!\n       getlogjoint_internal(last(DynamicPPL.evaluate!!(m, vi)))\n-Inf\n\nIndexing\n\nUsing NamedTuple as underlying storage.\n\njulia> svi_nt = SimpleVarInfo((m = (a = [1.0], ), ));\n\njulia> svi_nt[@varname(m)]\n(a = [1.0],)\n\njulia> svi_nt[@varname(m.a)]\n1-element Vector{Float64}:\n 1.0\n\njulia> svi_nt[@varname(m.a[1])]\n1.0\n\njulia> svi_nt[@varname(m.a[2])]\nERROR: BoundsError: attempt to access 1-element Vector{Float64} at index [2]\n[...]\n\njulia> svi_nt[@varname(m.b)]\nERROR: FieldError: type NamedTuple has no field `b`, available fields: `a`\n[...]\n\nUsing OrderedDict as underlying storage.\n\njulia> svi_dict = SimpleVarInfo(OrderedDict(@varname(m) => (a = [1.0], )));\n\njulia> svi_dict[@varname(m)]\n(a = [1.0],)\n\njulia> svi_dict[@varname(m.a)]\n1-element Vector{Float64}:\n 1.0\n\njulia> svi_dict[@varname(m.a[1])]\n1.0\n\njulia> svi_dict[@varname(m.a[2])]\nERROR: m.a[2] was not found in the dictionary provided\n[...]\n\njulia> svi_dict[@varname(m.b)]\nERROR: m.b was not found in the dictionary provided\n[...]\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.AbstractAccumulator",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.AbstractAccumulator",
    "title": "DynamicPPL.AbstractAccumulator",
    "section": "type",
    "text": "AbstractAccumulator\n\nAn abstract type for accumulators.\n\nAn accumulator is an object that may change its value at every tildeassume!! or tildeobserve!! call based on the random variable in question. The obvious examples of accumulators are the log prior and log likelihood. Other examples might be a variable that counts the number of observations in a trace, or a list of the names of random variables seen so far.\n\nAn accumulator type T <: AbstractAccumulator must implement the following methods:\n\naccumulator_name(acc::T) or accumulator_name(::Type{T})\naccumulate_observe!!(acc::T, dist, val, vn)\naccumulate_assume!!(acc::T, val, logjac, vn, dist)\nreset(acc::T)\nBase.copy(acc::T)\n\nIn these functions:\n\nval is the new value of the random variable sampled from a distribution (always in the original unlinked space), or the value on the left-hand side of an observe statement.\ndist is the distribution on the RHS of the tilde statement.\nvn is the VarName that is on the left-hand side of the tilde-statement. If the tilde-statement is a literal observation like 0.0 ~ Normal(), then vn is nothing.\nlogjac is the log determinant of the Jacobian of the link transformation, if the variable is stored as a linked value in the VarInfo. If the variable is stored in its original, unlinked form, then logjac is zero.\n\nTo be able to work with multi-threading, it should also implement:\n\nsplit(acc::T)\ncombine(acc::T, acc2::T)\n\nSee the documentation for each of these functions for more details.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.LogPriorAccumulator",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.LogPriorAccumulator",
    "title": "DynamicPPL.LogPriorAccumulator",
    "section": "type",
    "text": "LogPriorAccumulator{T<:Real} <: LogProbAccumulator{T}\n\nAn accumulator that tracks the cumulative log prior during model execution.\n\nNote that the log prior stored in here is always calculated based on unlinked parameters, i.e., the value of logp is independent of whether tha VarInfo is linked or not.\n\nFields\n\nlogp::Real: the scalar log prior value\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.LogJacobianAccumulator",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.LogJacobianAccumulator",
    "title": "DynamicPPL.LogJacobianAccumulator",
    "section": "type",
    "text": "LogJacobianAccumulator{T<:Real} <: LogProbAccumulator{T}\n\nAn accumulator that tracks the cumulative log Jacobian (technically, log(abs(det(J)))) during model execution. Specifically, J refers to the Jacobian of the link transform, i.e., from the space of the original distribution to unconstrained space.\n\nnote: Note\nThis accumulator is only incremented if the variable is transformed by a link function, i.e., if the VarInfo is linked (for the particular variable that is currently being accumulated). If the variable is not linked, the log Jacobian term will be 0.In general, for the forward Jacobian mathbfJ corresponding to the function mathbfy = f(mathbfx),log(q(mathbfy)) = log(p(mathbfx)) - log (mathbfJ)and correspondingly:getlogjoint_internal(vi) = getlogjoint(vi) - getlogjac(vi)\n\nFields\n\nlogjac::Real: the logabsdet of the link transform Jacobian\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.LogLikelihoodAccumulator",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.LogLikelihoodAccumulator",
    "title": "DynamicPPL.LogLikelihoodAccumulator",
    "section": "type",
    "text": "LogLikelihoodAccumulator{T<:Real} <: LogProbAccumulator{T}\n\nAn accumulator that tracks the cumulative log likelihood during model execution.\n\nFields\n\nlogp::Real: the scalar log likelihood value\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.getlogp",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.getlogp",
    "title": "DynamicPPL.getlogp",
    "section": "function",
    "text": "getlogp(vi::AbstractVarInfo)\n\nReturn a NamedTuple of the log prior, log Jacobian, and log likelihood probabilities.\n\nThe keys are called logprior, logjac, and loglikelihood. If any of them are not present in vi an error will be thrown.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.setlogp!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.setlogp!!",
    "title": "DynamicPPL.setlogp!!",
    "section": "function",
    "text": "setlogp!!(vi::AbstractVarInfo, logp::NamedTuple)\n\nSet both the log prior and the log likelihood probabilities in vi.\n\nlogp should have fields logprior and loglikelihood and no other fields.\n\nSee also: setlogprior!!, setloglikelihood!!, getlogp.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.acclogp!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.acclogp!!",
    "title": "DynamicPPL.acclogp!!",
    "section": "function",
    "text": "acclogp!!(vi::AbstractVarInfo, logp::NamedTuple; ignore_missing_accumulator::Bool=false)\n\nAdd to both the log prior and the log likelihood probabilities in vi.\n\nlogp should have fields logprior and/or loglikelihood, and no other fields.\n\nBy default if the necessary accumulators are not in vi an error is thrown. If ignore_missing_accumulator is set to true then this is silently ignored instead.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.getlogjoint",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.getlogjoint",
    "title": "DynamicPPL.getlogjoint",
    "section": "function",
    "text": "getlogjoint(vi::AbstractVarInfo)\n\nReturn the log of the joint probability of the observed data and parameters in vi.\n\nSee also: getlogprior, getloglikelihood.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.getlogjoint_internal",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.getlogjoint_internal",
    "title": "DynamicPPL.getlogjoint_internal",
    "section": "function",
    "text": "getlogjoint_internal(vi::AbstractVarInfo)\n\nReturn the log of the joint probability of the observed data and parameters as they are stored internally in vi, including the log-Jacobian for any linked parameters.\n\nIn general, we have that:\n\ngetlogjoint_internal(vi) == getlogjoint(vi) - getlogjac(vi)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.getlogjac",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.getlogjac",
    "title": "DynamicPPL.getlogjac",
    "section": "function",
    "text": "getlogjac(vi::AbstractVarInfo)\n\nReturn the accumulated log-Jacobian term for any linked parameters in vi. The Jacobian here is taken with respect to the forward (link) transform.\n\nSee also: setlogjac!!.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.setlogjac!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.setlogjac!!",
    "title": "DynamicPPL.setlogjac!!",
    "section": "function",
    "text": "setlogjac!!(vi::AbstractVarInfo, logjac)\n\nSet the accumulated log-Jacobian term for any linked parameters in vi. The Jacobian here is taken with respect to the forward (link) transform.\n\nSee also: getlogjac, acclogjac!!.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.acclogjac!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.acclogjac!!",
    "title": "DynamicPPL.acclogjac!!",
    "section": "function",
    "text": "acclogjac!!(vi::AbstractVarInfo, logjac)\n\nAdd logjac to the value of the log Jacobian in vi.\n\nSee also: getlogjac, setlogjac!!.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.getlogprior",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.getlogprior",
    "title": "DynamicPPL.getlogprior",
    "section": "function",
    "text": "getlogprior(vi::AbstractVarInfo)\n\nReturn the log of the prior probability of the parameters in vi.\n\nSee also: getlogjoint, getloglikelihood, setlogprior!!.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.getlogprior_internal",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.getlogprior_internal",
    "title": "DynamicPPL.getlogprior_internal",
    "section": "function",
    "text": "getlogprior_internal(vi::AbstractVarInfo)\n\nReturn the log of the prior probability of the parameters as stored internally in vi. This includes the log-Jacobian for any linked parameters.\n\nIn general, we have that:\n\ngetlogprior_internal(vi) == getlogprior(vi) - getlogjac(vi)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.setlogprior!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.setlogprior!!",
    "title": "DynamicPPL.setlogprior!!",
    "section": "function",
    "text": "setlogprior!!(vi::AbstractVarInfo, logp)\n\nSet the log of the prior probability of the parameters sampled in vi to logp.\n\nSee also: setloglikelihood!!, setlogp!!, getlogprior.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.acclogprior!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.acclogprior!!",
    "title": "DynamicPPL.acclogprior!!",
    "section": "function",
    "text": "acclogprior!!(vi::AbstractVarInfo, logp)\n\nAdd logp to the value of the log of the prior probability in vi.\n\nSee also: accloglikelihood!!, acclogp!!, getlogprior, setlogprior!!.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.getloglikelihood",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.getloglikelihood",
    "title": "DynamicPPL.getloglikelihood",
    "section": "function",
    "text": "getloglikelihood(vi::AbstractVarInfo)\n\nReturn the log of the likelihood probability of the observed data in vi.\n\nSee also: getlogjoint, getlogprior, setloglikelihood!!.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.setloglikelihood!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.setloglikelihood!!",
    "title": "DynamicPPL.setloglikelihood!!",
    "section": "function",
    "text": "setloglikelihood!!(vi::AbstractVarInfo, logp)\n\nSet the log of the likelihood probability of the observed data sampled in vi to logp.\n\nSee also: setlogprior!!, setlogp!!, getloglikelihood.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.accloglikelihood!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.accloglikelihood!!",
    "title": "DynamicPPL.accloglikelihood!!",
    "section": "function",
    "text": "accloglikelihood!!(vi::AbstractVarInfo, logp)\n\nAdd logp to the value of the log of the likelihood in vi.\n\nSee also: accloglikelihood!!, acclogp!!, getloglikelihood, setloglikelihood!!.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Base.keys",
    "href": "../DynamicPPL.jl/api/#Base.keys",
    "title": "Base.keys",
    "section": "function",
    "text": "keys(vi::AbstractVarInfo)\n\nReturn an iterator over all vns in vi.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Base.getindex",
    "href": "../DynamicPPL.jl/api/#Base.getindex",
    "title": "Base.getindex",
    "section": "function",
    "text": "getindex(vi::AbstractVarInfo, vn::VarName[, dist::Distribution])\ngetindex(vi::AbstractVarInfo, vns::Vector{<:VarName}[, dist::Distribution])\n\nReturn the current value(s) of vn (vns) in vi in the support of its (their) distribution(s).\n\nIf dist is specified, the value(s) will be massaged into the representation expected by dist.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#BangBang.push!!",
    "href": "../DynamicPPL.jl/api/#BangBang.push!!",
    "title": "BangBang.push!!",
    "section": "function",
    "text": "push!!(vi::VarInfo, vn::VarName, r, dist::Distribution)\n\nPush a new random variable vn with a sampled value r from a distribution dist to the VarInfo vi, mutating if it makes sense.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#BangBang.empty!!",
    "href": "../DynamicPPL.jl/api/#BangBang.empty!!",
    "title": "BangBang.empty!!",
    "section": "function",
    "text": "empty!!(vi::AbstractVarInfo)\n\nEmpty vi of variables and reset all accumulators.\n\nThis is useful when using a sampling algorithm that assumes an empty vi, e.g. SMC.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Base.isempty",
    "href": "../DynamicPPL.jl/api/#Base.isempty",
    "title": "Base.isempty",
    "section": "function",
    "text": "isempty(vi::AbstractVarInfo)\n\nReturn true if vi is empty and false otherwise.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.getindex_internal",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.getindex_internal",
    "title": "DynamicPPL.getindex_internal",
    "section": "function",
    "text": "getindex_internal(vi::AbstractVarInfo, vn::VarName)\ngetindex_internal(vi::AbstractVarInfo, vns::Vector{<:VarName})\ngetindex_internal(vi::AbstractVarInfo, ::Colon)\n\nReturn the internal value of the varname vn, varnames vns, or all varnames in vi respectively. The internal value is the value of the variables that is stored in the varinfo object; this may be the actual realisation of the random variable (i.e. the value sampled from the distribution), or it may have been transformed to Euclidean space, depending on whether the varinfo was linked.\n\nSee https://turinglang.org/docs/developers/transforms/dynamicppl/ for more information on how transformed variables are stored in DynamicPPL.\n\nSee also: getindex(vi::AbstractVarInfo, vn::VarName, dist::Distribution)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.setindex_internal!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.setindex_internal!",
    "title": "DynamicPPL.setindex_internal!",
    "section": "function",
    "text": "setindex_internal!(vnv::VarNamedVector, val, i::Int)\n\nSets the ith element of the internal storage vector, ignoring inactive entries.\n\n\n\n\n\nsetindex_internal!(vnv::VarNamedVector, val, vn::VarName[, transform])\n\nLike setindex!, but sets the values as they are stored internally in vnv.\n\nOptionally can set the transformation, such that transform(val) is the original value of the variable. By default, the transform is the identity if creating a new entry in vnv, or the existing transform if updating an existing entry.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.update_internal!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.update_internal!",
    "title": "DynamicPPL.update_internal!",
    "section": "function",
    "text": "update_internal!(vnv::VarNamedVector, vn::VarName, val::AbstractVector[, transform])\n\nUpdate an existing entry for vn in vnv with the value val.\n\nLike setindex_internal!, but errors if the key vn doesn't exist.\n\ntransform should be a function that converts val to the original representation. By default it's the same as the old transform for vn.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.insert_internal!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.insert_internal!",
    "title": "DynamicPPL.insert_internal!",
    "section": "function",
    "text": "insert_internal!(vnv::VarNamedVector, val::AbstractVector, vn::VarName[, transform])\n\nAdd a variable with given value to vnv.\n\nLike setindex_internal!, but errors if the key vn already exists.\n\ntransform should be a function that converts val to the original representation. By default it's identity.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.length_internal",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.length_internal",
    "title": "DynamicPPL.length_internal",
    "section": "function",
    "text": "length_internal(vnv::VarNamedVector)\n\nReturn the length of the internal storage vector of vnv, ignoring inactive entries.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.reset!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.reset!",
    "title": "DynamicPPL.reset!",
    "section": "function",
    "text": "reset!(vnv::VarNamedVector, val, vn::VarName)\n\nReset the value of vn in vnv to val.\n\nThis differs from setindex! in that it will always change the transform of the variable to be the default vectorisation transform. This undoes any possible linking.\n\nExamples\n\njulia> using DynamicPPL: VarNamedVector, @varname, reset!\n\njulia> vnv = VarNamedVector{VarName,Any,Any}();\n\njulia> vnv[@varname(x)] = reshape(1:9, (3, 3));\n\njulia> setindex!(vnv, 2.0, @varname(x))\nERROR: An error occurred while assigning the value 2.0 to variable x. If you are changing the type or size of a variable you'll need to call reset!\n[...]\n\njulia> reset!(vnv, 2.0, @varname(x));\n\njulia> vnv[@varname(x)]\n2.0\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.update!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.update!",
    "title": "DynamicPPL.update!",
    "section": "function",
    "text": "update!(vnv::VarNamedVector, val, vn::VarName)\n\nUpdate the value of vn in vnv to val.\n\nLike setindex!, but errors if the key vn doesn't exist.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Base.insert!",
    "href": "../DynamicPPL.jl/api/#Base.insert!",
    "title": "Base.insert!",
    "section": "function",
    "text": "insert!(vnv::VarNamedVector, val, vn::VarName)\n\nAdd a variable with given value to vnv.\n\nLike setindex!, but errors if the key vn already exists.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.loosen_types!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.loosen_types!!",
    "title": "DynamicPPL.loosen_types!!",
    "section": "function",
    "text": "loosen_types!!(vnv::VarNamedVector, ::Type{KNew}, ::Type{VNew}, ::Type{TNew})\n\nLoosen the types of vnv to allow varname type KNew and transformation type TransNew.\n\nIf KNew is a subtype of K and TransNew is a subtype of the element type of the TTrans then this is a no-op and vnv is returned as is. Otherwise a new VarNamedVector is returned with the same data but more abstract types, so that variables of type KNew and transformations of type TransNew can be pushed to it. Some of the underlying storage is shared between vnv and the return value, and thus mutating one may affect the other.\n\nSee also\n\ntighten_types!!\n\nExamples\n\njulia> using DynamicPPL: VarNamedVector, @varname, loosen_types!!, setindex_internal!\n\njulia> vnv = VarNamedVector(@varname(x) => [1.0]);\n\njulia> y_trans(x) = reshape(x, (2, 2));\n\njulia> setindex_internal!(vnv, collect(1:4), @varname(y), y_trans)\nERROR: MethodError: Cannot `convert` an object of type\n[...]\n\njulia> vnv_loose = DynamicPPL.loosen_types!!(\n           vnv, typeof(@varname(y)), Float64, typeof(y_trans)\n       );\n\njulia> setindex_internal!(vnv_loose, collect(1:4), @varname(y), y_trans)\n\njulia> vnv_loose[@varname(y)]\n2×2 Matrix{Float64}:\n 1.0  3.0\n 2.0  4.0\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.tighten_types!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.tighten_types!!",
    "title": "DynamicPPL.tighten_types!!",
    "section": "function",
    "text": "tighten_types!!(vnv::VarNamedVector)\n\nReturn a VarNamedVector like vnv with the most concrete types possible.\n\nThis function either returns vnv itself or new VarNamedVector with the same values in it, but with the element types of various containers made as concrete as possible.\n\nFor instance, if vnv has its vector of transforms have eltype Any, but all the transforms are actually identity transformations, this function will return a new VarNamedVector with the transforms vector having eltype typeof(identity).\n\nThis is a lot like the reverse of loosen_types!!. Like with loosen_types!!, the return value may share some of its underlying storage with vnv, and thus mutating one may affect the other.\n\nSee also\n\nloosen_types!!\n\nExamples\n\njulia> using DynamicPPL: VarNamedVector, @varname, loosen_types!!, setindex_internal!\n\njulia> vnv = VarNamedVector(@varname(x) => Real[23], @varname(y) => randn(2,2));\n\njulia> vnv = delete!(vnv, @varname(y));\n\njulia> eltype(vnv)\nReal\n\njulia> vnv.transforms\n1-element Vector{Any}:\n identity (generic function with 1 method)\n\njulia> vnv_tight = DynamicPPL.tighten_types!!(vnv);\n\njulia> eltype(vnv_tight) == Int\ntrue\n\njulia> vnv_tight.transforms\n1-element Vector{typeof(identity)}:\n identity (generic function with 1 method)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.values_as",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.values_as",
    "title": "DynamicPPL.values_as",
    "section": "function",
    "text": "values_as(varinfo[, Type])\n\nReturn the values/realizations in varinfo as Type, if implemented.\n\nIf no Type is provided, return values as stored in varinfo.\n\nExamples\n\nSimpleVarInfo with NamedTuple:\n\njulia> data = (x = 1.0, m = [2.0]);\n\njulia> values_as(SimpleVarInfo(data))\n(x = 1.0, m = [2.0])\n\njulia> values_as(SimpleVarInfo(data), NamedTuple)\n(x = 1.0, m = [2.0])\n\njulia> values_as(SimpleVarInfo(data), OrderedDict)\nOrderedDict{VarName{sym, typeof(identity)} where sym, Any} with 2 entries:\n  x => 1.0\n  m => [2.0]\n\njulia> values_as(SimpleVarInfo(data), Vector)\n2-element Vector{Float64}:\n 1.0\n 2.0\n\nSimpleVarInfo with OrderedDict:\n\njulia> data = OrderedDict{Any,Any}(@varname(x) => 1.0, @varname(m) => [2.0]);\n\njulia> values_as(SimpleVarInfo(data))\nOrderedDict{Any, Any} with 2 entries:\n  x => 1.0\n  m => [2.0]\n\njulia> values_as(SimpleVarInfo(data), NamedTuple)\n(x = 1.0, m = [2.0])\n\njulia> values_as(SimpleVarInfo(data), OrderedDict)\nOrderedDict{Any, Any} with 2 entries:\n  x => 1.0\n  m => [2.0]\n\njulia> values_as(SimpleVarInfo(data), Vector)\n2-element Vector{Float64}:\n 1.0\n 2.0\n\nVarInfo with NamedTuple of Metadata:\n\njulia> # Just use an example model to construct the `VarInfo` because we're lazy.\n       vi = DynamicPPL.typed_varinfo(DynamicPPL.TestUtils.demo_assume_dot_observe());\n\njulia> vi[@varname(s)] = 1.0; vi[@varname(m)] = 2.0;\n\njulia> # For the sake of brevity, let's just check the type.\n       md = values_as(vi); md.s isa Union{DynamicPPL.Metadata, DynamicPPL.VarNamedVector}\ntrue\n\njulia> values_as(vi, NamedTuple)\n(s = 1.0, m = 2.0)\n\njulia> values_as(vi, OrderedDict)\nOrderedDict{VarName{sym, typeof(identity)} where sym, Float64} with 2 entries:\n  s => 1.0\n  m => 2.0\n\njulia> values_as(vi, Vector)\n2-element Vector{Float64}:\n 1.0\n 2.0\n\nVarInfo with Metadata:\n\njulia> # Just use an example model to construct the `VarInfo` because we're lazy.\n       vi = DynamicPPL.untyped_varinfo(DynamicPPL.TestUtils.demo_assume_dot_observe());\n\njulia> vi[@varname(s)] = 1.0; vi[@varname(m)] = 2.0;\n\njulia> # For the sake of brevity, let's just check the type.\n       values_as(vi) isa Union{DynamicPPL.Metadata, Vector}\ntrue\n\njulia> values_as(vi, NamedTuple)\n(s = 1.0, m = 2.0)\n\njulia> values_as(vi, OrderedDict)\nOrderedDict{VarName{sym, typeof(identity)} where sym, Float64} with 2 entries:\n  s => 1.0\n  m => 2.0\n\njulia> values_as(vi, Vector)\n2-element Vector{Real}:\n 1.0\n 2.0\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.AbstractTransformation",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.AbstractTransformation",
    "title": "DynamicPPL.AbstractTransformation",
    "section": "type",
    "text": "abstract type AbstractTransformation\n\nRepresents a transformation to be used in link!! and invlink!!, amongst others.\n\nA concrete implementation of this should implement the following methods:\n\nlink!!: transforms the AbstractVarInfo to the unconstrained space.\ninvlink!!: transforms the AbstractVarInfo to the constrained space.\n\nAnd potentially:\n\nmaybe_invlink_before_eval!!: hook to decide whether to transform before evaluating the model.\n\nSee also: link!!, invlink!!, maybe_invlink_before_eval!!.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.NoTransformation",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.NoTransformation",
    "title": "DynamicPPL.NoTransformation",
    "section": "type",
    "text": "struct NoTransformation <: DynamicPPL.AbstractTransformation\n\nTransformation which applies the identity function.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.DynamicTransformation",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.DynamicTransformation",
    "title": "DynamicPPL.DynamicTransformation",
    "section": "type",
    "text": "struct DynamicTransformation <: DynamicPPL.AbstractTransformation\n\nTransformation which transforms the variables on a per-need-basis in the execution of a given Model.\n\nThis is in constrast to StaticTransformation which transforms all variables before the execution of a given Model.\n\nSee also: StaticTransformation.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.StaticTransformation",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.StaticTransformation",
    "title": "DynamicPPL.StaticTransformation",
    "section": "type",
    "text": "struct StaticTransformation{F} <: DynamicPPL.AbstractTransformation\n\nTransformation which transforms all variables before the execution of a given Model.\n\nThis is done through the maybe_invlink_before_eval!! method.\n\nSee also: DynamicTransformation, maybe_invlink_before_eval!!.\n\nFields\n\nbijector::Any: The function, assumed to implement the Bijectors interface, to be applied to the variables\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.transformation",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.transformation",
    "title": "DynamicPPL.transformation",
    "section": "function",
    "text": "transformation(vi::AbstractVarInfo)\n\nReturn the AbstractTransformation related to vi.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Bijectors.link",
    "href": "../DynamicPPL.jl/api/#Bijectors.link",
    "title": "Bijectors.link",
    "section": "function",
    "text": "link([t::AbstractTransformation, ]vi::AbstractVarInfo, model::Model)\nlink([t::AbstractTransformation, ]vi::AbstractVarInfo, vns::NTuple{N,VarName}, model::Model)\n\nTransform variables in vi to their linked space without mutating vi.\n\nEither transform all variables, or only ones specified in vns.\n\nUse the  transformation t, or default_transformation(model, vi) if one is not provided.\n\nSee also: default_transformation, invlink.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Bijectors.invlink",
    "href": "../DynamicPPL.jl/api/#Bijectors.invlink",
    "title": "Bijectors.invlink",
    "section": "function",
    "text": "invlink([t::AbstractTransformation, ]vi::AbstractVarInfo, model::Model)\ninvlink([t::AbstractTransformation, ]vi::AbstractVarInfo, vns::NTuple{N,VarName}, model::Model)\n\nTransform variables in vi to their constrained space without mutating vi.\n\nEither transform all variables, or only ones specified in vns.\n\nUse the (inverse of) transformation t, or default_transformation(model, vi) if one is not provided.\n\nSee also: default_transformation, link.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.link!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.link!!",
    "title": "DynamicPPL.link!!",
    "section": "function",
    "text": "link!!([t::AbstractTransformation, ]vi::AbstractVarInfo, model::Model)\nlink!!([t::AbstractTransformation, ]vi::AbstractVarInfo, vns::NTuple{N,VarName}, model::Model)\n\nTransform variables in vi to their linked space, mutating vi if possible.\n\nEither transform all variables, or only ones specified in vns.\n\nUse the  transformation t, or default_transformation(model, vi) if one is not provided.\n\nSee also: default_transformation, invlink!!.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.invlink!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.invlink!!",
    "title": "DynamicPPL.invlink!!",
    "section": "function",
    "text": "invlink!!([t::AbstractTransformation, ]vi::AbstractVarInfo, model::Model)\ninvlink!!([t::AbstractTransformation, ]vi::AbstractVarInfo, vns::NTuple{N,VarName}, model::Model)\n\nTransform variables in vi to their constrained space, mutating vi if possible.\n\nEither transform all variables, or only ones specified in vns.\n\nUse the (inverse of) transformation t, or default_transformation(model, vi) if one is not provided.\n\nSee also: default_transformation, link!!.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.default_transformation",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.default_transformation",
    "title": "DynamicPPL.default_transformation",
    "section": "function",
    "text": "default_transformation(model::Model[, vi::AbstractVarInfo])\n\nReturn the AbstractTransformation currently related to model and, potentially, vi.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.link_transform",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.link_transform",
    "title": "DynamicPPL.link_transform",
    "section": "function",
    "text": "link_transform(dist)\n\nReturn the constrained-to-unconstrained bijector for distribution dist.\n\nBy default, this is just Bijectors.bijector(dist).\n\nwarning: Warning\nNote that currently this is not used by Bijectors.logpdf_with_trans, hence that needs to be overloaded separately if the intention is to change behavior of an existing distribution.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.invlink_transform",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.invlink_transform",
    "title": "DynamicPPL.invlink_transform",
    "section": "function",
    "text": "invlink_transform(dist)\n\nReturn the unconstrained-to-constrained bijector for distribution dist.\n\nBy default, this is just inverse(link_transform(dist)).\n\nwarning: Warning\nNote that currently this is not used by Bijectors.logpdf_with_trans, hence that needs to be overloaded separately if the intention is to change behavior of an existing distribution.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.maybe_invlink_before_eval!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.maybe_invlink_before_eval!!",
    "title": "DynamicPPL.maybe_invlink_before_eval!!",
    "section": "function",
    "text": "maybe_invlink_before_eval!!([t::Transformation,] vi, model)\n\nReturn a possibly invlinked version of vi.\n\nThis will be called prior to model evaluation, allowing one to perform a single invlink!! before evaluation rather than lazyily evaluating the transforms on as-we-need basis as is done with DynamicTransformation.\n\nSee also: StaticTransformation, DynamicTransformation.\n\nExamples\n\njulia> using DynamicPPL, Distributions, Bijectors\n\njulia> @model demo() = x ~ Normal()\ndemo (generic function with 2 methods)\n\njulia> # By subtyping `Transform`, we inherit the `(inv)link!!`.\n       struct MyBijector <: Bijectors.Transform end\n\njulia> # Define some dummy `inverse` which will be used in the `link!!` call.\n       Bijectors.inverse(f::MyBijector) = identity\n\njulia> # We need to define `with_logabsdet_jacobian` for `MyBijector`\n       # (`identity` already has `with_logabsdet_jacobian` defined)\n       function Bijectors.with_logabsdet_jacobian(::MyBijector, x)\n           # Just using a large number of the logabsdet-jacobian term\n           # for demonstration purposes.\n           return (x, 1000)\n       end\n\njulia> # Change the `default_transformation` for our model to be a\n       # `StaticTransformation` using `MyBijector`.\n       function DynamicPPL.default_transformation(::Model{typeof(demo)})\n           return DynamicPPL.StaticTransformation(MyBijector())\n       end\n\njulia> model = demo();\n\njulia> vi = SimpleVarInfo(x=1.0)\nSimpleVarInfo((x = 1.0,), 0.0)\n\njulia> # Uses the `inverse` of `MyBijector`, which we have defined as `identity`\n       vi_linked = link!!(vi, model)\nTransformed SimpleVarInfo((x = 1.0,), 0.0)\n\njulia> # Now performs a single `invlink!!` before model evaluation.\n       logjoint(model, vi_linked)\n-1001.4189385332047\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#Base.merge-Tuple{AbstractVarInfo}",
    "href": "../DynamicPPL.jl/api/#Base.merge-Tuple{AbstractVarInfo}",
    "title": "Base.merge",
    "section": "method",
    "text": "merge(varinfo, other_varinfos...)\n\nMerge varinfos into one, giving precedence to the right-most varinfo when sensible.\n\nThis is particularly useful when combined with subset(varinfo, vns).\n\nSee docstring of subset(varinfo, vns) for examples.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.subset",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.subset",
    "title": "DynamicPPL.subset",
    "section": "function",
    "text": "subset(varinfo::AbstractVarInfo, vns::AbstractVector{<:VarName})\n\nSubset a varinfo to only contain the variables vns.\n\nThe ordering of variables in the return value will be the same as in varinfo.\n\nExamples\n\njulia> @model function demo()\n           s ~ InverseGamma(2, 3)\n           m ~ Normal(0, sqrt(s))\n           x = Vector{Float64}(undef, 2)\n           x[1] ~ Normal(m, sqrt(s))\n           x[2] ~ Normal(m, sqrt(s))\n       end\ndemo (generic function with 2 methods)\n\njulia> model = demo();\n\njulia> varinfo = VarInfo(model);\n\njulia> keys(varinfo)\n4-element Vector{VarName}:\n s\n m\n x[1]\n x[2]\n\njulia> for (i, vn) in enumerate(keys(varinfo))\n           varinfo[vn] = i\n       end\n\njulia> varinfo[[@varname(s), @varname(m), @varname(x[1]), @varname(x[2])]]\n4-element Vector{Float64}:\n 1.0\n 2.0\n 3.0\n 4.0\n\njulia> # Extract one with only `m`.\n       varinfo_subset1 = subset(varinfo, [@varname(m),]);\n\n\njulia> keys(varinfo_subset1)\n1-element Vector{VarName{:m, typeof(identity)}}:\n m\n\njulia> varinfo_subset1[@varname(m)]\n2.0\n\njulia> # Extract one with both `s` and `x[2]`.\n       varinfo_subset2 = subset(varinfo, [@varname(s), @varname(x[2])]);\n\njulia> keys(varinfo_subset2)\n2-element Vector{VarName}:\n s\n x[2]\n\njulia> varinfo_subset2[[@varname(s), @varname(x[2])]]\n2-element Vector{Float64}:\n 1.0\n 4.0\n\nsubset is particularly useful when combined with merge(varinfo::AbstractVarInfo)\n\njulia> # Merge the two.\n       varinfo_subset_merged = merge(varinfo_subset1, varinfo_subset2);\n\njulia> keys(varinfo_subset_merged)\n3-element Vector{VarName}:\n m\n s\n x[2]\n\njulia> varinfo_subset_merged[[@varname(s), @varname(m), @varname(x[2])]]\n3-element Vector{Float64}:\n 1.0\n 2.0\n 4.0\n\njulia> # Merge the two with the original.\n       varinfo_merged = merge(varinfo, varinfo_subset_merged);\n\njulia> keys(varinfo_merged)\n4-element Vector{VarName}:\n s\n m\n x[1]\n x[2]\n\njulia> varinfo_merged[[@varname(s), @varname(m), @varname(x[1]), @varname(x[2])]]\n4-element Vector{Float64}:\n 1.0\n 2.0\n 3.0\n 4.0\n\nNotes\n\nType-stability\n\nwarning: Warning\nThis function is only type-stable when vns contains only varnames with the same symbol. For exmaple, [@varname(m[1]), @varname(m[2])] will be type-stable, but [@varname(m[1]), @varname(x)] will not be.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.unflatten",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.unflatten",
    "title": "DynamicPPL.unflatten",
    "section": "function",
    "text": "unflatten(vi::AbstractVarInfo, x::AbstractVector)\n\nReturn a new instance of vi with the values of x assigned to the variables.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#AbstractPPL.evaluate!!",
    "href": "../DynamicPPL.jl/api/#AbstractPPL.evaluate!!",
    "title": "AbstractPPL.evaluate!!",
    "section": "function",
    "text": "evaluate!!(model::Model, varinfo)\n\nEvaluate the model with the given varinfo.\n\nIf the model has been marked as requiring threadsafe evaluation, are available, the varinfo provided will be wrapped in a ThreadSafeVarInfo before evaluation.\n\nReturns a tuple of the model's return value, plus the updated varinfo (unwrapped if necessary).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.DefaultContext",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.DefaultContext",
    "title": "DynamicPPL.DefaultContext",
    "section": "type",
    "text": "struct DefaultContext <: AbstractContext end\n\nDefaultContext, as the name suggests, is the default context used when instantiating a model.\n\njulia> @model f() = x ~ Normal();\n\njulia> model = f(); model.context\nDefaultContext()\n\nAs an evaluation context, the behaviour of DefaultContext is to require all variables to be present in the AbstractVarInfo used for evaluation. Thus, semantically, evaluating a model with DefaultContext means 'calculating the log-probability associated with the variables in the AbstractVarInfo'.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.InitContext",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.InitContext",
    "title": "DynamicPPL.InitContext",
    "section": "type",
    "text": "InitContext(\n        [rng::Random.AbstractRNG=Random.default_rng()],\n        [strategy::AbstractInitStrategy=InitFromPrior()],\n)\n\nA leaf context that indicates that new values for random variables are currently being obtained through sampling. Used e.g. when initialising a fresh VarInfo. Note that, if leafcontext(model.context) isa InitContext, then evaluate!!(model, varinfo) will override all values in the VarInfo.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.tilde_assume!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.tilde_assume!!",
    "title": "DynamicPPL.tilde_assume!!",
    "section": "function",
    "text": "DynamicPPL.tilde_assume!!(\n    context::AbstractContext,\n    right::Distribution,\n    vn::VarName,\n    vi::AbstractVarInfo\n)\n\nHandle assumed variables, i.e. anything which is not observed (see tilde_observe!!). Accumulate the associated log probability, and return the sampled value and updated vi.\n\nvn is the VarName on the left-hand side of the tilde statement.\n\nThis function should return a tuple (x, vi), where x is the sampled value (which must be in unlinked space!) and vi is the updated VarInfo.\n\n\n\n\n\nDynamicPPL.tilde_assume!!(\n    ::DefaultContext, right::Distribution, vn::VarName, vi::AbstractVarInfo\n)\n\nHandle assumed variables. For DefaultContext, this function extracts the value associated with vn from vi, If vi does not contain an appropriate value then this will error.\n\n\n\n\n\nDynamicPPL.tilde_assume!!(\n    context::AbstractContext,\n    right::DynamicPPL.Submodel,\n    vn::VarName,\n    vi::AbstractVarInfo\n)\n\nEvaluate the submodel with the given context.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.tilde_observe!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.tilde_observe!!",
    "title": "DynamicPPL.tilde_observe!!",
    "section": "function",
    "text": "DynamicPPL.tilde_observe!!(\n    context::AbstractContext,\n    right::Distribution,\n    left,\n    vn::Union{VarName, Nothing},\n    vi::AbstractVarInfo\n)\n\nThis function handles observed variables, which may be:\n\nliterals on the left-hand side, e.g., 3.0 ~ Normal()\na model input, e.g. x ~ Normal() in a model @model f(x) ... end\na conditioned or fixed variable, e.g. x ~ Normal() in a model model | (; x = 3.0).\n\nThe relevant log-probability associated with the observation is computed and accumulated in the VarInfo object vi (except for fixed variables, which do not contribute to the log-probability).\n\nleft is the actual value that the left-hand side evaluates to. vn is the VarName on the left-hand side, or nothing if the left-hand side is a literal value.\n\nObservations of submodels are not yet supported in DynamicPPL.\n\nThis function should return a tuple (left, vi), where left is the same as the input, and vi is the updated VarInfo.\n\n\n\n\n\nDynamicPPL.tilde_observe!!(\n    ::DefaultContext,\n    right::Distribution,\n    left,\n    vn::Union{VarName,Nothing},\n    vi::AbstractVarInfo,\n)\n\nHandle observed variables. This just accumulates the log-likelihood for left.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.AbstractParentContext",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.AbstractParentContext",
    "title": "DynamicPPL.AbstractParentContext",
    "section": "type",
    "text": "AbstractParentContext\n\nAn abstract context that has a child context.\n\nSubtypes of AbstractParentContext must implement the following interface:\n\nDynamicPPL.childcontext(context::AbstractParentContext): Return the child context.\nDynamicPPL.setchildcontext(parent::AbstractParentContext, child::AbstractContext): Reconstruct parent but now using child as its child context.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.childcontext",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.childcontext",
    "title": "DynamicPPL.childcontext",
    "section": "function",
    "text": "childcontext(context::AbstractParentContext)\n\nReturn the descendant context of context.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.setchildcontext",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.setchildcontext",
    "title": "DynamicPPL.setchildcontext",
    "section": "function",
    "text": "setchildcontext(parent::AbstractParentContext, child::AbstractContext)\n\nReconstruct parent but now using child is its childcontext, effectively updating the child context.\n\nExamples\n\njulia> using DynamicPPL: DynamicTransformationContext, ConditionContext\n\njulia> ctx = ConditionContext((; a = 1));\n\njulia> DynamicPPL.childcontext(ctx)\nDefaultContext()\n\njulia> ctx_prior = DynamicPPL.setchildcontext(ctx, DynamicTransformationContext{true}());\n\njulia> DynamicPPL.childcontext(ctx_prior)\nDynamicTransformationContext{true}()\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.leafcontext",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.leafcontext",
    "title": "DynamicPPL.leafcontext",
    "section": "function",
    "text": "leafcontext(context::AbstractContext)\n\nReturn the leaf of context, i.e. the first descendant context that is not an AbstractParentContext.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.setleafcontext",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.setleafcontext",
    "title": "DynamicPPL.setleafcontext",
    "section": "function",
    "text": "setleafcontext(left::AbstractContext, right::AbstractContext)\n\nReturn left but now with its leaf context replaced by right.\n\nNote that this also works even if right is not a leaf context, in which case effectively append right to left, dropping the original leaf context of left.\n\nExamples\n\njulia> using DynamicPPL: leafcontext, setleafcontext, childcontext, setchildcontext, AbstractContext, DynamicTransformationContext\n\njulia> struct ParentContext{C} <: AbstractParentContext\n           context::C\n       end\n\njulia> DynamicPPL.childcontext(context::ParentContext) = context.context\n\njulia> DynamicPPL.setchildcontext(::ParentContext, child) = ParentContext(child)\n\njulia> Base.show(io::IO, c::ParentContext) = print(io, \"ParentContext(\", childcontext(c), \")\")\n\njulia> ctx = ParentContext(ParentContext(DefaultContext()))\nParentContext(ParentContext(DefaultContext()))\n\njulia> # Replace the leaf context with another leaf.\n       leafcontext(setleafcontext(ctx, DynamicTransformationContext{true}()))\nDynamicTransformationContext{true}()\n\njulia> # Append another parent context.\n       setleafcontext(ctx, ParentContext(DefaultContext()))\nParentContext(ParentContext(ParentContext(DefaultContext())))\n\n\n\n\n\nsetleafcontext(model::Model, context::AbstractContext)\n\nReturn a new Model with its leaf context set to context. This is a convenience shortcut for contextualize(model, setleafcontext(model.context, context)).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.init!!",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.init!!",
    "title": "DynamicPPL.init!!",
    "section": "function",
    "text": "init!!(\n    [rng::Random.AbstractRNG,]\n    model::Model,\n    varinfo::AbstractVarInfo,\n    [init_strategy::AbstractInitStrategy=InitFromPrior()]\n)\n\nEvaluate the model and replace the values of the model's random variables in the given varinfo with new values, using a specified initialisation strategy. If the values in varinfo are not set, they will be added using a specified initialisation strategy.\n\nIf init_strategy is not provided, defaults to InitFromPrior().\n\nReturns a tuple of the model's return value, plus the updated varinfo object.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.InitFromPrior",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.InitFromPrior",
    "title": "DynamicPPL.InitFromPrior",
    "section": "type",
    "text": "InitFromPrior()\n\nObtain new values by sampling from the prior distribution.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.InitFromUniform",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.InitFromUniform",
    "title": "DynamicPPL.InitFromUniform",
    "section": "type",
    "text": "InitFromUniform()\nInitFromUniform(lower, upper)\n\nObtain new values by first transforming the distribution of the random variable to unconstrained space, then sampling a value uniformly between lower and upper, and transforming that value back to the original space.\n\nIf lower and upper are unspecified, they default to (-2, 2), which mimics Stan's default initialisation strategy.\n\nRequires that lower <= upper.\n\nReferences\n\nStan reference manual page on initialization\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.InitFromParams",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.InitFromParams",
    "title": "DynamicPPL.InitFromParams",
    "section": "type",
    "text": "InitFromParams(\n    params::Any\n    fallback::Union{AbstractInitStrategy,Nothing}=InitFromPrior()\n)\n\nObtain new values by extracting them from the given set of params.\n\nThe most common use case is to provide a NamedTuple or AbstractDict{<:VarName}, which provides a mapping from variable names to values. However, we leave the type of params open in order to allow for custom parameter storage types.\n\nCustom parameter storage types\n\nFor InitFromParams to work correctly with a custom params::P, you need to implement\n\nDynamicPPL.init(rng, vn::VarName, dist::Distribution, p::InitFromParams{P}) where {P}\n\nThis tells you how to obtain values for the random variable vn from p.params. Note that the last argument is InitFromParams(params), not just params itself. Please see the docstring of DynamicPPL.init for more information on the expected behaviour.\n\nIf you only use InitFromParams with DynamicPPL.OnlyAccsVarInfo, as is usually the case, then you will not need to implement anything else. So far, this is the same as you would do for creating any new AbstractInitStrategy subtype.\n\nHowever, to use InitFromParams with a full DynamicPPL.VarInfo, you may also need to implement\n\nDynamicPPL.get_param_eltype(p::InitFromParams{P}) where {P}\n\nSee the docstring of DynamicPPL.get_param_eltype for more information on when this is needed.\n\nThe argument fallback specifies how new values are to be obtained if they cannot be found in params, or they are specified as missing. fallback can either be an initialisation strategy itself, in which case it will be used to obtain new values, or it can be nothing, in which case an error will be thrown. The default for fallback is InitFromPrior().\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.AbstractInitStrategy",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.AbstractInitStrategy",
    "title": "DynamicPPL.AbstractInitStrategy",
    "section": "type",
    "text": "AbstractInitStrategy\n\nAbstract type representing the possible ways of initialising new values for the random variables in a model (e.g., when creating a new VarInfo).\n\nAny subtype of AbstractInitStrategy must implement the DynamicPPL.init method, and in some cases, DynamicPPL.get_param_eltype (see its docstring for details).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.init",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.init",
    "title": "DynamicPPL.init",
    "section": "function",
    "text": "init(rng::Random.AbstractRNG, vn::VarName, dist::Distribution, strategy::AbstractInitStrategy)\n\nGenerate a new value for a random variable with the given distribution.\n\nThis function must return a tuple (x, trf), where\n\nx is the generated value\ntrf is a function that transforms the generated value back to the unlinked space. If the value is already in unlinked space, then this should be DynamicPPL.typed_identity. You can also use Base.identity, but if you use this, you must be confident that zero(eltype(x)) will never error. See the docstring of typed_identity for more information.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.get_param_eltype",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.get_param_eltype",
    "title": "DynamicPPL.get_param_eltype",
    "section": "function",
    "text": "DynamicPPL.get_param_eltype(strategy::AbstractInitStrategy)\n\nReturn the element type of the parameters generated from the given initialisation strategy.\n\nThe default implementation returns Any. However, for InitFromParams which provides known parameters for evaluating the model, methods are implemented in order to return more specific types.\n\nIn general, if you are implementing a custom AbstractInitStrategy, correct behaviour can only be guaranteed if you implement this method as well. However, quite often, the default return value of Any will actually suffice. The cases where this does not suffice, and where you do have to manually implement get_param_eltype, are explained in the extended help (see ??DynamicPPL.get_param_eltype in the REPL).\n\nExtended help\n\nThere are a few edge cases in DynamicPPL where the element type is needed. These largely relate to determining the element type of accumulators ahead of time (before evaluation), as well as promoting type parameters in model arguments. The classic case is when evaluating a model with ForwardDiff: the accumulators must be set to Duals, and any Vector{Float64} arguments must be promoted to Vector{Dual}. Other tracer types, for example those in SparseConnectivityTracer.jl, also require similar treatment.\n\nIf the AbstractInitStrategy is never used in combination with tracer types, then it is perfectly safe to return Any. This does not lead to type instability downstream because the actual accumulators will still be created with concrete Float types (the Any is just used to determine whether the float type needs to be modified).\n\nIn case that wasn't enough: in fact, even the above is not always true. Firstly, the accumulator argument is only true when evaluating with ThreadSafeVarInfo. See the comments in DynamicPPL.unflatten for more details. For non-threadsafe evaluation, Julia is capable of automatically promoting the types on its own. Secondly, the promotion only matters if you are trying to directly assign into a Vector{Float64} with a ForwardDiff.Dual or similar tracer type, for example using xs[i] = MyDual. This doesn't actually apply to tilde-statements like xs[i] ~ ... because those use Accessors.@set under the hood, which also does the promotion for you. For the gory details, see the following issues:\n\nhttps://github.com/TuringLang/DynamicPPL.jl/issues/906 for accumulator types\nhttps://github.com/TuringLang/DynamicPPL.jl/issues/823 for type argument promotion\n\n\n\n\n\nget_param_eltype(varinfo::AbstractVarInfo, context::AbstractContext)\n\nGet the element type of the parameters being used to evaluate a model, using a varinfo under the given context. For example, when evaluating a model with ForwardDiff AD, this should return ForwardDiff.Dual.\n\nBy default, this uses eltype(varinfo) which is slightly cursed. This relies on the fact that typically, before evaluation, the parameters will have been inserted into the VarInfo's metadata field.\n\nFor InitContext, it's quite different: because InitContext is responsible for supplying the parameters, we can avoid using eltype(varinfo) and instead query the parameters inside it. See the docstring of get_param_eltype(strategy::AbstractInitStrategy) for more explanation.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.Experimental.determine_suitable_varinfo",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.Experimental.determine_suitable_varinfo",
    "title": "DynamicPPL.Experimental.determine_suitable_varinfo",
    "section": "function",
    "text": "determine_suitable_varinfo(model; only_dppl::Bool=true)\n\nReturn a suitable varinfo for the given model.\n\nSee also: DynamicPPL.Experimental.is_suitable_varinfo.\n\nwarning: Warning\nFor full functionality, this requires JET.jl to be loaded. If JET.jl is not loaded, this function will assume the model is compatible with typed varinfo.\n\nArguments\n\nmodel: The model for which to determine the varinfo.\n\nKeyword Arguments\n\nonly_dppl: If true, only consider error reports within DynamicPPL.jl.\n\nExamples\n\njulia> using DynamicPPL.Experimental: determine_suitable_varinfo\n\njulia> using JET: JET  # needs to be loaded for full functionality\n\njulia> @model function model_with_random_support()\n           x ~ Bernoulli()\n           if x\n               y ~ Normal()\n           else\n               z ~ Normal()\n           end\n       end\nmodel_with_random_support (generic function with 2 methods)\n\njulia> model = model_with_random_support();\n\njulia> # Typed varinfo cannot handle this random support model properly\n       # as using a single execution of the model will not see all random variables.\n       # Hence, this this model requires untyped varinfo.\n       vi = determine_suitable_varinfo(model);\n┌ Warning: Model seems incompatible with typed varinfo. Falling back to untyped varinfo.\n└ @ DynamicPPLJETExt ~/.julia/dev/DynamicPPL.jl/ext/DynamicPPLJETExt.jl:48\n\njulia> vi isa typeof(DynamicPPL.untyped_varinfo(model))\ntrue\n\njulia> # In contrast, a simple model with no random support can be handled by typed varinfo.\n       @model model_with_static_support() = x ~ Normal()\nmodel_with_static_support (generic function with 2 methods)\n\njulia> vi = determine_suitable_varinfo(model_with_static_support());\n\njulia> vi isa typeof(DynamicPPL.typed_varinfo(model_with_static_support()))\ntrue\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.Experimental.is_suitable_varinfo",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.Experimental.is_suitable_varinfo",
    "title": "DynamicPPL.Experimental.is_suitable_varinfo",
    "section": "function",
    "text": "is_suitable_varinfo(model::Model, varinfo::AbstractVarInfo; kwargs...)\n\nCheck if the model supports evaluation using the provided varinfo.\n\nwarning: Warning\nLoading JET.jl is required before calling this function.\n\nArguments\n\nmodel: The model to verify the support for.\nvarinfo: The varinfo to verify the support for.\n\nKeyword Arguments\n\nonly_dppl: If true, only consider error reports occuring in the tilde pipeline. Default: true.\n\nReturns\n\nissuccess: true if the model supports the varinfo, otherwise false.\nreport: The result of report_call from JET.jl.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#DynamicPPL.ParamsWithStats",
    "href": "../DynamicPPL.jl/api/#DynamicPPL.ParamsWithStats",
    "title": "DynamicPPL.ParamsWithStats",
    "section": "type",
    "text": "ParamsWithStats\n\nA struct which contains parameter values extracted from a VarInfo, along with any statistics associated with the VarInfo. The statistics are provided as a NamedTuple and are optional.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#AbstractMCMC.from_samples-Tuple{Type{Chains}, AbstractMatrix{<:ParamsWithStats}}",
    "href": "../DynamicPPL.jl/api/#AbstractMCMC.from_samples-Tuple{Type{Chains}, AbstractMatrix{<:ParamsWithStats}}",
    "title": "AbstractMCMC.from_samples",
    "section": "method",
    "text": "AbstractMCMC.from_samples(\n    ::Type{MCMCChains.Chains},\n    params_and_stats::AbstractMatrix{<:ParamsWithStats}\n)\n\nConvert an array of DynamicPPL.ParamsWithStats to an MCMCChains.Chains object.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "api/#AbstractMCMC.to_samples-Tuple{Type{ParamsWithStats}, Chains}",
    "href": "../DynamicPPL.jl/api/#AbstractMCMC.to_samples-Tuple{Type{ParamsWithStats}, Chains}",
    "title": "AbstractMCMC.to_samples",
    "section": "method",
    "text": "AbstractMCMC.to_samples(\n    ::Type{DynamicPPL.ParamsWithStats},\n    chain::MCMCChains.Chains\n)\n\nConvert an MCMCChains.Chains object to an array of DynamicPPL.ParamsWithStats.\n\nFor this to work, chain must contain the varname_to_symbol mapping in its info field.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "#DynamicPPL.jl",
    "href": "../DynamicPPL.jl/#DynamicPPL.jl",
    "title": "DynamicPPL.jl",
    "section": "section",
    "text": "A domain-specific language and backend for probabilistic programming languages, used by Turing.jl.\n\n",
    "crumbs": null
  },
  {
    "objectID": "types/#Types-API",
    "href": "../Bijectors.jl/types/#Types-API",
    "title": "Types API",
    "section": "section",
    "text": "This page includes docstrings for some types defined in Bijectors.",
    "crumbs": null
  },
  {
    "objectID": "types/#General-types",
    "href": "../Bijectors.jl/types/#General-types",
    "title": "General types",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "types/#Specific-bijectors",
    "href": "../Bijectors.jl/types/#Specific-bijectors",
    "title": "Specific bijectors",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "types/#Bijectors.Transform",
    "href": "../Bijectors.jl/types/#Bijectors.Transform",
    "title": "Bijectors.Transform",
    "section": "type",
    "text": "Abstract type for a transformation.\n\nImplementing\n\nA subtype of Transform of should at least implement transform(b, x).\n\nIf the Transform is also invertible:\n\nRequired:\nEither of the following:\ntransform(::Inverse{<:MyTransform}, x): the transform for its inverse.\nInverseFunctions.inverse(b::MyTransform): returns an existing Transform.\nlogabsdetjac: computes the log-abs-det jacobian factor.\nOptional:\nwith_logabsdet_jacobian: transform and logabsdetjac combined. Useful in cases where we can exploit shared computation in the two.\n\nFor the above methods, there are mutating versions which can optionally be implemented:\n\nwith_logabsdet_jacobian!\nlogabsdetjac!\nwith_logabsdet_jacobian!\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "types/#Bijectors.Bijector",
    "href": "../Bijectors.jl/types/#Bijectors.Bijector",
    "title": "Bijectors.Bijector",
    "section": "type",
    "text": "Abstract type of a bijector, i.e. differentiable bijection with differentiable inverse.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "types/#Bijectors.Inverse",
    "href": "../Bijectors.jl/types/#Bijectors.Inverse",
    "title": "Bijectors.Inverse",
    "section": "type",
    "text": "inverse(b::Transform)\nInverse(b::Transform)\n\nA Transform representing the inverse transform of b.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "types/#Bijectors.CorrBijector",
    "href": "../Bijectors.jl/types/#Bijectors.CorrBijector",
    "title": "Bijectors.CorrBijector",
    "section": "type",
    "text": "CorrBijector <: Bijector\n\nA bijector implementation of Stan's parametrization method for Correlation matrix: https://mc-stan.org/docs/reference-manual/transforms.html#correlation-matrix-transform.section\n\nBasically, a unconstrained strictly upper triangular matrix y is transformed to  a correlation matrix by following readable but not that efficient form:\n\nK = size(y, 1)\nz = tanh.(y)\n\nfor j=1:K, i=1:K\n    if i>j\n        w[i,j] = 0\n    elseif 1==i==j\n        w[i,j] = 1\n    elseif 1<i==j\n        w[i,j] = prod(sqrt(1 .- z[1:i-1, j].^2))\n    elseif 1==i<j\n        w[i,j] = z[i,j]\n    elseif 1<i<j\n        w[i,j] = z[i,j] * prod(sqrt(1 .- z[1:i-1, j].^2))\n    end\nend\n\nIt is easy to see that every column is a unit vector, for example:\n\nw3' w3 ==\nw[1,3]^2 + w[2,3]^2 + w[3,3]^2 ==\nz[1,3]^2 + (z[2,3] * sqrt(1 - z[1,3]^2))^2 + (sqrt(1-z[1,3]^2) * sqrt(1-z[2,3]^2))^2 ==\nz[1,3]^2 + z[2,3]^2 * (1-z[1,3]^2) + (1-z[1,3]^2) * (1-z[2,3]^2) ==\nz[1,3]^2 + z[2,3]^2 - z[2,3]^2 * z[1,3]^2 + 1 -z[1,3]^2 - z[2,3]^2 + z[1,3]^2 * z[2,3]^2 ==\n1\n\nAnd diagonal elements are positive, so w is a cholesky factor for a positive matrix.\n\nx = w' * w\n\nConsider block matrix representation for x\n\nx = [w1'; w2'; ... wn'] * [w1 w2 ... wn] == \n[w1'w1 w1'w2 ... w1'wn;\n w2'w1 w2'w2 ... w2'wn;\n ...\n]\n\nThe diagonal elements are given by wk'wk = 1, thus x is a correlation matrix.\n\nEvery step is invertible, so this is a bijection(bijector).\n\nNote: The implementation doesn't follow their \"manageable expression\" directly, because their equation seems wrong (7/30/2020). Insteadly it follows definition  above the \"manageable expression\" directly, which is also described in above doc.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "types/#Bijectors.LeakyReLU",
    "href": "../Bijectors.jl/types/#Bijectors.LeakyReLU",
    "title": "Bijectors.LeakyReLU",
    "section": "type",
    "text": "LeakyReLU{T}(α::T) <: Bijector\n\nDefines the invertible mapping\n\nx ↦ x if x ≥ 0 else αx\n\nwhere α > 0.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "types/#Bijectors.Stacked",
    "href": "../Bijectors.jl/types/#Bijectors.Stacked",
    "title": "Bijectors.Stacked",
    "section": "type",
    "text": "Stacked(bs)\nStacked(bs, ranges)\nStacked(bs::Bijector...)\n\nA Bijector which stacks bijectors together which can then be applied to a vector where bs[i]::Bijector is applied to x[ranges[i]]::UnitRange{Int}.\n\nArguments\n\nbs can be either a Tuple or an AbstractArray of 0- and/or 1-dimensional bijectors\nIf bs is a Tuple, implementations are type-stable using generated functions\nIf bs is an AbstractArray, implementations are not type-stable and use iterative methods\nranges needs to be an iterable consisting of UnitRange{Int}\nlength(bs) == length(ranges) needs to be true.\n\nExamples\n\nusing Bijectors: Logit, Stacked\nb1 = Logit(0.0, 1.0)\nb2 = identity\nb = Stacked(b1, b2)\nb([0.0, 1.0]) == [b1(0.0), 1.0]  # => true\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "types/#Bijectors.RationalQuadraticSpline",
    "href": "../Bijectors.jl/types/#Bijectors.RationalQuadraticSpline",
    "title": "Bijectors.RationalQuadraticSpline",
    "section": "type",
    "text": "RationalQuadraticSpline{T} <: Bijector\n\nImplementation of the Rational Quadratic Spline flow [1].\n\nOutside of the interval [minimum(widths), maximum(widths)], this mapping is given  by the identity map. \nInside the interval it's given by a monotonic spline (i.e. monotonic polynomials  connected at intermediate points) with endpoints fixed so as to continuously transform into the identity map.\n\nFor the sake of efficiency, there are separate implementations for 0-dimensional and 1-dimensional inputs.\n\nNotes\n\nThere are two constructors for RationalQuadraticSpline:\n\nRationalQuadraticSpline(widths, heights, derivatives): it is assumed that widths, \n\nheights, and derivatives satisfy the constraints that makes this a valid bijector, i.e.\n\nwidths: monotonically increasing and length(widths) == K,\nheights: monotonically increasing and length(heights) == K,\nderivatives: non-negative and derivatives[1] == derivatives[end] == 1.\nRationalQuadraticSpline(widths, heights, derivatives, B): other than than the lengths,  no assumptions are made on parameters. Therefore we will transform the parameters s.t.:\nwidths_new ∈ [-B, B]ᴷ⁺¹, where K == length(widths),\nheights_new ∈ [-B, B]ᴷ⁺¹, where K == length(heights),\nderivatives_new ∈ (0, ∞)ᴷ⁺¹ with derivatives_new[1] == derivates_new[end] == 1,  where (K - 1) == length(derivatives).\n\nExamples\n\nUnivariate\n\njulia> using StableRNGs: StableRNG; rng = StableRNG(42);  # For reproducibility.\n\njulia> using Bijectors: RationalQuadraticSpline\n\njulia> K = 3; B = 2;\n\njulia> # Monotonic spline on '[-B, B]' with `K` intermediate knots/\"connection points\".\n       b = RationalQuadraticSpline(randn(rng, K), randn(rng, K), randn(rng, K - 1), B);\n\njulia> b(0.5) # inside of `[-B, B]` → transformed\n1.1943325397834206\n\njulia> b(5.) # outside of `[-B, B]` → not transformed\n5.0\n\njulia> b = RationalQuadraticSpline(b.widths, b.heights, b.derivatives);\n\njulia> b(0.5) # inside of `[-B, B]` → transformed\n1.1943325397834206\n\njulia> d = 2; K = 3; B = 2;\n\njulia> b = RationalQuadraticSpline(randn(rng, d, K), randn(rng, d, K), randn(rng, d, K - 1), B);\n\njulia> b([-1., 1.])\n2-element Vector{Float64}:\n -1.5660106244288925\n  0.5384702734738573\n\njulia> b([-5., 5.])\n2-element Vector{Float64}:\n -5.0\n  5.0\n\njulia> b([-1., 5.])\n2-element Vector{Float64}:\n -1.5660106244288925\n  5.0\n\nReferences\n\n[1] Durkan, C., Bekasov, A., Murray, I., & Papamakarios, G., Neural Spline Flows, CoRR, arXiv:1906.04032 [stat.ML],  (2019). \n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "types/#Bijectors.Coupling",
    "href": "../Bijectors.jl/types/#Bijectors.Coupling",
    "title": "Bijectors.Coupling",
    "section": "type",
    "text": "Coupling{F, M}(θ::F, mask::M)\n\nImplements a coupling-layer as defined in [1].\n\nExamples\n\njulia> using Bijectors: Shift, Coupling, PartitionMask, coupling, couple\n\njulia> m = PartitionMask(3, [1], [2]); # <= going to use x[2] to parameterize transform of x[1]\n\njulia> cl = Coupling(Shift, m); # <= will do `y[1:1] = x[1:1] + x[2:2]`;\n\njulia> x = [1., 2., 3.];\n\njulia> cl(x)\n3-element Vector{Float64}:\n 3.0\n 2.0\n 3.0\n\njulia> inverse(cl)(cl(x))\n3-element Vector{Float64}:\n 1.0\n 2.0\n 3.0\n\njulia> coupling(cl) # get the `Bijector` map `θ -> b(⋅, θ)`\nShift\n\njulia> couple(cl, x) # get the `Bijector` resulting from `x`\nShift{Vector{Float64}}([2.0])\n\njulia> with_logabsdet_jacobian(cl, x)\n([3.0, 2.0, 3.0], 0.0)\n\nReferences\n\n[1] Kobyzev, I., Prince, S., & Brubaker, M. A., Normalizing flows: introduction and ideas, CoRR, (),  (2019). \n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "types/#Bijectors.OrderedBijector",
    "href": "../Bijectors.jl/types/#Bijectors.OrderedBijector",
    "title": "Bijectors.OrderedBijector",
    "section": "type",
    "text": "OrderedBijector()\n\nA bijector mapping unordered vectors in ℝᵈ to ordered vectors in ℝᵈ.\n\nSee also\n\nStan's documentation\nNote that this transformation and its inverse are the opposite of in this reference.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "types/#Bijectors.NamedTransform",
    "href": "../Bijectors.jl/types/#Bijectors.NamedTransform",
    "title": "Bijectors.NamedTransform",
    "section": "type",
    "text": "NamedTransform <: AbstractNamedTransform\n\nWraps a NamedTuple of key -> Bijector pairs, implementing evaluation, inversion, etc.\n\nExamples\n\njulia> using Bijectors: NamedTransform, Scale\n\njulia> b = NamedTransform((a = Scale(2.0), b = exp));\n\njulia> x = (a = 1., b = 0., c = 42.);\n\njulia> b(x)\n(a = 2.0, b = 1.0, c = 42.0)\n\njulia> (a = 2 * x.a, b = exp(x.b), c = x.c)\n(a = 2.0, b = 1.0, c = 42.0)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "types/#Bijectors.NamedCoupling",
    "href": "../Bijectors.jl/types/#Bijectors.NamedCoupling",
    "title": "Bijectors.NamedCoupling",
    "section": "type",
    "text": "NamedCoupling{target, deps, F} <: AbstractNamedTransform\n\nImplements a coupling layer for named bijectors.\n\nSee also: Coupling\n\nExamples\n\njulia> using Bijectors: NamedCoupling, Scale\n\njulia> b = NamedCoupling(:b, (:a, :c), (a, c) -> Scale(a + c));\n\njulia> x = (a = 1., b = 2., c = 3.);\n\njulia> b(x)\n(a = 1.0, b = 8.0, c = 3.0)\n\njulia> (a = x.a, b = (x.a + x.c) * x.b, c = x.c)\n(a = 1.0, b = 8.0, c = 3.0)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "flows/#Example:-Normalizing-flows",
    "href": "../Bijectors.jl/flows/#Example:-Normalizing-flows",
    "title": "Example: Normalizing flows",
    "section": "section",
    "text": "A very interesting application of bijectors is in normalizing flows. Usually this is done by sampling from a multivariate normal distribution, and then transforming this to a target distribution using invertible neural networks. Currently there are two such transforms available in Bijectors.jl: PlanarLayer and RadialLayer. Let's create a flow with a single PlanarLayer:\n\nusing Bijectors\nusing StableRNGs: StableRNG\nrng = StableRNG(42)\n\nd = MvNormal(zeros(2), ones(2))\nb = PlanarLayer(2)\nflow = transformed(d, b)\n\nflow is itself a multivariate distribution, so we can sample from it using rand and compute the logpdf, like any other Distribution.\n\ny = rand(rng, flow)\nlogpdf(flow, y)         # uses inverse of `b`\n\nSimilarily to the multivariate ADVI example, we could use Stacked to get a bounded flow:\n\nd = MvNormal(zeros(2), ones(2));\nibs = inverse.(bijector.((InverseGamma(2, 3), Beta())));\nsb = Stacked(ibs) # == Stacked(ibs, [i:i for i = 1:length(ibs)]\nb = sb ∘ PlanarLayer(2)\ntd = transformed(d, b);\ny = rand(rng, td)\n\n(As required, we have that 0 < y[1] and 0 ≤ y[2] ≤ 1.)\n\nTo fit the flow, we can define an objective function that computes the negative log-likelihood of some data. We will need to use automatic differentiation to compute gradients of the objective with respect to the parameters. Since most AD packages require vectorised inputs, this means we also need a way to convert between the vectorised parameters and the PlanarLayer struct.\n\nusing ForwardDiff\n\n# Construct the flow.\nb = PlanarLayer(2)\n\n# Obtain a vectorised version of the parameters.\nxs_init = vcat(b.w, b.u, b.b)\n\n# Function to reconstruct the `PlanarLayer` from vectorised parameters.\nfunction reconstruct_planarlayer(xs::AbstractVector)\n    dim = 2\n    w = xs[1:dim]\n    u = xs[(dim + 1):(2 * dim)]\n    b = xs[end:end]\n    return PlanarLayer(w, u, b)\nend\n\n# Check that the reconstruction does work...\nreconstruct_planarlayer(xs_init) == b\n\nHere is the objective function:\n\n# Make the objective a `struct` to avoid capturing global variables.\nstruct NLLObjective{R,D,T}\n    reconstruct::R\n    basedist::D\n    data::T\nend\n\nfunction (obj::NLLObjective)(xs::AbstractVector)\n    transformed_dist = transformed(obj.basedist, obj.reconstruct(xs))\n    return -sum(Base.Fix1(logpdf, transformed_dist), eachcol(obj.data))\nend\n\n# Some random data to estimate the density of.\nxs = randn(2, 1000)\n\n# Construct the objective.\nf = NLLObjective(reconstruct_planarlayer, MvNormal(2, 1), xs)\n\nprintln(\"Initial loss = $(f(xs_init)) at xs_init = $(xs_init)\")\n\nNow we can train the flow using gradient descent:\n\nusing ForwardDiff: ForwardDiff\n\nfunction train(xs_init, niters; stepsize=1e-3)\n    xs = xs_init\n    for i in 1:niters\n        grad = ForwardDiff.gradient(f, xs)\n        @. xs = xs - (stepsize * grad)\n    end\n    return xs\nend\nxs_trained = train(xs_init, 1000)\n\nprintln(\"Final loss = $(f(xs_trained)) at xs_trained = $(xs_trained)\")\n\nFinally, we can sample from the trained flow and check that the samples have approximately zero mean and identity covariance (as expected given that our data was sampled using randn):\n\nsamples = rand(transformed(f.basedist, f.reconstruct(xs_trained)), 1000);\n\n# mean ≈ [0, 0], cov ≈ I\nmean(eachcol(samples)), cov(samples; dims=2)\n\nMore complex flows can be created by composing multiple layers, e.g. PlanarLayer(10) ∘ PlanarLayer(10) ∘ RadialLayer(10).",
    "crumbs": null
  },
  {
    "objectID": "interface/#Interface",
    "href": "../Bijectors.jl/interface/#Interface",
    "title": "Interface",
    "section": "section",
    "text": "This page describes the user-facing interface of Bijectors.jl. You should be able to use all the functions documented here with any bijector defined in Bijectors.jl.",
    "crumbs": null
  },
  {
    "objectID": "interface/#Transformation",
    "href": "../Bijectors.jl/interface/#Transformation",
    "title": "Transformation",
    "section": "section",
    "text": "Bijectors are also callable objects, so b(x) is equivalent to transform(b, x).",
    "crumbs": null
  },
  {
    "objectID": "interface/#Inverses",
    "href": "../Bijectors.jl/interface/#Inverses",
    "title": "Inverses",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "interface/#Log-absolute-determinant-of-the-Jacobian",
    "href": "../Bijectors.jl/interface/#Log-absolute-determinant-of-the-Jacobian",
    "title": "Log-absolute determinant of the Jacobian",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "interface/#Transform-wrappers",
    "href": "../Bijectors.jl/interface/#Transform-wrappers",
    "title": "Transform wrappers",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "interface/#Elementwise-transformation",
    "href": "../Bijectors.jl/interface/#Elementwise-transformation",
    "title": "Elementwise transformation",
    "section": "section",
    "text": "Some transformations are well-defined for different types of inputs, e.g. exp can also act elementwise on an N-dimensional Array{<:Real,N}. To specify that a transformation should act elementwise, we can wrap it in the elementwise wrapper:",
    "crumbs": null
  },
  {
    "objectID": "interface/#Columnwise-transformation",
    "href": "../Bijectors.jl/interface/#Columnwise-transformation",
    "title": "Columnwise transformation",
    "section": "section",
    "text": "Likewise:",
    "crumbs": null
  },
  {
    "objectID": "interface/#Working-with-distributions",
    "href": "../Bijectors.jl/interface/#Working-with-distributions",
    "title": "Working with distributions",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "interface/#Utilities",
    "href": "../Bijectors.jl/interface/#Utilities",
    "title": "Utilities",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.transform",
    "href": "../Bijectors.jl/interface/#Bijectors.transform",
    "title": "Bijectors.transform",
    "section": "function",
    "text": "transform(b, x)\n\nTransform x using b.\n\nIf with_logabsdet_jacobian is already implemented for b, the default implementation of transform will call first(with_logabsdet_jacobian(b, x)).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.transform!",
    "href": "../Bijectors.jl/interface/#Bijectors.transform!",
    "title": "Bijectors.transform!",
    "section": "function",
    "text": "transform!(b, x[, y])\n\nTransform x using b, storing the result in y.\n\nIf y is not provided, x is used as the output.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#InverseFunctions.inverse",
    "href": "../Bijectors.jl/interface/#InverseFunctions.inverse",
    "title": "InverseFunctions.inverse",
    "section": "function",
    "text": "inverse(t::Transform)\n\nReturns the inverse of transform t.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.logabsdetjac",
    "href": "../Bijectors.jl/interface/#Bijectors.logabsdetjac",
    "title": "Bijectors.logabsdetjac",
    "section": "function",
    "text": "logabsdetjac(b, x)\n\nReturn log(abs(det(J(b, x)))), where J(b, x) is the jacobian of b at x.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.logabsdetjac!",
    "href": "../Bijectors.jl/interface/#Bijectors.logabsdetjac!",
    "title": "Bijectors.logabsdetjac!",
    "section": "function",
    "text": "logabsdetjac!(b, x[, logjac])\n\nCompute log(abs(det(J(b, x)))) and store the result in logjac, where J(b, x) is the jacobian of b at x.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.logabsdetjacinv",
    "href": "../Bijectors.jl/interface/#Bijectors.logabsdetjacinv",
    "title": "Bijectors.logabsdetjacinv",
    "section": "function",
    "text": "logabsdetjacinv(b, y)\n\nJust an alias for logabsdetjac(inverse(b), y).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#ChangesOfVariables.with_logabsdet_jacobian",
    "href": "../Bijectors.jl/interface/#ChangesOfVariables.with_logabsdet_jacobian",
    "title": "ChangesOfVariables.with_logabsdet_jacobian",
    "section": "function",
    "text": "with_logabsdet_jacobian(t::Transform, x)\n\nSemantically, this must return a tuple of (y, logabsdetjac), where y = transform(t, x) and logabsdetjac = logabsdetjac(t, x). However, you can implement this function to exploit shared computation between the two quantities.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.with_logabsdet_jacobian!",
    "href": "../Bijectors.jl/interface/#Bijectors.with_logabsdet_jacobian!",
    "title": "Bijectors.with_logabsdet_jacobian!",
    "section": "function",
    "text": "with_logabsdet_jacobian!(b, x[, y, logjac])\n\nCompute transform(b, x) and logabsdetjac(b, x), storing the result in y and logjac, respetively.\n\nIf y is not provided, then x will be used in its place.\n\nDefaults to calling with_logabsdet_jacobian(b, x) and updating y and logjac with the result.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.elementwise",
    "href": "../Bijectors.jl/interface/#Bijectors.elementwise",
    "title": "Bijectors.elementwise",
    "section": "function",
    "text": "elementwise(f)\n\nAlias for Base.Fix1(broadcast, f).\n\nIn the case where f::ComposedFunction, the result is Base.Fix1(broadcast, f.outer) ∘ Base.Fix1(broadcast, f.inner) rather than Base.Fix1(broadcast, f).\n\nExamples\n\njulia> x = [1.0, 2.0, 3.0];\n\njulia> f = elementwise(exp);\n\njulia> f(x)\n3-element Vector{Float64}:\n  2.718281828459045\n  7.38905609893065\n 20.085536923187668\n\njulia> with_logabsdet_jacobian(f, x)\n([2.718281828459045, 7.38905609893065, 20.085536923187668], 6.0)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.columnwise",
    "href": "../Bijectors.jl/interface/#Bijectors.columnwise",
    "title": "Bijectors.columnwise",
    "section": "function",
    "text": "columnwise(f)\n\nAlias for Base.Fix1(eachcolmaphcat, f).\n\nRepresents a function f which is applied to each column of an input.\n\nExamples\n\njulia> x = [4.0 5.0 6.0; 1.0 2.0 3.0];\n\njulia> my_reverse(v) = reverse(v);  # To avoid type piracy.\n\njulia> f = columnwise(my_reverse);\n\njulia> f(x)\n2×3 Matrix{Float64}:\n 1.0  2.0  3.0\n 4.0  5.0  6.0\n\njulia> # We can't use `with_logabsdet_jacobian` on `f` until we define it\n       # for `my_reverse`, since we need to sum over columns.\n       Bijectors.with_logabsdet_jacobian(::typeof(my_reverse), xs) = my_reverse(xs), 0.0;\n\njulia> with_logabsdet_jacobian(f, x)\n([1.0 2.0 3.0; 4.0 5.0 6.0], 0.0)\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.bijector",
    "href": "../Bijectors.jl/interface/#Bijectors.bijector",
    "title": "Bijectors.bijector",
    "section": "function",
    "text": "bijector(d::Distribution)\n\nReturns the constrained-to-unconstrained bijector for distribution d.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.link",
    "href": "../Bijectors.jl/interface/#Bijectors.link",
    "title": "Bijectors.link",
    "section": "function",
    "text": "link(d::Distribution, x)\n\nTransforms the input x using the constrained-to-unconstrained bijector for distribution d.\n\nSee also: invlink.\n\nExample\n\njulia> using Bijectors\n\njulia> d = LogNormal()   # support is (0, Inf)\nLogNormal{Float64}(μ=0.0, σ=1.0)\n\njulia> b = bijector(d)   # log function transforms to unconstrained space\n(::Base.Fix1{typeof(broadcast), typeof(log)}) (generic function with 2 methods)\n\njulia> b(1.0)\n0.0\n\njulia> link(LogNormal(), 1.0)\n0.0\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.invlink",
    "href": "../Bijectors.jl/interface/#Bijectors.invlink",
    "title": "Bijectors.invlink",
    "section": "function",
    "text": "invlink(d::Distribution, y)\n\nPerforms the inverse transform on a value y that was transformed using the constrained-to-unconstrained bijector for distribution d.\n\nIt should hold that invlink(d, link(d, x)) = x.\n\nSee also: link.\n\nExample\n\njulia> using Bijectors\n\njulia> d = LogNormal()    # support is (0, Inf)\nLogNormal{Float64}(μ=0.0, σ=1.0)\n\njulia> link(LogNormal(), 1.0)   # uses a log transform\n0.0\n\njulia> invlink(LogNormal(), 0.0)\n1.0\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.logpdf_with_trans",
    "href": "../Bijectors.jl/interface/#Bijectors.logpdf_with_trans",
    "title": "Bijectors.logpdf_with_trans",
    "section": "function",
    "text": "logpdf_with_trans(d::Distribution, x, transform::Bool)\n\nIf transform is false, logpdf_with_trans calculates the log probability density function (logpdf) of distribution d at x.\n\nIf transform is true, x is transformed using the constrained-to-unconstrained bijector for distribution d, and then the logpdf of the resulting value is calculated with respect to the unconstrained (transformed) distribution. Equivalently, if x is distributed according to d and y = link(d, x) is distributed according to td = transformed(d), then logpdf_with_trans(d, x, true) = logpdf(td, y). This is accomplished by subtracting the log Jacobian of the transformation.\n\nExample\n\njulia> using Bijectors\n\njulia> logpdf_with_trans(LogNormal(), ℯ, false)\n-2.4189385332046727\n\njulia> logpdf(LogNormal(), ℯ)  # Same as above\n-2.4189385332046727\n\njulia> logpdf_with_trans(LogNormal(), ℯ, true)\n-1.4189385332046727\n\njulia> # If x ~ LogNormal(), then log(x) ~ Normal()\n       logpdf(Normal(), 1.0)   \n-1.4189385332046727\n\njulia> # The difference between the two is due to the Jacobian\n       logabsdetjac(bijector(LogNormal()), ℯ)\n-1\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.output_size",
    "href": "../Bijectors.jl/interface/#Bijectors.output_size",
    "title": "Bijectors.output_size",
    "section": "function",
    "text": "output_size(f, sz)\n\nReturns the size of f(x) when given an input x of size sz.\n\n\n\n\n\noutput_size(f, dist::Distribution)\n\nReturns the output size of f given an input drawn from the distribution dist.\n\nBy default this just calls output_size(f, size(dist)), but this can be overloaded for specific distributions. This is useful when Base.size(dist) is not defined, e.g. for ProductNamedTupleDistribution and in particular is used by DynamicPPL when generating new random values for transformed distributions.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.transformed-Tuple{Distribution, Bijector}",
    "href": "../Bijectors.jl/interface/#Bijectors.transformed-Tuple{Distribution, Bijector}",
    "title": "Bijectors.transformed",
    "section": "method",
    "text": "transformed(d::Distribution)\ntransformed(d::Distribution, b::Bijector)\n\nCouples distribution d with the bijector b by returning a TransformedDistribution.\n\nIf no bijector is provided, i.e. transformed(d) is called, then  transformed(d, bijector(d)) is returned.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.ordered",
    "href": "../Bijectors.jl/interface/#Bijectors.ordered",
    "title": "Bijectors.ordered",
    "section": "function",
    "text": "ordered(d::Distribution)\n\nReturn a Distribution whose support are ordered vectors, i.e., vectors with increasingly ordered elements.\n\nSpecifically, d is restricted to the subspace of its domain containing only ordered elements.\n\nwarning: Warning\nrand is implemented using rejection sampling, which can be slow for high-dimensional distributions. In such cases, consider using MCMC methods to sample from the distribution instead.\n\nwarning: Warning\nThe resulting ordered distribution is un-normalized, which can cause issues in some contexts, e.g. in hierarchical models where the parameters of the ordered distribution are themselves sampled. See the notes below for a more detailed discussion.\n\nNotes on ordered being un-normalized\n\nThe resulting ordered distribution is un-normalized. This is not a problem if used in a context where the normalizing factor is irrelevant, but if the value of the normalizing factor impacts the resulting computation, the results may be inaccurate.\n\nFor example, if the distribution is used in sampling a posterior distribution with MCMC and the parameters of the ordered distribution are themselves sampled, then the normalizing factor would in general be needed for accurate sampling, and ordered should not be used. However, if the parameters are fixed, then since MCMC does not require distributions be normalized, ordered may be used without problems.\n\nA common case is where the distribution being ordered is a joint distribution of n identical univariate distributions. In this case the normalization factor works out to be the constant n!, and ordered can again be used without problems even if the parameters of the univariate distribution are sampled.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.isinvertible",
    "href": "../Bijectors.jl/interface/#Bijectors.isinvertible",
    "title": "Bijectors.isinvertible",
    "section": "function",
    "text": "isinvertible(t)\n\nReturn true if t is invertible, and false otherwise.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "interface/#Bijectors.isclosedform-Tuple{Bijectors.Transform}",
    "href": "../Bijectors.jl/interface/#Bijectors.isclosedform-Tuple{Bijectors.Transform}",
    "title": "Bijectors.isclosedform",
    "section": "method",
    "text": "isclosedform(b::Transform)::bool\nisclosedform(b⁻¹::Inverse{<:Transform})::bool\n\nReturns true or false depending on whether or not evaluation of b has a closed-form implementation.\n\nMost transformations have closed-form evaluations, but there are cases where this is not the case. For example the inverse evaluation of PlanarLayer requires an iterative procedure to evaluate.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "defining_examples/#Defining-a-bijector:-examples",
    "href": "../Bijectors.jl/defining_examples/#Defining-a-bijector:-examples",
    "title": "Defining a bijector: examples",
    "section": "section",
    "text": "Here we provide two different worked examples of defining a custom bijector.",
    "crumbs": null
  },
  {
    "objectID": "defining_examples/#Cyclic-permutation",
    "href": "../Bijectors.jl/defining_examples/#Cyclic-permutation",
    "title": "Cyclic permutation",
    "section": "section",
    "text": "We start with something simple: a bijector that performs a cyclic permutation of the elements of a vector.\n\nusing Bijectors\n\nstruct CircShift <: Bijector\n    shift::Int\nend\n\nAs described in the previous page, the only function you absolutely must implement is with_logabsdet_jacobian.\n\nLet's think for a moment about what the Jacobian is. CircShift is a mapping from ℝⁿ → ℝⁿ that permutes the elements of the input vector. For example, CircShift(1) would map a length-3 vector x = [x1, x2, x3] to y = [x3, x1, x2].\n\nThat means that the Jacobian matrix is\n\nJ = beginbmatrix\npartial y_1partial x_1  partial y_1partial x_2  partial y_1partial x_3 \npartial y_2partial x_1  partial y_2partial x _2  partial y_2partial x_3 \npartial y_3partial x_1  partial y_3partial x_2  partial y_3partial x_3\nendbmatrix\n= beginbmatrix\n0  0  1 \n1  0  0 \n0  1  0\nendbmatrix\n\nIn general, the Jacobian of such a transformation is a permutation matrix. The determinant of a permutation matrix is either 1 or -1, depending on whether the permutation is even or odd. (In this case, it is even; but it could be odd for other shifts and/or input sizes.) This means that the log-absolute determinant of the Jacobian is always 0.\n\nWe can now implement with_logabsdet_jacobian.\n\nfunction Bijectors.with_logabsdet_jacobian(\n    b::CircShift, x::AbstractVector{T}\n) where {T<:Real}\n    y = circshift(x, b.shift)\n    return y, zero(T)\nend\n\nIt is good practice to let the type of the input determine the type of the log-Jacobian term here. However, you might also ask: since a cyclic permutation is also well-defined for arrays of non-real types, should we also allow that? We can do so by creating a new method, but we would have to make a choice as to the type of the log-Jacobian term, since we cannot derive it from the input type. Here, we will choose Float64:\n\nimport ChangesOfVariables: with_logabsdet_jacobian\n\nfunction with_logabsdet_jacobian(b::CircShift, x::AbstractVector)\n    y = circshift(x, b.shift)\n    return y, 0.0\nend\n\nWith this defined, we can now benefit from a host of automatic definitions:\n\nb = CircShift(1)\nx = [1.0, 2.0, 3.0]\nb(x)\n\nlogabsdetjac(b, x)\n\nWe can also define the inverse bijector. A default definition for inverse(b) already exists: it would return Bijectors.Inverse(b). But, if we used this default definition, we would have to also define with_logabsdet_jacobian(::Inverse{CircShift}, y). We can save ourselves this hassle by overloading the method:\n\nimport InverseFunctions: inverse\n\ninverse(b::CircShift) = CircShift(-b.shift)\n\nNow we can use the inverse bijector:\n\ny = b(x)\ninverse(b)(y) == x\n\nnote: Note\nBijectors re-exports both with_logabsdet_jacobian as well as inverse, so you don't need to import them separately if Bijectors is already a dependency. Conversely, if you don't want to depend on Bijectors.jl directly, you can just import these functions from their respective packages.",
    "crumbs": null
  },
  {
    "objectID": "defining_examples/#Stereographic-projection",
    "href": "../Bijectors.jl/defining_examples/#Stereographic-projection",
    "title": "Stereographic projection",
    "section": "section",
    "text": "Now, we'll look at a more complex example: a stereographic projection mapping points on the unit sphere (i.e., length-3 vectors x for which x_1^2 + x_2^2 + x_3^2 = 1), to points in the plane ℝ² (i.e., length-2 vectors y whose elements are unconstrained).\n\nThe relevant formulae are given here on Wikipedia. The forward transform (from sphere to plane) is:\n\ny_1 = fracx_11 - x_3 qquad y_2 = fracx_21 - x_3\n\nusing Bijectors\n\nstruct StereographicProj <: Bijector end\nfunction (s::StereographicProj)(x::AbstractVector{T}) where {T<:Real}\n    y = similar(x, 2)\n    denom = one(T) - x[3]\n    y[1] = x[1] / denom\n    y[2] = x[2] / denom\n    return y\nend\n\nwarning: Warning\nThis will return [Inf, Inf] if x[3] == 1 (the 'north pole' of the sphere), which may potentially make downstream computations fail. One potential way around this is to add eps(T) to the denominator to avoid it ever being zero: you will sometimes see this trick used in Bijectors.jl. However, be aware that the reverse transform has to also be modified accordingly so that the two transforms remain inverses of each other!\n\nWhen it comes to computing the Jacobian, we find ourselves in a spot of bother. The partial derivatives themselves can ostensibly be computed using fairly straightforward calculus:\n\nJ = beginbmatrix\npartial y_1partial x_1  partial y_1partial x_2  partial y_1partial x_3 \npartial y_2partial x_1  partial y_2partial x_2  partial y_2partial x_3\nendbmatrix\n= beginbmatrix\n1(1 - x_3)  0  x_1(1 - x_3)^2 \n0  1(1 - x_3)  x_2(1 - x_3)^2\nendbmatrix\n\nbut since our mapping is from ℝ³ → ℝ², the Jacobian matrix is not square, and so we cannot compute its determinant!\n\nTo fix this, we need to realise that x_1, x_2, and x_3 are not really independent at all. The partial derivatives we computed above treated them as independent variables! In reality, they must satisfy the constraint x_1^2 + x_2^2 + x_3^2 = 1, which means that\n\nx_3 = pm sqrt1 - x_1^2 - x_2^2\n\nand thus\n\nfracpartial x_3partial x_1 = -fracx_1x_3 qquad fracpartial x_3partial x_2 = -fracx_2x_3\n\n(Note that this is true regardless of which sign x_3 has.)\n\nIn effect, we are treating x_3 as a function of x_1 and x_2, rather than as an independent variable. This means that we can construct a Jacobian using only x_1 and x_2 as inputs, and thus obtain a square Jacobian matrix.\n\nFor example, we can recompute the derivative of y_1 with respect to x_1, but this time also making sure to include the dependence of x_3 on x_1.\n\nbeginalign*\nfracpartial y_1partial x_1\n= fracpartialpartial x_1 x_1(1 - x_3)^-1 \n= (1 - x_3)^-1 + x_1 (-1)(1 - x_3)^-2 left(fracpartialpartial x_1(1 - x_3)right) \n= (1 - x_3)^-1 - x_1 (1 - x_3)^-2 left(fracx_1x_3right) \n= frac11 - x_3 - fracx_1^2x_3 (1 - x_3)^2\nendalign*\n\nA similar strategy for all the other partial derivatives gives us the Jacobian\n\nJ = beginbmatrix\ndfrac11 - x_3 - dfracx_1^2x_3 (1 - x_3)^2  -dfracx_1 x_2x_3 (1 - x_3)^2 \n-dfracx_1 x_2x_3 (1 - x_3)^2  dfrac11 - x_3 - dfracx_2^2x_3 (1 - x_3)^2\nendbmatrix\n\nnote: Note\nWhen you see x_3 here, don't think 'the variable x_3': it's just shorthand for pm sqrt1 - x_1^2 - x_2^2. (And recall that these formulae hold for both choices of sign.)\n\nIts determinant very nicely simplifies to\n\ndet(J) = -frac1x_3 (1 - x_3)^2\n\nthe absolute determinant being\n\ndet(J) = frac1x_3 (1 - x_3)^2\n\n((1 - x_3)^2 is always non-negative, of course); and thus\n\nfunction Bijectors.logabsdetjac(b::StereographicProj, x::AbstractVector{T}) where {T<:Real}\n    return -log(abs(x[3])) - (2 * log(one(T) - x[3]))\nend\n\nPhew!\n\nLet's take a moment and check that we did indeed do this correctly. To verify that the implementation of logabsdetjac is indeed correct, we can compare it against a Jacobian obtained via automatic differentiation.\n\nIf we try to calculate a Jacobian for StereographicProj(), we will just get a 2x3 matrix, which is not what we want. So, we need to take an extra step to map from the independent coordinates of x (i.e., x_1 and x_2) to the full 3D coordinates, and then to the plane:\n\nsgn = 1\n\nfunction full_transform(x12)\n    x3 = sgn * sqrt(one(eltype(x12)) - sum(x12 .^ 2))\n    x123 = vcat(x12, x3)\n    return StereographicProj()(x123)\nend\n\nimport DifferentiationInterface as DI\nusing FiniteDifferences, LinearAlgebra\nx = [0.3, 0.4, sgn * sqrt(1 - 0.3^2 - 0.4^2)]\n\nadtype = DI.AutoFiniteDifferences(; fdm=central_fdm(5, 1))\njac = DI.jacobian(full_transform, adtype, x[1:2])\nlogjac = logabsdet(jac)[1]\n\nHopefully this is approximately the same!\n\nlogabsdetjac(StereographicProj(), x)\n\nYou can also rerun the code blocks above with sgn = -1 to verify that our logabsdetjac implementation does indeed behave correctly for both positive and negative values of x_3.\n\nWhen writing unit tests for a new bijector, it is a good idea to include comparisons like this to verify that the Jacobian is computed correctly. The strategy used above to get square Jacobians is quite generally applicable, and is used for testing the bijectors for (e.g.) simplices and Cholesky factors.\n\nReturning to the Bijectors interface, because we have defined the forward transform as well as logabsdetjac, we can just use these to implement with_logabsdet_jacobian:\n\nfunction Bijectors.with_logabsdet_jacobian(s::StereographicProj, x)\n    return s(x), logabsdetjac(s, x)\nend\n\nOr if we wanted to be more efficient, we might notice that one(T) - x[3] is computed both in s(x) as well as in logabsdetjac. So we could also write:\n\nfunction Bijectors.with_logabsdet_jacobian(\n    s::StereographicProj, x::AbstractVector{T}\n) where {T<:Real}\n    denom = one(T) - x[3]  # Shared computation\n    y = similar(x, 2)\n    y[1] = x[1] / denom\n    y[2] = x[2] / denom\n    logjac = -log(abs(x[3])) - (2 * log(denom))\n    return y, logjac\nend\n\nOf course, this alone is unlikely to save any meaningful amount of time, but other bijectors may have more expensive computations that may be shared between both transform and log-Jacobian calculations.\n\nThe inverse bijector can be implemented in a very similar way (Wikipedia has the formulae as well), but is left as an exercise for the very willing reader!\n\nFinally, suppose we had a distribution UnitSphere, where rand(UnitSphere()) returned a random point on the unit sphere. Something similar to this technically exists in Manifolds.jl, but we can also define a hacky version ourselves:\n\nusing Distributions\n\nstruct UnitSphere <: Distributions.ContinuousMultivariateDistribution end\nBase.size(::UnitSphere) = (3,)\nBase.rand(::UnitSphere) = normalize(rand(3))\n\nThen, we could define\n\nBijectors.bijector(::UnitSphere) = StereographicProj()\n\n# Not strictly needed for this example, but other usage may require it\nBijectors.output_size(::StereographicProj, ::UnitSphere) = (2,)\n\nand that would allow us to construct, for example, transformed distributions:\n\ntd = transformed(UnitSphere())\nrand(td)  # returns a random point in ℝ²\n\nWe didn't define logpdf for UnitSphere, but if we had, then we would also be able to make use of logpdf(td, y) and Bijectors.logpdf_with_trans.",
    "crumbs": null
  },
  {
    "objectID": "distributions/#Usage-with-distributions",
    "href": "../Bijectors.jl/distributions/#Usage-with-distributions",
    "title": "Usage with distributions",
    "section": "section",
    "text": "Bijectors provides many utilities for working with probability distributions.\n\nusing Bijectors\n\ndist = LogNormal()\nx = rand(dist)\nb = bijector(dist)  # bijection (0, ∞) → ℝ\n\ny = b(x)\n\nHere, bijector(d::Distribution) returns the corresponding constrained-to-unconstrained bijection for Beta, which is a log function. The resulting bijector can be called, just like any other function, to transform samples from the distribution to the unconstrained space.\n\nThe function link provides a short way of doing the above:\n\nlink(dist, x) ≈ b(x)\n\nSee the Turing.jl docs for more information about how this is used in probabilistic programming.",
    "crumbs": null
  },
  {
    "objectID": "distributions/#Transforming-distributions",
    "href": "../Bijectors.jl/distributions/#Transforming-distributions",
    "title": "Transforming distributions",
    "section": "section",
    "text": "We can also couple a distribution together with its bijector to create a transformed Distribution, i.e. a Distribution defined by sampling from a given Distribution and then transforming using a given transformation:\n\ndist = LogNormal()          # support on (0, ∞)\ntdist = transformed(dist)   # support on ℝ\n\nWe can then sample from, and compute the logpdf for, the resulting distribution:\n\ny = rand(tdist)\n\nlogpdf(tdist, y)\n\nWe should expect here that\n\nlogpdf(tdist, y) ≈ logpdf(dist, x) - logabsdetjac(b, x)\n\nwhere b = bijector(dist) and y = b(x).\n\nTo verify this, we can calculate the value of x using the inverse bijector:\n\nb = bijector(dist)\nbinv = inverse(b)\n\nx = binv(y)\n\n(Because b is just a log function, binv is an exponential function, i.e. x = exp(y).)\n\nThen we can check the equality:\n\nlogpdf(tdist, y) ≈ logpdf(dist, x) - logabsdetjac(b, x)\n\nYou can also use Bijectors.logpdf_with_trans with the original distribution:\n\nlogpdf_with_trans(dist, x, false) ≈ logpdf(dist, x)\n\nlogpdf_with_trans(dist, x, true) ≈ logpdf(tdist, y)",
    "crumbs": null
  },
  {
    "objectID": "advi/#Example:-Variational-inference",
    "href": "../Bijectors.jl/advi/#Example:-Variational-inference",
    "title": "Example: Variational inference",
    "section": "section",
    "text": "The real utility of TransformedDistribution becomes more apparent when using transformed(dist, b) for any bijector b. To get the transformed distribution corresponding to the Beta(2, 2), we called transformed(dist) before. This is an alias for transformed(dist, bijector(dist)). Remember bijector(dist) returns the constrained-to-constrained bijector for that particular Distribution. But we can of course construct a TransformedDistribution using different bijectors with the same dist.\n\nThis is particularly useful in Automatic Differentiation Variational Inference (ADVI).",
    "crumbs": null
  },
  {
    "objectID": "advi/#Univariate-ADVI",
    "href": "../Bijectors.jl/advi/#Univariate-ADVI",
    "title": "Univariate ADVI",
    "section": "section",
    "text": "An important part of ADVI is to approximate a constrained distribution, e.g. Beta, as follows:\n\nSample x from a Normal with parameters μ and σ, i.e. x ~ Normal(μ, σ).\nTransform x to y s.t. y ∈ support(Beta), with the transform being a differentiable bijection with a differentiable inverse (a \"bijector\").\n\nThis then defines a probability density with the same support as Beta! Of course, it's unlikely that it will be the same density, but it's an approximation.\n\nCreating such a distribution can be done with Bijector and TransformedDistribution:\n\nusing Bijectors\nusing StableRNGs: StableRNG\nrng = StableRNG(42)\n\ndist = Beta(2, 2)\nb = bijector(dist)                # (0, 1) → ℝ\nb⁻¹ = inverse(b)                  # ℝ → (0, 1)\ntd = transformed(Normal(), b⁻¹)   # x ∼ 𝓝(0, 1) then b(x) ∈ (0, 1)\nx = rand(rng, td)                 # ∈ (0, 1)\n\nIt's worth noting that support(Beta) is the closed interval [0, 1], while the constrained-to-unconstrained bijection, Logit in this case, is only well-defined as a map (0, 1) → ℝ for the open interval (0, 1). This is of course not an implementation detail. ℝ is itself open, thus no continuous bijection exists from a closed interval to ℝ. But since the boundaries of a closed interval has what's known as measure zero, this doesn't end up affecting the resulting density with support on the entire real line. In practice, this means that\n\ntd = transformed(Beta())\ninverse(td.transform)(rand(rng, td))\n\nwill never result in 0 or 1 though any sample arbitrarily close to either 0 or 1 is possible. Disclaimer: numerical accuracy is limited, so you might still see 0 and 1 if you're 'lucky'.",
    "crumbs": null
  },
  {
    "objectID": "advi/#Multivariate-ADVI-example",
    "href": "../Bijectors.jl/advi/#Multivariate-ADVI-example",
    "title": "Multivariate ADVI example",
    "section": "section",
    "text": "We can also do multivariate ADVI using the Stacked bijector. Stacked gives us a way to combine univariate and/or multivariate bijectors into a singe multivariate bijector. Say you have a vector x of length 2 and you want to transform the first entry using Exp and the second entry using Log. Stacked gives you an easy and efficient way of representing such a bijector.\n\nusing Bijectors: SimplexBijector\n\n# Original distributions\ndists = (Beta(), InverseGamma(), Dirichlet(2, 3))\n\n# Construct the corresponding ranges\nfunction make_ranges(dists)\n    ranges = []\n    idx = 1\n    for i in 1:length(dists)\n        d = dists[i]\n        push!(ranges, idx:(idx + length(d) - 1))\n        idx += length(d)\n    end\n    return ranges\nend\n\nranges = make_ranges(dists)\nranges\n\n# Base distribution; mean-field normal\nnum_params = ranges[end][end]\n\nd = MvNormal(zeros(num_params), ones(num_params));\n\n# Construct the transform\nbs = bijector.(dists)       # constrained-to-unconstrained bijectors for dists\nibs = inverse.(bs)          # invert, so we get unconstrained-to-constrained\nsb = Stacked(ibs, ranges)   # => Stacked <: Bijector\n\n# Mean-field normal with unconstrained-to-constrained stacked bijector\ntd = transformed(d, sb)\ny = rand(td)\n\nAs can be seen from this, we now have a y for which 0.0 ≤ y[1] ≤ 1.0, 0.0 < y[2], and sum(y[3:4]) ≈ 1.0.",
    "crumbs": null
  },
  {
    "objectID": "#Bijectors.jl",
    "href": "../Bijectors.jl/#Bijectors.jl",
    "title": "Bijectors.jl",
    "section": "section",
    "text": "This package implements functionality for transforming random variables to Euclidean space (and back).\n\nFor example, consider a random variable X sim mathrmBeta(2 2), which has support on (0 1):\n\nusing Bijectors\n\nx = rand(Beta(2, 2))\n\nIn this case, the logit function is used as the transformation:\n\nY = mathrmlogit(X) = logleft(fracX1 - Xright)\n\nWe can construct this function\n\nb = bijector(Beta(2, 2))\n\nand apply it to x:\n\ny = b(x)\n\nYou can also obtain the log absolute determinant of the Jacobian of the transformation:\n\ny, ladj = with_logabsdet_jacobian(b, x)",
    "crumbs": null
  },
  {
    "objectID": "defining/#Defining-a-bijector",
    "href": "../Bijectors.jl/defining/#Defining-a-bijector",
    "title": "Defining a bijector",
    "section": "section",
    "text": "This page describes the minimum expected interface to implement a bijector.\n\nIn general, there are two pieces of information needed to define a bijector:\n\nThe transformation itself, i.e., the map b mathbbR^d to mathbbR^d.\nThe log-absolute determinant of the Jacobian of that transformation. For a transformation b mathbbR^d to mathbbR^d, the Jacobian at point x in mathbbR^d is defined as:\nJ_b(x) = beginbmatrix\npartial y_1partial x_1  partial y_1partial x_2  cdots  partial y_1partial x_d \npartial y_2partial x_1  partial y_2partial x_2  cdots  partial y_2partial x_d \nvdots  vdots  ddots  vdots \npartial y_dpartial x_1  partial y_dpartial x_2  cdots  partial y_dpartial x_d\nendbmatrix\nwhere y = b(x).",
    "crumbs": null
  },
  {
    "objectID": "defining/#The-transform-itself",
    "href": "../Bijectors.jl/defining/#The-transform-itself",
    "title": "The transform itself",
    "section": "section",
    "text": "The most efficient way to implement a bijector is to provide an implementation of:\n\nnote: Note\nwith_logabsdet_jacobian is re-exported from ChangesOfVariables.jl, so if you want to avoid importing Bijectors.jl, you can implement ChangesOfVariables.with_logabsdet_jacobian instead.\n\nIf you define with_logabsdet_jacobian(b, x), then you will automatically get default implementations of both transform(b, x) and logabsdetjac(b, x), which respectively return the first and second value of that tuple. So, in fact, you can implement a bijector by defining only with_logabsdet_jacobian.\n\nIf you prefer, you can implement transform and logabsdetjac separately, as described below. Having manual implementations of these may also be useful if you expect either to be used heavily without the other.",
    "crumbs": null
  },
  {
    "objectID": "defining/#Transformation",
    "href": "../Bijectors.jl/defining/#Transformation",
    "title": "Transformation",
    "section": "section",
    "text": "If transform(b, x) is defined, then you will automatically get a default implementation of b(x) which calls that.",
    "crumbs": null
  },
  {
    "objectID": "defining/#Log-absolute-determinant-of-the-Jacobian",
    "href": "../Bijectors.jl/defining/#Log-absolute-determinant-of-the-Jacobian",
    "title": "Log-absolute determinant of the Jacobian",
    "section": "section",
    "text": "",
    "crumbs": null
  },
  {
    "objectID": "defining/#Inverse",
    "href": "../Bijectors.jl/defining/#Inverse",
    "title": "Inverse",
    "section": "section",
    "text": "Often you will want to define an inverse bijector as well. To do so, you will have to implement:\n\nnote: Note\ninverse is re-exported from InverseFunctions.jl, so the same note as for with_logabsdet_jacobian applies.\n\nIf b is a bijector, then inverse(b) should return the inverse bijector b^-1.\n\nIf your bijector subtypes Bijectors.Bijector, then you will get a default implementation of inverse which constructs Bijectors.Inverse(b). This may be easier than creating a second type for the inverse bijector. Note that you will also need to implement the methods for with_logabsdet_jacobian (and/or transform and logabsdetjac) for the inverse bijector type.\n\nIf your bijector is not invertible, you can specify this here:",
    "crumbs": null
  },
  {
    "objectID": "defining/#Distributions",
    "href": "../Bijectors.jl/defining/#Distributions",
    "title": "Distributions",
    "section": "section",
    "text": "If your bijector is intended for use with a distribution, i.e., it transforms random variables drawn from that distribution to Euclidean space, then you should also implement:\n\nwhich should return your bijector.\n\nOn top of that, you should also implement a method for Bijectors.output_size(b, dist::Distribution):",
    "crumbs": null
  },
  {
    "objectID": "defining/#Closed-form",
    "href": "../Bijectors.jl/defining/#Closed-form",
    "title": "Closed-form",
    "section": "section",
    "text": "If your bijector does not have a closed-form expression (e.g. if it uses an iterative procedure), then this should be set to false:\n\nThe default is true so you only need to set this if your bijector is not closed-form.",
    "crumbs": null
  },
  {
    "objectID": "defining/#ChangesOfVariables.with_logabsdet_jacobian-defining",
    "href": "../Bijectors.jl/defining/#ChangesOfVariables.with_logabsdet_jacobian-defining",
    "title": "ChangesOfVariables.with_logabsdet_jacobian",
    "section": "function",
    "text": "with_logabsdet_jacobian(t::Transform, x)\n\nSemantically, this must return a tuple of (y, logabsdetjac), where y = transform(t, x) and logabsdetjac = logabsdetjac(t, x). However, you can implement this function to exploit shared computation between the two quantities.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "defining/#Bijectors.transform-defining",
    "href": "../Bijectors.jl/defining/#Bijectors.transform-defining",
    "title": "Bijectors.transform",
    "section": "function",
    "text": "transform(b, x)\n\nTransform x using b.\n\nIf with_logabsdet_jacobian is already implemented for b, the default implementation of transform will call first(with_logabsdet_jacobian(b, x)).\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "defining/#Bijectors.logabsdetjac-defining",
    "href": "../Bijectors.jl/defining/#Bijectors.logabsdetjac-defining",
    "title": "Bijectors.logabsdetjac",
    "section": "function",
    "text": "logabsdetjac(b, x)\n\nReturn log(abs(det(J(b, x)))), where J(b, x) is the jacobian of b at x.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "defining/#InverseFunctions.inverse-defining",
    "href": "../Bijectors.jl/defining/#InverseFunctions.inverse-defining",
    "title": "InverseFunctions.inverse",
    "section": "function",
    "text": "inverse(t::Transform)\n\nReturns the inverse of transform t.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "defining/#Bijectors.isinvertible-defining",
    "href": "../Bijectors.jl/defining/#Bijectors.isinvertible-defining",
    "title": "Bijectors.isinvertible",
    "section": "function",
    "text": "isinvertible(t)\n\nReturn true if t is invertible, and false otherwise.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "defining/#Bijectors.bijector-defining",
    "href": "../Bijectors.jl/defining/#Bijectors.bijector-defining",
    "title": "Bijectors.bijector",
    "section": "function",
    "text": "bijector(d::Distribution)\n\nReturns the constrained-to-unconstrained bijector for distribution d.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "defining/#Bijectors.output_size-defining",
    "href": "../Bijectors.jl/defining/#Bijectors.output_size-defining",
    "title": "Bijectors.output_size",
    "section": "function",
    "text": "output_size(f, sz)\n\nReturns the size of f(x) when given an input x of size sz.\n\n\n\n\n\noutput_size(f, dist::Distribution)\n\nReturns the output size of f given an input drawn from the distribution dist.\n\nBy default this just calls output_size(f, size(dist)), but this can be overloaded for specific distributions. This is useful when Base.size(dist) is not defined, e.g. for ProductNamedTupleDistribution and in particular is used by DynamicPPL when generating new random values for transformed distributions.\n\n\n\n\n\n",
    "crumbs": null
  },
  {
    "objectID": "defining/#Bijectors.isclosedform-defining",
    "href": "../Bijectors.jl/defining/#Bijectors.isclosedform-defining",
    "title": "Bijectors.isclosedform",
    "section": "function",
    "text": "isclosedform(b::Transform)::bool\nisclosedform(b⁻¹::Inverse{<:Transform})::bool\n\nReturns true or false depending on whether or not evaluation of b has a closed-form implementation.\n\nMost transformations have closed-form evaluations, but there are cases where this is not the case. For example the inverse evaluation of PlanarLayer requires an iterative procedure to evaluate.\n\n\n\n\n\n",
    "crumbs": null
  }
]

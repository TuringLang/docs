{
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": "# Install necessary dependencies.\nusing Pkg\nPkg.activate(; temp=true)\nPkg.add([\"Turing\", \"OptimizationOptimJL\", \"StatsBase\", \"Mooncake\", \"OptimizationNLopt\"])",
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "#| echo: false\n#| output: false\nusing Pkg;\nPkg.instantiate();",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "After defining a statistical model, in addition to sampling from its distributions, one may be interested in finding the parameter values that maximise for instance the posterior distribution density function or the likelihood. This is called mode estimation. Turing provides support for two mode estimation techniques, [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) (MLE) and [maximum a posteriori](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) (MAP) estimation.\n\nTo demonstrate mode estimation, let us load Turing and declare a model:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "using Turing\n\n@model function gdemo(x)\n    s² ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s²))\n\n    for i in eachindex(x)\n        x[i] ~ Normal(m, sqrt(s²))\n    end\nend",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Once the model is defined, we can construct a model instance as we normally would:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "# Instantiate the gdemo model with our data.\ndata = [1.5, 2.0]\nmodel = gdemo(data)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Finding the maximum a posteriori or maximum likelihood parameters is as simple as",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "# Generate a MLE estimate.\nmle_estimate = maximum_likelihood(model)\n\n# Generate a MAP estimate.\nmap_estimate = maximum_a_posteriori(model)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The estimates are returned as instances of the `ModeResult` type. It has the fields `values` for the parameter values found and `lp` for the log probability at the optimum, as well as `f` for the objective function and `optim_result` for more detailed results of the optimisation procedure.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "@show mle_estimate.values\n@show mle_estimate.lp;",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Controlling the optimisation process\n\nUnder the hood `maximum_likelihood` and `maximum_a_posteriori` use the [Optimisation.jl](https://github.com/SciML/Optimisation.jl) package, which provides a unified interface to many other optimisation packages. By default Turing typically uses the [LBFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) method from [Optim.jl](https://github.com/JuliaNLSolvers/Optim.jl) to find the mode estimate, but we can easily change that:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "using OptimizationOptimJL: NelderMead\n@show maximum_likelihood(model, NelderMead())\n\nusing OptimizationNLopt: NLopt.LD_TNEWTON_PRECOND_RESTART\n@show maximum_likelihood(model, LD_TNEWTON_PRECOND_RESTART());",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The above are just two examples, Optimisation.jl supports [many more](https://docs.sciml.ai/Optimisation/stable/).\n\nWe can also help the optimisation by giving it a starting point we know is close to the final solution, or by specifying an automatic differentiation method",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "import Mooncake\n\nmaximum_likelihood(\n    model, NelderMead(); initial_params=[0.1, 2], adtype=AutoMooncake()\n)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "When providing values to arguments like `initial_params` the parameters are typically specified in the order in which they appear in the code of the model, so in this case first `s²` then `m`. More precisely it's the order returned by `Turing.Inference.getparams(model, DynamicPPL.VarInfo(model))`.\n\n> ## Initialisation strategies and consistency with MCMC sampling\n> \n> Since Turing v0.41, for MCMC sampling, the `initial_params` argument must be a `DynamicPPL.AbstractInitStrategy` as described in [the sampling options page]({{< meta usage-sampling-options >}}#specifying-initial-parameters)).\n> The optimisation interface has not yet been updated to use this; thus, initial parameters are still specified as Vectors.\n> We expect that this will be changed in the near future.\n\nWe can also do constrained optimisation, by providing either intervals within which the parameters must stay, or constraint functions that they need to respect.\nFor instance, here's how one can find the MLE with the constraint that the variance must be less than 0.01 and the mean must be between -1 and 1.:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "maximum_likelihood(model; lb=[0.0, -1.0], ub=[0.01, 1.0])",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The arguments for lower (`lb`) and upper (`ub`) bounds follow the arguments of `Optimisation.OptimizationProblem`, as do other parameters for providing [constraints](https://docs.sciml.ai/Optimisation/stable/tutorials/constraints/), such as `cons`.\nAny extraneous keyword arguments given to `maximum_likelihood` or `maximum_a_posteriori` are passed to `Optimisation.solve`.\nSome often useful ones are `maxiters` for controlling the maximum number of iterations and `abstol` and `reltol` for the absolute and relative convergence tolerances:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "badly_converged_mle = maximum_likelihood(\n    model, NelderMead(); maxiters=10, reltol=1e-9\n)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We can check whether the optimisation converged using the `optim_result` field of the result:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "@show badly_converged_mle.optim_result;",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "For more details, such as a full list of possible arguments, we encourage the reader to read the docstring of the function `Turing.Optimisation.estimate_mode`, which is what `maximum_likelihood` and `maximum_a_posteriori` call, and the documentation of [Optimisation.jl](https://docs.sciml.ai/Optimisation/stable/).\n\n## Analyzing your mode estimate\n\nTuring extends several methods from `StatsBase` that can be used to analyse your mode estimation results. Methods implemented include `vcov`, `informationmatrix`, `coeftable`, `params`, and `coef`, among others.\n\nFor example, let's examine our ML estimate from above using `coeftable`:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "using StatsBase: coeftable\ncoeftable(mle_estimate)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Standard errors are calculated from the Fisher information matrix (inverse Hessian of the log likelihood or log joint).\nNote that standard errors calculated in this way may not always be appropriate for MAP estimates, so please be cautious in interpreting them.\n\n## Sampling with the MAP/MLE as initial states\n\nYou can begin sampling your chain from an MLE/MAP estimate by wrapping it in `InitFromParams` and providing it to the `sample` function with the keyword `initial_params`.\nFor example, here is how to sample from the full posterior using the MAP estimate as the starting point:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "#| eval: false\nmap_estimate = maximum_a_posteriori(model)\nchain = sample(model, NUTS(), 1_000; initial_params=InitFromParams(map_estimate))",
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia",
      "language": "julia"
    }
  },
  "nbformat": 4
}
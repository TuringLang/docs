{
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": "# Install necessary dependencies.\nusing Pkg\nPkg.activate(; temp=true)\nPkg.add([])",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| echo: false\n#| output: false\nusing Pkg;\nPkg.instantiate();\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The following tutorial illustrates the use of Turing for an unsupervised task, namely, clustering data using a Bayesian mixture model.\nThe aim of this task is to infer a latent grouping (hidden structure) from unlabelled data.\n\n## Synthetic Data\n\nWe generate a synthetic dataset of $N = 60$ two-dimensional points $x_i \\in \\mathbb{R}^2$ drawn from a Gaussian mixture model.\nFor simplicity, we use $K = 2$ clusters with\n\n- equal weights, i.e., we use mixture weights $w = [0.5, 0.5]$, and\n- isotropic Gaussian distributions of the points in each cluster.\n\nMore concretely, we use the Gaussian distributions $\\mathcal{N}([\\mu_k, \\mu_k]^\\mathsf{T}, I)$ with parameters $\\mu_1 = -3.5$ and $\\mu_2 = 0.5$.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\nusing Distributions\nusing FillArrays\nusing StatsPlots\n\nusing LinearAlgebra\nusing Random\n\n# Set a random seed.\nRandom.seed!(3)\n\n# Define Gaussian mixture model.\nw = [0.5, 0.5]\nμ = [-2.0, 2.0]\nmixturemodel = MixtureModel([MvNormal(Fill(μₖ, 2), 0.2 * I) for μₖ in μ], w)\n\n# We draw the data points.\nN = 30\nx = rand(mixturemodel, N);\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The following plot shows the dataset.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "scatter(x[1, :], x[2, :]; legend=false, title=\"Synthetic Dataset\")",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Gaussian Mixture Model in Turing\n\nWe are interested in recovering the grouping from the dataset.\nMore precisely, we want to infer the mixture weights, the parameters $\\mu_1$ and $\\mu_2$, and the assignment of each datum to a cluster for the generative Gaussian mixture model.\n\nIn a Bayesian [Gaussian mixture model](https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model) with $K$ components, each data point $x_i$ ($i = 1,\\ldots,N$) is generated according to the following generative process.\nFirst we draw the model parameters: the cluster means $\\mu_k$ and the mixture weights $w$ that determine the probability of each cluster.\nWe use standard normal distributions as priors for $\\mu_k$ and a Dirichlet distribution with parameters $\\alpha_1 = \\cdots = \\alpha_K = 1$ as prior for $w$:\n$$\n\\begin{aligned}\n\\mu_k &\\sim \\mathcal{N}(0, 1) \\qquad (k = 1,\\ldots,K)\\\\\nw &\\sim \\operatorname{Dirichlet}(\\alpha_1, \\ldots, \\alpha_K)\n\\end{aligned}\n$$\nAfter having constructed all the necessary model parameters, we can generate an observation by first selecting one of the clusters\n$$\nz_i \\sim \\operatorname{Categorical}(w) \\qquad (i = 1,\\ldots,N),\n$$\nand then drawing the datum accordingly, i.e., in our example drawing\n$$\nx_i \\sim \\mathcal{N}([\\mu_{z_i}, \\mu_{z_i}]^\\mathsf{T}, I) \\qquad (i=1,\\ldots,N).\n$$\nFor more details on Gaussian mixture models, refer to Chapter 9 of Christopher M. Bishop, *Pattern Recognition and Machine Learning*.\n\nWe specify the model in Turing:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\nusing Turing\n\n@model function gaussian_mixture_model(x)\n    # Draw the parameters for each of the K=2 clusters from a standard normal distribution.\n    K = 2\n    μ ~ MvNormal(Zeros(K), I)\n\n    # Draw the weights for the K clusters from a Dirichlet distribution with parameters αₖ = 1.\n    w ~ Dirichlet(K, 1.0)\n    # Alternatively, one could use a fixed set of weights.\n    # w = fill(1/K, K)\n\n    # Construct categorical distribution of assignments.\n    distribution_assignments = Categorical(w)\n\n    # Construct multivariate normal distributions of each cluster.\n    D, N = size(x)\n    distribution_clusters = [MvNormal(Fill(μₖ, D), I) for μₖ in μ]\n\n    # Draw assignments for each datum and generate it from the multivariate normal distribution.\n    k = Vector{Int}(undef, N)\n    for i in 1:N\n        k[i] ~ distribution_assignments\n        x[:, i] ~ distribution_clusters[k[i]]\n    end\n\n    return k\nend\n\nmodel = gaussian_mixture_model(x);\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We run a MCMC simulation to obtain an approximation of the posterior distribution of the parameters $\\mu$ and $w$ and assignments $k$.\nWe use a `Gibbs` sampler that combines a [particle Gibbs](https://www.stats.ox.ac.uk/%7Edoucet/andrieu_doucet_holenstein_PMCMC.pdf) sampler for the discrete parameters (assignments $k$) and a Hamiltonian Monte Carlo sampler for the continuous parameters ($\\mu$ and $w$).\nWe generate multiple chains in parallel using multi-threading.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| output: false\n#| echo: false\nsetprogress!(false)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| output: false\nsampler = Gibbs(:k => PG(100), (:μ, :w) => HMC(0.05, 10))\nnsamples = 150\nnchains = 4\nburn = 10\nchains = sample(model, sampler, MCMCThreads(), nsamples, nchains, discard_initial = burn);\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "> ## Sampling With Multiple Threads\n> The `sample()` call above assumes that you have at least two threads available in your Julia instance.\n> If you do not, the multiple chains will run sequentially, and you may notice a warning.\n> For more information, see [the Turing documentation on sampling multiple chains.]({{<meta core-functionality>}}#sampling-multiple-chains)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| echo: false\nlet\n    # Verify that the output of the chain is as expected.\n    for i in MCMCChains.chains(chains)\n        # μ[1] and μ[2] can switch places, so we sort the values first.\n        chain = Array(chains[:, [\"μ[1]\", \"μ[2]\"], i])\n        μ_mean = vec(mean(chain; dims=1))\n        @assert isapprox(sort(μ_mean), μ; atol=0.5) \"Difference between estimated mean of μ ($(sort(μ_mean))) and data-generating μ ($μ) unexpectedly large!\"\n    end\nend\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Inferred Mixture Model\n\nAfter sampling we can visualise the trace and density of the parameters of interest.\n\nWe consider the samples of the location parameters $\\mu_1$ and $\\mu_2$ for the two clusters.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "plot(chains[[\"μ[1]\", \"μ[2]\"]]; legend=true)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "From the plots above, we can see that the chains have converged to seemingly different values for the parameters $\\mu_1$ and $\\mu_2$.\nHowever, these actually represent the same solution: it does not matter whether we assign $\\mu_1$ to the first cluster and $\\mu_2$ to the second, or vice versa, since the resulting sum is the same.\n(In principle it is also possible for the parameters to swap places _within_ a single chain, although this does not happen in this example.)\nFor more information see the [Stan documentation](https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html), or Bishop's book, where the concept of _identifiability_ is discussed.\n\nHaving $\\mu_1$ and $\\mu_2$ swap can complicate the interpretation of the results, especially when different chains converge to different assignments.\nOne solution here is to enforce an ordering on our $\\mu$ vector, requiring $\\mu_k \\geq \\mu_{k-1}$ for all $k$.\n`Bijectors.jl` [provides](https://turinglang.org/Bijectors.jl/stable/transforms/#Bijectors.OrderedBijector) a convenient function, `ordered()`, which can be applied to a (continuous multivariate) distribution to enforce this:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\nusing Bijectors: ordered\n\n@model function gaussian_mixture_model_ordered(x)\n    # Draw the parameters for each of the K=2 clusters from a standard normal distribution.\n    K = 2\n    μ ~ ordered(MvNormal(Zeros(K), I))\n    # Draw the weights for the K clusters from a Dirichlet distribution with parameters αₖ = 1.\n    w ~ Dirichlet(K, 1.0)\n    # Alternatively, one could use a fixed set of weights.\n    # w = fill(1/K, K)\n    # Construct categorical distribution of assignments.\n    distribution_assignments = Categorical(w)\n    # Construct multivariate normal distributions of each cluster.\n    D, N = size(x)\n    distribution_clusters = [MvNormal(Fill(μₖ, D), I) for μₖ in μ]\n    # Draw assignments for each datum and generate it from the multivariate normal distribution.\n    k = Vector{Int}(undef, N)\n    for i in 1:N\n        k[i] ~ distribution_assignments\n        x[:, i] ~ distribution_clusters[k[i]]\n    end\n    return k\nend\n\nmodel = gaussian_mixture_model_ordered(x);\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now, re-running our model, we can see that the assigned means are consistent between chains:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| output: false\nchains = sample(model, sampler, MCMCThreads(), nsamples, nchains, discard_initial = burn);\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| echo: false\nlet\n    # Verify that the output of the chain is as expected\n    for i in MCMCChains.chains(chains)\n        # μ[1] and μ[2] can no longer switch places. Check that they've found the mean\n        chain = Array(chains[:, [\"μ[1]\", \"μ[2]\"], i])\n        μ_mean = vec(mean(chain; dims=1))\n        @assert isapprox(sort(μ_mean), μ; atol=0.5) \"Difference between estimated mean of μ ($(sort(μ_mean))) and data-generating μ ($μ) unexpectedly large!\"\n    end\nend\n```",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "plot(chains[[\"μ[1]\", \"μ[2]\"]]; legend=true)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We also inspect the samples of the mixture weights $w$.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "plot(chains[[\"w[1]\", \"w[2]\"]]; legend=true)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "As the distributions of the samples for the parameters $\\mu_1$, $\\mu_2$, $w_1$, and $w_2$ are unimodal, we can safely visualise the density region of our model using the average values.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# Model with mean of samples as parameters.\nμ_mean = [mean(chains, \"μ[$i]\") for i in 1:2]\nw_mean = [mean(chains, \"w[$i]\") for i in 1:2]\nmixturemodel_mean = MixtureModel([MvNormal(Fill(μₖ, 2), I) for μₖ in μ_mean], w_mean)\ncontour(\n    range(-7.5, 3; length=1_000),\n    range(-6.5, 3; length=1_000),\n    (x, y) -> logpdf(mixturemodel_mean, [x, y]);\n    widen=false,\n)\nscatter!(x[1, :], x[2, :]; legend=false, title=\"Synthetic Dataset\")\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Inferred Assignments\n\nFinally, we can inspect the assignments of the data points inferred using Turing.\nAs we can see, the dataset is partitioned into two distinct groups.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "assignments = [mean(chains, \"k[$i]\") for i in 1:N]\nscatter(\n    x[1, :],\n    x[2, :];\n    legend=false,\n    title=\"Assignments on Synthetic Dataset\",\n    zcolor=assignments,\n)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Marginalizing Out The Assignments\n\nWe can write out the marginal posterior of (continuous) $w, \\mu$ by summing out the influence of our (discrete) assignments $z_i$ from our likelihood:\n\n$$p(y \\mid w, \\mu ) = \\sum_{k=1}^K w_k p_k(y \\mid \\mu_k)$$\n\nIn our case, this gives us:\n\n$$p(y \\mid w, \\mu) = \\sum_{k=1}^K w_k \\cdot \\operatorname{MvNormal}(y \\mid \\mu_k, I)$$\n\n\n### Marginalizing By Hand\n\nWe could implement the above version of the Gaussian mixture model in Turing as follows.\n\nFirst, Turing uses log-probabilities, so the likelihood above must be converted into log-space:\n\n$$\\log \\left( p(y \\mid w, \\mu) \\right) = \\text{logsumexp} \\left[\\log (w_k) + \\log(\\operatorname{MvNormal}(y \\mid \\mu_k, I)) \\right]$$\n\nWhere we sum the components with `logsumexp` from the [`LogExpFunctions.jl` package](https://juliastats.org/LogExpFunctions.jl/stable/).\nThe manually incremented likelihood can be added to the log-probability with `@addlogprob!`, giving us the following model:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| output: false\nusing LogExpFunctions\n\n@model function gmm_marginalized(x)\n    K = 2\n    D, N = size(x)\n    μ ~ ordered(MvNormal(Zeros(K), I))\n    w ~ Dirichlet(K, 1.0)\n    dists = [MvNormal(Fill(μₖ, D), I) for μₖ in μ]\n    for i in 1:N\n        lvec = Vector(undef, K)\n        for k in 1:K\n            lvec[k] = (w[k] + logpdf(dists[k], x[:, i]))\n        end\n        @addlogprob! logsumexp(lvec)\n    end\nend\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "> ## Manually Incrementing Probablity\n> \n> When possible, use of `@addlogprob!` should be avoided, as it exists outside the usual structure of a Turing model.\n> In most cases, a custom distribution should be used instead.\n> \n> The next section demonstrates the preferred method: using the `MixtureModel` distribution we have seen already to perform the marginalization automatically.\n\n### Marginalizing For Free With Distribution.jl's `MixtureModel` Implementation\n\nWe can use Turing's `~` syntax with anything that `Distributions.jl` provides `logpdf` and `rand` methods for.\nIt turns out that the `MixtureModel` distribution it provides has, as its `logpdf` method, `logpdf(MixtureModel([Component_Distributions], weight_vector), Y)`, where `Y` can be either a single observation or vector of observations.\n\nIn fact, `Distributions.jl` provides [many convenient constructors](https://juliastats.org/Distributions.jl/stable/mixture/) for mixture models, allowing further simplification in common special cases.\n\nFor example, when mixtures distributions are of the same type, one can write: `~ MixtureModel(Normal, [(μ1, σ1), (μ2, σ2)], w)`, or when the weight vector is known to allocate probability equally, it can be ommited.\n\nThe `logpdf` implementation for a `MixtureModel` distribution is exactly the marginalization defined above, and so our model can be simplified to:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| output: false\n@model function gmm_marginalized(x)\n    K = 2\n    D, _ = size(x)\n    μ ~ ordered(MvNormal(Zeros(K), I))\n    w ~ Dirichlet(K, 1.0)\n    x ~ MixtureModel([MvNormal(Fill(μₖ, D), I) for μₖ in μ], w)\nend\nmodel = gmm_marginalized(x);\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "As we have summed out the discrete components, we can perform inference using `NUTS()` alone.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| output: false\nsampler = NUTS()\nchains = sample(model, sampler, MCMCThreads(), nsamples, nchains; discard_initial = burn);\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| echo: false\nlet\n    # Verify for marginalized model that the output of the chain is as expected\n    for i in MCMCChains.chains(chains)\n        # μ[1] and μ[2] can no longer switch places. Check that they've found the mean\n        chain = Array(chains[:, [\"μ[1]\", \"μ[2]\"], i])\n        μ_mean = vec(mean(chain; dims=1))\n        @assert isapprox(sort(μ_mean), μ; atol=0.5) \"Difference between estimated mean of μ ($(sort(μ_mean))) and data-generating μ ($μ) unexpectedly large!\"\n    end\nend\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "`NUTS()` significantly outperforms our compositional Gibbs sampler, in large part because our model is now Rao-Blackwellized thanks to the marginalization of our assignment parameter.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "plot(chains[[\"μ[1]\", \"μ[2]\"]], legend=true)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Inferred Assignments With The Marginalized Model\n\nAs we have summed over possible assignments, the latent parameter representing the assignments is no longer available in our chain.\nThis is not a problem, however, as given any fixed sample $(\\mu, w)$, the assignment probability $p(z_i \\mid y_i)$ can be recovered using Bayes's theorme:\n\n$$p(z_i \\mid y_i) = \\frac{p(y_i \\mid z_i) p(z_i)}{\\sum_{k = 1}^K \\left(p(y_i \\mid z_i) p(z_i) \\right)}$$\n\nThis quantity can be computed for every $p(z = z_i \\mid y_i)$, resulting in a probability vector, which is then used to sample posterior predictive assignments from a categorial distribution.\nFor details on the mathematics here, see [the Stan documentation on latent discrete parameters](https://mc-stan.org/docs/stan-users-guide/latent-discrete.html).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| output: false\nfunction sample_class(xi, dists, w)\n    lvec = [(logpdf(d, xi) + log(w[i])) for (i, d) in enumerate(dists)]\n    rand(Categorical(softmax(lvec)))\nend\n\n@model function gmm_recover(x)\n    K = 2\n    D, N =  size(x)\n    μ ~ ordered(MvNormal(Zeros(K), I))\n    w ~ Dirichlet(K, 1.0)\n    dists = [MvNormal(Fill(μₖ, D), I) for μₖ in μ]\n    x ~ MixtureModel(dists, w)\n    # Return assignment draws for each datapoint.\n    return [sample_class(x[:, i], dists, w) for i in 1:N]\nend\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We sample from this model as before:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| output: false\nmodel = gmm_recover(x)\nchains = sample(model, sampler, MCMCThreads(), nsamples, nchains, discard_initial = burn);\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Given a sample from the marginalized posterior, these assignments can be recovered with:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "assignments = mean(returned(gmm_recover(x), chains));",
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "scatter(\n    x[1, :],\n    x[2, :];\n    legend=false,\n    title=\"Assignments on Synthetic Dataset - Recovered\",\n    zcolor=assignments,\n)",
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia",
      "language": "julia"
    }
  },
  "nbformat": 4
}
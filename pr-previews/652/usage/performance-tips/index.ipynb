{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using Pkg; Pkg.activate(; temp=true)\nPkg.add(\"DynamicPPL\")\nPkg.add(\"FillArrays\")\nPkg.add(\"Turing\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "This section briefly summarises a few common techniques to ensure good performance when using Turing.\nWe refer to [the Julia documentation](https://docs.julialang.org/en/v1/manual/performance-tips/index.html) for general techniques to ensure good performance of Julia programs.\n\n## Use multivariate distributions\n\nIt is generally preferable to use multivariate distributions if possible.\n\nThe following example:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using Turing\n@model function gmodel(x)\n    m ~ Normal()\n    for i in eachindex(x)\n        x[i] ~ Normal(m, 0.2)\n    end\nend"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "can be directly expressed more efficiently with a simple transformation:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using FillArrays\n\n@model function gmodel(x)\n    m ~ Normal()\n    return x ~ MvNormal(Fill(m, length(x)), 0.04 * I)\nend"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Choose your AD backend\n\nAutomatic differentiation (AD) makes it possible to use modern, efficient gradient-based samplers like NUTS and HMC.\nThis, however, also means that using a performant AD system is incredibly important.\nTuring currently supports several AD backends, including [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl) (the default), [Mooncake](https://github.com/chalk-lab/Mooncake.jl), and [ReverseDiff](https://github.com/JuliaDiff/ReverseDiff.jl).\n\nFor many common types of models, the default ForwardDiff backend performs well, and there is no need to worry about changing it.\nHowever, if you need more speed, you can try different backends via the standard [ADTypes](https://github.com/SciML/ADTypes.jl) interface by passing an `AbstractADType` to the sampler with the optional `adtype` argument, e.g. `NUTS(; adtype = AutoMooncake())`.\n\nGenerally, `adtype = AutoForwardDiff()` is likely to be the fastest and most reliable for models with few parameters (say, less than 20 or so), while reverse-mode backends such as `AutoMooncake()` or `AutoReverseDiff()` will perform better for models with many parameters or linear algebra operations.\nIf in doubt, you can benchmark your model with different backends to see which one performs best.\nSee the [Automatic Differentiation]({{<meta usage-automatic-differentiation>}}) page for details.\n\n### Special care for ReverseDiff with a compiled tape\n\nFor large models, the fastest option is often ReverseDiff with a compiled tape, specified as `adtype=AutoReverseDiff(; compile=true)`.\nHowever, it is important to note that if your model contains any branching code, such as `if`-`else` statements, **the gradients from a compiled tape may be inaccurate, leading to erroneous results**.\nIf you use this option for the (considerable) speedup it can provide, make sure to check your code for branching and ensure that it does not affect the gradients.\nIt is also a good idea to verify your gradients with another backend.\n\n## Ensure that types in your model can be inferred\n\nFor efficient gradient-based inference, e.g. using HMC, NUTS or ADVI, it is important to ensure the types in your model can be inferred.\n\nThe following example with abstract types"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function tmodel(x, y)\n    p, n = size(x)\n    params = Vector{Real}(undef, n)\n    for i in 1:n\n        params[i] ~ truncated(Normal(); lower=0)\n    end\n\n    a = x * params\n    return y ~ MvNormal(a, I)\nend"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "can be transformed into the following representation with concrete types:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function tmodel(x, y, ::Type{T}=Float64) where {T}\n    p, n = size(x)\n    params = Vector{T}(undef, n)\n    for i in 1:n\n        params[i] ~ truncated(Normal(); lower=0)\n    end\n\n    a = x * params\n    return y ~ MvNormal(a, I)\nend"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Alternatively, you could use `filldist` in this example:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function tmodel(x, y)\n    params ~ filldist(truncated(Normal(); lower=0), size(x, 2))\n    a = x * params\n    return y ~ MvNormal(a, I)\nend"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "You can use DynamicPPL's debugging utilities to find types in your model definition that the compiler cannot infer.\nThese will be marked in red in the Julia REPL (much like when using the `@code_warntype` macro).\n\nFor example, consider the following model:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function tmodel(x)\n    p = Vector{Real}(undef, 1)\n    p[1] ~ Normal()\n    p = p .+ 1\n    return x ~ Normal(p[1])\nend"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Because the element type of `p` is an abstract type (`Real`), the compiler cannot infer a concrete type for `p[1]`.\nTo detect this, we can use"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#| eval: false\nmodel = tmodel(1.0)\n\nusing DynamicPPL\nDynamicPPL.DebugUtils.model_warntype(model)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In this particular model, the following call to `getindex` should be highlighted in red (the exact numbers may vary):\n\n```\n[...]\n│    %120 = p::AbstractVector\n│    %121 = Base.getindex(%120, 1)::Any\n[...]\n```"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
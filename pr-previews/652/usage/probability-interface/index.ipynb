{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Querying Model Probabilities\n",
        "engine: julia\n",
        "aliases:\n",
        "  - ../../tutorials/usage-probability-interface/index.html\n",
        "---\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "#| output: false\n",
        "using Pkg;\n",
        "Pkg.instantiate();\n",
        "```\n",
        "\n",
        "The easiest way to manipulate and query Turing models is via the DynamicPPL probability interface.\n",
        "\n",
        "Let's use a simple model of normally-distributed data as an example.\n",
        "\n",
        "```{julia}\n",
        "using Turing\n",
        "using DynamicPPL\n",
        "using Random\n",
        "\n",
        "@model function gdemo(n)\n",
        "    μ ~ Normal(0, 1)\n",
        "    x ~ MvNormal(fill(μ, n), I)\n",
        "end\n",
        "```\n",
        "\n",
        "We generate some data using `μ = 0`:\n",
        "\n",
        "```{julia}\n",
        "Random.seed!(1776)\n",
        "dataset = randn(100)\n",
        "dataset[1:5]\n",
        "```\n",
        "\n",
        "## Conditioning and Deconditioning\n",
        "\n",
        "Bayesian models can be transformed with two main operations, conditioning and deconditioning (also known as marginalization).\n",
        "Conditioning takes a variable and fixes its value as known.\n",
        "We do this by passing a model and a collection of conditioned variables to `|`, or its alias, `condition`:\n",
        "\n",
        "```{julia}\n",
        "# (equivalently)\n",
        "# conditioned_model = condition(gdemo(length(dataset)), (x=dataset, μ=0))\n",
        "conditioned_model = gdemo(length(dataset)) | (x=dataset, μ=0)\n",
        "```\n",
        "\n",
        "This operation can be reversed by applying `decondition`:\n",
        "\n",
        "```{julia}\n",
        "original_model = decondition(conditioned_model)\n",
        "```\n",
        "\n",
        "We can also decondition only some of the variables:\n",
        "\n",
        "```{julia}\n",
        "partially_conditioned = decondition(conditioned_model, :μ)\n",
        "```\n",
        "\n",
        "We can see which of the variables in a model have been conditioned with `DynamicPPL.conditioned`:\n",
        "\n",
        "```{julia}\n",
        "DynamicPPL.conditioned(partially_conditioned)\n",
        "```\n",
        "\n",
        "::: {.callout-note}\n",
        "Sometimes it is helpful to define convenience functions for conditioning on some variable(s).\n",
        "For instance, in this example we might want to define a version of `gdemo` that conditions on some observations of `x`:\n",
        "\n",
        "```julia\n",
        "gdemo(x::AbstractVector{<:Real}) = gdemo(length(x)) | (; x)\n",
        "```\n",
        "\n",
        "For illustrative purposes, however, we do not use this function in the examples below.\n",
        ":::\n",
        "\n",
        "## Probabilities and Densities\n",
        "\n",
        "We often want to calculate the (unnormalized) probability density for an event.\n",
        "This probability might be a prior, a likelihood, or a posterior (joint) density.\n",
        "DynamicPPL provides convenient functions for this.\n",
        "To begin, let's define a model `gdemo`, condition it on a dataset, and draw a sample.\n",
        "The returned sample only contains `μ`, since the value of `x` has already been fixed:\n",
        "\n",
        "```{julia}\n",
        "model = gdemo(length(dataset)) | (x=dataset,)\n",
        "\n",
        "Random.seed!(124)\n",
        "sample = rand(model)\n",
        "```\n",
        "\n",
        "We can then calculate the joint probability of a set of samples (here drawn from the prior) with `logjoint`.\n",
        "\n",
        "```{julia}\n",
        "logjoint(model, sample)\n",
        "```\n",
        "\n",
        "For models with many variables `rand(model)` can be prohibitively slow since it returns a `NamedTuple` of samples from the prior distribution of the unconditioned variables.\n",
        "We recommend working with samples of type `DataStructures.OrderedDict` in this case (which Turing re-exports, so can be used directly):\n",
        "\n",
        "```{julia}\n",
        "Random.seed!(124)\n",
        "sample_dict = rand(OrderedDict, model)\n",
        "```\n",
        "\n",
        "`logjoint` can also be used on this sample:\n",
        "\n",
        "```{julia}\n",
        "logjoint(model, sample_dict)\n",
        "```\n",
        "\n",
        "The prior probability and the likelihood of a set of samples can be calculated with the functions `logprior` and `loglikelihood` respectively.\n",
        "The log joint probability is the sum of these two quantities:\n",
        "\n",
        "```{julia}\n",
        "logjoint(model, sample) ≈ loglikelihood(model, sample) + logprior(model, sample)\n",
        "```\n",
        "\n",
        "```{julia}\n",
        "logjoint(model, sample_dict) ≈ loglikelihood(model, sample_dict) + logprior(model, sample_dict)\n",
        "```\n",
        "\n",
        "## Example: Cross-validation\n",
        "\n",
        "To give an example of the probability interface in use, we can use it to estimate the performance of our model using cross-validation.\n",
        "In cross-validation, we split the dataset into several equal parts.\n",
        "Then, we choose one of these sets to serve as the validation set.\n",
        "Here, we measure fit using the cross entropy (Bayes loss).[^1]\n",
        "(For the sake of simplicity, in the following code, we enforce that `nfolds` must divide the number of data points.\n",
        "For a more competent implementation, see [MLUtils.jl](https://juliaml.github.io/MLUtils.jl/dev/api/#MLUtils.kfolds).)\n",
        "\n",
        "```{julia}\n",
        "# Calculate the train/validation splits across `nfolds` partitions, assume `length(dataset)` divides `nfolds`\n",
        "function kfolds(dataset::Array{<:Real}, nfolds::Int)\n",
        "    fold_size, remaining = divrem(length(dataset), nfolds)\n",
        "    if remaining != 0\n",
        "        error(\"The number of folds must divide the number of data points.\")\n",
        "    end\n",
        "    first_idx = firstindex(dataset)\n",
        "    last_idx = lastindex(dataset)\n",
        "    splits = map(0:(nfolds - 1)) do i\n",
        "        start_idx = first_idx + i * fold_size\n",
        "        end_idx = start_idx + fold_size\n",
        "        train_set_indices = [first_idx:(start_idx - 1); end_idx:last_idx]\n",
        "        return (view(dataset, train_set_indices), view(dataset, start_idx:(end_idx - 1)))\n",
        "    end\n",
        "    return splits\n",
        "end\n",
        "\n",
        "function cross_val(\n",
        "    dataset::Vector{<:Real};\n",
        "    nfolds::Int=5,\n",
        "    nsamples::Int=1_000,\n",
        "    rng::Random.AbstractRNG=Random.default_rng(),\n",
        ")\n",
        "    # Initialize `loss` in a way such that the loop below does not change its type\n",
        "    model = gdemo(1) | (x=[first(dataset)],)\n",
        "    loss = zero(logjoint(model, rand(rng, model)))\n",
        "\n",
        "    for (train, validation) in kfolds(dataset, nfolds)\n",
        "        # First, we train the model on the training set, i.e., we obtain samples from the posterior.\n",
        "        # For normally-distributed data, the posterior can be computed in closed form.\n",
        "        # For general models, however, typically samples will be generated using MCMC with Turing.\n",
        "        posterior = Normal(mean(train), 1)\n",
        "        samples = rand(rng, posterior, nsamples)\n",
        "\n",
        "        # Evaluation on the validation set.\n",
        "        validation_model = gdemo(length(validation)) | (x=validation,)\n",
        "        loss += sum(samples) do sample\n",
        "            logjoint(validation_model, (μ=sample,))\n",
        "        end\n",
        "    end\n",
        "\n",
        "    return loss\n",
        "end\n",
        "\n",
        "cross_val(dataset)\n",
        "```\n",
        "\n",
        "[^1]: See [ParetoSmooth.jl](https://github.com/TuringLang/ParetoSmooth.jl) for a faster and more accurate implementation of cross-validation than the one provided here."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
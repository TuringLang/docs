{
    "sourceFile": "tutorials/05-gaussian-mixture-model/05_gaussian-mixture-model.jmd",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1629513822298,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1629513822298,
            "name": "Commit-0",
            "content": "---\ntitle: Unsupervised Learning using Bayesian Mixture Models\npermalink: /:collection/:name/\nredirect_from: tutorials/1-gaussianmixturemodel/\nredirect_from: tutorials/gaussian_mixture/\n---\n\nThe following tutorial illustrates the use *Turing* for clustering data using a Bayesian mixture model. The aim of this task is to infer a latent grouping (hidden structure) from unlabelled data.\n\nMore specifically, we are interested in discovering the grouping illustrated in figure below. This example consists of 2-D data points, i.e. $\\boldsymbol{x} = \\\\{x_i\\\\}_{i=1}^N, x_i \\in \\mathbb{R}^2$, which are distributed according to Gaussian distributions. For simplicity, we use isotropic Gaussian distributions but this assumption can easily be relaxed by introducing additional parameters. \n\n\n```julia\nusing Distributions, StatsPlots, Random\n\n# Set a random seed.\nRandom.seed!(3)\n\n# Construct 30 data points for each cluster.\nN = 30\n\n# Parameters for each cluster, we assume that each cluster is Gaussian distributed in the example.\nμs = [-3.5, 0.0]\n\n# Construct the data points.\nx = mapreduce(c -> rand(MvNormal([μs[c], μs[c]], 1.), N), hcat, 1:2)\n\n# Visualization.\nscatter(x[1,:], x[2,:], legend = false, title = \"Synthetic Dataset\")\n```\n\n## Gaussian Mixture Model in Turing\n\nTo cluster the data points shown above, we use a model that consists of two mixture components (clusters) and assigns each datum to one of the components. The assignment thereof determines the distribution that the data point is generated from.\n\nIn particular, in a Bayesian Gaussian mixture model with $1 \\leq k \\leq K$ components for 1-D data each data point $x_i$ with $1 \\leq i \\leq N$ is generated according to the following generative process.\nFirst we draw the parameters for each cluster, i.e. in our example we draw location of the distributions from a Normal:\n$$\n\\mu_k \\sim \\mathrm{Normal}() \\, , \\;  \\forall k\n$$\nand then draw mixing weight for the $K$ clusters from a Dirichlet distribution, i.e.\n$$\n    w \\sim \\mathrm{Dirichlet}(K, \\alpha) \\, .\n$$\nAfter having constructed all the necessary model parameters, we can generate an observation by first selecting one of the clusters and then drawing the datum accordingly, i.e.\n$$\n    z_i \\sim \\mathrm{Categorical}(w) \\, , \\;  \\forall i \\\\\n    x_i \\sim \\mathrm{Normal}(\\mu_{z_i}, 1.) \\, , \\;  \\forall i\n$$\n\nFor more details on Gaussian mixture models, we refer to Christopher M. Bishop, *Pattern Recognition and Machine Learning*, Section 9.\n\n\n```julia\nusing Turing, MCMCChains\n\n# Turn off the progress monitor.\nTuring.setprogress!(false);\n```\n\n```julia\n@model GaussianMixtureModel(x) = begin\n    \n    D, N = size(x)\n\n    # Draw the parameters for cluster 1.\n    μ1 ~ Normal()\n    \n    # Draw the parameters for cluster 2.\n    μ2 ~ Normal()\n    \n    μ = [μ1, μ2]\n    \n    # Uncomment the following lines to draw the weights for the K clusters \n    # from a Dirichlet distribution.\n    \n    # α = 1.0\n    # w ~ Dirichlet(2, α)\n    \n    # Comment out this line if you instead want to draw the weights.\n    w = [0.5, 0.5]\n    \n    # Draw assignments for each datum and generate it from a multivariate normal.\n    k = Vector{Int}(undef, N)\n    for i in 1:N\n        k[i] ~ Categorical(w)\n        x[:,i] ~ MvNormal([μ[k[i]], μ[k[i]]], 1.)\n    end\n    return k\nend\n```\n\nAfter having specified the model in Turing, we can construct the model function and run a MCMC simulation to obtain assignments of the data points.\n\n\n```julia\ngmm_model = GaussianMixtureModel(x);\n```\n\nTo draw observations from the posterior distribution, we use a [particle Gibbs](https://www.stats.ox.ac.uk/~doucet/andrieu_doucet_holenstein_PMCMC.pdf) sampler to draw the discrete assignment parameters as well as a Hamiltonion Monte Carlo sampler for continous parameters.\n\nNote that we use a `Gibbs` sampler to combine both samplers for Bayesian inference in our model.\nWe are also calling `MCMCThreads` to generate multiple chains, particularly so we test for convergence.\n\n```julia\ngmm_sampler = Gibbs(PG(100, :k), HMC(0.05, 10, :μ1, :μ2))\ntchain = sample(gmm_model, gmm_sampler, MCMCThreads(), 100, 3);\n```\n\n## Visualize the Density Region of the Mixture Model\n\nAfter sucessfully doing posterior inference, we can first visualize the trace and density of the parameters of interest.\n\nIn particular, in this example we consider the sample values of the location parameter for the two clusters.\n\n\n```julia\nids = findall(map(name -> occursin(\"μ\", string(name)), names(tchain)));\np=plot(tchain[:, ids, :], legend=true, labels = [\"Mu 1\" \"Mu 2\"], colordim=:parameter)\n```\n\nYou'll note here that it appears the location means are switching between chains. We will address this in future tutorials. For those who are keenly interested, see [this](https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html) article on potential solutions.\n\nFor the moment, we will just use the first chain to ensure the validity of our inference.\n\n\n```julia\ntchain = tchain[:, :, 1];\n```\n\nAs the samples for the location parameter for both clusters are unimodal, we can safely visualize the density region of our model using the average location.\n\n\n```julia\n# Helper function used for visualizing the density region.\nfunction predict(x, y, w, μ)\n    # Use log-sum-exp trick for numeric stability.\n    return Turing.logaddexp(\n        log(w[1]) + logpdf(MvNormal([μ[1], μ[1]], 1.), [x, y]), \n        log(w[2]) + logpdf(MvNormal([μ[2], μ[2]], 1.), [x, y])\n    )\nend\n```\n\n```julia\ncontour(range(-5, stop = 3), range(-6, stop = 2), \n    (x, y) -> predict(x, y, [0.5, 0.5], [mean(tchain[:μ1]), mean(tchain[:μ2])])\n)\nscatter!(x[1,:], x[2,:], legend = false, title = \"Synthetic Dataset\")\n```\n\n## Infered Assignments\n\nFinally, we can inspect the assignments of the data points infered using Turing. As we can see, the dataset is partitioned into two distinct groups.\n\n\n```julia\n# TODO: is there a better way than this icky `.nt.mean` stuff?\nassignments = mean(MCMCChains.group(tchain, :k)).nt.mean\nscatter(x[1,:], x[2,:], \n    legend = false, \n    title = \"Assignments on Synthetic Dataset\", \n    zcolor = assignments)\n```\n\n```julia, echo=false, skip=\"notebook\"\nif isdefined(Main, :TuringTutorials)\n    Main.TuringTutorials.tutorial_footer(WEAVE_ARGS[:folder], WEAVE_ARGS[:file])\nend\n```\n"
        }
    ]
}
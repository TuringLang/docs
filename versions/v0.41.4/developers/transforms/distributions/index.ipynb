{
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": "# Install necessary dependencies.\nusing Pkg\nPkg.activate(; temp=true)\nPkg.add([\"Distributions\", \"Plots\", \"Random\"])",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| echo: false\n#| output: false\nusing Pkg;\nPkg.instantiate();\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This series of articles will seek to motivate the [Bijectors.jl package](https://github.com/TuringLang/Bijectors.jl/), which provides the tools for transforming distributions in the Turing.jl probabilistic programming language.\n\nIt assumes:\n\n- some basic knowledge of probability distributions (the notions of sampling from them and calculating the probability density function for a given distribution); and\n- some calculus (the chain and product rules for differentiation, and changes of variables in integrals).",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "import Random\nRandom.seed!(468);\n\nusing Distributions: Normal, LogNormal, logpdf, Distributions\nusing Plots: histogram",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Sampling from a distribution\n\nTo sample from a distribution (as defined in [Distributions.jl](https://juliastats.org/Distributions.jl/)), we can use the `rand` function.\nLet's sample from a normal distribution and then plot a histogram of the samples.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "samples = rand(Normal(), 5000)\nhistogram(samples, bins=50)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "(Calling `Normal()` without any arguments, as we do here, gives us a normal distribution with mean 0 and standard deviation 1.)\nIf you want to know the log probability density of observing any of the samples, you can use `logpdf`:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "println(\"sample: $(samples[1])\")\nprintln(\"logpdf: $(logpdf(Normal(), samples[1]))\")",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The probability density function for the normal distribution with mean 0 and standard deviation 1 is\n\n$$p(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp{\\left(-\\frac{x^2}{2}\\right)},$$\n\nso we could also have calculated this manually using:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "log(1 / sqrt(2π) * exp(-samples[1]^2 / 2))",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "(or more efficiently, `-(samples[1]^2 + log2π) / 2`, where `log2π` is from the [IrrationalConstants.jl package](https://github.com/JuliaMath/IrrationalConstants.jl)).\n\n## Sampling from a transformed distribution\n\nSay that $x$ is distributed according to `Normal()`, and we want to draw samples of $y = \\exp(x)$.\nNow, $y$ is itself a random variable, and like any other random variable, will have a probability distribution, which we'll call $q(y)$.\n\nIn this specific case, the distribution of $y$ is known as a [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution).\nFor the purposes of this tutorial, let's implement our own `MyLogNormal` distribution that we can sample from.\n(Distributions.jl already defines its own `LogNormal`, so we have to use a different name.)\nTo do this, we need to overload `Base.rand` for our new distribution.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "struct MyLogNormal <: Distributions.ContinuousUnivariateDistribution\n    μ::Float64\n    σ::Float64\nend\nMyLogNormal() = MyLogNormal(0.0, 1.0)\n\nfunction Base.rand(rng::Random.AbstractRNG, d::MyLogNormal)\n  exp(rand(rng, Normal(d.μ, d.σ)))\nend",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Now we can do the same as above:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\nsamples_lognormal = rand(MyLogNormal(), 5000)\n# Cut off the tail for clearer visualisation\nhistogram(samples_lognormal, bins=0:0.1:5; xlims=(0, 5))\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "How do we implement `logpdf` for our new distribution, though?\nOr in other words, if we observe a sample $y$, how do we know what the probability of drawing that sample was?\n\nNaively, we might think to just un-transform the variable `y` by reversing the exponential, i.e. taking the logarithm.\nWe could then use the `logpdf` of the original distribution of `x`.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "naive_logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y))",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We can compare this function against the logpdf implemented in Distributions.jl:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "println(\"Sample   : $(samples_lognormal[1])\")\nprintln(\"Expected : $(logpdf(LogNormal(), samples_lognormal[1]))\")\nprintln(\"Actual   : $(naive_logpdf(MyLogNormal(), samples_lognormal[1]))\")",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Clearly this approach is not quite correct!\n\n## The derivative\n\nThe reason why this doesn't work is because transforming a (continuous) distribution causes probability density to be stretched and otherwise moved around.\nFor example, in the normal distribution, half of the probability density is between $-\\infty$ and $0$, and half is between $0$ and $\\infty$.\nWhen exponentiated (i.e. in the log-normal distribution), the first half of the density is mapped to the interval $(0, 1)$, and the second half to $(1, \\infty)$.\n\nThis 'explanation' on its own does not really mean much, though.\nA perhaps more useful approach is to not talk about _probability densities_, but instead to make it more concrete by relating them to actual _probabilities_.\nIf we think about the normal distribution as a continuous curve, what the probability density function $p(x)$ really tells us is that: for any two points $a$ and $b$ (where $a \\leq b$), the probability of drawing a sample between $a$ and $b$ is the corresponding area under the curve, i.e.\n\n$$\\int_a^b p(x) \\, \\mathrm{d}x.$$\n\nFor example, if $(a, b) = (-\\infty, \\infty)$, then the probability of drawing a sample between $a$ and $b$ is 1.\n\nLet's say that the probability density function of the log-normal distribution is $q(y)$.\nThen, the area under the curve between the two points $\\exp(a)$ and $\\exp(b)$ is:\n\n$$\\int_{\\exp(a)}^{\\exp(b)} q(y) \\, \\mathrm{d}y.$$\n\nThis integral should be equal to the one above, because the probability of drawing from $[a, b]$ in the original distribution should be the same as the probability of drawing from $[\\exp(a), \\exp(b)]$ in the transformed distribution.\nThe question we have to solve here is: how do we find a function $q(y)$ such that this equality holds?\n\nWe can approach this by making the substitution $y = \\exp(x)$ in the first integral (see [Wikipedia](https://en.wikipedia.org/wiki/Integration_by_substitution) for a refresher on substitutions in integrals, if needed).\nWe have that:\n\n$$\\frac{\\mathrm{d}y}{\\mathrm{d}x} = \\exp(x) = y \\implies \\mathrm{d}x = \\frac{1}{y}\\,\\mathrm{d}y$$\n\nand so\n\n$$\\int_{x=a}^{x=b} p(x) \\, \\mathrm{d}x\n  = \\int_{y=\\exp(a)}^{y=\\exp(b)} p(\\log(y)) \\frac{1}{y} \\,\\mathrm{d}y\n  = \\int_{\\exp(a)}^{\\exp(b)} q(y) \\, \\mathrm{d}y,\n$$\n\nfrom which we can read off $q(y) = p(\\log(y)) / y$.\n\nIn contrast, when we implemented `naive_logpdf`",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "naive_logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y))",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "that was the equivalent of saying that $q(y) = p(\\log(y))$.\nWe left out a factor of $1/y$!\n\nIndeed, now we can define the correct `logpdf` function.\nSince everything is a logarithm here, instead of multiplying by $1/y$ we subtract $\\log(y)$:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "Distributions.logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y)) - log(y)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "and check that it works:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "println(\"Sample   : $(samples_lognormal[1])\")\nprintln(\"Expected : $(logpdf(LogNormal(), samples_lognormal[1]))\")\nprintln(\"Actual   : $(logpdf(MyLogNormal(), samples_lognormal[1]))\")",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The same process can be applied to any kind of (invertible) transformation.\nIf we have some transformation from $x$ to $y$, and the probability density functions of $x$ and $y$ are $p(x)$ and $q(y)$ respectively, then we have a general formula that:\n\n$$q(y) = p(x) \\left| \\frac{\\mathrm{d}x}{\\mathrm{d}y} \\right|.$$\n\nIn this case, we had $y = \\exp(x)$, so $\\mathrm{d}x/\\mathrm{d}y = 1/y$.\n(This equation is (11.5) in Bishop's textbook.)\n\n> The absolute value here takes care of the case where $f$ is a decreasing function, i.e., $f(x) > f(y)$ when $x < y$.\n> You can try this out with the transformation $y = -\\exp(x)$.\n> If $a < b$, then $-\\exp(a) > -\\exp(b)$, and so you will have to swap the integration limits to ensure that the integral comes out positive.\n\nNote that $\\mathrm{d}y/\\mathrm{d}x$ is equal to $(\\mathrm{d}x/\\mathrm{d}y)^{-1}$, so the formula above can also be written as:\n\n$$q(y) \\left| \\frac{\\mathrm{d}y}{\\mathrm{d}x} \\right| = p(x).$$\n\n## The Jacobian\n\nIn general, we may have transforms that act on multivariate distributions: for example, something mapping $p(x_1, x_2)$ to $q(y_1, y_2)$.\nIn this case, we need to extend the rule above by introducing what is known as the Jacobian matrix:\n\nIn this case, the rule above has to be extended by replacing the derivative $\\mathrm{d}x/\\mathrm{d}y$ with the determinant of the inverse Jacobian matrix:\n\n$$\\mathbf{J} = \\begin{pmatrix}\n\\partial y_1/\\partial x_1 & \\partial y_1/\\partial x_2 \\\\\n\\partial y_2/\\partial x_1 & \\partial y_2/\\partial x_2\n\\end{pmatrix}.$$\n\nThis allows us to write the direct generalisation as:\n\n$$q(y_1, y_2) \\left| \\det(\\mathbf{J}) \\right| = p(x_1, x_2),$$\n\nor equivalently,\n\n$$q(y_1, y_2) = p(x_1, x_2) \\left| \\det(\\mathbf{J}^{-1}) \\right|.$$\n\nwhere $\\mathbf{J}^{-1}$ is the inverse of the Jacobian matrix.\nThis is the same as equation (11.9) in Bishop.\n\n> Instead of inverting the original Jacobian matrix to get $\\mathbf{J}^{-1}$, we could also use the Jacobian of the inverse function:\n> \n> $$\\mathbf{J}_\\text{inv} = \\begin{pmatrix}\n> \\partial x_1/\\partial y_1 & \\partial x_1/\\partial y_2 \\\\\n> \\partial x_2/\\partial y_1 & \\partial x_2/\\partial y_2\n> \\end{pmatrix}.$$\n> \n> As it turns out, these are entirely equivalent: the Jacobian of the inverse function is the inverse of the original Jacobian matrix.\n\nThe rest of this section will be devoted to an example to show that this works, and contains some slightly less pretty mathematics.\nIf you are already suitably convinced by this stage, then you can skip the rest of this section.\n(Or if you prefer something more formal, the Wikipedia article on integration by substitution [discusses the multivariate case as well](https://en.wikipedia.org/wiki/Integration_by_substitution#Substitution_for_multiple_variables).)\n\n### An example: the Box–Muller transform\n\nA motivating example where one might like to use a Jacobian is the [Box–Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform), which is a technique for sampling from a normal distribution.\n\nThe Box–Muller transform works by first sampling two random variables from the uniform distribution between 0 and 1:\n\n$$\\begin{align}\nx_1 &\\sim U(0, 1) \\\\\nx_2 &\\sim U(0, 1).\n\\end{align}$$\n\nBoth of these have a probability density function of $p(x) = 1$ for $0 < x \\leq 1$, and 0 otherwise.\nBecause they are independent, we can write that\n\n$$p(x_1, x_2) = p(x_1) p(x_2) = \\begin{cases}\n1 & \\text{if } 0 < x_1 \\leq 1 \\text{ and } 0 < x_2 \\leq 1, \\\\\n0 & \\text{otherwise}.\n\\end{cases}$$\n\nThe next step is to perform the transforms\n\n$$\\begin{align}\ny_1 &= \\sqrt{-2 \\log(x_1)} \\cos(2\\pi x_2); \\\\\ny_2 &= \\sqrt{-2 \\log(x_1)} \\sin(2\\pi x_2),\n\\end{align}$$\n\nand it turns out that with these transforms, both $y_1$ and $y_2$ are independent and normally distributed with mean 0 and standard deviation 1, i.e.\n\n$$q(y_1, y_2) = \\frac{1}{2\\pi} \\exp{\\left(-\\frac{y_1^2}{2}\\right)} \\exp{\\left(-\\frac{y_2^2}{2}\\right)}.$$\n\nHow can we show that this is the case?\n\nThere are many ways to work out the required calculus.\nSome are more elegant and some rather less so!\nOne of the less headache-inducing ways is to define the intermediate variables:\n\n$$r = \\sqrt{-2 \\log(x_1)}; \\quad \\theta = 2\\pi x_2,$$\n\nfrom which we can see that $y_1 = r\\cos\\theta$ and $y_2 = r\\sin\\theta$, and hence\n\n$$\\begin{align}\nx_1 &= \\exp{\\left(-\\frac{r^2}{2}\\right)} = \\exp{\\left(-\\frac{y_1^2}{2}\\right)}\\exp{\\left(-\\frac{y_2^2}{2}\\right)}; \\\\\nx_2 &= \\frac{\\theta}{2\\pi} = \\frac{1}{2\\pi} \\, \\arctan\\left(\\frac{y_2}{y_1}\\right).\n\\end{align}$$\n\nThis lets us obtain the requisite partial derivatives in a way that doesn't involve _too_ much algebra.\nAs an example, we have\n\n$$\\frac{\\partial x_1}{\\partial y_1} = -y_1 \\exp{\\left(-\\frac{y_1^2}{2}\\right)}\\exp{\\left(-\\frac{y_2^2}{2}\\right)} = -y_1 x_1,$$\n\n(where we used the product rule), and\n\n$$\\frac{\\partial x_2}{\\partial y_1} = \\frac{1}{2\\pi} \\left(\\frac{1}{1 + (y_2/y_1)^2}\\right) \\left(-\\frac{y_2}{y_1^2}\\right),$$\n\n(where we used the chain rule, and the derivative $\\mathrm{d}(\\arctan(a))/\\mathrm{d}a = 1/(1 + a^2)$).\n\nPutting together the Jacobian matrix, we have:\n\n$$\\mathbf{J} = \\begin{pmatrix}\n-y_1 x_1 & -y_2 x_1 \\\\\n-cy_2/y_1^2 & c/y_1 \\\\\n\\end{pmatrix},$$\n\nwhere $c = [2\\pi(1 + (y_2/y_1)^2)]^{-1}$.\nThe determinant of this matrix is\n\n$$\\begin{align}\n\\det(\\mathbf{J}) &= -cx_1 - cx_1(y_2/y_1)^2 \\\\\n&= -cx_1\\left[1 + \\left(\\frac{y_2}{y_1}\\right)^2\\right] \\\\\n&= -\\frac{1}{2\\pi} x_1 \\\\\n&= -\\frac{1}{2\\pi}\\exp{\\left(-\\frac{y_1^2}{2}\\right)}\\exp{\\left(-\\frac{y_2^2}{2}\\right)},\n\\end{align}$$\n\nComing right back to our probability density, we have that\n\n$$\\begin{align}\nq(y_1, y_2) &= p(x_1, x_2) \\cdot |\\det(\\mathbf{J})| \\\\\n&= \\frac{1}{2\\pi}\\exp{\\left(-\\frac{y_1^2}{2}\\right)}\\exp{\\left(-\\frac{y_2^2}{2}\\right)},\n\\end{align}$$\n\nas desired.\n\n> We haven't yet explicitly accounted for the fact that $p(x_1, x_2)$ is 0 if either $x_1$ or $x_2$ are outside the range $(0, 1]$.\n> For example, if this constraint on $x_1$ and $x_2$ were to result in inaccessible values of $y_1$ or $y_2$, then $q(y_1, y_2)$ should be 0 for those values.\n> Formally, for the transformation $f: X \\to Y$ where $X$ is the unit square (i.e. $0 < x_1, x_2 \\leq 1$), $q(y_1, y_2)$ should only take the above value for the [image](https://en.wikipedia.org/wiki/Image_(mathematics)) of $f$, and anywhere outside of the image it should be 0.\n> \n> In our case, the $\\log(x_1)$ term in the transform varies between 0 and $\\infty$, and the $\\cos(2\\pi x_2)$ term ranges from $-1$ to $1$.\n> Hence $y_1$, which is the product of these two terms, ranges from $-\\infty$ to $\\infty$, and likewise for $y_2$.\n> So the image of $f$ is the entire real plane, and we don't have to worry about this.\n\nHaving seen the theory that underpins how distributions can be transformed, let's now turn to how this is implemented in the Turing ecosystem.",
      "metadata": {}
    }
  ],
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia",
      "language": "julia"
    }
  },
  "nbformat": 4
}
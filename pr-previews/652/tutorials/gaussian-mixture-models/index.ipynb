{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Gaussian Mixture Models\n",
        "engine: julia\n",
        "aliases:\n",
        " - ../01-gaussian-mixture-model/index.html\n",
        "---\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "#| output: false\n",
        "using Pkg;\n",
        "Pkg.instantiate();\n",
        "```\n",
        "\n",
        "The following tutorial illustrates the use of Turing for an unsupervised task, namely, clustering data using a Bayesian mixture model.\n",
        "The aim of this task is to infer a latent grouping (hidden structure) from unlabelled data.\n",
        "\n",
        "## Synthetic Data\n",
        "\n",
        "We generate a synthetic dataset of $N = 60$ two-dimensional points $x_i \\in \\mathbb{R}^2$ drawn from a Gaussian mixture model.\n",
        "For simplicity, we use $K = 2$ clusters with\n",
        "\n",
        "- equal weights, i.e., we use mixture weights $w = [0.5, 0.5]$, and\n",
        "- isotropic Gaussian distributions of the points in each cluster.\n",
        "\n",
        "More concretely, we use the Gaussian distributions $\\mathcal{N}([\\mu_k, \\mu_k]^\\mathsf{T}, I)$ with parameters $\\mu_1 = -3.5$ and $\\mu_2 = 0.5$.\n",
        "\n",
        "```{julia}\n",
        "using Distributions\n",
        "using FillArrays\n",
        "using StatsPlots\n",
        "\n",
        "using LinearAlgebra\n",
        "using Random\n",
        "\n",
        "# Set a random seed.\n",
        "Random.seed!(3)\n",
        "\n",
        "# Define Gaussian mixture model.\n",
        "w = [0.5, 0.5]\n",
        "μ = [-3.5, 0.5]\n",
        "mixturemodel = MixtureModel([MvNormal(Fill(μₖ, 2), I) for μₖ in μ], w)\n",
        "\n",
        "# We draw the data points.\n",
        "N = 60\n",
        "x = rand(mixturemodel, N);\n",
        "```\n",
        "\n",
        "The following plot shows the dataset.\n",
        "\n",
        "```{julia}\n",
        "scatter(x[1, :], x[2, :]; legend=false, title=\"Synthetic Dataset\")\n",
        "```\n",
        "\n",
        "## Gaussian Mixture Model in Turing\n",
        "\n",
        "We are interested in recovering the grouping from the dataset.\n",
        "More precisely, we want to infer the mixture weights, the parameters $\\mu_1$ and $\\mu_2$, and the assignment of each datum to a cluster for the generative Gaussian mixture model.\n",
        "\n",
        "In a Bayesian Gaussian mixture model with $K$ components each data point $x_i$ ($i = 1,\\ldots,N$) is generated according to the following generative process.\n",
        "First we draw the model parameters, i.e., in our example we draw parameters $\\mu_k$ for the mean of the isotropic normal distributions and the mixture weights $w$ of the $K$ clusters.\n",
        "We use standard normal distributions as priors for $\\mu_k$ and a Dirichlet distribution with parameters $\\alpha_1 = \\cdots = \\alpha_K = 1$ as prior for $w$:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mu_k &\\sim \\mathcal{N}(0, 1) \\qquad (k = 1,\\ldots,K)\\\\\n",
        "w &\\sim \\operatorname{Dirichlet}(\\alpha_1, \\ldots, \\alpha_K)\n",
        "\\end{aligned}\n",
        "$$\n",
        "After having constructed all the necessary model parameters, we can generate an observation by first selecting one of the clusters\n",
        "$$\n",
        "z_i \\sim \\operatorname{Categorical}(w) \\qquad (i = 1,\\ldots,N),\n",
        "$$\n",
        "and then drawing the datum accordingly, i.e., in our example drawing\n",
        "$$\n",
        "x_i \\sim \\mathcal{N}([\\mu_{z_i}, \\mu_{z_i}]^\\mathsf{T}, I) \\qquad (i=1,\\ldots,N).\n",
        "$$\n",
        "For more details on Gaussian mixture models, refer to Chapter 9 of Christopher M. Bishop, *Pattern Recognition and Machine Learning*.\n",
        "\n",
        "We specify the model in Turing:\n",
        "\n",
        "```{julia}\n",
        "using Turing\n",
        "\n",
        "@model function gaussian_mixture_model(x)\n",
        "    # Draw the parameters for each of the K=2 clusters from a standard normal distribution.\n",
        "    K = 2\n",
        "    μ ~ MvNormal(Zeros(K), I)\n",
        "\n",
        "    # Draw the weights for the K clusters from a Dirichlet distribution with parameters αₖ = 1.\n",
        "    w ~ Dirichlet(K, 1.0)\n",
        "    # Alternatively, one could use a fixed set of weights.\n",
        "    # w = fill(1/K, K)\n",
        "\n",
        "    # Construct categorical distribution of assignments.\n",
        "    distribution_assignments = Categorical(w)\n",
        "\n",
        "    # Construct multivariate normal distributions of each cluster.\n",
        "    D, N = size(x)\n",
        "    distribution_clusters = [MvNormal(Fill(μₖ, D), I) for μₖ in μ]\n",
        "\n",
        "    # Draw assignments for each datum and generate it from the multivariate normal distribution.\n",
        "    k = Vector{Int}(undef, N)\n",
        "    for i in 1:N\n",
        "        k[i] ~ distribution_assignments\n",
        "        x[:, i] ~ distribution_clusters[k[i]]\n",
        "    end\n",
        "\n",
        "    return k\n",
        "end\n",
        "\n",
        "model = gaussian_mixture_model(x);\n",
        "```\n",
        "\n",
        "We run a MCMC simulation to obtain an approximation of the posterior distribution of the parameters $\\mu$ and $w$ and assignments $k$.\n",
        "We use a `Gibbs` sampler that combines a [particle Gibbs](https://www.stats.ox.ac.uk/%7Edoucet/andrieu_doucet_holenstein_PMCMC.pdf) sampler for the discrete parameters (assignments $k$) and a Hamiltonian Monte Carlo sampler for the continuous parameters ($\\mu$ and $w$).\n",
        "We generate multiple chains in parallel using multi-threading.\n",
        "\n",
        "```{julia}\n",
        "#| output: false\n",
        "#| echo: false\n",
        "setprogress!(false)\n",
        "```\n",
        "\n",
        "```{julia}\n",
        "#| output: false\n",
        "sampler = Gibbs(:k => PG(100), (:μ, :w) => HMC(0.05, 10))\n",
        "nsamples = 150\n",
        "nchains = 4\n",
        "burn = 10\n",
        "chains = sample(model, sampler, MCMCThreads(), nsamples, nchains, discard_initial = burn);\n",
        "```\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Sampling With Multiple Threads\n",
        "The `sample()` call above assumes that you have at least two threads available in your Julia instance.\n",
        "If you do not, the multiple chains will run sequentially, and you may notice a warning.\n",
        "For more information, see [the Turing documentation on sampling multiple chains.]({{<meta core-functionality>}}#sampling-multiple-chains)\n",
        ":::\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "let\n",
        "    # Verify that the output of the chain is as expected.\n",
        "    for i in MCMCChains.chains(chains)\n",
        "        # μ[1] and μ[2] can switch places, so we sort the values first.\n",
        "        chain = Array(chains[:, [\"μ[1]\", \"μ[2]\"], i])\n",
        "        μ_mean = vec(mean(chain; dims=1))\n",
        "        @assert isapprox(sort(μ_mean), μ; rtol=0.1) \"Difference between estimated mean of μ ($(sort(μ_mean))) and data-generating μ ($μ) unexpectedly large!\"\n",
        "    end\n",
        "end\n",
        "```\n",
        "\n",
        "## Inferred Mixture Model\n",
        "\n",
        "After sampling we can visualize the trace and density of the parameters of interest.\n",
        "\n",
        "We consider the samples of the location parameters $\\mu_1$ and $\\mu_2$ for the two clusters.\n",
        "\n",
        "```{julia}\n",
        "plot(chains[[\"μ[1]\", \"μ[2]\"]]; legend=true)\n",
        "```\n",
        "\n",
        "From the plots above, we can see that the chains have converged to seemingly different values for the parameters $\\mu_1$ and $\\mu_2$.\n",
        "However, these actually represent the same solution: it does not matter whether we assign $\\mu_1$ to the first cluster and $\\mu_2$ to the second, or vice versa, since the resulting sum is the same.\n",
        "(In principle it is also possible for the parameters to swap places _within_ a single chain, although this does not happen in this example.)\n",
        "For more information see the [Stan documentation](https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html), or Bishop's book, where the concept of _identifiability_ is discussed.\n",
        "\n",
        "Having $\\mu_1$ and $\\mu_2$ swap can complicate the interpretation of the results, especially when different chains converge to different assignments.\n",
        "One solution here is to enforce an ordering on our $\\mu$ vector, requiring $\\mu_k \\geq \\mu_{k-1}$ for all $k$.\n",
        "`Bijectors.jl` [provides](https://turinglang.org/Bijectors.jl/stable/transforms/#Bijectors.OrderedBijector) a convenient function, `ordered()`, which can be applied to a (continuous multivariate) distribution to enforce this:\n",
        "\n",
        "```{julia}\n",
        "using Bijectors: ordered\n",
        "\n",
        "@model function gaussian_mixture_model_ordered(x)\n",
        "    # Draw the parameters for each of the K=2 clusters from a standard normal distribution.\n",
        "    K = 2\n",
        "    μ ~ ordered(MvNormal(Zeros(K), I))\n",
        "    # Draw the weights for the K clusters from a Dirichlet distribution with parameters αₖ = 1.\n",
        "    w ~ Dirichlet(K, 1.0)\n",
        "    # Alternatively, one could use a fixed set of weights.\n",
        "    # w = fill(1/K, K)\n",
        "    # Construct categorical distribution of assignments.\n",
        "    distribution_assignments = Categorical(w)\n",
        "    # Construct multivariate normal distributions of each cluster.\n",
        "    D, N = size(x)\n",
        "    distribution_clusters = [MvNormal(Fill(μₖ, D), I) for μₖ in μ]\n",
        "    # Draw assignments for each datum and generate it from the multivariate normal distribution.\n",
        "    k = Vector{Int}(undef, N)\n",
        "    for i in 1:N\n",
        "        k[i] ~ distribution_assignments\n",
        "        x[:, i] ~ distribution_clusters[k[i]]\n",
        "    end\n",
        "    return k\n",
        "end\n",
        "\n",
        "model = gaussian_mixture_model_ordered(x);\n",
        "```\n",
        "\n",
        "Now, re-running our model, we can see that the assigned means are consistent between chains:\n",
        "\n",
        "```{julia}\n",
        "#| output: false\n",
        "chains = sample(model, sampler, MCMCThreads(), nsamples, nchains, discard_initial = burn);\n",
        "```\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "let\n",
        "    # Verify that the output of the chain is as expected\n",
        "    for i in MCMCChains.chains(chains)\n",
        "        # μ[1] and μ[2] can no longer switch places. Check that they've found the mean\n",
        "        chain = Array(chains[:, [\"μ[1]\", \"μ[2]\"], i])\n",
        "        μ_mean = vec(mean(chain; dims=1))\n",
        "        @assert isapprox(sort(μ_mean), μ; rtol=0.4) \"Difference between estimated mean of μ ($(sort(μ_mean))) and data-generating μ ($μ) unexpectedly large!\"\n",
        "    end\n",
        "end\n",
        "```\n",
        "\n",
        "```{julia}\n",
        "plot(chains[[\"μ[1]\", \"μ[2]\"]]; legend=true)\n",
        "```\n",
        "\n",
        "We also inspect the samples of the mixture weights $w$.\n",
        "\n",
        "```{julia}\n",
        "plot(chains[[\"w[1]\", \"w[2]\"]]; legend=true)\n",
        "```\n",
        "\n",
        "As the distributions of the samples for the parameters $\\mu_1$, $\\mu_2$, $w_1$, and $w_2$ are unimodal, we can safely visualize the density region of our model using the average values.\n",
        "\n",
        "```{julia}\n",
        "# Model with mean of samples as parameters.\n",
        "μ_mean = [mean(chains, \"μ[$i]\") for i in 1:2]\n",
        "w_mean = [mean(chains, \"w[$i]\") for i in 1:2]\n",
        "mixturemodel_mean = MixtureModel([MvNormal(Fill(μₖ, 2), I) for μₖ in μ_mean], w_mean)\n",
        "contour(\n",
        "    range(-7.5, 3; length=1_000),\n",
        "    range(-6.5, 3; length=1_000),\n",
        "    (x, y) -> logpdf(mixturemodel_mean, [x, y]);\n",
        "    widen=false,\n",
        ")\n",
        "scatter!(x[1, :], x[2, :]; legend=false, title=\"Synthetic Dataset\")\n",
        "```\n",
        "\n",
        "## Inferred Assignments\n",
        "\n",
        "Finally, we can inspect the assignments of the data points inferred using Turing.\n",
        "As we can see, the dataset is partitioned into two distinct groups.\n",
        "\n",
        "```{julia}\n",
        "assignments = [mean(chains, \"k[$i]\") for i in 1:N]\n",
        "scatter(\n",
        "    x[1, :],\n",
        "    x[2, :];\n",
        "    legend=false,\n",
        "    title=\"Assignments on Synthetic Dataset\",\n",
        "    zcolor=assignments,\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "## Marginalizing Out The Assignments\n",
        "\n",
        "We can write out the marginal posterior of (continuous) $w, \\mu$ by summing out the influence of our (discrete) assignments $z_i$ from our likelihood:\n",
        "\n",
        "$$p(y \\mid w, \\mu ) = \\sum_{k=1}^K w_k p_k(y \\mid \\mu_k)$$\n",
        "\n",
        "In our case, this gives us:\n",
        "\n",
        "$$p(y \\mid w, \\mu) = \\sum_{k=1}^K w_k \\cdot \\operatorname{MvNormal}(y \\mid \\mu_k, I)$$\n",
        "\n",
        "\n",
        "### Marginalizing By Hand\n",
        "\n",
        "We could implement the above version of the Gaussian mixture model in Turing as follows.\n",
        "\n",
        "First, Turing uses log-probabilities, so the likelihood above must be converted into log-space:\n",
        "\n",
        "$$\\log \\left( p(y \\mid w, \\mu) \\right) = \\text{logsumexp} \\left[\\log (w_k) + \\log(\\operatorname{MvNormal}(y \\mid \\mu_k, I)) \\right]$$\n",
        "\n",
        "Where we sum the components with `logsumexp` from the [`LogExpFunctions.jl` package](https://juliastats.org/LogExpFunctions.jl/stable/).\n",
        "The manually incremented likelihood can be added to the log-probability with `@addlogprob!`, giving us the following model:\n",
        "\n",
        "```{julia}\n",
        "#| output: false\n",
        "using LogExpFunctions\n",
        "\n",
        "@model function gmm_marginalized(x)\n",
        "    K = 2\n",
        "    D, N = size(x)\n",
        "    μ ~ ordered(MvNormal(Zeros(K), I))\n",
        "    w ~ Dirichlet(K, 1.0)\n",
        "    dists = [MvNormal(Fill(μₖ, D), I) for μₖ in μ]\n",
        "    for i in 1:N\n",
        "        lvec = Vector(undef, K)\n",
        "        for k in 1:K\n",
        "            lvec[k] = (w[k] + logpdf(dists[k], x[:, i]))\n",
        "        end\n",
        "        @addlogprob! logsumexp(lvec)\n",
        "    end\n",
        "end\n",
        "```\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Manually Incrementing Probablity\n",
        "\n",
        "When possible, use of `@addlogprob!` should be avoided, as it exists outside the usual structure of a Turing model.\n",
        "In most cases, a custom distribution should be used instead.\n",
        "\n",
        "The next section demonstrates the preferred method: using the `MixtureModel` distribution we have seen already to perform the marginalization automatically.\n",
        ":::\n",
        "\n",
        "### Marginalizing For Free With Distribution.jl's `MixtureModel` Implementation\n",
        "\n",
        "We can use Turing's `~` syntax with anything that `Distributions.jl` provides `logpdf` and `rand` methods for.\n",
        "It turns out that the `MixtureModel` distribution it provides has, as its `logpdf` method, `logpdf(MixtureModel([Component_Distributions], weight_vector), Y)`, where `Y` can be either a single observation or vector of observations.\n",
        "\n",
        "In fact, `Distributions.jl` provides [many convenient constructors](https://juliastats.org/Distributions.jl/stable/mixture/) for mixture models, allowing further simplification in common special cases.\n",
        "\n",
        "For example, when mixtures distributions are of the same type, one can write: `~ MixtureModel(Normal, [(μ1, σ1), (μ2, σ2)], w)`, or when the weight vector is known to allocate probability equally, it can be ommited.\n",
        "\n",
        "The `logpdf` implementation for a `MixtureModel` distribution is exactly the marginalization defined above, and so our model can be simplified to:\n",
        "\n",
        "```{julia}\n",
        "#| output: false\n",
        "@model function gmm_marginalized(x)\n",
        "    K = 2\n",
        "    D, _ = size(x)\n",
        "    μ ~ ordered(MvNormal(Zeros(K), I))\n",
        "    w ~ Dirichlet(K, 1.0)\n",
        "    x ~ MixtureModel([MvNormal(Fill(μₖ, D), I) for μₖ in μ], w)\n",
        "end\n",
        "model = gmm_marginalized(x);\n",
        "```\n",
        "\n",
        "As we have summed out the discrete components, we can perform inference using `NUTS()` alone.\n",
        "\n",
        "```{julia}\n",
        "#| output: false\n",
        "sampler = NUTS()\n",
        "chains = sample(model, sampler, MCMCThreads(), nsamples, nchains; discard_initial = burn);\n",
        "```\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "let\n",
        "    # Verify for marginalized model that the output of the chain is as expected\n",
        "    for i in MCMCChains.chains(chains)\n",
        "        # μ[1] and μ[2] can no longer switch places. Check that they've found the mean\n",
        "        chain = Array(chains[:, [\"μ[1]\", \"μ[2]\"], i])\n",
        "        μ_mean = vec(mean(chain; dims=1))\n",
        "        @assert isapprox(sort(μ_mean), μ; rtol=0.4) \"Difference between estimated mean of μ ($(sort(μ_mean))) and data-generating μ ($μ) unexpectedly large!\"\n",
        "    end\n",
        "end\n",
        "```\n",
        "\n",
        "`NUTS()` significantly outperforms our compositional Gibbs sampler, in large part because our model is now Rao-Blackwellized thanks to the marginalization of our assignment parameter.\n",
        "\n",
        "```{julia}\n",
        "plot(chains[[\"μ[1]\", \"μ[2]\"]], legend=true)\n",
        "```\n",
        "\n",
        "## Inferred Assignments With The Marginalized Model\n",
        "\n",
        "As we have summed over possible assignments, the latent parameter representing the assignments is no longer available in our chain.\n",
        "This is not a problem, however, as given any fixed sample $(\\mu, w)$, the assignment probability $p(z_i \\mid y_i)$ can be recovered using Bayes's theorme:\n",
        "\n",
        "$$p(z_i \\mid y_i) = \\frac{p(y_i \\mid z_i) p(z_i)}{\\sum_{k = 1}^K \\left(p(y_i \\mid z_i) p(z_i) \\right)}$$\n",
        "\n",
        "This quantity can be computed for every $p(z = z_i \\mid y_i)$, resulting in a probability vector, which is then used to sample posterior predictive assignments from a categorial distribution.\n",
        "For details on the mathematics here, see [the Stan documentation on latent discrete parameters](https://mc-stan.org/docs/stan-users-guide/latent-discrete.html).\n",
        "\n",
        "```{julia}\n",
        "#| output: false\n",
        "function sample_class(xi, dists, w)\n",
        "    lvec = [(logpdf(d, xi) + log(w[i])) for (i, d) in enumerate(dists)]\n",
        "    rand(Categorical(softmax(lvec)))\n",
        "end\n",
        "\n",
        "@model function gmm_recover(x)\n",
        "    K = 2\n",
        "    D, N =  size(x)\n",
        "    μ ~ ordered(MvNormal(Zeros(K), I))\n",
        "    w ~ Dirichlet(K, 1.0)\n",
        "    dists = [MvNormal(Fill(μₖ, D), I) for μₖ in μ]\n",
        "    x ~ MixtureModel(dists, w)\n",
        "    # Return assignment draws for each datapoint.\n",
        "    return [sample_class(x[:, i], dists, w) for i in 1:N]\n",
        "end\n",
        "```\n",
        "\n",
        "We sample from this model as before:\n",
        "\n",
        "```{julia}\n",
        "#| output: false\n",
        "model = gmm_recover(x)\n",
        "chains = sample(model, sampler, MCMCThreads(), nsamples, nchains, discard_initial = burn);\n",
        "```\n",
        "\n",
        "Given a sample from the marginalized posterior, these assignments can be recovered with:\n",
        "\n",
        "```{julia}\n",
        "assignments = mean(returned(gmm_recover(x), chains));\n",
        "```\n",
        "\n",
        "```{julia}\n",
        "scatter(\n",
        "    x[1, :],\n",
        "    x[2, :];\n",
        "    legend=false,\n",
        "    title=\"Assignments on Synthetic Dataset - Recovered\",\n",
        "    zcolor=assignments,\n",
        ")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
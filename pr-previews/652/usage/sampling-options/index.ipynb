{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using Pkg; Pkg.activate(; temp=true)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Markov chain Monte Carlo sampling in Turing.jl is performed using the `sample()` function.\nAs described on the [Core Functionality page]({{< meta core-functionality >}}), single-chain and multiple-chain sampling can be done using, respectively,\n\n```julia\nsample(model, sampler, niters)\nsample(model, sampler, MCMCThreads(), niters, nchains)  # or MCMCSerial() or MCMCDistributed()\n```\n\nOn top of this, both methods also accept a number of keyword arguments that allow you to control the sampling process.\nThis page will detail these options.\n\nTo begin, let's create a simple model:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using Turing\n\n@model function demo_model()\n    x ~ Normal()\n    y ~ Normal(x)\n    4.0 ~ Normal(y)\n    return nothing\nend"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Controlling logging\n\nProgress bars can be controlled with the `progress` keyword argument.\nThe exact values that can be used depend on whether you are using single-chain or multi-chain sampling.\n\nFor single-chain sampling, `progress=true` and `progress=false` enable and disable the progress bar, respectively.\n\nFor multi-chain sampling, `progress` can take the following values:\n\n- `:none` or `false`: no progress bar\n- (default) `:overall` or `true`: creates one overall progress bar for all chains\n- `:perchain`: creates one overall progress bar, plus one extra progress bar per chain (note that this can lead to visual clutter if you have many chains)\n\nIf you want to globally enable or disable the progress bar, you can use:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "Turing.setprogress!(false);   # or true"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "(This handily also disables progress logging for the rest of this document.)\n\nFor NUTS in particular, you can also specify `verbose=false` to disable the \"Found initial step size\" info message.\n\n## Ensuring sampling reproducibility\n\nLike many other Julia functions, a `Random.AbstractRNG` object can be passed as the first argument to `sample()` to ensure reproducibility of results."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using Random\nchn1 = sample(Xoshiro(468), demo_model(), MH(), 5);\nchn2 = sample(Xoshiro(468), demo_model(), MH(), 5);\n(chn1[:x] == chn2[:x], chn1[:y] == chn2[:y])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Alternatively, you can set the global RNG using `Random.seed!()`, although we recommend this less as it modifies global state."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "Random.seed!(468)\nchn3 = sample(demo_model(), MH(), 5);\nRandom.seed!(468)\nchn4 = sample(demo_model(), MH(), 5);\n(chn3[:x] == chn4[:x], chn3[:y] == chn4[:y])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "::: {.callout-note}\nThe outputs of pseudorandom number generators in the standard `Random` library are not guaranteed to be the same across different Julia versions or platforms.\nIf you require absolute reproducibility, you should use [the StableRNGs.jl package](https://github.com/JuliaRandom/StableRNGs.jl).\n:::\n\n## Switching the output type\n\nBy default, the results of MCMC sampling are bundled up in an `MCMCChains.Chains` object."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "chn = sample(demo_model(), HMC(0.1, 20), 5)\ntypeof(chn)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "If you wish to use a different chain format provided in another package, you can specify the `chain_type` keyword argument.\nYou should refer to the documentation of the respective package for exact details.\n\nAnother situation where specifying `chain_type` can be useful is when you want to obtain the raw MCMC outputs as a vector of transitions.\nThis can be used for profiling or debugging purposes (often, chain construction can take a surprising amount of time compared to sampling, especially for very simple models).\nTo do so, you can use `chain_type=Any` (i.e., do not convert the output to any specific chain format):"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "transitions = sample(demo_model(), MH(), 5; chain_type=Any)\ntypeof(transitions)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Specifying initial parameters\n\nIn Turing.jl, initial parameters for MCMC sampling can be specified using the `initial_params` keyword argument.\n\nFor **single-chain sampling**, the AbstractMCMC interface generally expects that you provide a completely flattened vector of parameters."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "chn = sample(demo_model(), MH(), 5; initial_params=[1.0, -5.0])\nchn[:x][1], chn[:y][1]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "::: {.callout-note}\nNote that a number of samplers use warm-up steps by default (see the [Thinning and Warmup section below](#thinning-and-warmup)), so `chn[:param][1]` may not correspond to the exact initial parameters you provided.\n`MH()` does not do this, which is why we use it here.\n:::\n\nNote that for Turing models, the use of `Vector` can be extremely error-prone as the order of parameters in the flattened vector is not always obvious (especially if there are parameters with non-trivial types).\nIn general, parameters should be provided in the order they are defined in the model.\nA relatively 'safe' way of obtaining parameters in the correct order is to first generate a `VarInfo`, and then linearise that:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "using DynamicPPL\nvi = VarInfo(demo_model())\ninitial_params = vi[:]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "To avoid this situation, you can also use `NamedTuple` to specify initial parameters."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "chn = sample(demo_model(), MH(), 5; initial_params=(y=2.0, x=-6.0))\nchn[:x][1], chn[:y][1]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "This works even for parameters with more complex types."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@model function demo_complex()\n    x ~ LKJCholesky(3, 0.5)\n    y ~ MvNormal(zeros(3), I)\nend\ninit_x, init_y = rand(LKJCholesky(3, 0.5)), rand(MvNormal(zeros(3), I))\nchn = sample(demo_complex(), MH(), 5; initial_params=(x=init_x, y=init_y));"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "For **multiple-chain sampling**, the `initial_params` keyword argument should be a vector with length equal to the number of chains being sampled.\nEach element of this vector should be the initial parameters for the corresponding chain, as described above.\nThus, for example, a vector of vectors, or a vector of `NamedTuple`s, can be used.\nIf you want to use the same initial parameters for all chains, you can use `fill`:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "initial_params = fill((x=1.0, y=-5.0), 3)\nchn = sample(demo_model(), MH(), MCMCThreads(), 5, 3; initial_params=initial_params)\nchn[:x][1,:], chn[:y][1,:]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "::: {.callout-important}\n## Upcoming changes in Turing v0.41\n\nIn Turing v0.41, instead of providing _initial parameters_, users will have to provide what is conceptually an _initialisation strategy_.\nThe keyword argument is still `initial_params`, but the permitted values (for single-chain sampling) will either be:\n\n- `InitFromPrior()`: generate initial parameters by sampling from the prior\n- `InitFromUniform(lower, upper)`: generate initial parameters by sampling uniformly from the given bounds in linked space\n- `InitFromParams(namedtuple_or_dict)`: use the provided initial parameters, supplied either as a `NamedTuple` or a `Dict{<:VarName}`\n\nInitialisation with `Vector` will be fully removed due to its inherent ambiguity.\nInitialisation with a raw `NamedTuple` will still be supported (it will simply be wrapped in `InitFromParams()`); but we expect to remove this eventually, so it will be more future-proof to use `InitFromParams()` directly.\n\nFor multiple chains, the same as above applies: the `initial_params` keyword argument should be a vector of initialisation strategies, one per chain.\n:::\n\n## Saving and resuming sampling\n\nBy default, MCMC sampling starts from scratch, using the initial parameters provided.\nYou can, however, resume sampling from a previous chain.\nThis is useful to, for example, perform sampling in batches, or to inspect intermediate results.\n\nFirstly, the previous chain _must_ have been run using the `save_state=true` argument."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "rng = Xoshiro(468)\n\nchn1 = sample(rng, demo_model(), MH(), 5; save_state=true);"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "For `MCMCChains.Chains`, this results in the final sampler state being stored inside the chain metadata.\nYou can access it using `DynamicPPL.loadstate`:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "saved_state = DynamicPPL.loadstate(chn1)\ntypeof(saved_state)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "::: {.callout-note}\nYou can also directly access the saved sampler state with `chn1.info.samplerstate`, but we recommend _not_ using this as it relies on the internal structure of `MCMCChains.Chains`.\n:::\n\nSampling can then be resumed from this state by providing it as the `initial_state` keyword argument."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "chn2 = sample(demo_model(), MH(), 5; initial_state=saved_state)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Note that the exact format saved in `chn.info.samplerstate`, and that expected by `initial_state`, depends on the invocation of `sample` used.\nFor single-chain sampling, the saved state, and the required initial state, is just a single sampler state.\nFor multiple-chain sampling, it is a vector of states, one per chain.\n\nThis means that, for example, after sampling a single chain, you could sample three chains that branch off from that final state:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "initial_states = fill(saved_state, 3)\nchn3 = sample(demo_model(), MH(), MCMCThreads(), 5, 3; initial_state=initial_states)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "::: {.callout-note}\n## Initial states versus initial parameters\n\nThe `initial_state` and `initial_params` keyword arguments are mutually exclusive.\nIf both are provided, `initial_params` will be silently ignored."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "chn2 = sample(rng, demo_model(), MH(), 5;\n    initial_state=saved_state, initial_params=(x=0.0, y=0.0)\n)\nchn2[:x][1], chn2[:y][1]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In general, the saved state will contain a set of parameters (which will be the last parameters in the previous chain).\nHowever, the saved state not only specifies parameters but also other internal variables required by the sampler.\nFor example, the MH state contains a cached log-density of the current parameters, which is later used for calculating the acceptance ratio.\n\nFinally, note that the first sample in the resumed chain will not be the same as the last sample in the previous chain; it will be the sample immediately after that."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# In general these will not be the same (although it _could_ be if the MH step\n# was rejected -- that is why we seed the sampling in this section).\nchn1[:x][end], chn2[:x][1]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ":::\n\n## Thinning and warmup\n\nThe `num_warmup` and `discard_initial` keyword arguments can be used to control MCMC warmup.\nBoth of these are integers, and respectively specify the number of warmup steps to perform, and the number of iterations at the start of the chain to discard.\nNote that the value of `discard_initial` should also include the `num_warmup` steps if you want the warmup steps to be discarded.\n\nHere are some examples of how these two keyword arguments interact:\n\n| `num_warmup=`  | `discard_initial=`   | Description                                                                                                             |\n| -------------- | -------------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| 10             | 10                  | Perform 10 warmup steps, discard them; the chain starts from the first non-warmup step                                  |\n| 10             | 15                  | Perform 10 warmup steps, discard them and the next 5 steps; the chain starts from the 6th non-warmup step               |\n| 10             | 5                   | Perform 10 warmup steps, discard the first 5; the chain will contain 5 warmup steps followed by the rest of the chain   |\n| 0              | 10                  | No warmup steps, discard the first 10 steps; the chain starts from the 11th step                                        |\n| 0              | 0                   | No warmup steps, do not discard any steps; the chain starts from the 1st step (corresponding to the initial parameters) |\n\nEach sampler has its own default value for `num_warmup`, but `discard_initial` always defaults to `num_warmup`.\n\nWarmup steps and 'regular' non-warmup steps differ in that warmup steps call `AbstractMCMC.step_warmup`, whereas regular steps call `AbstractMCMC.step`.\nFor all the samplers defined in Turing, these two functions are identical; however, they may in general differ for other samplers.\nPlease consult the documentation of the respective sampler for details.\n\nA thinning factor can be specified using the `thinning` keyword argument.\nFor example, `thinning=10` will keep every tenth sample, discarding the other nine.\n\nNote that thinning is not applied to the first `discard_initial` samples; it is only applied to the remaining samples.\nThus, for example, if you use `discard_initial=50` and `thinning=10`, the chain will contain samples 51, 61, 71, and so on.\n\n## Performing model checks\n\nDynamicPPL by default performs a number of checks on the model before any sampling is done.\nThis catches a number of potential errors in a model, such as having repeated variables (see [the DynamicPPL documentation](https://turinglang.org/DynamicPPL.jl/stable/api/#DynamicPPL.DebugUtils.check_model_and_trace) for details).\n\nIf you wish to disable this you can pass `check_model=false` to `sample()`.\n\n\n## Callbacks\n\nThe `callback` keyword argument can be used to specify a function that is called at the end of each sampler iteration.\nThis function should have the signature `callback(rng, model, sampler, sample, iteration::Int; kwargs...)`.\n\nIf you are performing multi-chain sampling, `kwargs` will additionally contain `chain_number::Int`, which ranges from 1 to the number of chains.\n\nThe [TuringCallbacks.jl package](https://github.com/TuringLang/TuringCallbacks.jl) contains a `TensorBoardCallback`, which can be used to obtain live progress visualisations using [TensorBoard](https://www.tensorflow.org/tensorboard).\n\n## Automatic differentiation\n\nFinally, please note that for samplers which use automatic differentiation (e.g., HMC and NUTS), the AD type should be specified in the sampler constructor itself, rather than as a keyword argument to `sample()`.\n\nIn other words, this is correct:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "spl = NUTS(; adtype=AutoForwardDiff())\nchn = sample(demo_model(), spl, 10);"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "and not this:\n\n```julia\nspl = NUTS()\nchn = sample(demo_model(), spl, 10; adtype=AutoForwardDiff())\n```"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
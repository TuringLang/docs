{
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": "# Install necessary dependencies.\nusing Pkg\nPkg.activate(; temp=true)\nPkg.add([])",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| echo: false\n#| output: false\nusing Pkg;\nPkg.instantiate();\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Using External Samplers on Turing Models\n\n`Turing` provides several wrapped samplers from external sampling libraries, e.g., HMC samplers from `AdvancedHMC`.\nThese wrappers allow new users to seamlessly sample statistical models without leaving `Turing`\nHowever, these wrappers might not always be complete, missing some functionality from the wrapped sampling library.\nMoreover, users might want to use samplers currently not wrapped within `Turing`.\n\nFor these reasons, `Turing` also makes running external samplers on Turing models easy without any necessary modifications or wrapping!\nThroughout, we will use a 10-dimensional Neal's funnel as a running example::",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# Import libraries.\nusing Turing, Random, LinearAlgebra\n\nd = 10\n@model function funnel()\n    θ ~ Truncated(Normal(0, 3), -3, 3)\n    z ~ MvNormal(zeros(d - 1), exp(θ) * I)\n    return x ~ MvNormal(z, I)\nend\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now we sample the model to generate some observations, which we can then condition on.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "(; x) = rand(funnel() | (θ=0,))\nmodel = funnel() | (; x);",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Users can use any sampler algorithm to sample this model if it follows the `AbstractMCMC` API.\nBefore discussing how this is done in practice, giving a high-level description of the process is interesting.\nImagine that we created an instance of an external sampler that we will call `spl` such that `typeof(spl)<:AbstractMCMC.AbstractSampler`.\nIn order to avoid type ambiguity within Turing, at the moment, it is necessary to declare `spl` as an external sampler to Turing `espl = externalsampler(spl)`, where `externalsampler(s::AbstractMCMC.AbstractSampler)` is a Turing function that types our external sampler adequately.\n\nAn excellent point to start to show how this is done in practice is by looking at the sampling library `AdvancedMH` ([`AdvancedMH`'s GitHub](https://github.com/TuringLang/AdvancedMH.jl)) for Metropolis-Hastings (MH) methods.\nLet's say we want to use a random walk Metropolis-Hastings sampler without specifying the proposal distributions.\nThe code below constructs an MH sampler using a multivariate Gaussian distribution with zero mean and unit variance in `d` dimensions as a random walk proposal.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# Importing the sampling library\nusing AdvancedMH\nrwmh = AdvancedMH.RWMH(d)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| output: false\nsetprogress!(false)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Sampling is then as easy as:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "chain = sample(model, externalsampler(rwmh), 10_000)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Going beyond the Turing API\n\nAs previously mentioned, the Turing wrappers can often limit the capabilities of the sampling libraries they wrap.\n`AdvancedHMC`[^1] ([`AdvancedHMC`'s GitHub](https://github.com/TuringLang/AdvancedHMC.jl)) is a clear example of this. A common practice when performing HMC is to provide an initial guess for the mass matrix.\nHowever, the native HMC sampler within Turing only allows the user to specify the type of the mass matrix despite the two options being possible within `AdvancedHMC`.\nThankfully, we can use Turing's support for external samplers to define an HMC sampler with a custom mass matrix in `AdvancedHMC` and then use it to sample our Turing model.\n\nWe can use the library `Pathfinder`[^2] ([`Pathfinder`'s GitHub](https://github.com/mlcolab/Pathfinder.jl)) to construct our estimate of mass matrix.\n`Pathfinder` is a variational inference algorithm that first finds the maximum a posteriori (MAP) estimate of a target posterior distribution and then uses the trace of the optimisation to construct a sequence of multivariate normal approximations to the target distribution.\nIn this process, `Pathfinder` computes an estimate of the mass matrix the user can access.\nYou can see an example of how to use `Pathfinder` with Turing in [`Pathfinder`'s docs](https://mlcolab.github.io/Pathfinder.jl/stable/examples/turing/).\n\n## Using new inference methods\n\nSo far we have used Turing's support for external samplers to go beyond the capabilities of the wrappers.\nThis is made possible by an interface for external samplers, which is [described in the Turing.jl documentation here](https://turinglang.org/Turing.jl/stable/api/Inference/#Turing.Inference.ExternalSampler): if you are implementing your own sampler and would like it to work with Turing.jl models, that link describes the methods that you need to overload.\n\nFor an example of an 'external sampler' that works in this way with Turing, we recommend the [SliceSampling.jl library](https://github.com/TuringLang/SliceSampling.jl).\nNote that although this library is hosted under the TuringLang GitHub organisation, it is not a Turing.jl dependency, and thus from Turing's perspective it is truly an 'external' sampler.\n\nIn this section, we will briefly go through the interface requirements for external samplers.\nFirst and foremost, the sampler `MySampler` should be a subtype of `AbstractMCMC.AbstractSampler`.\nSecond, the stepping function of the MCMC algorithm must be defined as new methods of `AbstractMCMC.step` following the structure below:\n\n```julia\n# First step\nfunction AbstractMCMC.step(\n    rng::Random.AbstractRNG,\n    model::AbstractMCMC.LogDensityModel,\n    spl::MySampler;\n    kwargs...,\n)\n    [...]\n    return transition, state\nend\n\n# N+1 step\nfunction AbstractMCMC.step(\n    rng::Random.AbstractRNG,\n    model::AbstractMCMC.LogDensityModel,\n    sampler::MySampler,\n    state;\n    kwargs...,\n) \n    [...]\n    return transition, state\nend\n```\n\nNote that the `model` argument here must be [an `AbstractMCMC.LogDensityModel`](https://turinglang.org/AbstractMCMC.jl/stable/api/#AbstractMCMC.LogDensityModel).\nThis is a thin wrapper around an object which satisfies the `LogDensityProblems.jl` interface.\nThus, in your external sampler, you can access the inner object with `model.logdensity` and call `LogDensityProblems.logdensity(model.logdensity, params)` to calculate the (unnormalised) log density of the model at `params`.\n\nAs shown above, there must be two `step` methods:\n    \n - A method that performs the first step, performing any initialisation it needs to; and\n - A method that performs the following steps and takes an extra input, `state`, which carries the initialisation information.\n\nThe output of both of these methods must be a tuple containing:\n - a 'transition', which is essentially the 'visible output' of the sampler: this object is later used to construct an `MCMCChains.Chains`;\n - a 'state', representing the current state of the sampler, which is passed to the next step of the MCMC algorithm.\n\nApart from this, your sampler state should also implement `Turing.Inference.getparams(model, transition)` to return the parameters of the model as a vector.\nHere, `transition` represents the first output of the `step` function.\n\n```julia\nfunction Turing.Inference.getparams(model::DynamicPPL.Model, state::MyTransition)\n    # Return a vector containing the parameters of the model.\nend\n```\n\nThese functions are the bare minimum that your external sampler must implement to work with Turing models.\nThere are other methods which can be overloaded to improve the performance or other features of the sampler; please refer to the documentation linked above for more details.\n\nIn general, we recommend that the `AbstractMCMC` interface is implemented directly in your library.\nHowever, any DynamicPPL- or Turing-specific functionality is best implemented in a `MySamplerTuringExt` extension.\n\n[^1]: Xu et al., [AdvancedHMC.jl: A robust, modular and efficient implementation of advanced HMC algorithms](http://proceedings.mlr.press/v118/xu20a/xu20a.pdf), 2019\n[^2]: Zhang et al., [Pathfinder: Parallel quasi-Newton variational inference](https://arxiv.org/abs/2108.03782), 2021",
      "metadata": {}
    }
  ],
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia",
      "language": "julia"
    }
  },
  "nbformat": 4
}
{
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": "# Install necessary dependencies.\nusing Pkg\nPkg.activate(; temp=true)\nPkg.add([\"Turing\", \"DynamicPPL\", \"Chairmarks\"])",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| echo: false\n#| output: false\nusing Pkg;\nPkg.instantiate();\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "A common technique to speed up Julia code is to use multiple threads to run computations in parallel.\nThe Julia manual [has a section on multithreading](https://docs.julialang.org/en/v1/manual/multi-threading), which is a good introduction to the topic.\n\nWe assume that the reader is familiar with some threading constructs in Julia, and the general concept of data races.\nThis page specificaly discusses Turing's support for threadsafe model evaluation.\n\n> Please note that this is a rapidly-moving topic, and things may change in future releases of Turing.\n> If you are ever unsure about what works and doesn't, please don't hesitate to ask on [Slack](https://julialang.slack.com/archives/CCYDC34A0) or [Discourse](https://discourse.julialang.org/c/domain/probprog/48)",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "println(\"This notebook is being run with $(Threads.nthreads()) threads.\")",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Threading in Turing models\n\nGiven that Turing models mostly contain 'plain' Julia code, one might expect that all threading constructs such as `Threads.@threads` or `Threads.@spawn` can be used inside Turing models.\n\nThis is, to some extent, true: for example, you can use threading constructs to speed up deterministic computations.\nFor example, here we use parallelism to speed up a transformation of `x`:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "using Turing\n\n@model function parallel(y)\n    x ~ dist\n    x_transformed = similar(x)\n    Threads.@threads for i in eachindex(x)\n        x_transformed[i] = some_expensive_function(x[i])\n    end\n    y ~ some_likelihood(x_transformed)\nend",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "In general, for code that does not involve tilde-statements (`x ~ dist`), threading works exactly as it does in regular Julia code.\n\n**However, extra care must be taken when using tilde-statements (`x ~ dist`), or `@addlogprob!`, inside threaded blocks.**\n\n> ## Why are tilde-statements special?\n> Tilde-statements are expanded by the `@model` macro into something that modifies the internal VarInfo object used for model evaluation.\n> Essentially, `x ~ dist` expands to something like\n> \n> ```julia\n> x, __varinfo__ = DynamicPPL.tilde_assume!!(..., __varinfo__)\n> ```\n> \n> and writing into `__varinfo__` is, _in general_, not threadsafe.\n> Thus, parallelising tilde-statements can lead to data races [as described in the Julia manual](https://docs.julialang.org/en/v1/manual/multi-threading/#Using-@threads-without-data-races).\n\n## Threaded observations\n\n**As of version 0.42, Turing only supports the use of tilde-statements inside threaded blocks when these are observations (i.e., likelihood terms).**\n\nHowever, such models **must** be marked by the user as requiring threadsafe evaluation, using `setthreadsafe`.\n\nThis means that the following code is safe to use:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "@model function threaded_obs(N)\n    x ~ Normal()\n    y = Vector{Float64}(undef, N)\n    Threads.@threads for i in 1:N\n        y[i] ~ Normal(x)\n    end\nend\n\nN = 100\ny = randn(N)\nthreadunsafe_model = threaded_obs(N) | (; y = y)\nthreadsafe_model = setthreadsafe(threadunsafe_model, true)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Evaluating this model is threadsafe, in that Turing guarantees to provide the correct result in functions such as:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "logjoint(threadsafe_model, (; x = 0.0))",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "(we can compare with the true value)",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "logpdf(Normal(), 0.0) + sum(logpdf.(Normal(0.0), y))",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Note that if you do not use `setthreadsafe`, the above code may give wrong results, or even error:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "logjoint(threadunsafe_model, (; x = 0.0))",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "You can sample from this model and safely use functions such as `predict` or `returned`, as long as the model is always marked as threadsafe:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "model = setthreadsafe(threaded_obs(N) | (; y = y), true)\nchn = sample(model, NUTS(), 100; check_model=false, progress=false)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "```julia\npmodel = setthreadsafe(threaded_obs(N), true)  # don't condition on data\npredict(pmodel, chn)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "> ## Previous versions\n> \n> Up until Turing v0.41, you did not need to use `setthreadsafe` to enable threadsafe evaluation, and it was automatically enabled whenever Julia was launched with more than one thread.\n> \n> There were several reasons for changing this: one major one is because threadsafe evaluation comes with a performance cost, which can sometimes be substantial (see below).\n> \n> Furthermore, the number of threads is not an appropriate way to determine whether threadsafe evaluation is needed!\n\n## Threaded assumptions / sampling latent values\n\n**On the other hand, parallelising the sampling of latent values is not supported.**\nAttempting to do this will either error or give wrong results.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| error: true\n@model function threaded_assume_bad(N)\n    x = Vector{Float64}(undef, N)\n    Threads.@threads for i in 1:N\n        x[i] ~ Normal()\n    end\n    return x\nend\n\nmodel = threaded_assume_bad(100)\n\n# This will throw an error (and probably a different error\n# each time it's run...)\nmodel()\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## When is threadsafe evaluation really needed?\n\nYou only need to enable threadsafe evaluation if you are using tilde-statements or `@addlogprob!` inside threaded blocks.\n\nSpecifically, you do *not* need to enable threadsafe evaluation if:\n\n- You have parallelism inside the model, but it does not involve tilde-statements or `@addlogprob!`.\n\n  ```julia\n  @model function parallel_no_tilde(y)\n      x ~ Normal()\n      fy = similar(y)\n      Threads.@threads for i in eachindex(y)\n          fy[i] = some_expensive_function(x, y[i])\n      end\n  end\n  # This does not need setthreadsafe\n  model = parallel_no_tilde(y)\n  ```\n\n- You are sampling from a model using `MCMCThreads()`, but the model itself does not contain any parallel tilde-statements or `@addlogprob!`.\n\n  ```julia\n  @model function no_parallel(y)\n      x ~ Normal()\n      y ~ Normal(x)\n  end\n\n  # This does not need setthreadsafe\n  model = no_parallel(1.0)\n  chn = sample(model, NUTS(), MCMCThreads(), 100)\n  ```\n\n## Performance considerations\n\nAs described above, one of the major considerations behind the introduction of `setthreadsafe` is that threadsafe evaluation comes with a performance cost.\n\nConsider a simple model that does not use threading:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "@model function gdemo()\n    s ~ InverseGamma(2, 3)\n    m ~ Normal(0, sqrt(s))\n    1.5 ~ Normal(m, sqrt(s))\n    2.0 ~ Normal(m, sqrt(s))\nend\nmodel_no_threadsafe = gdemo()\nmodel_threadsafe = setthreadsafe(gdemo(), true)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "One can see that evaluation of the threadsafe model is substantially slower:",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "using Chairmarks, DynamicPPL\n\nfunction benchmark_eval(m)\n    vi = VarInfo(m)\n    display(median(@be DynamicPPL.evaluate!!($m, $vi)))\nend\n\nbenchmark_eval(model_no_threadsafe)\nbenchmark_eval(model_threadsafe)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "In previous versions of Turing, this cost would **always** be incurred whenever Julia was launched with multiple threads, even if the model did not use any threading at all!\n\n## Alternatives to threaded observation\n\nAn alternative to using threaded observations is to manually calculate the log-likelihood term (which can be parallelised using any of Julia's standard mechanisms), and then _outside_ of the threaded block, [add it to the model using `@addlogprob!`]({{< meta usage-modifying-logprob >}}).\n\nFor example:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# Note that `y` has to be passed as an argument; you can't\n# condition on it because otherwise `y[i]` won't be defined.\n@model function threaded_obs_addlogprob(N, y)\n    x ~ Normal()\n\n    # Instead of this:\n    # Threads.@threads for i in 1:N\n    #     y[i] ~ Normal(x)\n    # end\n\n    # Do this instead:\n    lls = map(1:N) do i\n        Threads.@spawn begin\n            logpdf(Normal(x), y[i])\n        end\n    end\n    @addlogprob! sum(fetch.(lls))\nend\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In a similar way, you can also use your favourite parallelism package, such as `FLoops.jl` or `OhMyThreads.jl`.\nSee [this Discourse post](https://discourse.julialang.org/t/parallelism-within-turing-jl-model/54064/9) for some examples.\n\nWe make no promises about the use of tilde-statements _with_ these packages (indeed it will most likely error), but as long as you use them to only parallelise regular Julia code (i.e., not tilde-statements), they will work as intended.\n\nThe main downside of this approach is:\n\n1. You can't use conditioning syntax to provide data; it has to be passed as an argument or otherwise included inside the model.\n2. You can't use `predict` to sample new data.\n\nOn the other hand, one benefit of rewriting the model this way is that sampling from this model with `MCMCThreads()` will always be reproducible.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\nusing Random\nN = 100\ny = randn(N)\n# Note that since `@addlogprob!` is outside of the threaded block, we don't\n# need to use `setthreadsafe`.\nmodel = threaded_obs_addlogprob(N, y)\nnuts_kwargs = (progress=false, verbose=false)\n\nchain1 = sample(Xoshiro(468), model, NUTS(), MCMCThreads(), 1000, 4; nuts_kwargs...)\nchain2 = sample(Xoshiro(468), model, NUTS(), MCMCThreads(), 1000, 4; nuts_kwargs...)\nmean(chain1[:x]), mean(chain2[:x])  # should be identical\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In contrast, the original `threaded_obs` (which used tilde inside `Threads.@threads`) is not reproducible when using `MCMCThreads()`.\n(In principle, we would like to fix this bug, but we haven't yet investigated where it stems from.)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\nmodel = setthreadsafe(threaded_obs(N) | (; y = y), true)\nnuts_kwargs = (progress=false, verbose=false)\nchain1 = sample(Xoshiro(468), model, NUTS(), MCMCThreads(), 1000, 4; nuts_kwargs...)\nchain2 = sample(Xoshiro(468), model, NUTS(), MCMCThreads(), 1000, 4; nuts_kwargs...)\nmean(chain1[:x]), mean(chain2[:x])  # oops!\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## AD support\n\nFinally, if you are [using Turing with automatic differentiation]({{< meta usage-automatic-differentiation >}}), you also need to keep track of which AD backends support threadsafe evaluation.\n\nForwardDiff is the only AD backend that we find to work reliably with threaded model evaluation.\n\nIn particular:\n\n - ReverseDiff sometimes gives right results, but quite often gives incorrect gradients.\n - Mooncake [currently does not support multithreading at all](https://github.com/chalk-lab/Mooncake.jl/issues/570).\n - Enzyme [mostly gives the right result, but sometimes gives incorrect gradients](https://github.com/TuringLang/DynamicPPL.jl/issues/1131).\n\n## Under the hood\n\n> This part will likely only be of interest to DynamicPPL developers and the very curious user.\n\n### Why is VarInfo not threadsafe?\n\nAs alluded to above, the issue with threaded tilde-statements stems from the fact that these tilde-statements modify the VarInfo object used for model evaluation, leading to potential data races.\n\nTraditionally, VarInfo objects contain both *metadata* as well as *accumulators*.\nMetadata is where information about the random variables' values are stored.\nIt is a Dict-like structure, and pushing to it from multiple threads is therefore not threadsafe (Julia's `Dict` has similar limitations).\n\nOn the other hand, accumulators are used to store outputs of the model, such as log-probabilities\nThe way DynamicPPL's threadsafe evaluation works is to create one set of accumulators per thread, and then combine the results at the end of model evaluation.\n\nIn this way, any function call that _solely_ involving accumulators can be made threadsafe.\nFor example, this is why observations are supported: there is no need to modify metadata, and only the log-likelihood accumulator needs to be updated.\n\nHowever, `assume` tilde-statements always modify the metadata, and thus cannot currently be made threadsafe.\n\n### OnlyAccsVarInfo\n\nAs it happens, much of what is needed in DynamicPPL can be constructed such that they *only* rely on accumulators.\n\nFor example, as long as there is no need to *sample* new values of random variables, it is actually fine to completely omit the metadata object.\nThis is the case for `LogDensityFunction`: since values are provided as the input vector, there is no need to store it in metadata.\nWe need only calculate the associated log-prior probability, which is stored in an accumulator.\nThus, since DynamicPPL v0.39, `LogDensityFunction` itself is completely threadsafe.\n\nTechnically speaking, this is achieved using `OnlyAccsVarInfo`, which is a subtype of `VarInfo` that only contains accumulators, and no metadata at all.\nIt implements enough of the `VarInfo` interface to be used in model evaluation, but will error if any functions attempt to modify or read its metadata.\n\nThere is currently an ongoing push to use `OnlyAccsVarInfo` in as many settings as we possibly can.\nFor example, this is why `predict` is threadsafe in DynamicPPL v0.39: instead of modifying metadata to store the predicted values, we store them inside a `ValuesAsInModelAccumulator` instead, and combine them at the end of evaluation.\n\nHowever, propagating these changes up to Turing will require a substantial amount of additional work, since there are many places in Turing which currently rely on a full VarInfo (with metadata).\nSee, e.g., [this PR](https://github.com/TuringLang/DynamicPPL.jl/pull/1154) for more information.",
      "metadata": {}
    }
  ],
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia",
      "language": "julia"
    }
  },
  "nbformat": 4
}
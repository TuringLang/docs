---
title: Variational Inference
engine: julia
aliases:
 - ../09-variational-inference/index.html
---

```{julia}
#| echo: false
#| output: false
using Pkg;
Pkg.instantiate();
```

In this post will look at **variational inference (VI)**, an optimization approach to _approximate_ Bayesian inference, and how to use it in Turing.jl as an alternative to other approaches such as MCMC.
This post will be focusing on the usage of VI in Turing rather than the principles and theory underlying VI.
If you are interested in understanding the mathematics you can checkout [our write-up]({{<meta using-turing-variational-inference>}}) or any other resource online (there a lot of great ones).

Let's start with a minimal example. 
Consider a `Turing.Model` we denote as `model`.
Approximating the posterior associated with `model` via VI is as simple as

```{julia}
#| eval: false
m = model(data...)               # instantiate model on the data
q_init = q_fullrank_gaussian(m)  # initial variational approximation
q, _, _, _ = vi(m, q_init, 1000) # perform VI with the default algorithm on `m` for 1000 iterations
```
Thus it's no more work than standard MCMC sampling in Turing.
The default algorithm uses stochastic gradient descent to minimize the (exclusive) KL divergence.
This is commonly referred to as *automatic differentiation variational inference* or *black-box variational inference*.

To get a bit more into what we can do with VI, let's have a look at a more concrete example.
We will reproduce the [tutorial on Bayesian linear regression]({{<meta linear-regression>}}) using VI instead of MCMC.
After that we will discuss how to customize the behavior of `vi` for more advanced usage.

Let's first import the necessary packages:

```{julia}
using Random
using Turing
using Turing: Variational
using AdvancedVI
using Bijectors: bijector
using StatsPlots, Measures
using Plots

Random.seed!(42);
```

## Basic example: Normal-Gamma model

The Normal-(Inverse)Gamma model is defined by the following generative process

\begin{align}
s &\sim \mathrm{InverseGamma}(2, 3) \\
m &\sim \mathcal{N}(0, s) \\
x_i &\overset{\text{i.i.d.}}{=} \mathcal{N}(m, s), \quad i = 1, \dots, n
\end{align}

Note that this model is conjugate, meaning that we can obtain a closed-form expression for the posterior.
This will enable us to compare the result of VI against the true posterior.
(Of course, in practice, it doesn't make sense to use VI on a conjugate model. This is simply for illustration purposes.)

Let's first generate some synthetic data:
```{julia}
# generate data
x = randn(1000);
```

A corresponding `Turing.Model` can be defined as follows:
```{julia}
@model function model(x)
    s ~ InverseGamma(2, 3)
    m ~ Normal(0.0, sqrt(s))
    for i in 1:length(x)
        x[i] ~ Normal(m, sqrt(s))
    end
end;
```
and instantiated as follows:
```{julia}
m = model(x);
```

To run VI, we must first set a *variational family*.
For instance, the most commonly used family is the mean-field Gaussian family.
For this, Turing `provides` functions that automatically constructs the initialization corresponding to the model `m`:
```{julia}
q_init = q_meanfield_gaussian(m);
```
`vi` will automatically recognize the variational family through the type of `q_init`.

Let's run VI with the default setting:
```{julia}
n_iters = 300
q_avg, q_last, info, state = vi(m, q_init, n_iters; show_progress=false);
```

Here is the full documentation for `vi`:

```{julia}
@doc(Variational.vi)
```
The default setting uses the `AdvancedVI.RepGardELBO` objective, which corresponds to a variant of what is known as *automatic differentiation VI* or *black-box VI* with the reparameterization gradient.
The default optimizer we use is `AdvancedVI.DoWG` combined with a proximal operator.

## Values Returned by `vi`
The returned values of `vi` are the following:
- `q_avg`: The average of the parameters generated by the optimization algorithm.
- `q_last`: The last-iterate of the optimization algorithms. This can be used for resuming optimization from a previous run.
- `info`: Information generated during optimization.
- `state`: State used for optimization. This can be used for resuming optimization from a previous run.

The default setting uses what is known as polynomial averaginge for computing `q_avg`.
`q_avg` will usually perform better than `q_last`.
For instance, we can compare the ELBO of the two:
```{julia}
@info("Objective of q_avg and q_last",
    ELBO_q_avg = estimate_objective(AdvancedVI.RepGradELBO(32), q_avg, Turing.Variational.make_logdensity(m)),
    ELBO_q_last = estimate_objective(AdvancedVI.RepGradELBO(32), q_last, Turing.Variational.make_logdensity(m)) 
)
```
We can see that `ELBO_q_avg` is slightly more optimal.

Now, `info` contains information generated during optimization that could be useful for diagonstics.
For the default setting, which is `RepGradELBO`, it contains the ELBO estimated at each step, which can be plotted as follows:

```{julia}
Plots.plot([i.elbo for i in info], xlabel="Iterations", ylabel="ELBO", label="info")
```
Since the ELBO is estimated by a small number of samples, it appears noisy.
Furthermore, at each step, the ELBO is evaluated on `q_last` not `q_avg`, which is the actual output that we care about.
To obtain more accurate ELBO estimates evaluated on `q_avg`, we have to define a custom callback funtion.

## Custom Callback Functions
To inspect the progress of optimization in more detail, one can define a custom callback function.
For example, the following callback function estimates the ELBO on `q_avg` every 10 step with a larger number of samples:

```{julia}
function callback(; stat, averaged_params, restructure, kwargs...)
    if mod(stat.iteration, 10) == 1
        q_avg    = restructure(averaged_params)
        obj      = AdvancedVI.RepGradELBO(128)
        elbo_avg = estimate_objective(obj, q_avg, Turing.Variational.make_logdensity(m))
        (elbo_avg = elbo_avg,)
    else
        nothing
    end
end;
```
The `NamedTuple` returned by `callback` will be appended to the corresponding entry of `info`, and it will also be displayed on the progress meter if `show_progress` is set as `true`.

The custom callback can be supplied to `vi` as a keyword argument:
```{julia}
_, _, info_mf, _ = vi(m, q_init, n_iters; show_progress=false, callback=callback);
```

Let's plot the result:
```{julia}
iters   = 1:10:length(info_mf)
elbo_mf = [i.elbo_avg for i in info_mf[iters]]
Plots.plot!(iters, elbo_mf, xlabel="Iterations", ylabel="ELBO", label="callback", ylims=(-1450,Inf))
```
We can see that the ELBO values are less noisy and progress more smoothly due to averaging.

## Using Different Optimisers
The default optimiser we use is a proximal variant of [DoWG](https://arxiv.org/abs/2305.16284).
For Gaussian variational families, this works well as a default option.
Sometimes, the step size of DoWG could be too large resulting in unstable behavior.
Or, for whatever reason, it might be desirable to use a different optimiser.
Our implementation supports any optimiser that implements the [Optimisers.jl]() interface.

For instance, let's try using `Optimiers.Adam`, which is a popular choice.
Since `AdvancedVI` does not implement a proximal operator for `Optimisers.Adam`, we must use the `AdvancedVI.Identity()` operator, which results in regular (non-proximal) gradient descent.
```{julia}
using Optimisers

_, _, info_adam, _ = vi(m, q_init, n_iters; show_progress=false, callback=callback, optimizer=Optimisers.Adam(1e-2), operator=IdentityOperator());
```

```{julia}
iters     = 1:10:length(info_mf)
elbo_adam = [i.elbo_avg for i in info_adam[iters]]
Plots.plot(iters, elbo_mf, xlabel="Iterations", ylabel="ELBO", label="DoWG()")
Plots.plot!(iters, elbo_adam, xlabel="Iterations", ylabel="ELBO", label="Adam(1e-2)")
```
Compared to the default option `AdvancedVI.DoWG()`, we can see that `Optimisers.Adam(1e-2)` is converging more slowly.
With more step size tuning, it is possible that `Optimisers.Adam` could perform better or equial.
That is, most common optimisers require some degree of tuning to perform better or comparably to `AdvancedVI.DoWG()` or `AdvancedVI.DoG()`, which do not require much tuning at all.
Due to this fact, they are referred to as parameter-free optimizers.

## Using Full-Rank Variational Families
So far, we have only used the mean-field Gaussian family.
This, however, approximates the posterior covariance with a diagonal matrix.
To model the full covariance matrix, we can use the *full-rank* Gaussian family:
```{julia}
q_init_fr = q_fullrank_gaussian(m) 
```
The term *full-rank* might seem a bit peculiar since covariance matrices are always full-rank.
This term, however, traditionally comes from the fact that full-rank families use full-rank factors in addition to the diagonal of the covariance.

In contrast to the mean-field family, the full-rank family will often result in more computation per optimization step and slower convergence, especially in high-dimensions:
```{julia}
q_init_fr = q_fullrank_gaussian(m)
q_fr, _, info_fr, _ = vi(m, q_init_fr, 300; show_progress=false, callback)

Plots.plot(elbo_mf, xlabel="Iterations", ylabel="ELBO", label="Mean-Field", ylims=(-1450, Inf))

elbo_fr = [i.elbo_avg for i in info_fr[iters]]
Plots.plot!(elbo_fr, xlabel="Iterations", ylabel="ELBO", label="Full-Rank", ylims=(-1450, Inf))
```
This trade-off between statistical accuracy and optimization speed is often referred to as the *statistical-computational trade-off*.
The fact that we can control this trade-off through the choice of variational family is a strength, rather than a limitation, of variational inference.



Now we'll produce some samples from the posterior using a MCMC method, which in constrast to VI is guaranteed to converge to the *exact* posterior (as the number of samples go to infinity).

We'll produce 10 000 samples with 200 steps used for adaptation and a target acceptance rate of 0.65

If you don't understand what "adaptation" or "target acceptance rate" refers to, all you really need to know is that `NUTS` is known to be one of the most accurate and efficient samplers (when applicable) while requiring little to no hand-tuning to work well.

```{julia}
#| output: false
setprogress!(false)
```

```{julia}
samples_nuts = sample(m, NUTS(), 10_000);
```

{
    "sourceFile": "tutorials/07-bayesian-neural-network/07_bayesian-neural-network.jmd",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1629523550010,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1629523550010,
            "name": "Commit-0",
            "content": "---\ntitle: Bayesian Neural Networks\npermalink: /:collection/:name/\nredirect_from: tutorials/3-bayesnn/\nredirect_from: tutorials/bayes_nn/\n---\n\nIn this tutorial, we demonstrate how one can implement a Bayesian Neural Network using a combination of Turing and [Flux](https://github.com/FluxML/Flux.jl), a suite of machine learning tools. We will use Flux to specify the neural network's layers and Turing to implement the probabilistic inference, with the goal of implementing a classification algorithm.\n\nWe will begin with importing the relevant libraries.\n\n\n```julia\n# Import libraries.\nusing Turing, Flux, Plots, Random, ReverseDiff\n\n# Hide sampling progress.\nTuring.setprogress!(false);\n\n# Use reverse_diff due to the number of parameters in neural networks.\nTuring.setadbackend(:reversediff)\n```\n\nOur goal here is to use a Bayesian neural network to classify points in an artificial dataset. The code below generates data points arranged in a box-like pattern and displays a graph of the dataset we'll be working with.\n\n\n```julia\n# Number of points to generate.\nN = 80\nM = round(Int, N / 4)\nRandom.seed!(1234)\n\n# Generate artificial data.\nx1s = rand(M) * 4.5; x2s = rand(M) * 4.5; \nxt1s = Array([[x1s[i] + 0.5; x2s[i] + 0.5] for i = 1:M])\nx1s = rand(M) * 4.5; x2s = rand(M) * 4.5; \nappend!(xt1s, Array([[x1s[i] - 5; x2s[i] - 5] for i = 1:M]))\n\nx1s = rand(M) * 4.5; x2s = rand(M) * 4.5; \nxt0s = Array([[x1s[i] + 0.5; x2s[i] - 5] for i = 1:M])\nx1s = rand(M) * 4.5; x2s = rand(M) * 4.5; \nappend!(xt0s, Array([[x1s[i] - 5; x2s[i] + 0.5] for i = 1:M]))\n\n# Store all the data for later.\nxs = [xt1s; xt0s]\nts = [ones(2*M); zeros(2*M)]\n\n# Plot data points.\nfunction plot_data()\n    x1 = map(e -> e[1], xt1s)\n    y1 = map(e -> e[2], xt1s)\n    x2 = map(e -> e[1], xt0s)\n    y2 = map(e -> e[2], xt0s)\n\n    Plots.scatter(x1,y1, color=\"red\", clim = (0,1))\n    Plots.scatter!(x2, y2, color=\"blue\", clim = (0,1))\nend\n\nplot_data()\n```\n\n## Building a Neural Network\n\nThe next step is to define a [feedforward neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network) where we express our parameters as distribtuions, and not single points as with traditional neural networks. The two functions below, `unpack` and `nn_forward` are helper functions we need when we specify our model in Turing.\n\n`unpack` takes a vector of parameters and partitions them between weights and biases. `nn_forward` constructs a neural network with the variables generated in `unpack` and returns a prediction based on the weights provided.\n\nThe `unpack` and `nn_forward` functions are explicity designed to create a neural network with two hidden layers and one output layer, as shown below.\n\n<img width=\"320\" alt=\"nn-diagram\" src=\"https://user-images.githubusercontent.com/422990/47970321-bd172080-e038-11e8-9c6d-6c2bd790bd8a.png\">\n\nThe end of this tutorial provides some code that can be used to generate more general network shapes.\n\n\n```julia\n# Turn a vector into a set of weights and biases.\nfunction unpack(nn_params::AbstractVector)\n    W₁ = reshape(nn_params[1:6], 3, 2);   \n    b₁ = nn_params[7:9]\n    \n    W₂ = reshape(nn_params[10:15], 2, 3); \n    b₂ = nn_params[16:17]\n    \n    Wₒ = reshape(nn_params[18:19], 1, 2); \n    bₒ = nn_params[20:20]\n    return W₁, b₁, W₂, b₂, Wₒ, bₒ\nend\n\n# Construct a neural network using Flux and return a predicted value.\nfunction nn_forward(xs, nn_params::AbstractVector)\n    W₁, b₁, W₂, b₂, Wₒ, bₒ = unpack(nn_params)\n    nn = Chain(Dense(W₁, b₁, tanh),\n               Dense(W₂, b₂, tanh),\n               Dense(Wₒ, bₒ, σ))\n    return nn(xs)\nend;\n```\n\nThe probabilistic model specification below creates a `params` variable, which has 20 normally distributed variables. Each entry in the `params` vector represents weights and biases of our neural net.\n\n\n```julia\n# Create a regularization term and a Gaussain prior variance term.\nalpha = 0.09\nsig = sqrt(1.0 / alpha)\n\n# Specify the probabilistic model.\n@model function bayes_nn(xs, ts)\n    # Create the weight and bias vector.\n    nn_params ~ MvNormal(zeros(20), sig .* ones(20))\n    \n    # Calculate predictions for the inputs given the weights\n    # and biases in theta.\n    preds = nn_forward(xs, nn_params)\n    \n    # Observe each prediction.\n    for i = 1:length(ts)\n        ts[i] ~ Bernoulli(preds[i])\n    end\nend;\n```\n\nInference can now be performed by calling `sample`. We use the `HMC` sampler here.\n\n```julia\n# Perform inference.\nN = 5000\nch = sample(bayes_nn(hcat(xs...), ts), HMC(0.05, 4), N);\n```\n\nNow we extract the weights and biases from the sampled chain. We'll use these primarily in determining how good a classifier our model is.\n\n```julia\n# Extract all weight and bias parameters.\ntheta = MCMCChains.group(ch, :nn_params).value;\n```\n\n## Prediction Visualization\n\nWe can use [MAP estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) to classify our population by using the set of weights that provided the highest log posterior.\n\n\n```julia\n# Plot the data we have.\nplot_data()\n\n# Find the index that provided the highest log posterior in the chain.\n_, i = findmax(ch[:lp])\n\n# Extract the max row value from i.\ni = i.I[1]\n\n# Plot the posterior distribution with a contour plot.\nx_range = collect(range(-6,stop=6,length=25))\ny_range = collect(range(-6,stop=6,length=25))\nZ = [nn_forward([x, y], theta[i, :])[1] for x=x_range, y=y_range]\ncontour!(x_range, y_range, Z)\n```\n\nThe contour plot above shows that the MAP method is not too bad at classifying our data.\n\nNow we can visualize our predictions.\n\n$$ \np(\\tilde{x} | X, \\alpha) = \\int_{\\theta} p(\\tilde{x} | \\theta) p(\\theta | X, \\alpha) \\approx \\sum_{\\theta \\sim p(\\theta | X, \\alpha)}f_{\\theta}(\\tilde{x}) \n$$\n\nThe `nn_predict` function takes the average predicted value from a network parameterized by weights drawn from the MCMC chain.\n\n```julia\n# Return the average predicted value across\n# multiple weights.\nfunction nn_predict(x, theta, num)\n    mean([nn_forward(x, theta[i,:])[1] for i in 1:10:num])\nend;\n```\n\nNext, we use the `nn_predict` function to predict the value at a sample of points where the `x` and `y` coordinates range between -6 and 6. As we can see below, we still have a satisfactory fit to our data.\n\n```julia\n# Plot the average prediction.\nplot_data()\n\nn_end = 1500\nx_range = collect(range(-6,stop=6,length=25))\ny_range = collect(range(-6,stop=6,length=25))\nZ = [nn_predict([x, y], theta, n_end)[1] for x=x_range, y=y_range]\ncontour!(x_range, y_range, Z)\n```\n\nIf you are interested in how the predictive power of our Bayesian neural network evolved between samples, the following graph displays an animation of the contour plot generated from the network weights in samples 1 to 1,000. \n\n```julia\n# Number of iterations to plot.\nn_end = 500\n\nanim = @gif for i=1:n_end\n    plot_data()\n    Z = [nn_forward([x, y], theta[i,:])[1] for x=x_range, y=y_range]\n    contour!(x_range, y_range, Z, title=\"Iteration $i\", clim = (0,1))\nend every 5\n```\n\n## Generic Bayesian Neural Networks\n\nThe below code is intended for use in more general applications, where you need to be able to change the basic network shape fluidly. The code above is highly rigid, and adapting it for other architectures would be time consuming. Currently the code below only supports networks of `Dense` layers.\n\nHere, we solve the same problem as above, but with three additional 2x2 `tanh` hidden layers. You can modify the `network_shape` variable to specify differing architectures. A tuple `(3,2, :tanh)` means you want to construct a `Dense` layer with 3 outputs, 2 inputs, and a `tanh` activation function. You can provide any activation function found in Flux by entering it as a `Symbol` (e.g., the `tanh` function is entered in the third part of the tuple as `:tanh`).\n\n\n```julia\n# Specify the network architecture.\nnetwork_shape = [\n    (3,2, :tanh),\n    (2,3, :tanh), \n    (1,2, :σ)]\n\n# Regularization, parameter variance, and total number of\n# parameters.\nalpha = 0.09\nsig = sqrt(1.0 / alpha)\nnum_params = sum([i * o + i for (i, o, _) in network_shape])\n\n# This modification of the unpack function generates a series of vectors\n# given a network shape.\nfunction unpack(θ::AbstractVector, network_shape::AbstractVector)\n    index = 1\n    weights = []\n    biases = []\n    for layer in network_shape\n        rows, cols, _ = layer\n        size = rows * cols\n        last_index_w = size + index - 1\n        last_index_b = last_index_w + rows\n        push!(weights, reshape(θ[index:last_index_w], rows, cols))\n        push!(biases, reshape(θ[last_index_w+1:last_index_b], rows))\n        index = last_index_b + 1\n    end\n    return weights, biases\nend\n\n# Generate an abstract neural network given a shape, \n# and return a prediction.\nfunction nn_forward(x, θ::AbstractVector, network_shape::AbstractVector)\n    weights, biases = unpack(θ, network_shape)\n    layers = []\n    for i in eachindex(network_shape)\n        push!(layers, Dense(weights[i],\n            biases[i],\n            eval(network_shape[i][3])))\n    end\n    nn = Chain(layers...)\n    return nn(x)\nend\n\n# General Turing specification for a BNN model.\n@model bayes_nn_general(xs, ts, network_shape, num_params) = begin\n    θ ~ MvNormal(zeros(num_params), sig .* ones(num_params))\n    preds = nn_forward(xs, θ, network_shape)\n    for i = 1:length(ts)\n        ts[i] ~ Bernoulli(preds[i])\n    end\nend\n\n# Perform inference.\nnum_samples = 500\nch2 = sample(bayes_nn_general(hcat(xs...), ts, network_shape, num_params), NUTS(0.65), num_samples);\n```\n\n```julia\n# This function makes predictions based on network shape.\nfunction nn_predict(x, theta, num, network_shape)\n    mean([nn_forward(x, theta[i,:], network_shape)[1] for i in 1:10:num])\nend;\n\n# Extract the θ parameters from the sampled chain.\nparams2 = MCMCChains.group(ch2, :θ).value\n\nplot_data()\n\nx_range = collect(range(-6,stop=6,length=25))\ny_range = collect(range(-6,stop=6,length=25))\nZ = [nn_predict([x, y], params2, length(ch2), network_shape)[1] for x=x_range, y=y_range]\ncontour!(x_range, y_range, Z)\n```\n\nThis has been an introduction to the applications of Turing and Flux in defining Bayesian neural networks.\n\n```julia, echo=false, skip=\"notebook\"\nif isdefined(Main, :TuringTutorials)\n    Main.TuringTutorials.tutorial_footer(WEAVE_ARGS[:folder], WEAVE_ARGS[:file])\nend\n```\n"
        }
    ]
}
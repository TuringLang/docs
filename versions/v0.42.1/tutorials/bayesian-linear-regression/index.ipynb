{
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "source": "# Install necessary dependencies.\nusing Pkg\nPkg.activate(; temp=true)\nPkg.add([])",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| echo: false\n#| output: false\nusing Pkg;\nPkg.instantiate();\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Turing is powerful when applied to complex hierarchical models, but it can also be applied to common statistical procedures, like [linear regression](https://en.wikipedia.org/wiki/Linear_regression).\nThis tutorial covers how to implement a linear regression model in Turing.\n\n## Set Up\n\nWe begin by importing all the necessary libraries.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# Import Turing.\nusing Turing\n\n# Package for loading the data set.\nusing RDatasets\n\n# Package for visualisation.\nusing StatsPlots\n\n# Functionality for splitting the data.\nusing MLUtils: splitobs\n\n# Functionality for constructing arrays with identical elements efficiently.\nusing FillArrays\n\n# Functionality for normalising the data and evaluating the model predictions.\nusing StatsBase\n\n# Functionality for working with scaled identity matrices.\nusing LinearAlgebra\n\n# For ensuring reproducibility.\nusing StableRNGs: StableRNG\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| output: false\nsetprogress!(false)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We will use the `mtcars` dataset from the [RDatasets](https://github.com/JuliaStats/RDatasets.jl) package.\n`mtcars` contains a variety of statistics on different car models, including their miles per gallon, number of cylinders, and horsepower, among others.\n\nWe want to know if we can construct a Bayesian linear regression model to predict the miles per gallon of a car, given the other statistics it has.\nLet us take a look at the data we have.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# Load the dataset.\ndata = RDatasets.dataset(\"datasets\", \"mtcars\")\n\n# Show the first six rows of the dataset.\nfirst(data, 6)\n```",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "size(data)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The next step is to get our data ready for testing. We'll split the `mtcars` dataset into two subsets, one for training our model and one for evaluating our model. Then, we separate the targets we want to learn (`MPG`, in this case) and standardise the datasets by subtracting each column's mean and dividing by the standard deviation of that column. This [standardisation](https://en.wikipedia.org/wiki/Feature_scaling) ensures all features have similar scales (mean 0, standard deviation 1), which helps the sampler explore the parameter space more efficiently.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# Remove the model column.\nselect!(data, Not(:Model))\n\n# Split our dataset 70%/30% into training/test sets.\ntrainset, testset = map(DataFrame, splitobs(StableRNG(468), data; at=0.7, shuffle=true))\n\n# Turing requires data in matrix form.\ntarget = :MPG\ntrain = Matrix(select(trainset, Not(target)))\ntest = Matrix(select(testset, Not(target)))\ntrain_target = trainset[:, target]\ntest_target = testset[:, target]\n\n# Standardise the features.\ndt_features = fit(ZScoreTransform, train; dims=1)\nStatsBase.transform!(dt_features, train)\nStatsBase.transform!(dt_features, test)\n\n# Standardise the targets.\ndt_targets = fit(ZScoreTransform, train_target)\nStatsBase.transform!(dt_targets, train_target)\nStatsBase.transform!(dt_targets, test_target);\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Model Specification\n\nIn a traditional frequentist model using [OLS](https://en.wikipedia.org/wiki/Ordinary_least_squares), our model might look like:\n\n$$\n\\mathrm{MPG}_i = \\alpha + \\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{X_i}\n$$\n\nwhere $\\boldsymbol{\\beta}$ is a vector of coefficients and $\\boldsymbol{X}$ is a vector of inputs for observation $i$. The Bayesian model we are more concerned with is the following:\n\n$$\n\\mathrm{MPG}_i \\sim \\mathcal{N}(\\alpha + \\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{X_i}, \\sigma^2)\n$$\n\nwhere $\\alpha$ is an intercept term common to all observations, $\\boldsymbol{\\beta}$ is a coefficient vector, $\\boldsymbol{X_i}$ is the observed data for car $i$, and $\\sigma^2$ is a common variance term.\n\nFor $\\sigma^2$, we assign a prior of `truncated(Normal(0, 100); lower=0)`.\nThis is consistent with [Andrew Gelman's recommendations](http://www.stat.columbia.edu/%7Egelman/research/published/taumain.pdf) on noninformative priors for variance.\nThe intercept term ($\\alpha$) is assumed to be normally distributed with a mean of zero and a variance of three.\nThis represents our assumptions that miles per gallon can be explained mostly by our various variables, but a high variance term indicates our uncertainty about that.\nEach coefficient is assumed to be normally distributed with a mean of zero and a variance of 10.\nWe do not know that our coefficients are different from zero, and we do not know which ones are likely to be the most important, so the variance term is quite high.\nLastly, each observation $y_i$ is distributed according to the calculated `mu` term given by $\\alpha + \\boldsymbol{\\beta}^\\mathsf{T}\\boldsymbol{X_i}$.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# Bayesian linear regression.\n@model function linear_regression(x, y)\n    # Set variance prior.\n    σ² ~ truncated(Normal(0, 100); lower=0)\n\n    # Set intercept prior.\n    intercept ~ Normal(0, sqrt(3))\n\n    # Set the priors on our coefficients.\n    nfeatures = size(x, 2)\n    coefficients ~ MvNormal(Zeros(nfeatures), 10.0 * I)\n\n    # Calculate all the mu terms.\n    mu = intercept .+ x * coefficients\n    return y ~ MvNormal(mu, σ² * I)\nend\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "With our model specified, we can call the sampler. We will use the No U-Turn Sampler ([NUTS](https://turinglang.org/stable/docs/library/#Turing.Inference.NUTS)) here.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "model = linear_regression(train, train_target)\nchain = sample(StableRNG(468), model, NUTS(), 20_000)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We can also check the densities and traces of the parameters visually using the `plot` functionality.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "plot(chain)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "It looks like all parameters have converged.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| echo: false\nlet\n    ess_df = ess(chain)\n    @assert minimum(ess_df[:, :ess]) > 500 \"Minimum ESS: $(minimum(ess_df[:, :ess])) - not > 500\"\n    @assert mean(ess_df[:, :ess]) > 2_000 \"Mean ESS: $(mean(ess_df[:, :ess])) - not > 2000\"\n    @assert maximum(ess_df[:, :ess]) > 3_500 \"Maximum ESS: $(maximum(ess_df[:, :ess])) - not > 3500\"\nend\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Comparing to OLS\n\nA satisfactory test of our model is to evaluate how well it predicts. Importantly, we want to compare our model to existing tools like OLS. The code below uses the [GLM.jl](https://juliastats.org/GLM.jl/stable/) package to generate a traditional OLS multiple regression model on the same data as our probabilistic model.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# Import the GLM package.\nusing GLM\n\n# Perform multiple regression OLS.\ntrain_with_intercept = hcat(ones(size(train, 1)), train)\nols = lm(train_with_intercept, train_target)\n\n# Compute predictions on the training data set and unstandardise them.\ntrain_prediction_ols = GLM.predict(ols)\nStatsBase.reconstruct!(dt_targets, train_prediction_ols)\n\n# Compute predictions on the test data set and unstandardise them.\ntest_with_intercept = hcat(ones(size(test, 1)), test)\ntest_prediction_ols = GLM.predict(ols, test_with_intercept)\nStatsBase.reconstruct!(dt_targets, test_prediction_ols);\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The function below accepts a chain and an input matrix and calculates predictions. We use the samples starting from sample 200 onwards, discarding the initial samples as [burn-in](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) to allow the sampler to reach the typical set.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# Make a prediction given an input vector.\nfunction prediction(chain, x)\n    p = get_params(chain[200:end, :, :])\n    targets = p.intercept' .+ x * reduce(hcat, p.coefficients)'\n    return vec(mean(targets; dims=2))\nend\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "When we make predictions, we unstandardise them so they are more understandable.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n# Calculate the predictions for the training and testing sets and unstandardise them.\ntrain_prediction_bayes = prediction(chain, train)\nStatsBase.reconstruct!(dt_targets, train_prediction_bayes)\ntest_prediction_bayes = prediction(chain, test)\nStatsBase.reconstruct!(dt_targets, test_prediction_bayes)\n\n# Show the predictions on the test data set.\nDataFrame(; MPG=testset[!, target], Bayes=test_prediction_bayes, OLS=test_prediction_ols)\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now let's evaluate the loss for each method, and each prediction set. We will use the mean squared error to evaluate loss, given by\n$$\n\\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^n {(y_i - \\hat{y_i})^2}\n$$\nwhere $y_i$ is the actual value (true MPG) and $\\hat{y_i}$ is the predicted value using either OLS or Bayesian linear regression. A lower MSE indicates a closer fit to the data.",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": "println(\n    \"Training set:\",\n    \"\\n\\tBayes loss: \",\n    msd(train_prediction_bayes, trainset[!, target]),\n    \"\\n\\tOLS loss: \",\n    msd(train_prediction_ols, trainset[!, target]),\n)\n\nprintln(\n    \"Test set:\",\n    \"\\n\\tBayes loss: \",\n    msd(test_prediction_bayes, testset[!, target]),\n    \"\\n\\tOLS loss: \",\n    msd(test_prediction_ols, testset[!, target]),\n)",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "```julia\n#| echo: false\nlet\n    bayes_train_loss = msd(train_prediction_bayes, trainset[!, target])\n    bayes_test_loss = msd(test_prediction_bayes, testset[!, target])\n    ols_train_loss = msd(train_prediction_ols, trainset[!, target])\n    ols_test_loss = msd(test_prediction_ols, testset[!, target])\n    @assert bayes_train_loss < bayes_test_loss \"Bayesian training loss ($bayes_train_loss) >= Bayesian test loss ($bayes_test_loss)\"\n    @assert ols_train_loss < ols_test_loss \"OLS training loss ($ols_train_loss) >= OLS test loss ($ols_test_loss)\"\n    @assert bayes_train_loss > ols_train_loss \"Bayesian training loss ($bayes_train_loss) <= OLS training loss ($bayes_train_loss)\"\n    @assert bayes_test_loss < ols_test_loss \"Bayesian test loss ($bayes_test_loss) >= OLS test loss ($ols_test_loss)\"\nend\n```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can see from this that both linear regression techniques perform fairly similarly.\nThe Bayesian linear regression approach performs worse on the training set, but better on the test set.\nThis indicates that the Bayesian approach is more able to generalise to unseen data, i.e., it is not overfitting the training data as much.",
      "metadata": {}
    }
  ],
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia",
      "language": "julia"
    }
  },
  "nbformat": 4
}